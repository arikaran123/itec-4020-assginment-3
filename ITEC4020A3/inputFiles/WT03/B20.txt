WT03-B20-1IA005-000053-B038-165http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs201.html 138.80.61.12 19970221164037 text/html 6755HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:10:12 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 6584Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact Sheet 201 How policy-makers are responding to global climate change The first time climate change was recognized as a serious problem by a major intergovernmental meeting was in 1979. The First World Climate Conference, held in February of that year, was an important scientific event. It issued a declaration calling on the world's governments "to foresee and prevent potential man-made changes in climate that might be adverse to the well-being of humanity." A large number of international conferences on climate change have been convened since then. Attended by government policy-makers, scientists, and environmental groups, they have addressed both scientific and policy issues. Important meetings have been held in Toronto, the Hague, Noordwijk, Bergen, and elsewhere. The Second World Climate Conference, held in 1990 in Geneva, was a particularly crucial step towards a binding global convention on climate change. Some of these meetings have taken place under the auspices of the United Nations and its specialized agencies. Others have been held within regional and global fora such as the European Community, the Commonwealth, and the South Pacific Forum, or have been convened by individual governments. A number of meetings have been dedicated to the particular concerns of small island states and of developing countries. The 1992 UN Framework Convention on Climate Change is the first binding international legal instrument to address the issue specifically.1 Adopted after 15 months of intensive negotiations within the Intergovernmental Negotiating Committee for a Framework Convention on Climate Change (INC/FCCC - see fact sheet 209), it was opened for signature in Rio de Janeiro at the June 1992 UN Conference on Environment and Development (UNCED). The INC negotiators drew on the First Assessment Report of the Intergovernmental Panel on Climate Change (IPCC), a body established jointly by the United Nations Environment Programme (UNEP) and the World Meteorological Organization (WMO). They were also influenced by the Ministerial Declaration issued by the Second World Climate Conference and by policy statements adopted by numerous other climate conferences. The Convention incorporates a number of newly emerging legal principles that had been developed or affirmed by various climate conferences. The Convention will provide a general framework for addressing the climate change issue . . . The Convention was signed by 154 states and the European Community during UNCED. Other states have signed since then, and some national legislatives have ratified. It will enter into force after it has been ratified by 50 states. In the meantime, states are expected to start working towards the aims set by the Convention on a voluntary basis. . . . but much work remains to be done. States must now strive to ensure that the Convention enters into force as soon as possible. At the same time, government experts must decide whether to adopt additional measures in future annexes and protocols to the Convention. These protocols may set out more specific commitments, such as timetables for reducing greenhouse gas emissions. Even before the Convention was adopted, some countries had already taken unilateral action at the national level. Most OECD member states have set national targets for stabilizing or reducing their emissions of greenhouse gases.2 In 1990, the Council of the European Communities (EC) adopted a policy that provides for stabilizing the emissions of carbon dioxide - the most significant greenhouse gas - at 1990 levels by the year 2000. A strategy to limit carbon dioxide emissions and to improve energy efficiency is currently being elaborated by the EC Commission. In addition, two other international environmental treaties address climate change indirectly. The amended 1987 Montreal Protocol on Substances That Deplete the Ozone Layer legally obliges its parties to phase out chlorofluorocarbons (CFCs) by the year 1996 (fact sheet 224). Although inspired by concern over the destruction of the ozone layer, this protocol is significant also for climate change since CFCs are greenhouse gases. Similarly, the 1979 Geneva Convention on Long-Range Transboundary Air Pollution and its protocols regulate the emission of noxious gases, some of which are precursors of greenhouse gases. These treaties, however, do not address the complex set of inter-related climate issues. For further reading: Churchill and Freestone, eds., "International Law and Global Climate Change", London (1991). "Environmental Policy and Law", Nos. 19 (1989), 20 (1990), 21 (1991). and 22/1 (1992) pp. 5-15. Goldman and Hajost, in "Yearbook of International Environmental Law", No. 2 (1991), pp. 111-115. Hajost, in "Yearbook of International Environmental Law", No. 1 (1990), pp. 100-104. Lang, Neuhold and Zemanek (eds.), "Environmental Protection and International Law", London (1991) (see in particular chapter 6, "Ozone Layer and Climate Change", by P. Sz�ll and J. Temple Lang). Stone, "Beyond Rio: 'Insuring' Against Global Warming", "American Journal of International Law", 86 (1992), pp. 445 et. seq. Zaelke and Cameron, "Global Warming and Climate Change: An Overview of the International Legal Process", in "American University Journal of International Law and Policy", Vol. 5/2 (1990), pp. 249-290. Notes: 1 Report of the Intergovernmental Negotiating Committee for a Framework Convention on Climate Change, May 1992, A/AC.237/18 (Part II)/Add.1. 2 International Energy Agency, "Climate Change Policy Initiatives Update", 15 July 1991. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-2IA005-000053-B037-526http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs128.html 138.80.61.12 19970221163846 text/html 7221HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:08:02 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 7050Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact Sheet 128 Zimbabwe's vulnerablility to climate change Zimbabwe is a developing country with a semi-arid climate. As such, it is particularly vulnerable to the possibility that global climate change will lead to an even drier climate and create conditions that undermine economic development. It is not yet possible, however, to predict how greenhouse gas-induced climate change will express itself at the regional or national level. At the global level, it is expected to raise average temperatures by 1.5-4.5 C over the next 100 years. It is also likely to increase rainfall, but because evapo-transpiration would also accelerate, soils would tend to become drier than before. Climate change may also affect the frequency, magnitude, and location of extreme events such as windstorms and droughts. The catastrophic drought of 1991/92 offers valuable insights into Zimbabwe's vulnerabilities. During the drought, which was probably linked to an El Ni�o/Southern Oscillation (ENSO) event, Zimbabwe's temperatures reached record heights. Rainfall levels fell to just 40% of normal, the water table dropped by 100-200 m, ground water (including traditional shallow wells and boreholes) dried up, and numberless rivers, lakes, reservoirs, and their related ecosystems disappeared. Only the very oldest people had ever experienced such a severe drought. In addition to highlighting the vulnerabilities of Zimbabwe's various economic sectors, the drought created widespread awareness among policy-makers and the general public of the need to address the country's dependence on climatic conditions. Under the best conditions, Zimbabwe's limited water resources are in great demand for both domestic and industrial use. During the drought, people in remote areas often walked 10-15 km for their daily supplies. Schools, hospitals, and rural service centres were threatened with closure due to water shortages. Meanwhile, irrigation programmes failed completely and will require more than just one or two good rainy seasons to recover. The rains during the 1992/93 season were 80% of normal and so were not sufficient to sustain river flow or to raise the water table to safe levels. The 1991/92 drought caused a near-total collapse of the nation's agricultural system, including irrigated and rain fed crops and animal herds. The southern limit of the grain production zone, normally around 20 S latitude, shifted northwards to 15 S, across the central districts of neighbouring Zambia. Similar shifts in agricultural and ecological zones are likely to be a key feature of long-term climate change. Meanwhile, the national herd was reduced by up to 50%. The drought recovery programme, including the importation and distribution of grain, cost over Z$200 million (US$40 million). Since some 80% of Zimbabwe's 10.5 million inhabitants are farmers, with many of the remaining city-dwellers also engaged in agro-industry, the economic damage and human suffering were enormous. Because Zimbabwe is dependent on hydropower, its energy sector is particularly vulnerable to drought. Some 80% of the country's energy comes from hydro-generators on the Lake Kariba dam (which helps to limit Zimbabwe's carbon dioxide emissions). During the 1991/92 season the lake level dropped to 40% of capacity; any further drop would have made generating electricity impossible. The result was frequent power cuts during 1992 and a serious challenge to Zimbabwe's economic structural adjustment programme. Productivity may have fallen by 30% or more. To maintain the programme, the Government was forced to import power at great expense from Zaire, Zambia, and South Africa. The forestry sector, essential for both energy and manufacturing, is also dependent on water resources. Most of Zimbabwe's rural population uses fuel-wood and other biomass for its domestic energy needs. Unfortunately, population pressure and the unsustainable use of this resource has deforested much of the rural areas. The drought then created such harsh conditions that a number of indigenous and exotic tree species actually perished. Although the Government has embarked on an extensive tree-planting programme, it cannot succeed if the climate is not supportive. Drought usually has negative effects on human health. The problems of under-nourishment, water and air borne diseases, and the lack of resources to purchase essential drugs are exacerbated during times of drought. While Government efforts prevented human deaths due to famine during the 1991/92 drought, new and drug-resistant strains of malaria and diarrhoea were reported in all districts of Zimbabwe. Logistical problems plagued the delivery of essential drugs to rural clinics and hospitals, and reports of sudden outbreaks of disease were not uncommon. Economic development can be undermined by extreme climate events such as drought. During the recent drought, the diversion of funds to emergency relief weakened investment in the country's industrial reform programme. Since the success of economic development in most developing countries depends to a large extent on good agricultural performance, a supportive climate is a prerequisite. Climate extremes will have an adverse ripple effect on key economic sectors such as agriculture, energy development, industry, forestry, and health. In addition to enhancing the resilience of these vulnerable sectors, a careful mix of new industries with a limited agricultural bias could be introduced to minimise the country's dependence on climate-sensitive development programmes. Zimbabwe signed the United Nations Framework convention on Climate Change at the Rio Earth Summit and on 3 November 1992 became the fifth country to ratify. Before the drought, the government had already initiated programmes to address this vulnerability. For example, the trend towards a late start to the rainy season, prolonged mid-season droughts, and shorter growing seasons had prompted the development of seed varieties that require much shorter growing seasons. Zimbabwe has also taken advantage of support from the United Nations and from other countries to initiate country studies and programmes on climate change impacts, response options, and awareness-raising. This fact sheet was prepared by the Department of Meteorology, BOX BE. 150, Belvedere, Harare, Zimbabwe. Last revised 1 March 1994 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-3IA005-000053-B039-160http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs206.html 138.80.61.12 19970221164839 text/html 7965HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:18:55 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 7794Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact Sheet 206 How UNEP and WMO are responding to climate change The United Nations Environment Programme (UNEP) is the "environmental conscience" of the United Nations System. Together with the World Meteorological Organization (see below), it plays a leading role in the UN�s work on climate change. UNEP is a programme of the UN General Assembly. Established as a result of the 1972 Stockholm Conference on the Human Environment, UNEP has headquarters in Nairobi and regional offices and specialized units around the world. UNEP�s primary function is to promote environmental action and awareness by spurring others to act and by working through and with other organizations. The policies and activities of UNEP are determined by its Governing Council, which normally meets once a year. The Council�s decisions are implemented by the Executive Director, who heads the UNEP Secretariat. In the last few years, the issue of climate change has been increasingly high on the Governing Council�s agenda. A number of units and programme activity centres within the UNEP Secretariat deal extensively with climate change. The Global Environment Monitoring System (GEMS) links together some 25 major global monitoring networks. It provides a rigorous scientific basis for all of UNEP�s other activities as well as for the environmental management programmes of governments and of international agencies and institutions. GEMS concentrates on climate, transboundary pollution, terrestrial renewable natural resources, oceans, and environmental pollution. The Global Resources Information Database (GRID) was founded in 1985 to make the findings of GEMS available to decision-makers. It uses geographic information system (GIS) and satellite image processing technology to present environmental data and analyses in easily-understood maps and print-outs. The Information Unit on Climate Change (IUCC) was set up in Geneva in 1991 and also produces information on climate change for decision-makers. It provides presentations, fact sheets, videos, and other information services (WMO became a co-sponsor of the Unit in 1992). The World Meteorological Organization (WMO) also plays a leading role in the UN�s activities on climate change. A Geneva-based UN specialized agency, WMO is responsible for the world-wide coordination of meteorological activities. It ensures the provision of authoritative international scientific information on the state and behaviour of the global atmosphere, the climate it produces, its interaction with the oceans, and the resulting distribution of water resources on the earth. WMO also promotes research on climate change and supports a number of mainly science-oriented climate programmes and institutions. WMO�s fundamental policies and activities are determined by the World Meteorological Congress, which meets every four years. WMO�s Executive Council, meeting at least once a year, monitors the implementation of the Congress�s decisions by the WMO Secretariat and guarantees the continuity of work between the Congress�s meetings. UNEP and WMO cooperate extensively on the climate change issue. They joined forces for the first time in 1979 when, together with a number of other organizations, they convened the First World Climate Conference. Among other things, the Conference endorsed the establishment of a World Climate Programme (WCP) for monitoring global climate. In 1991, the Congress of WMO decided that the functions of the WCP, which is under the joint responsibility of WMO, UNEP, and the International Council of Scientific Unions (ICSU), should be considerably broadened to allow it to respond to new developments.1 In 1985, the two organizations and ICSU sponsored the Villach Conference on the scientific aspects of climate change. In 1988, UNEP and WMO jointly established the Intergovernmental Panel on Climate Change (IPCC). Both organizations subsequently followed and gave guidance to IPCC�s work. In 1990 they sponsored the Second World Climate Conference and set up an intergovernmental working group to prepare for negotiations on a framework climate treaty; this group�s activities were later taken over bythe Intergovernmental Negotiating Committee on Climate Change (INC). Both organizations stressed the continuing importance of IPCC and gave it a mandate for providing scientific support to INC. UNEP also recommended that the Preparatory Committee of the United Nations Conference on Environment and Development (UNCED) consider climate change as a priority area.2 The UNEP/WMO partnership will continue to be a leading force in the international response to the threat of climate change. In 1991 the WMO Congress and the UNEP Governing Council reaffirmed the importance of the ongoing work of IPCC and INC and of the results of the Second World Climate Conference.3 The WMO Congress asked WMO�s Secretary-General and UNEP�s Executive Director to consider convening a third world climate conference at a later stage.4 UNEP and WMO plan to commit more of their financial and organizational resources to the climate change issue. In 1991, the UNEP Governing Council asked the Executive Director to consider contributing financially towards the work of INC. It drew the attention of states and organizations to the special vulnerability of low-lying coastal and island states and commissioned the UNEP Regional Seas Programme to help such states build up their capacity for response measures.5 Climate change was included as a priority area in UNEP's Montevideo Programme for the Development and Periodic Review of Environmental Law.6 UNEP is also studying ways and means for implementing Agenda 21's proposals to protect the atmosphere. Meanwhile, in 1991 WMO�s Congress reaffirmed a 1989 decision by the WMO Executive Council7 to establish a WMO Special Trust Fund for Climate and Atmospheric Environment Activities. This will be used for financing activities in 15 priority topic areas, most of which are directly relevant to developing countries.8 In accordance with a recommendation by the Second World Climate Conference, the Congress also decided to establish a Global Climate Observing System to coordinate and complement ongoing monitoring activities and to provide comprehensive information on the climate system.9 In its 1992 Resolution the Executive Council emphasized WMO's need for more funding for climate change work, and in particular urged states to contribute to the Special Trust Fund.10 Notes: 1 WMO Congress Resolution 3.2.5/5 (Cg.-XI), 1991. 2 UNEP Governing Council Decision SS.II/9, 3 August 1990. 3 WMO Congress Res. 3.2.5/4 (Cg-XI) and 3.2.5/5 (Cg.-XI), 1991; UNEP Governing Council Decisions 16/41 and 16/27, 31 May 1991. 4 WMO Cg-XI/PINK 9, Appendix, 1991. 5 UNEP Governing Council Decision 16/27, 31 May 1991. 6 UNEP Governing Council Decision 16/23. 7 WMO Executive Council Resolution 5 (EC-XLI), 1989. 8 WMO Congress Res. 3.2.5/1 (Cg-XI), 1991. 9 WMO Congress Res. 3.2.5/2 (Cg-XI), 1991. 10 WMO Executive Council Resolution 1202/2 (EC-XLIV), 1992. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-4IA005-000053-B041-324http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs218.html 138.80.61.12 19970221170234 text/html 6300HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:32:48 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 6129Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact Sheet 218 The Noordwijk Ministerial Declaration on climate change The Noordwijk Ministerial Conference was a critical milestone on the road to international emission targets for carbon dioxide. Held in November 1989 in Noordwijk, The Netherlands, the Conference was attended by representatives of 67 countries, 11 international organizations, and the Commission of the European Community (EC). The conclusions of the resulting Noordwijk Declaration on Atmospheric Pollution and Climate Change1 have often been referred to in subsequent climate debates.2 The Declaration includes an important statement on stabilizing emissions of carbon dioxide (CO2): "The Conference recognizes the need to stabilize, while ensuring stable development of the world economy, CO2 emissions and emissions of other greenhouse gases not controlled by the Montreal Protocol. Industrialized nations agree that such stabilization should be achieved by them as soon as possible, at levels to be considered by the IPCC and the Second World Climate Conference of November 1990. In the view of many industrialized nations such stabilization of CO2 emissions should be achieved as a first step at the latest by the year 2000." Not all industrialized states represented at the conference were ready to make concrete commitments on emission targets. Nevertheless, this statement was a first step towards the recognition of such targets. The EC, whose member states were among those agreeing to specific emission targets, referred back to the Noordwijk discussions when it established its policy targets on climate change in 1990.3 The Conference participants recognized a number of key principles of relevance to a climate treaty. These included the concept of climate change as a common concern of humankind, the common but differentiated responsibilities of states, the sovereign right of states to manage their own natural resources, and the necessity of sustainable development. The participants also endorsed the work of the Intergovernmental Panel on Climate Change (IPCC) and the convening of the Second World Climate Conference. Urging all states to participate in these efforts, they set out the issues which, in their view, the future climate treaty and its protocols should regulate. These included research on climate change, action to deal with greenhouse gas emissions and the effects of global warming, financial assistance and transfer of technology to developing countries, and sustainable management of forests. The Conference Declaration asked the IPCC to conduct further research into emission targets and reforestation. These requests were reiterated by the UN/ECE Bergen Conference in May 1990. The declaration suggested that the IPCC consider the feasibility of a 20% reduction in CO2 emissions by the year 2005, as suggested by the conference held in Toronto in 1988. To this end, industrialized states were called upon to support IPCC. Another request was that the IPCC consider the feasibility of achieving, as a provisional target, a net forest growth of 12 million hectares a year by the beginning of the 21st century. The Declaration contains specific proposals for action on emissions, funding, and research and monitoring. These proposals relate to: Carbon dioxide In addition to collective action within the framework of the IPCC, countries should also take individual measures to reduce CO2 emissions by promoting better energy conservation and using environmentally sound energy sources. Industrial countries in particular were urged to work towards these aims. The need of developing countries for time to implement response measures was recognized, but the declaration also called upon them to make such efforts as were within their capabilities. CFCs The Conference welcomed the efforts made within the framework of the Vienna Convention and the Montreal Protocol for protecting the ozone layer and urged all states to become parties to these instruments (fact sheet 224). Other greenhouse gases As with CO2, efforts should be made to limit atmospheric concentrations of other greenhouse gases. Funding Like many other conference statements, the Noordwijk Declaration recognized that financial assistance to developing countries is needed to help them cope with the causes and effects of climate change and to enable them to participate in the international climate negotiations. It recommended that existing development institutions give more weight to climate problems when planning their projects and that additional funds be made available. Funded projects should initially focus on, among other things, phasing out CFCs, improving energy efficiency, and reforesting the land. Research and monitoring More research should be undertaken on sources and sinks of greenhouse gases other than CO2. The Declaration urged all states to cooperate in research and monitoring activities. Notes: 1 Reprinted in "American University Journal of International Law and Policy", Vol. 5 (1990), pp. 592-601. 2 For example, the UN/ECE Bergen Conference (1990) and the 1989 Cairo Conference on the atmosphere. (See relevant fact sheets.) 3 See communication from the EC Commission to the Council on community policy targets on the greenhouse issue, 16 March 1990 (Document SEC(90) final). Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-5IA005-000053-B044-243http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs232.html 138.80.61.12 19970221171522 text/html 6429HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:45:35 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 6258Last-modified: Tue, 09 Jul 1996 02:25:58 GMT Climate Change Fact Sheet 232 The economics of internationally-agreed climate change policies From a narrow economic perspective, individual countries do not have a strong incentive to reduce their net greenhouse gas emissions unilaterally. Few countries emit more than 1-2% of mankind's current fossil fuel emissions of carbon dioxide (CO2). So if a country reduced its emissions, it would receive some benefit in the form of a small reduction in global climate change, but that benefit would be only a tiny fraction of the world-wide benefit. As a result, individual countries have very little economic incentive for incurring the costs of abating emissions or of enhancing forests or other "carbon sinks". To be truly effective, climate policies will have to be international. The OECD countries account for about one-half of mankind�s current CO2 emissions. However, even if all the OECD countries work together to cut their emissions, they would have only a limited impact on future climate change because emissions are expected to increase rapidly in the developing countries. It must be remembered that, while the industrialized countries are largely responsible for the historical build-up of atmospheric concentrations, new policies can only impact current and future emissions. Resource transfers will be required to make an internationally coordinated policy attractive to developing countries. Developing countries may be relatively more vulnerable to climate change than are industrialized countries. However, the total benefit they would receive from policies to reduce climate change damages is likely to be smaller than the benefit realized by industrial countries, since the latter have larger economies (this analysis considers only market goods and services - see fact sheet 229). Resources will therefore have to be transferred to developing countries to make it attractive for them to incur the costs of substantially reducing greenhouse gas emissions and enhancing carbon sinks. Precedents for such transfers exist. For example, under the amended Montreal Protocol industrialized countries compensate developing countries for the "incremental costs" of substituting new chemicals for CFCs. In the case of climate change, rich countries would find transfers economically attractive because the total cost of abatement can be reduced if the abatement burden is distributed widely. While the UN Convention on Climate Change calls for such transfers, agreeing on their magnitude and distribution will not be easy. Free rider incentives will be hard to overcome. The problem is that any country that decides not to participate in the global effort to reduce emissions saves substantially on abatement costs. At the same time, because this country�s emissions are small relative to the total, it would suffer only a minute loss in benefit as a consequence of its own decision not to participate. So this country would be better off economically if it did not participate. It will be harder to reduce emissions of carbon dioxide than it has been to reduce emissions of gases that destroy the ozone layer. While it is true that the Montreal Protocol successfully harnessed international cooperation to phase out CFCs, climate change is a very different environmental problem. Reducing CFCs will be relatively cheap, and the economically-measurable benefits quite high - largely because ozone depletion causes cancer, which people are willing to pay a lot to avoid. This gave countries strong unilateral incentives to agree to phasing out CFCs. Such is not the case with climate change. To be cost-effective, international climate change policies should include more than just emissions targets. Many other actions can be taken to reduce atmospheric concentrations of greenhouse gases at a low cost. The destruction of rain forests, for example, has been hastened by domestic tax policies that provide incentives for excessive deforestation; removing these policies would not only reduce climate change, but would benefit national economic development. Eliminating subsidies on fossil fuels would also reduce CO2 emissions while improving economic performance. A different type of problem relates to large-scale leaks of methane from the former USSR�s natural gas pipelines. These leaks occur largely because the Soviet incentive system did not reward efficiency. Economic reforms in Eastern Europe and the former USSR are almost certain to lead to more efficient energy use and thus fewer emissions per unit of GNP. International cooperation on these various issues would help to speed the adjustment to more efficient economies. An international programme based solely on economic self-interest would not have to include measures for adapting to climate change. If the Netherlands builds dikes to protect Amsterdam, it incurs the cost but also receives all of the benefit. This gives the Netherlands an economic incentive to adapt to climate change to the extent necessary. The same would hold true for all other countries. Consequently, apart from transferring funds to poor countries and cooperating on research and development, an international agreement on adaptation is not really needed. For further reading: S Barrett, "Economic Analysis of International Environmental Agreements: Lessons for a Global Warming Treaty", OECD, "Responding to Climate Change: Selected Economic Issues", Paris: OECD (1991). S Barrett, "Climate Change: Economic Issues in Negotiating a Framework Convention", OECD: Paris (1992). T.C Schelling, "Economic Responses to Global Warming: Prospects for Cooperative Approaches", in R. Dornbusch and J.M. Poterba, "Global Warming: Economic Policy Responses", Cambridge: MIT Press (1991). Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-6IA005-000053-B041-513http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs223.html 138.80.61.12 19970221170433 text/html 6715HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:34:34 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 6544Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact Sheet 223 Regional climate conferences in Africa, Asia, and Latin America Among the most important points are:Africa, Asia, and Latin America each held a regional conference on climate change in 1990-91. These three meetings were convened to address the concerns of developing countries in the context of climate change. The meetings were co-sponsored by UNEP and various other organizations and attended by both scientists and policy-makers. The statements adopted by the three conferences stressed the same four key issues raised by developing countries at all climate meetings: The shared but differentiated responsibilities of states to counter-act climate change, the need for funding and technology transfer from industrial to developing countries, the importance of sustainable development, and the need for developing countries to participate fully in the international climate negotiations. The statements contain specific and action-oriented recommendations but no concrete commitments by participating states. (See fact sheet 204.) The Nairobi Conference on Global Warming and Climate Change addressed the concerns of African countries. Held at UNEP Headquarters in Nairobi from 2-4 May 1990, the conference examined the possible impact of climate change on African countries. It also issued a set of recommendations based on the premise that "there is sufficient scientific data to justify action being taken now to reduce greenhouse gas emissions and implement strategic planning to deal with climate change". Most of the recommendations are addressed to African governments, but some are also addressed to industrialized countries, the United Nations, NGOs (non-governmental organizations), and the private sector. Specific proposals for African countries include afforestation, family planning, and conservation of resources. Industrialized countries are asked, among other things, to reduce their greenhouse gas emissions and to transfer technology to developing countries. In addition, the conference statement advocates promoting negotiations on binding legal instruments on climate change and raising public awareness. The Sao Paolo Regional Conference on Global Warming and Sustainable Development focused on Latin America. Held from 18-20 June 1990, the Conference brought together 200 scientists and government officials from the Latin American region. They adopted a Conference Statement consisting of 20 bullet-point recommendations based on the conclusions of the Conference�s eight working groups. Climate change is a serious threat to humankind. Global action must be taken immediately to stop it. The United Nations Conference on Environment and Development (UNCED) should play an important role in promoting action; The world community should be prepared to reduce carbon dioxide emissions to 60%-80% of 1990 levels by the year 2000. This reduction is necessary, according to the Intergovernmental Panel on Climate Change (IPCC), to stabilize atmospheric concentrations of carbon dioxide; States must end the destruction of forests and adopt reforestation programs; Since economic development is a priority for developing countries, they should pioneer new forms of sustainable development, especially forms adapted to the circumstances prevailing in developing countries. Industrialized countries should contribute by removing economic obstacles by, for example, adopting a new approach to the foreign debt problem; Climate change presents a potential source of conflict between developed and developing countries. This problem should be addressed by UNCED; and Because some degree of global warming is inevitable, developing countries must elaborate strategies for adapting to the consequences of climate change, such as a rise in the sea-level. The Bangkok Conference on Global Warming and Sustainable Development established Asia�s priorities vis a vis climate change. As the last of the regional climate meetings of developing countries, the 10-12 June 1991 Bangkok Conference was able to draw upon the conclusions of prior meetings. The Conference brought together scientists and policy makers from 20 South-East Asian, East Asian, and Pacific countries, as well as from other states and international organizations. Because the Asia/Pacific region includes both highly industrialized and developing countries, the Conference provided a good forum for discussing cooperation between these two groups. The Conference Statement reviews the scientific findings on climate change and its effects as well as the conclusions of previous climate conferences. It "proposed policies and activities to move the region towards a pattern of development designed for the long-term amelioration of these problems". In particular, it advised: That the world community treat climate change as a crucial security issue in light of the seriousness of the potential impacts of global warming; That the equitable involvement of all social groups within each country was essential to addressing climate change in the context of sustainable economic development. Women play a particularly important role in both economic development and environmental protection and should therefore be involved; That transfers of funds and technology to the developing countries are critical to the ability of these countries to respond to climate change. The Statement suggests new approaches to the international debt problem and proposes that current military assistance to developing countries be replaced by environmental assistance; That a regional network of policy research institutes focusing on global warming and sustainable development and a parallel network of scientific institutions to investigate climate change be established; and That the International Negotiating Committee on Climate Change (INC) represents a welcome and rare opportunity for all states, but especially developing countries, to address many of their environment and development concerns within a global forum. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-7IA005-000053-B044-341http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs234.html 138.80.61.12 19970221171631 text/html 7023HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:46:44 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 6852Last-modified: Tue, 09 Jul 1996 02:25:58 GMT Climate Change Fact Sheet 234 The economics of risk avoidance under uncertainty Scientists are not yet certain just how the climate will respond to increasing atmospheric concentrations of greenhouse gases (GHGs). Their best estimate is that current GHG emissions trends will cause the global average temperature to increase by 1.5-4.5 C over the next 100 years and the global average sea level to rise by 3-10 cm per decade. Scientists have significantly less confidence in regional or national climate forecasts. Other uncertainties include future population and economic growth rates and how these rates will impact future GHG emissions levels. What's more, since the majority of studies that assess the likely impacts of climate change take the doubling of carbon dioxide (CO2) concentrations as their benchmark scenario, little information exists on the likely scale of damages either before or - more importantly - after this doubling occurs. As a result, there is a great deal of uncertainty about how great the risks of climate change truly are. Economists recognise that people dislike uncertainty and tend to be risk averse . . . People are often willing to exchange a risky asset with a high expected return for an asset with a lower, yet more certain rate of return. For example, suppose a lottery winner is given a choice between receiving a certain prize of $450,000 and selecting one of two identical envelopes. One envelope contains a prize of $1 million and the other is empty, making the expected payoff to the gamble $500,000 ([$1 million x 0.5] + [$0 x 0.5]). This is higher than the certain payoff, but most people would, of course, prefer to take the certain $450,000 prize. By comparison, a "risk neutral individual" would be someone who cares only about the expected outcome and would thus choose to open an envelope. but economic analysis that treats climate change as if it were a problem involving perfect certainty does offer valuable insights. One of the most useful services economists could render to policy-makers would be to calculate the optimal level of GHG emissions, given the costs and benefits of making emissions reductions. To make the necessary calculations for such a cost-benefit analysis, economists usually assign definite values to uncertain parameters such as climate sensitivity. They use the expected, or most likely, values. This allows economists to offer some insights, but it does lead to treating the problem as if it were one of perfect certainty. An analysis that ignores uncertainty could lend support to policies that would be appropriate for society "on average". They might not, however, reflect the interest that individuals would have in avoiding extreme consequences, even if there is a low probability that they would occur. Critics therefore argue that people would probably want to make even larger cutbacks in GHG emissions than suggested by cost-benefit analyses to avoid this high risk. Improving the treatment of risk in cost-benefit analyses requires making much more complex calculations. Few economists have attempted to do this. They would have to describe variations in key parameters mathematically - for example, to say that there is a 95% probability that climate sensitivity lies between two particular values. Scientists, however, are generally reluctant to make such statements. Furthermore, some risks are inherently unquantifiable, and economists have no widely accepted method for dealing with such risks. Research that would reduce climate change uncertainties would be extremely valuable for policy-making. By calculating how much it would be worth paying to get immediate, perfect information about key uncertainties, economists can provide a strong argument for greatly increased research budgets. If cost-benefit analyses could be based on perfect information, we could more easily avoid restricting emissions unnecessarily in the event that global warming involves low risks, or cut back further on excessive emissions if the climate is acutely sensitive to greenhouse gas emissions. One study1 calculates that the value of perfect information regarding climate sensitivity is an impressive US$148 billion. However, not only does scientific research seldom provide perfect information, but some scientific improvements must await the passage of time and cannot simply be bought. Some economists even speculate that permitting a small amount of global warming to occur would provide scientific information that is more valuable than the damage associated with the warming (although given the predictions of climate change models that some climate change seems inevitable, there would seem to be little or nothing to gain from postponing abatement). On the other hand, because the expected value of perfect information is high, there is a good argument for substantial funding for scientific research. Because the greenhouse problem is so full of uncertainties, policy-making will remain relatively subjective. Some people argue that society should adopt a "maxi-mini' approach to decision-making. This approach of maximising the minimum payoff suggests that we should prepare for the worst possible outcome irrespective of the cost and no matter how unlikely this eventuality is. This is not, of course, very typical real-life behaviour. An alternative proposal is to regard some rate of warming, such as 0.1o C per decade, as the maximum acceptable and to set an emissions ceiling for GHGs to guarantee that such a rate is not exceeded. This is based on the argument that it will remain impossible to quantify the climatic risks of surpassing either a rate of warming or an absolute temperature that is greater than what has been recorded in the earth's recent history. The truth of this argument, too, is hotly debated. For economists, the challenge remains to develop policy prescriptions that take into account current scientific uncertainties. For further reading: S. Peck and T. Tiesberg, "Global Warming Uncertainties and the Value of Information: An Analysis Using CETA", The Electric Power Institute, Paolo Alto CA, USA, 1992. A. Manne and R. Richels, "Buying Greenhouse Insurance: The Economics of Carbon Dioxide Emission Limits", MIT Press, Cambridge MA, 1992. Note: 1 Peck and Tiesberg (1992). Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-8IA005-000053-B044-365http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs235.html 138.80.61.12 19970221171641 text/html 5876HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:47:01 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5705Last-modified: Tue, 09 Jul 1996 02:25:58 GMT Climate Change Fact Sheet 235 The links between climate change and acid rain policies The problems of climate change and acid are distinct but related. Climate change is a global problem that concerns all countries, while acid rain is a regional problem concerning individual countries and their neighbours. Climate change is caused by man-made emissions of the so-called greenhouse gases (GHGs). These non-toxic gases, most of which occur naturally in the atmosphere and are not pollutants, include carbon dioxide, methane, CFCs, and nitrous oxide. Acid rain is caused by toxic industrial smoke containing sulphur dioxide, nitrogen oxides, and chloride. These chemical compounds break down in the atmosphere into their respective acids - sulphuric, nitric, and hydrochloric - which are deposited on the earth in the form of dry gas, rain, snow, mist, or fog. These two environmental problems are clearly different - but they are also linked in important ways. Reducing emissions of carbon dioxide to combat climate change would also reduce acid rain. There are at present no economically competitive "end-of-pipe" or "scrubbing" technologies for removing carbon dioxide (CO2) from industrial smokestacks. CO2 emissions can only be cut by reducing the use of fossil fuel, whether by lowering energy use per unit of economic production or substituting low- and non-carbon fuel for high-carbon fuels. On average, reducing one tonne (1,000 kg) of carbon simultaneously reduces about 20 kg of sulphur dioxide and eight kg of nitrogen oxides from stationary sources such as factories, and about 0.5 and 9 kg respectively from mobile sources such as cars. From the point of view of policies to cut GHG emissions, this secondary benefit of reducing acid rain may be considerable; it may even, at least initially, exceed the primary benefit of reduced warming by a factor of 10 or 20. If these figures are accurate, they lend additional support to economic arguments for carbon taxes and other policies for reducing GHG emissions. Reducing emissions of sulphur dioxide to combat acid rain would probably affect the rate of global warming. Rather than reducing fossil fuel use, which cuts emissions of all fuel-related gases, policies to control SO2 emissions call for installing flue gas desulphurisation and other end-of-pipe technologies and for substituting low-sulphur fuels for high-sulphur fuels. While leaving GHGs untouched, these technologies will reduce the sulphur dioxide and other sulphuric particles floating in the atmosphere as "aerosols". By scattering and reflecting the sun's light away from the earth (causing bright red sunsets), these particles have a cooling effect on the lower atmosphere. In the highly industrialised Northern Hemisphere, sulphur particles in the atmosphere may be offsetting at least half of the current man-made warming. In the less-industrialised southern hemisphere, the offsetting effect may be almost negligible. The potency of sulphur cooling has recently been demonstrated with the volcanic eruption of Mount Pinatubo in June 1991. This massive release of sulphur particles into the stratosphere seems to have cooled the earth by 0.5 C on average between May 1991 and May 1992. A warmer atmosphere would influence the occurrence and severity of acid rain. Many of the chemical reactions that lead to the formation of acid rain are affected by temperature. Higher temperatures usually accelerate reactions, suggesting that global warming is likely to increase the formation of acidic materials. The likely net effect on acid rain is unclear, however, since global warming will also cause changes in cloud, wind, and precipitation patterns. Because of these linkages, there are benefits to designing acid rain and climate change policies simultaneously. Cost benefit analyses of policies to reduce GHG emissions normally suggest that reductions should take place as long as the marginal costs of reduction are smaller than the marginal benefits of avoided global warming. Because reducing GHG emissions also reduces acid rain, however, the marginal benefits of this secondary reduction should also be considered in the analysis. Similarly, a cost benefit analysis of reducing sulphur emissions should consider the marginal benefits of avoided acid rain, the marginal costs of making the reductions, plus the additional costs of increased global warming due to the reduction in atmospheric aerosols. For further Reading: D.M. Newberry, Acid Rain, Economic Policy 11(October), 297-346, 1992. D.W. Pearce, The Secondary Benefits of Greenhouse Gas Control, Centre for Social and Economic Research on the Global Environment (CSERGE), University of East Anglia and University College London, Working Paper GEC 92-12, 1992. IPCC, Climate Change 1992: The Supplementary Report to the IPCC Scientific Assessment, Cambridge: Cambridge University Press, 1992. J.B. Smith and D.A. Tirpak, ed., The Potential Effects of Global Climate Change on the United States. Appendix F: Air Quality, US Environmental Protection Agency, Washington, DC, 1989. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-9IA005-000053-B043-102http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs225.html 138.80.61.12 19970221171153 text/html 5979HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:41:58 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5808Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact Sheet 225 Climate change and the Geneva Convention on Long-Range Transboundary Air Pollution Although drafted in response to acid rain, the Geneva Convention is potentially relevant to climate change. The treaty was adopted in 1979, when greenhouse gases and their effects on climate were not yet a major issue. It seeks to control the emission and cross-border flow of "air pollutants", defined as substances which, when released into the air, have harmful effects on human well-being and on flora, fauna, and ecosystems. This broad definition does not envisage the greenhouse effect as such, but it can be applied to a number of greenhouse gases and their precursors. The Geneva Convention was adopted under the auspices of the UN Economic Commission for Europe (ECE) and is limited in scope and membership to European and North American countries. It entered into force in 1983. As of January 1993, 35 states and the European Community are parties to it. The Convention provides a general framework for the control of air pollution. More specific obligations are set out in the accompanying protocols. The Convention�s Executive Body reviews the progress that party states are making on their commitments under the Convention to endeavour to limit and then, as far as possible, to reduce air pollution; to attempt to develop "without undue delay" policies and strategies to combat air pollution; to initiate and coordinate research on the causes and effects of air pollution and on new technologies for reducing emissions; to exchange information on scientific research, policies, and measures relating to air pollution; to hold consultations between upwind "polluter" states and downwind "victim" states. The four protocols adopted within the framework of the Geneva Convention have some relevance to climate change. The 1988 Sofia Protocol on nitrogen oxides (NOx) and the 1991 Geneva Protocol on volatile organic compounds (VOCs) both seek to limit emissions of substances which are precursors of the greenhouse gas tropospheric ozone (O3).1 The 1985 Helsinki Protocol aims to limit sulphur emissions. As the presence of sulphates in the stratosphere tends to promote global cooling, this may also have some impact on climate change. Under the EMEP programme, governed by the 1984 Geneva Protocol, data on air pollution are collected and monitored. Parties to the Sofia Protocol must stabilise nitrogen oxide emissions. The Protocol entered into force in 1991 and, as of January 1993, has 20 Parties. It imposes the following obligations: to stabilise the emissions or cross-border flows of nitrogen oxides at 1987 levels. This must be done by 1994 at the latest, and should represent just a first step. Parties are free to choose a base year from before 1987; to apply national emissions standards to major new emissions sources. States must also introduce pollution control measures for all major existing stationary sources; to start negotiations on further reductions of nitrogen oxide emissions; to facilitate the exchange of information and technology for reducing nitrogen oxide emissions; to make unleaded fuel for vehicles equipped with catalytic converters available within two years of the Protocol�s entry into force; to review the Protocol regularly as new scientific and technical information becomes available; to carry out research and to develop national policies and strategies for implementing the Protocol. The 1991 Geneva Protocol provides that annual VOC emissions must be reduced by at least 30% by 1999. It sets 1988 as the base year, but Parties may choose a different base year from between 1984 and 1990. Parties with low 1988 emissions levels (specified in the protocol) must at least stabilise emissions by 1999. The Protocol also obliges states to apply emission standards to all new sources of VOCs and to products containing solvents, use the best available technologies that are economically feasible, review the Protocol regularly and negotiate further emissions reductions, establish a mechanism for compliance monitoring, set up national programmes to fulfill their obligations, and co-operate in the exchange of technology and information. It also specifies that compliance with its provisions does not relieve parties from fulfilling other international obligations, including those related to combating climate change. The Protocol was signed by 23 states and will enter into force upon ratification by 16 states. For further reading: Lang, Neuhold and Zemanek (Eds.), "Environmental Protection and International Law", (London 1991); Chapter 3: Protection of the Environment by International Law: Air Pollution, (L. G�ndling and P. Fauteux). "Yearbook of International Environmental Law", Vol. 2(1991); Section II./1. Transboundary Air Pollution, (W.J. Kakebeeke), p. 103. P. Sand (Ed.), "The Effectiveness of International Environmental Agreements: A Survey of Existing Legal Instruments", (Cambridge 1992); Section III. Atmosphere and Outer Space, (R.E.Benedick and R. Pronove), p. 123. Notes: 1 The Intergovernmental Panel on Climate Change, "Climate Change: The IPCC Scientific Assessment Report", Cambridge University Press, 1990, pp. 28-30. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-10IA005-000053-B038-611http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs251.html 138.80.61.12 19970221164548 text/html 7414HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:14:25 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 7243Last-modified: Tue, 09 Jul 1996 02:25:58 GMT Climate Change Fact Sheet  251 <IMG SRC = "iucctiny.gif" > Key legal aspects of the Climate Change Convention The United Nations Framework Convention on Climate Change is part of the body of legal rules that govern relationships among states and international organisations. International law determines the obligations and rights that members of the community of states have towards each other. It does not normally apply directly to individuals or to private companies. Treaties (often called conventions) are one of the most important sources of international law. They are formal agreements between two or more states to do or to not do certain things, or to do them in a certain way. The Convention is legally binding only on those states that have agreed to be bound by it. A state that is bound by a convention is called a party. To meet its obligations under a convention, a party may need to impose legal obligations on its nationals. For example, the Climate Change Convention obliges developed country parties to take measures aimed at limiting their greenhouse gas emissions. To meet this commitment, these states may need to adopt national legislation that will in turn encourage or require companies and individuals to limit their emissions. The Convention was concluded through a formal procedure. The task of drafting a treaty involving numerous states and a complicated issue is often assigned to a body of government representatives set up by a UN agency or other international organisation. The Climate Change Convention was negotiated and drafted by the Intergovernmental Negotiating Committee for a Framework Convention on Climate Change (INC/FCCC), a body established by the UN General Assembly (fact sheet 209). Participation in the INC/FCCC was open to all member states of the UN and its specialised agencies, and efforts were made to involve as many states as possible. Meetings were organised and serviced by a Geneva-based secretariat. The INC/FCCC took just 15 months to draft the Convention, a remarkably short time given the complexity of the issue and the diversity of positions represented. The INC/FCCC negotiated and then formally adopted the Convention. At the end of a treaty negotiation, ministers and other duly empowered representatives of the negotiating states formally express their collective consent to the draft text. The treaty is then considered to be adopted, although not yet legally binding. The Climate Change Convention was adopted by the INC/FCCC, the same body that drafted it. The Convention was officially opened for signature at the UN Conference on Environment and Development (UNCED). Signature is generally the first step towards accepting a treaty. During the Conference, also known as the Rio Earth Summit, the Convention was signed by 154 heads of state or other senior national representatives, as well as by one regional economic organisation (the European Economic Community, now the European Union). It remained open for signature until 19 June 1993, by which time 165 states plus the EEC had signed. After signing, states must ratify the Convention in order to become parties. Ratifying can take time because many states must have treaties formally approved by parliament or some other legal body. They may also need to adjust their national laws to reflect their new treaty obligations. States that ratify a treaty accept it as binding and are obliged to implement it. The Climate Change Convention entered into force (became legally binding on its parties) on 21 March 1994, 90 days after the 50th ratification. Those states that were not involved in the drafting and adoption of the treaty, or that join after the closing date for signature, may still become a party to the Convention through a procedure called accession. International law does not provide a single, clearly defined way to make states respect the treaties to which they are parties. Some experts argue that international law provides rules on liability and responsibility which require states or individuals that have caused environmental damage to restore the affected parts of the environment, pay compensation to those that suffered damage, or both. However, these rules are of limited value for problems such as greenhouse gas emissions, where the link between cause and effect can be difficult to prove. The Convention creates its own institutions for monitoring compliance. Parties to the Convention must regularly provide information on their performance to the Conference of the Parties (COP). These documents are made available to all other parties and discussed by the COP. Two subsidiary bodies are created by the Convention to provide expert advice for assessing the reports and the effectiveness of the measures that parties take. In this way, all parties will know if a state is not meeting its commitments. This can provide an incentive for a state to comply with its obligations so as to avoid a negative international reputation. Many other treaties also use this so-called "institutional supervision" method to monitor compliance. It is becoming an increasingly important mechanism for issues where a country's failure to meet its commitments affects not just one other state, but the entire international community. Any party can raise questions about whether another party has complied with its commitments under the Convention. If possible this should be done through negotiation or other peaceful means. A multilateral "non-compliance procedure" could be adopted to allow the issue to be addressed in a non-confrontational setting aimed at facilitating compliance with the Convention. Alternatively, the parties may use more traditional methods of dispute settlement, including conciliation, arbitration, and adjudication. The parties concerned could agree to bring the dispute before the International Court of Justice or to another agreed forum. Legally, the resulting decision would have to be respected. The decision of the court or arbitrator may oblige a state to honour its commitments in the future, restore the damaged parts of the environment, or pay compensation. One possible drawback of this approach is that all states involved in the dispute must have previously accepted the jurisdiction of the arbitrator or court. For further reading: Brownlie, I., "Principles of Public International Law", 4th edition, Oxford (1990). Francioni, F. and T. Scovazzi (Eds.), "International Responsibility for Environmental Harm", London (1991). Reuter, P., "Introduction to the Law of Treaties", London (1989). Sinclair, I., "The Vienna Convention on the Law of Treaties", 2nd edition, Manchester (1989). Last revised 1 April 1994 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-11IA005-000053-B037-278http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs126.html 138.80.61.12 19970221163629 text/html 5277HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:05:53 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5106Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact sheet 126 Poland and climate change As a developed country with inefficient industry, Poland is a major emitter of greenhouse gases. In 1988 its emissions of carbon dioxide (CO2) exceeded 460 million tonnes, representing 2.3% of total world-wide emissions, or 12.1 tonnes per capita and 2.66 tonnes per US$1,000 of GDP. These figures result partly from the country's heavy reliance on coal and lignite, which are carbon-intensive fuels. Poland is also vulnerable to many of the potential impacts of global climate change. Over the next 100 years, annual mean temperatures in Poland may increase by about 3.5 C, while January temperatures could rise by as much as 5 C. Agriculture may be both positively and negatively affected by climate change. Higher temperatures could extend the summer growing season by 50 to 70 days. Combined with higher humidity, this could triple the harvest of fodder and double the production of cereal and other stubble crops. Shorter and gentler winters might also permit livestock to remain in pasture almost the entire year. Maize soybeans and sunflowers would thrive, and the variety of fruits and vegetables that could be raised would expand. One study has suggested that total food production will increase by about 34%. On the negative side, climate change could encourage new and more virulent pests and diseases to attack crops. And, while most crop yields are expected to rise, the staple potato crop could decline under warmer and drier conditions. A warmer climate would probably accelerate the cycle of precipitation, evapo-transpiration, and water run-off. Researchers predict that evapo-transpiration may increase by 30% by the year 2100, while the annual runoff in central Poland may increase by 50-100% over the next 100 years. This could worsen the erosion of arable land and reduce the water content of the soil. Greater run-off in the early spring followed by a water deficit during the growing season would require a more extensive artificial reservoir system to store excess water. The species composition of Polish forests would continue to change. Due in part to a slight warming since the 1920s, deciduous trees such as oak and beech have been gradually replacing evergreens such as pine, spruce and fir. If temperatures continue to rise, broad-leafed species are likely to continue displacing conifers. On the other hand, an increase in droughts and heat-waves could slow their growth and raise the risk of forest fires. The mean sea-level of the southern Baltic Sea would continue to rise. It has already been rising by about 1.5-2.9 mm per year over the past century, and higher levels could subject Poland's northern low-lying areas - which are mostly farmland - to flooding. Meanwhile, 18 holiday centres situated on cliffs would be threatened by severe erosion. With a higher sea-level would come increased salinity in river estuaries, coastal lakes, and ground water. The area at risk is home to 700,000 people, five large harbours, and a number of industrial plants. The cost of a "full protection" response strategy for Poland's Baltic coastline has been estimated at US$7 billion, while the total cost of a "no protection" strategy would amount to $70 billion. Poland signed the Climate Change Convention on 4 June 1992 during the UNCED "Earth Summit" in Rio de Janeiro. By 21 September 1994, the government must present a summary of its national climate change strategy to the Conference of the Parties to the Convention. This summary will include an "inventory" of Poland's sources and "sinks" of carbon dioxide and other greenhouse gases. It will also outline Poland's strategies for limiting its emissions; a key strategy will be to reduce its dependence on coal and lignite for 75% of its energy needs to 50% by the year 2010 by shifting to gas and oil. The report will also explain how Poland plans to adapt to the possible impacts described above and to promote research and education programmes. For further reading: Kundzewicz, Z.W., A. Kedziora, K. Kowalski, and L. Ryszkowski, "Climate change impacts on eco-hydrology: the Polish experience", in "Natural bases for protection and management of agricultural landscape", (CPBP 04.10.03), Ministry of Education in Poland (1991). L. Ryszkowski, A. Kedziora, "Potential effects of climate and land use changes on the water balance structure in Poland", in "Land use changes in Europe", Kluwer Academic Publishers: The Netherlands (1991). R. B. Zeidler, "Assessment of the vulnerability of Poland's coastal areas to sea-level rise", H*T*S: Gdansk (1992). Last revised 1 February 1994 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-12IA005-000053-B040-201http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs213.html 138.80.61.12 19970221165553 text/html 3025HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:26:06 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 2854Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact Sheet 213 The First World Climate Conference The First World Climate Conference was one of the first major international meeting on climate change. Held on 12-23 February 1979 in Geneva, the Conference was sponsored by the World Meteorological Organization WMO) and a number of other international bodies. Essentially a scientific conference, it was attended by scientists from a wide range of disciplines. In addition to the main plenary sessions, the conference organized four working groups to look into climate data, the identification of climate topics, integrated impact studies, and research on climate variability and change. The Conference focused mostly on how climate change might impact human activities. It examined the possible impacts on specific activities such as agriculture, fishing, forestry, hydrology, and urban planning. The conclusions were summarized in the Declaration of the World Climate Conference1, which highlighted the international community�s emerging perception of the climate as a vital natural resource. It recognized that humanity�s survival requires us to live in harmony with nature, and it urged governments "to foresee and to prevent potential man-made changes in climate that might be adverse to the well-being of humanity." The declaration also identified the leading cause of global warming as increased atmospheric concentrations of carbon dioxide resulting from the burning of fossil fuels, deforestation, and changes in land use. The Conference led to the establishment of the World Climate Programme (WCP) to research climate change. Emphasizing the need for more scientific research into the causes and effects of climate change, the First World Climate Conference endorsed a proposal by WMO to establish a new climate research programme. The WCP was set up under the joint responsibility of WMO, the United Nations Environment Programme UNEP), and the International Council of Scientific Unions (ICSU). It is composed of the World Climate Data Programme, the World Climate Applications Programme, the World Climate Research Programme, and the World Climate Impact Programme. For further reading: "Environmental Policy and Law", No. 5 (1979), p. 65; and No. 6 (1980), p. 103. Notes: 1 WMO, World Climate Conference: Extended Summaries of Papers Presented at the Conference, Geneva, February 1979. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-13IA005-000053-B038-304http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs202.html 138.80.61.12 19970221164215 text/html 9639HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:12:13 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 9468Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact Sheet 202 International law and climate change The UN Framework Convention on Climate Change was adopted in May 1992 and opened for signature in June at UNCED in Rio de Janeiro.1 Conventions among states are a key source of international law. Also called treaties, they set out obligations that are binding on their party states. As a framework convention, the climate treaty contains important principles and general obligations. Additional commitments may be agreed upon later in one or more protocols. Customary international law also provides some general guidance on the legal implications of climate change. An unwritten international norm becomes part of customary law if it is consistently followed over a long period of time by a significant number of states which accept it as a legal obligation. For example, if a particular commitment to act is repeatedly expressed at important international conferences, and if all the participating states act in accordance with it, then the commitment may become an obligation under customary law. Existing customary law affirms the sovereign right of states to manage their own natural resources, although this right is by no means absolute. Customary law also prohibits a state from allowing activities on its territory to inflict serious damage on the environment of other states or on parts of the environment that do not belong to any state. Although states are not prohibited from causing any environmental damage at all, they must make "reasonable use" of common resources such as the atmosphere. The problem is, what is "reasonable"? Exactly how much carbon dioxide is a state permitted to release into the atmosphere? How much forest may it turn into agricultural or industrial land? Customary law has no definitive answer. Until 1992, international law did not address climate change directly. Because climate change is a phenomenon of unprecedented scale and character, traditional legal concepts and mechanisms provided by treaties and customary law do not help much. A number of treaties already in force, notably the Geneva Convention on Long-Range Transboundary Air Pollution and the Montreal Protocol on Substances That Deplete The Ozone Layer, do deal with atmospheric pollution. However, they do not specifically address the causes and effects of climate change as such. Non-binding statements by international climate conferences influenced the drafting of the Climate Change Convention by the Intergovernmental Negotiating Committee for a Framework Convention on Climate Change INC/FCCC). The treaty drafters referred to the statements to evaluate the concerns and proposals of various states and regions. In this way, a number of concepts and principles were reaffirmed and highlighted. The following three paragraphs describe the most important of these principles: Climate change is a "common concern of humankind". Representing an effort to provide a basis for international action to protect the global climate, this concept was first introduced in a 1988 resolution of the United Nations General Assembly.2 It has since been supported by numerous international climate meetings.3 The legal problem is that climate change is not imposed by one state upon another state. As a result, the traditional legal principles governing transboundary pollution (which is imposed by one state upon another) do not apply. But if the atmosphere is a "common concern of humankind", all states have an interest and duty to protect it from serious harm. A state on one side of the globe is thus "affected" by a state on the other side of globe that is emitting greenhouse gases into the atmosphere. This principle is affirmed in the preamble to the Climate Convention. States have "common but differentiated responsibilities" for combating climate change. It is widely recognized that all states contribute to climate change and that all states may, to different degrees, suffer from it. But the industrialized states developed their economies over the past 150 years in part by treating the atmosphere as a free and unlimited resource, and they continue to generate the greatest quantity of greenhouse gases. Developing countries are now attempting to industrialize at a time when the atmosphere is no longer considered as free and unlimited. In addition, they still make a smaller contribution to climate change (although it will increase in the decades to come). The principle of "common but differentiated responsibilities" proposes that, while all states should act to prevent damage to the atmosphere, developed countries should take the lead. This principle is widely recognized.4 It was incorporated into the 1987 Montreal Protocol and it underlies the dual standard of commitments for developed and developing countries established by the Climate Convention. Potentially dangerous activities should be restricted or prohibited even before they can be proven to cause serious damage (the precautionary principle). Traditionally, activities were often not restricted or prohibited by legal rules until they had been proven to cause environmental damage. In other words, states were free in their activities unless and until a causal link between an activity and a particular damage had been established. This approach may not work, however, in the case of activities contributing to climate change. Scientists are still unsure about the exact timing and nature of climate change impacts, but if efforts to limit net greenhouse gas emissions are not initiated before scientific certainty is achieved, it may be too late to undo the damage. Therefore, the precautionary principle provides that activities threatening serious or irreversible damage should be restricted or even prohibited before there is absolute scientific certainty about their impact. This principle was discussed at many international climate conferences,5 and it has also been included in some recent environmental policy statements and conventions.6 The climate treaty embodies a precautionary approach, since states agreed to take action despite the remaining scientific uncertainties about climate change. For further reading: R.R. Churchill and D.F. Freestone (Eds.), International Law and Global Climate Change, London, 1991. See in particular Chapter 1: International Law and the Protection of the Global Atmosphere: Concepts, Categories and Principles (A.E. Boyle). L. G�ndling, "The Precautionary Principle", in International Journal of Estuarine and Coastal Law,No. 5 (1990), p. 23. W. Lang, H. Neuhold, and K. Zemanek (eds.), "Environmental Protection and International Law", London, 1991. See in particular Chapter 6, "Ozone Layer and Climate Change". Notes: 1 Report of the Intergovernmental Negotiating Committee for a Framework Convention on Climate Change, May 1992, A/AC.237/18(Part II)Add.1 2 UN General Assembly Resolution 43/53 on the protection of global climate for present and future generations of mankind, 6 December 1988. 3 E.g. Decision 15/36 of the UNEP Governing Council; the Second World Climate Conference; Working Group I of the Intergovernmental Negotiating Committee on Climate Change (INC); the Noordwijk Declaration; the Mal� Declaration; the Langkawi Declaration; the Beijing Declaration; and The Meeting of the Group of Legal Experts to Examine the Concept of the Common Concern of Mankind in Relation to Global Environmental Issues, Malta, 13-15 Dec., 1990, (summary of the discussions and reports, edited by David J. Attard (Nairobi, 1991)). 4 The principle was strongly supported by all "developing countries climate conferences" and was recognized by the UN General Assembly, the Second World Climate Conference, the Preparatory Committee of UNCED, the Toronto Conference Statement, the Hague Declaration, and the Noordwijk Declaration. It is also reflected in Principle 7 of the 1992 Rio Declaration (see relevant fact sheets). 5 E.g. the "Brundtland Commission"; the Toronto Conference Statement; the EC-EFTA policy decision; Working Group I of the Intergovernmental Panel on Climate Change (IPCC), Working Group I of the International Negotiation Committee on Climate Change (INC), and the UNCED Preparatory Committee. 6 E.g. the 1991 Bamako Convention on hazardous wastes adopted by the African States; the 1992 Paris Convention for the Protection of the Marine Environment of the North-East Atlantic; the 1992 Helsinki Convention on the Protection of the Marine Environment of the Baltic Sea Area; the preamble of the amended Montreal Protocol (1987/90); Resolution 44 (14) of the 1991 conference of the parties to the London Dumping Convention , and Principle 15 of the 1992 Rio Declaration. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-14IA005-000053-B036-321http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs121.html 138.80.61.12 19970221163156 text/html 6579HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:02:15 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 6408Last-modified: Tue, 09 Jul 1996 02:25:56 GMT Climate Change Fact Sheet 121 How climate change could impact Southeast Asia Scientists believe that mankind�s greenhouse gas emissions may raise temperatures and change rainfall patterns in Southeast Asia. According to computerized General Circulation Models GCMs) , the mean annual near-surface temperature of the earth will rise by 1.5 - 4.5 C over the next 100 years. Scientists recognize that, while GCMs can offer global climate forecasts, they are still unable to generate climate scenarios at the regional level. Nevertheless, to assist policy-makers who must start preparing national climate change response strategies, researchers have begun to use the global results of GCMs to conduct studies on the likely socio-economic impacts of climate change on particular regions, such as Southeast Asia. The most serious threat to livelihoods in Southeast Asia would come from coastal inundation caused by rising sea levels. Scientists believe that higher atmospheric temperatures would cause a sea-level rise of around 20 cm by the year 2030 and 65 cm by the end of next century. Sandy coastlines backed by densely populated, low-lying plains make the Southeast Asian region particularly vulnerable to inundation. As much as 20,000 km2 of land in Malaysia, Thailand, and Indonesia could be threatened with flooding, including some of the most economically productive land in these countries; the city of Bangkok and its surrounding industrial belt is all within one metre of sea level. Meanwhile, as the ocean rose the "natural" inland retreat of mangrove swamps - critical breeding grounds for fish and prawns - would be obstructed by towns and farms. Coastal farmland would also be lost, and the disappearance of beaches would undermine tourism, an important source of earnings in the area. The cost of improving coastal defenses against a sea-level rise of 20 cm may be around US$4 million per kilometre. Although the countries of Southeast Asia have seen impressive rates of development over the past decade, such enormous expenditures would put serious strains on their economies. Indonesia has 80,000 km of coastline, the longest of any country in the world. Clearly it would not be feasible to protect it all. An alternative strategy would be to expand land area through reclamation projects such as land-fills, with the income from the new land helping to offset the costs of construction. In 1989 Malaysia�s Prime Minister suggested that a series of sea walls could be constructed about 3.2 km offshore in the Strait of Malacca as a prelude to large-scale land reclamation. Whichever response is adopted, deciding what areas should be protected and which ones should be sacrificed to the sea would no doubt prove controversial. Higher temperatures and changing rainfall patterns would probably reduce rice production throughout the region. Besides the loss in output resulting from sea inundation, yields on remaining cropland would be reduced. In one research study, the climate scenarios developed by GCMs were fed into yield models for various crops. While there are uncertainties over such factors as how farmers would respond to the challenge, the results suggest that Malaysian rice yields could fall by around 20%. Excess demand for irrigation water would severely limit the potential for growing two crops per year as is the current practice in the more fertile areas of the country, and higher temperatures would shorten the time available for grains to mature. In Vietnam, increased drought in the south could seriously damage agriculture in the Mekong Delta, one of the country�s most productive areas. In Indonesia, excessive rainfall would cause erosion and soil leaching. In Thailand, rice yields per hectare might actually rise in response to increased temperatures, but a reduction in the amount of cultivated land would still cause total production levels to fall. If changing patterns of production in other countries did not compensate for this shortfall, the world price for rice could rise sharply, as Thailand currently accounts for about one third of the international market. New agricultural technologies and crop strains might mitigate some of the worst effects of climate change. Local and international agricultural research centres may be able to develop new seed varieties that can tolerate the expected changes in temperature and soil quality. However, this is more likely for some crops than for others. Rubber, for example, which earned revenues of M$3.6 billion for Malaysia in 1989, might adapt well to climate change, but Indonesia�s maize crop might suffer a 50% decrease in yields. Poverty and high population densities would ensure that even small changes in land area or crop productivity would have serious social consequences. Many of Southeast Asia�s farmers and fishermen must struggle for their daily existence and would not have the resources to adapt to climate change or to relocate. If the climate scenarios presented in the studies come to pass, government-sponsored resettlement programmes will become essential. While moving populations can create ethnic tensions and foment competition over economic resources, spontaneous migration could be even more damaging and might result in widespread destitution as well as in further harm to the environment. For Further Reading: Antonio Maghalhaes, Nguyen Huu Ninh, Martin Parry, eds., "The Potential Socio-Economic Effects of Climate Change in South-East Asia", Nairobi: UNEP 1991. A.L. Chong, S. Panich, Martin Parry, and Blantran de Rozari, eds., "The Potential Socio-Economic Effects of Climate Change: A Summary of Three Regional Assessments", Nairobi: UNEP 1991. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-15IA005-000053-B044-124http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs230.html 138.80.61.12 19970221171428 text/html 8635HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:44:48 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 8464Last-modified: Tue, 09 Jul 1996 02:25:58 GMT Climate Change Fact Sheet 230 The economics of carbon taxes A tax on the carbon content of oil, coal, and gas would discourage fossil-fuel use and so reduce carbon dioxide (CO2) emissions. Carbon taxes have already been introduced by a number of industrialized countries, including Finland, the Netherlands, Norway, and Sweden. Detailed proposals are also being debated in Denmark and Switzerland, and a proposal by the European Commission for a community-wide carbon tax is being considered by member states. Many economists believe that a carbon tax would achieve any given reduction in CO2 emissions at minimum cost . . . Although there are other ways to reduce carbon dioxide emissions, they can be less economically efficient than are carbon taxes. A tax just on petrol, for example, would reduce the use of oil, but much of the reduction would be negated as energy users switch over to coal and natural gas, which also emit carbon dioxide. It would also not affect the use of heating oil and other petroleum products. As for energy efficiency standards and other regulatory alternatives, because they would not raise the price of emitting carbon, they would neither discourage energy use nor provide electricity generators with incentives to move away from carbon-intensive fuels. A carbon tax, on the other hand, would give all users of fossil fuels the same incentive to reduce carbon emissions. but the tax must be well designed and administered. Because different fossil fuels have different carbon contents and can often be substituted for one another, it is important to tax carbon content. While such a tax might seem awkward to apply, estimates of each fuel�s carbon content could easily be used to translate the carbon tax into separate taxes on coal, oil, and natural gas. Since fossil fuels are already taxed in most countries, this would make the carbon tax easy to administer. Unfortunately, many of the national carbon tax programmes already in existence are not well designed. In one nation's programme, for example, the electricity industry is exempt from paying the carbon tax, and industrial companies pay the tax on the basis of their revenues, rather than on their energy use. These exceptions and exemptions have the effect of raising the country�s overall cost of abating any given level of CO2 emissions. A carbon tax cannot guarantee how great the reduction in emissions will be . . . The effect that a tax has on emissions depends on how energy users and suppliers respond to it. Their response would be difficult to predict in advance. Economists have made a range of estimates for the effect different carbon taxes would have. The firmest conclusion that emerges from their work is that the tax would have to be high - some US$100 per ton of carbon - if it were to reduce emissions substantially in the long run. A US$100 per-ton tax would raise the current price of crude oil (US$20 per barrel) by about two thirds. The tax would have to be this high in part because economic growth will, in the absence of abatement policy, increase emissions over time. Hence, the tax would have to be raised every now and then to keep emissions levels stable. On the other hand, future technological innovations may reduce the cost of abatement in the very long run, allowing carbon taxes to be reduced. but it can guarantee that the cost of abatement will not be excessive. Faced with a carbon tax of US$100 per ton of carbon, individuals and firms would seek to avoid this penalty by spending up to - but not more than - US$100 to abate a ton of carbon emissions. Quantitative emissions limits, on the other hand, would guarantee how much CO2 is emitted, but not the price for achieving that level. Therefore, a quantitative limit on emissions could turn out to be too costly (higher, for example, than the damage avoided) or not costly enough. In many industrialized countries, CO2 emissions are no higher today than they were in 1973 due to real increases in the price of energy. If energy prices, always volatile and unpredictable, increase in the future, a given quantitative ceiling on emissions might be reachable without devoting any additional resources to abatement; if prices fall, the costs of achieving a given quantitative target could turn out to be much higher than expected. A key virtue of the carbon tax is that it fixes the incentive to abate emissions independently of energy price fluctuations (that is, a US$100 tax provides a US$100 incentive). In addition, the tax could be adjusted up or down if new scientific information becomes available about the damages caused by particular emissions levels. A supplementary benefit is that a carbon tax would allow other, more distorting, taxes to be lowered, thus improving the economy�s performance. A high carbon tax would raise substantial sums of money; one study estimates that a tax of US$100 per ton would raise revenues equal to 3.4% of GNP in the US and 1.1% of GNP in energy-efficient Japan.1 Such high sums would allow income and other taxes to be reduced, making the carbon tax revenue neutral (i.e. keeping the total tax burden unchanged). While taxes on income and savings distort economic decision-making by individuals and firms, a carbon tax actually reduces distortions in the economy. This contribution to economic performance needs to be factored in when calculating the true cost of an abatement policy based on a carbon tax. The carbon tax is regressive, but other taxes and transfers can be adjusted to offset its negative impact on poorer households. Poor households tend to spend a greater percentage of their total income on energy than do rich ones. But the regressive nature of the carbon tax can be neutralized by indexing transfer payments to inflation (as is already done in many countries) and by increasing the personal deductions households can take on their income taxes. At the same time, it should be recognized that other policies for abating net greenhouse gas emissions would also impact the distribution of income, even if less transparently than would a carbon tax. For example, energy efficiency standards would hit the poor hardest because the poor tend to buy cheaper, less-energy-efficient appliances than do richer households. The most cost-efficient carbon tax would be one that is comprehensive and internationally coordinated. A one-ton reduction in carbon emissions has the same benefit as a one-ton increase in the amount of carbon absorbed by trees or other "sinks". The most efficient policy, then, would be to offer a subsidy for sink enhancement that is equivalent to the tax on CO2 emissions. In addition, taxes should also be imposed on other greenhouse gases; such taxes would be calculated according to the global warming potential of each particular gas (fact sheet 7). The US, for example, taxes chlorofluorocarbons (CFCs), and a few other countries are considering doing the same. If such taxes were set at the same level in every country, the total cost of reducing greenhouse gas emissions would be reduced (fact sheet 228). There would be no need for such an international tax to be imposed by an international agency, an arrangement many countries would anyway not accept. Instead, as exemplified by the current proposal by the European Community, a standard carbon tax could be coordinated internationally but administered nationally. For further reading: S. Barrett, "Global Warming: Economics of a Carbon Tax," in D. Pearce (ed.), "Blueprint 2: Greening the World Economy", London: Earthscan (1991). J. M. Poterba, "Tax Policy to Combat Global Warming: On Designing a Carbon Tax," in R. Dornbusch and J.M. Poterba (eds.), "Global Warming: Economic Policy Responses", Cambridge: MIT Press (1991). D. Pearce, "The Role of Carbon Taxes in Adjusting to Global Warming," The Economic Journal, 101, pp. 938-948 (1991). Notes: 1 Poterba (1991). Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-16IA005-000053-B041-356http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs219.html 138.80.61.12 19970221170255 text/html 4241HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:33:09 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 4070Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact Sheet 219 The Cairo Compact on climate change The 1989 Cairo Compact called for urgent international action on the threat of climate change. In December 1989 the Egyptian government, the United Nations Environment Programme UNEP), and the Climate Institute convened a World Conference on Preparing for Climate Change. Held in Cairo and attended by over 400 participants from all regions of the world, the conference adopted the Cairo Compact.1 The Compact states that climate change requires the cooperation of all nations on an unprecedented scale. To be effective, any measures would have to be supported not only by governments, but by multilateral organizations, the industrial and financial community, scientific and educational institutions, and environmental groups. Individuals may also make an important contribution through their consumption patterns and participation in the decision-making process. The Compact concludes by underlining the urgency of these actions: "For the sake of our planet and the lives of our children and generations to come, we must act now." The Compact outlines the key issues that should be at the top of the world agenda. They include: The elaboration of a framework climate treaty in time for adoption by the United Nations Conference on Environment and Development UNCED). Work on such a treaty should build on concepts already developed by international organizations and conferences, such as the Noordwijk Ministerial Conference of November 1989. The convention should set general targets for greenhouse gas emissions and reforestation. More specific commitments should be contained in protocols. Governments should give strong support to the work of the Inter-governmental Panel on Climate Change(IPCC). All nations should become parties to the Montreal Protocol on Substances That Deplete the Ozone Layer. Existing UN bodies such as UNEP and the World Meteorological Organization WMO) should be strengthened. (See fact sheets 206, 208 and 224.) While participating in the development of an international treaty, governments should take their own measures at the national and regional levels. These measures could then serve as a basis for international regulations. Wealthy nations should provide developing countries with the means to cope with climate change. International and national measures should focus on efforts to reduce the "historically unprecedented population growth" which is "a driving force behind the increase in greenhouse gas emissions." Other important efforts include curbing deforestation, promoting reforestation, saving energy through more efficient industrial processes and transport systems, and using renewable sources of energy. As a certain degree of climate change is inevitable, the necessary adaptation measures have to be taken. The resilience of agriculture under conditions of climate change must be increased. Island and coastal states should include the prospective sea-level rise in their long-term planning. Since climate change may lead to displacement of human populations, sufficient means are needed to cope with the problem of environmental refugees. Notes: 1 The Cairo Compact: Toward a Concerted World-Wide Response to the Climate Crisis, adopted on 21 December 1989. Reprinted in "American University Journal of International Law and Policy", Vol. 5 (1990), p. 631. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-17IA005-000053-B039-630http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs211.html 138.80.61.12 19970221165330 text/html 7328HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:23:41 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 7157Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact Sheet 211 The climate policy of the European Community The environmental policy of the European Community (EC) has addressed climate change since the mid-1980s. Between 1986 and 1989, several non-binding resolutions adopted by the Parliament and Council of the EC called for measures to combat climate change.1 In addition, a number of binding Directives on air pollution addressed the issue indirectly.2 The EC's Climate Change Policy of 1990,3 subsequently endorsed by the EFTA states,4 committed member states to stabilising carbon dioxide emissions at 1990 levels "in general" by the year 2000. According to the policy document, this target is only a first step, with reducing emissions to be considered at a later stage. The policy document also contains suggestions on how to achieve the target. The EC and its members signed the UN Framework Convention on Climate Change in June 1992 at the Rio "Earth Summit". On 23 April 1993, the EC Council decided that it would ratify the Convention by the end of the year. During the negotiations for the Convention, the Commission of the EC and the 12 member states had played a leading role. They had advocated the adoption of strong measures, in particular the incorporation of the EC emissions target, into the Convention. Although supported by a number of states, this stabilisation target was eventually replaced by a compromise formula. The EC has declared its intention to continue playing a lead role in the international effort to combat climate change. As a block of leading industrial states, the EC intends to adopt measures that go beyond the obligations imposed on developed countries by the Climate Convention. The focus of current EC initiatives is therefore on stabilising and eventually reducing CO2 emissions. The 5th Environmental Action Programme adopted in March 1992 sets objectives and targets for the year 2000.5 In addition, the EC Commission started work in 1990 on a comprehensive Community strategy for limiting carbon dioxide emissions and improving energy efficiency. The proposed strategy, submitted to the Council in May 1992,6 is based on the premise that reducing energy demand by increasing energy efficiency and promoting fuel-switching is the best way to reduce CO2 emissions. It proposes a wide range of specific measures, which are to be based on the principles of economic efficiency, equity, and subsidiarity. The strategy is currently under discussion. The proposed EC strategy for limiting CO2 emissions incorporates existing actions and initiatives for promoting energy efficiency. In October 1991, the EC Council approved a five-year energy efficiency programme called SAVE (Specific Actions for Vigorous Energy Efficiency). The proposed ALTENER Programme would promote the development of renewable energy sources. In April 1993, the EC Council decided to set up a system for monitoring the Community's CO2 emissions. Other programmes related to research and development in the field of energy efficiency are also under discussion. The most controversial measure under discussion is the introduction of a carbon tax. In June 1992, the Commission proposed a Council Directive for taxing carbon dioxide emissions and energy.7 The proposal calls for member states to levy taxes at the national level on the consumption of energy from non-renewable sources. The tax would be based on the energy source's CO2 content and energy value (measured in calories). The rate would start at US$3 per barrel of oil or its equivalent in 1993 and increase by $1 per barrel every year. There is still considerable debate over this measure. The EC is also a party to earlier international agreements that have a bearing on climate change. As a party to the Montreal Protocol, which controls the use of CFCs, it was among the first to advocate stricter measures than those originally imposed by the Protocol. The EC is also a party to the Geneva Convention on Long-Range Transboundary Air Pollution and its protocols. It has adopted implementing legislation in the areas addressed by these various legal instruments. Work is underway on a draft Directive on volatile organic compounds (VOCs), which are the subject of the 1991 protocol to the Geneva Convention (fact sheet 225). For further reading: "Report of the Commission of the European Communities to the United Nations Conference on Environment and Development", Luxembourg, Office for Official Publications of the EC, (June 1992). EC Commission/Directorate General for Energy (DG XVII), "Energy in Europe: Energy Policies and Trends in the European Community", Brussels, (December 1992). "The Climate Challenge - Economic Aspects of the Community's Strategy for Limiting CO2 Emissions", in "European Economy", No. 51, Luxembourg, Office for Official Publications of the EC (May 1992). Cameron, J., "European Community Law and Policy on Global Warming", in T. Iwama (ed.), "Policies and Law on Global Warming: International and Comparative Analysis", Tokyo: Environmental Research Centre (1991). Notes: 1 Parliament resolution on measures to combat rising CO2 concentrations in the atmosphere (Official Journal of the European Communities (OJ) No. C. 255/275, 1986); conclusions of the 40th session of the European Council, Rhodes, December 1988; Council Resolution on the greenhouse effect and the Community, 21 June 1989 (89/C 183/03), OJ No. C 183/4, 1989. 2 E.g. Council Directive 70/220/EEC, as amended and supplemented by Council Directives 74/290/EEC, 77/102/EEC, 78/665/EEC, 83/351/EEC, 88/76/EEC, 88/436/EEC, and 89/458/EEC; also Council Directive 85/203/EEC. 3 EC Climate Change Policy, adopted by the joint Energy and Environmental Council, Luxembourg, 29 October 1990. 4 Joint EC/EFTA Decision on a CO2 emission target, adopted at the Second Ministerial Conference of EFTA and EC on the Environment, Geneva, 5 November 1990. 5 "Towards Sustainability: A European Community Programme of Policy and Action in relation to the Environment and Sustainable Development", Vol. II, COM(92)23 final, 27 March 1992, p. 41-2. 6 Community Strategy to limit carbon dioxide emissions and improve energy efficiency (Communication from the Commission to the Council); Documents COM(92)246 final and SEC(91)1744 final. 7 Commission Proposal for a Council Directive introducing a tax on carbon dioxide emissions and energy, COM(92)226 final, 30 June 1992. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-18IA005-000053-B039-361http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs207.html 138.80.61.12 19970221165052 text/html 5876HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:19:11 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5705Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact Sheet 207 Climate change and the UNCED "Earth Summit" The United Nations Conference on Environment and Development (UNCED) was held in Rio de Janeiro on June 3-14, 1992. This so-called Earth Summit took place 20 years after the 1972 Stockholm Declaration1 first laid the foundations of contemporary environmental policy. As a high-level forum with universal participation, UNCED�s main aim was to show the way to a new global strategy for reconciling development needs with environmental protection. The key results of UNCED were the Rio Declaration, Agenda 21, Forest Principles, and two international conventions, one concerning climate change, the other biodiversity. The decisions taken and the documents adopted by UNCED are likely to have important implications for the future international response to climate change, as well as for virtually every other environmental issue. UNCED�s success was made possible by the two-year preparatory process that preceded the actual Earth Summit. A Preparatory Committee (PrepCom) for UNCED, established by the UN General Assembly,2 began its work in early 1990. The PrepCom received guidance from a number of UN bodies on how to address UNCED�s various topics.3 It established three working groups and held an organizational meeting and four substantive sessions between March 1990 and April 1992.4 The PrepCom dealt extensively with UNCED�s primary concern of integrating environmental and development aims to promote sustainable development. The work relating to climate change focused on assisting and monitoring efforts already underway in the Intergovernmental Negotiating Committee for a Framework Convention on Climate Change (INC/FCCC) and in other UN bodies.5 The United Nations Framework Convention on Climate Change was signed at UNCED by 153 States and the European Community. After 15 months of tough negotiations, the Convention was adopted in New York on May 9, 1992 by the INC/FCCC. The negotiations were concluded in time for the treaty to be officially opened for signature in Rio, as intended. UNCED was considered the proper forum for affirming the international community�s support for this important international treaty. (The other legally binding document signed at UNCED was the Convention on Biological Diversity.) The Rio Declaration on Environment and Development adopted at UNCED sets out key principles of relevance to climate change. Although not legally binding, the Declaration contains 27 principles intended to shape future international and national action on environment and development. Some of these principles had already been affirmed at climate change and other international conferences, such as the common but differentiated responsibilities of states at different levels of development, the need for sustainable development, and the precautionary principle. Agenda 21, UNCED�s 40-chapter environmental action agenda for the 21st century, contains a chapter on protecting the atmosphere. Chapter 9 of Agenda 21 recommends measures that governments and other bodies can take to improve the scientific understanding of climate change, to promote sustainable energy use and production, and to reduce the damage caused to the atmosphere by transport, industry, resource development, and land use. Chapter 9 also makes recommendations about the related issues of stratospheric ozone depletion and transboundary atmospheric pollution. This chapter was one of the most difficult to negotiate and caused substantive disagreements up to the last moment. For further reading: United Nations General Assembly, A/CONF.151/26 (Vols. 1-4). This is a report of the Rio Earth Summit, including Agenda 21, the Rio Declaration, Forest Principles, and references to the two Conventions adopted. It is available in mimeograph from UN Documents in NY and Geneva, and a printed version is planned for sale by UN Publications. UNCED, "A Global Partnership for Environment and Development - A Guide to Agenda 21". This 120 page summary will be available from UN Publications. Notes: 1 Declaration on the Human Environment, adopted by the United Nations Conference on the Human Environment, Stockholm, June 16, 1972; reprinted in UNEP, Environmental Law Guidelines and Principles, No. 1. 2 UNGA Resolutions 43/196 (20 December 1988) and 44/288 (22 December 1989). 3 UNEP Governing Council Decision SS.II/9, August. 3, 1990. 4 Organizational meeting: New York, March 5-16, 1990; 1st session: Nairobi, August 6-31 1991; 2nd session: Geneva March 18 - April 5, 1991; 3rd session: Geneva August 12 - September 4, 1991; 4th session: New York, March 3 - April 2, 1992. Cf. the reports of the PrepCom to the UN General Assembly; also "Environmental Policy and Law", Vol. 20 (1990), pp. 127, 164; Vol. 21 (1991) pp. 42, 186, 233, 242, 267; and Vol. 22 (1992) p. 67, 110, and 129. 5 UNCED Preparatory Committee Decision 2/8, pp. 30-32 of the report of the PrepCom to the UN General Assembly, Document A/46/48, April 5, 1991. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-19IA005-000053-B041-63http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs215.html 138.80.61.12 19970221165940 text/html 7489HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:28:10 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 7318Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact Sheet 215 The Toronto and Ottawa conferences and the "Law of the Atmosphere" The 1988 Toronto Conference called for a "comprehensive international framework that can address the interrelated problems of the global atmosphere." Convened by the government of Canada from 27-30 June, the Toronto Conference brought together over 300 scientists and policy-makers from 46 countries and organizations. Unlike most similar international gatherings, the "International Conference of the Changing Atmosphere: Implications for Global Security" addressed climatic change together with the closely related problems of ozone layer depletion and the long-range transport of toxic and acidifying substances. It concluded that, although some steps had been taken to develop international law for protecting the atmosphere, the existing rules were still relatively fragmentary. The Conference Statement noted that "humanity is conducting an unintended, uncontrolled, globally pervasive experiment whose ultimate consequences could be second only to a global nuclear war."1 Stressing the importance of immediate action, the Statement called for the adoption of an Action Plan for the Protection of the Atmosphere and the establishment of a World Atmosphere Fund to finance its implementation. The Action Plan should include an international framework convention on the atmosphere as well as national legislation and action plans. The Statement also outlined the scientific basis for concern, the economic and social concerns, and the legal aspects of the problems. The Conference issued an ambitious "call for action" to governments, industry, and international organizations. Among its recommendations were: 1) the reduction of carbon dioxide (CO2) emissions by 20% of 1988 levels by the year 2005. Recommended as an initial global goal, this goes beyond the emissions targets recommended by most later international conferences as well as the tentative target set by the 1992 UN Framework Convention on Climate Change; 2) the improvement of energy efficiency by as much as 10% by the year 2005; 3) the inception of the necessary technological changes to enable these goals to be reached; 4) the preparation of the principles and components of a framework treaty for the protection of the atmosphere (including climate change, ozone depletion, and atmospheric pollution) in time for the 1992 UN Conference on Environment and Development (UNCED); and 5) the promotion of the Intergovernmental Panel on Climate Change (the IPCC had been established shortly before the Toronto Conference), the channelling of resources to the World Climate Programme (WCP) and other research institutions, the backing of technology transfer, the reduction of deforestation, and the furtherance of public awareness of these issues. The 1989 Ottawa Meeting of Legal and Policy Experts generated ideas for treaties on the atmosphere and on climate change. Held from 20-22 February as a follow-up to the Toronto Conference, the meeting was attended by around 80 legal and policy experts acting in a personal capacity, including officials from governments, non-governmental organizations, and academic institutions around the world. Its goal was to further develop the idea of a "law of the atmosphere treaty" and a narrower convention on climate change. The non-binding statement adopted by the meeting2 amounted to rough drafts of such treaties. Both drafts contain some interesting and innovative ideas. The "law of the atmosphere" plan tackled not only climate change, but every threat to the global atmosphere. This contrasts with proposals made by other international conferences and was criticized as being over-ambitious. Although the plan was discussed in various international meetings, it was not taken up by the climate treaty negotiators. The 1992 UN Framework Convention on Climate Change focuses exclusively on climate change. This was generally considered to be a more realistic approach with a greater chance of leading to a treaty within a reasonable period of time. The proposed "law of the atmosphere" convention would have created a very broad framework for dealing comprehensively with threats to the atmosphere. These threats include climate change, ozone depletion, and acid rain, as well as any other problems that might arise in the future. Borrowing the approach taken by the 1982 Law of the Seas Convention, some participants proposed that conventions and protocols on specific issues such as climate change be elaborated within this general framework. The Ottawa Meeting�s proposed "law of the atmosphere" convention would have included: 1) broad definitions of "atmosphere" and "atmospheric interference"; 2) the obligation on the part of all states to protect the atmosphere, notwithstanding the sovereignty of states in the management of their own resources; 3) a provision that measures taken to protect the atmosphere should not lead to damage of other parts of the environment; 4) the development and transfer of technology, especially for the benefit of developing countries; 5) prior notice to other states, and environmental impact assessment, of potentially harmful activities; 6) the elaboration of a regime for liability and compensation and for dispute settlement; and 7) the possible establishment of a World Atmospheric Trust Fund to assist developing countries. The proposed climate treaty was to be a framework treaty with protocols. Like the Vienna Convention on the Protection of the Ozone Layer and its Montreal Protocol, the proposed treaty would have dealt with fundamental issues while leaving specific issues to protocols. Possible protocols could address the various greenhouse gases (carbon dioxide, methane, CFCs and halons, nitrous oxide, and tropospheric ozone), forest management and reforestation, and the setting up of a World Climate Trust Fund to help developing countries meet their obligations under the treaty system. The "framework treaty" approach also underlies the 1992 Climate Convention. For further reading: Zaelke and Cameron, in "American University Journal of International Law and Policy", Vol. 5/2 (1990), p. 276-278. Notes: 1 See Conference Proceedings, The Changing Atmosphere: Implications for Global Security (WMO/OMM-No. 710), p. 710. The reports of the working groups established by the conference are also reprinted in this document on p. 346. 2 Protection of the Atmosphere: Statement of the Meeting of Legal and Policy Experts (Ottawa, 22 February 1989). Reprinted in: American University Journal of International Law and Policy, Vol. 5/2 (1990), p. 529-542. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-20IA006-000057-B011-142http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind/dwind.html 138.80.61.12 19970221181626 text/html 2287HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:46:38 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 2116Last-modified: Tue, 09 Jul 1996 02:26:07 GMT Wind Plants of Germany Wind Plants of Germany Wind energy is growing more rapidly in Germany than anywhere else in the world. In 1994 the recently reunified country surpassed Denmark as the world's second largest wind energy powerhouse. By the end of 1995 Germany will exceed 1,000 MW of installed wind capacity and will outstrip California, long the world leader in installed capacity by 1998, if not sooner. Wind turbines in Germany generated about 1 Terawatt-hour (1,000,000,000 kWh) of electricity during 1994 and will produce nearly 2.5 TWh in 1996. German wind development is concentrated in the two northern länder of Niedersachsen (Lower Saxony) and Schleswig-Holstein. Because each länder has a a long coastline with good wind resources, both states haveadopted a goal of installing 1,000 MW of wind capacity. Enercon E40 near Groothusen Enercon is Germany's largest wind turbine manufacturer and is located in the northenmost province of Neidersachsen, Ostfriesland. Groothusen is a small village north of Emden in the Krummhörn peninsula. The turbine shown here uses a 40-meter rotor to drive a large-diameter ring generator directly without the aid of a gearbox. The 500 kW wind turbine is installed on a 40 meter tall concrete tower. Photo copyright © by Paul Gipe. All rights reserved. Paul Gipe, 20.08.95, pgipe@igc.apc.org More information on wind energy. Wind Energy Comes of Age Wind Power for Home & Business Workgroup Windturbines WT03-B20-21IA005-000053-B044-198http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs231.html 138.80.61.12 19970221171505 text/html 6593HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:45:11 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 6422Last-modified: Tue, 09 Jul 1996 02:25:58 GMT Climate Change Fact Sheet 231 Cutting back greenhouse gases with tradable emissions permits Tradable permits are a cost-efficient, market-driven approach to reducing greenhouse gas emissions. A government must start by deciding how many tons of a particular gas may be emitted each year. It then divides this quantity up into a number of tradable emissions entitlements - measured, perhaps, in CO2-equivalent tons - and allocates them to individual firms. This gives each firm a quota of greenhouse gases that it can emit over a specified interval of time. Then the market takes over. Those polluters that can reduce their emissions relatively cheaply may find it profitable to do so and to sell their emissions permits to other firms. Those that find it expensive to cut emissions may find it attractive to buy extra permits. Trading would continue until all profitable trading opportunities had been exhausted. Permits would ensure that emissions do not exceed a given level. They are not as good as carbon taxes, however, at guaranteeing that the costs of abatement will be neither too large nor too small. So in choosing between the two main market-oriented approaches of tradable entitlements and carbon taxes , a government must decide whether it is more important to be certain of the quantity of reduced emissions or of the costs involved. Tradable permits are already being used to address several other environmental problems. For example, the US regulates chlorofluorocarbons (CFCs) with tradable emissions entitlements, and it is introducing a similar system to limit emissions of pollutants that cause acid rain. The 1987 Montreal Protocol on Substances that Deplete the Ozone Layer also includes provisions for the international trading of emissions permits, although no such trades have yet taken place. To be successful, a permit scheme would have to be carefully designed. If the rules governing trading are complex, or if the market for permits consists of only a few players, trading may not be efficient. This will also be true if trading involves substantial transaction costs. On the other hand, even an inefficient permits system may be a more cost-efficient way to reduce emissions than using most forms of regulatory control. If implemented internationally, tradable permits could lead to resource transfers from rich countries to poor ones. International trading could take place between governments as well as between firms. But before trading could begin, governments would have to agree on how to make the initial allocation of permits. One proposal calls for allocating entitlements on an equal per-capita basis.1 Such an allocation would guarantee resource transfers from the North to the South because, having fewer greenhouse gas emissions per capita than do industrialized ones, developing countries would be net sellers of permits, while rich countries would be net buyers. However, it is highly unlikely that such an allocation would be acceptable to the rich countries. They would probably prefer to have no agreement at all than to make such large transfers. To win broad acceptance, an international scheme for tradable permits could not allocate quotas on a simple per-capita basis. The problem is that OECD countries have high per-capita emissions, the developing countries have low per-capita emissions, and the former Communist countries are somewhere in between. An allocation based on population would be attractive to developing countries, but probably unacceptable to OECD countries because it would require them to make huge transfers to poor countries (assuming that the agreed goal was to reduce global emissions on a large scale). Allocating entitlements according to a slightly more complex formula could reduce these transfers and ensure that every party to the agreement is better off than it would be without the agreement. Agreement would be far more likely if the entitlements were initially allocated according to a formula that reflected the different circumstances of these country groups. For example, the OECD could receive somewhat fewer permits than would be required for current emissions levels, and the poor countries a slight surplus, with the total quantity of entitlements being somewhat below current global emission levels. One study estimates that such an allocation would lower resource transfers to a fraction of current overseas development assistance.2 International tradable permits could be effective even if implemented on a small scale. Economists analysing schemes for emissions permits have usually focused on international agreements for making large-scale reductions in global greenhouse gas emissions. But trading may also be effective in more limited circumstances. For example, a country facing high abatement costs may meet its own national target for emissions reduction by providing incentives for other countries with low abatement costs to undertake abatement on its behalf. Such bilateral agreements (known as "offsets") may involve relatively high transactions costs, but their small scale may make them easier to negotiate and implement, at least in the short run. For further reading: S. Barrett (1991), "Economic Instruments for Climate Change Policy," in OECD, Responding to Climate Change: Selected Economic Issues, Paris: OECD. S. Barrett (1991), "�Acceptable� Allocations of Tradable Carbon Emission Entitlements in a Global Warming Treaty," United Nations Conference on Trade and Development, Geneva. M. Grubb (1989), The Greenhouse Effect: Negotiating Targets, London: Royal Institute for International Affairs. Notes: 1 Grubb (1989). 2. S.Barrett (1991), "Transfers and the Gains From Trading Carbon Emission Entitlements in a Global Warming Treaty," UNCTAD, Geneva. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-22IA005-000053-B036-401http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs122.html 138.80.61.12 19970221163253 text/html 7865HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:02:38 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 7694Last-modified: Tue, 09 Jul 1996 02:25:56 GMT Climate Change Fact Sheet 122 How rersearchers assess the implications of climate change for agriculture A growing number of research studies are trying to assess how climate change may affect agriculture and global food supplies . Because climate change would lead to a complex chain of physical and human interactions, methodologies for analysing agricultural impacts are still being refined. But policy-makers need guidance now. Although researchers cannot offer them definitive forecasts, they can provide their current "best guess" of future possibilities. The first steps in an agricultural impacts study are to define the problem and to select the research method. The research team must agree on the goals of the study, the crops and region to be studied, the time-frame and the data needs, and the ultimate use of the study findings. This in turn helps to determine the methodology to be used. The chosen methodology must always be tested. For example, a linear regression model that predicts grass yields based on mean monthly temperatures can be run with an external set of data unrelated to the study to ensure that the model is realistic and sensitive to a wide range of assumptions. Such testing is very difficult, however, and most current methodologies have still not been sufficiently refined through experimentation. A future, potential climate scenario is then selected. This can be done in one of three ways. The first way is to adopt an arbitrary scenario and to assume that the average regional temperature will rise by, say, 2 C. Another is to take the output data from a General Circulation Model (GCM). Researchers use GCMs to explore how emissions of carbon dioxide and other greenhouse gases will change average global temperatures, rainfall, and other climatic variables. However, these models cannot yet give reliable estimates at the regional level at which most impact studies are conducted, especially for rainfall, a key variable for agriculture. The third approach is to use a past or other actual climate as an analogue. One study of the US Midwest, for example, used the actual climate conditions of the 1930s, when temperatures were 1.1 C higher than during the base period, to model the possible future agricultural response to warmer and drier conditions. The next step is to assess the impact of the climate scenario on crops. Researchers use the scenario's temperature and moisture assumptions to estimate crop yields and animal growth rates. The required modelling, however, can be quite complex. For example, in addition to the climate, crop yields depend on air quality, the soil's moisture and nutrient content, farming practices (which are continuously evolving), and much more. And, since the expected combination of temperature, moisture, soil quality, and greenhouse gas concentrations is unprecedented, scientists must rely on data that has been generated under experimental conditions to validate their impact models. Unfortunately, the results of experiments with raising crops under artificial circumstances do not always correspond with data from actual farms. For example, models indicate that the rise in atmospheric concentrations of carbon dioxide from 280 parts per million in pre-industrial times to nearly 360 ppm today should already have increased actual farm yields by 1-5%. However, no statistically significant increases related to "CO2-fertilisation" have yet been detected. Researchers also consider how climate change might affect agricultural yields indirectly. For example, some crops might be damaged by diseases and pests that have been stimulated by the warmer climate. Other environmental problems, such as acid rain or the depletion of ground water resources by expanding populations, must also be considered. As impacts models improve, they will increasingly incorporate more complex interactions, such as how changing rainfall patterns may impact hydro-electricity and thus the availability of power for irrigation pumps. They will also account for economic factors, such as the influence of markets and the impact of changing food prices on production. Expected changes in yields are then used to assess the human consequences. Reduced yields combined with population growth and other factors would clearly have implications for food supplies. Lowered agricultural production would also worsen the problems of poverty, migration, and employment. Research studies can assess the possible impacts on individual households, especially poorer ones, as well as on entire regions or nations. Many research studies consider efforts by farmers and governments to respond to threats to crop yields or to new opportunities. Farmers can adapt to climate change by using new seed varieties or altering their cultivation methods. Governments and agri-business can support research into crops that are more suited to the changing climate. Although such responses are difficult to predict, researchers can influence them by outlining policy options in their studies' conclusions. Indeed, to serve the needs of policy-makers impacts studies are increasingly being designed to focus on the greatest risks to agriculture and on effective response strategies. There are also other methodologies for assessing the likely agricultural consequences of climate change. For example, rather than starting with a particular climate scenario, researchers can ask what are a region's most significant vulnerabilities. This approach can provide policy-makers with insights into the risks and the worse case scenarios that may deserve their early attention. It involves assessing which household or regional changes would be particularly damaging and then moving "backwards" to identify the changes in production and climate which would be likely to cause them. Yet another approach is to gather information on droughts, floods, and other adverse affects of natural climate variability to gain insights into the potential physical impacts of climate change, the relative vulnerability of different sectors of the population, and the most effective response options. For example, the gradual depletion of ground water in the U.S. Midwest or desertification in the Sahel are in many ways analogous to what could occur elsewhere in future years due to climate change. For Further Reading: Martin Parry, "Climate Change and World Agriculture", Earthscan, London, 1990. Carter, T.R., M.L. Parry, S. Nishioka, and H. Harasawa, "Preliminary Guidelines for Assessing Impacts of Climate Change", Environmental Change Unit, Oxford, and Center for Global Environmental Research, Tsukuba, 1992. William R. Cline, "Global Warming: Estimating the Economic Benefits of Abatement", OECD, Paris, 1992. Martin Parry, M. Blantran de Rozari, A.L. Chong, and S. Panich, eds., "The Potential Socio-Economic Effects of Climate Change in South-East Asia", Nairobi: UNEP, 1991. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-23IA005-000053-B038-651http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs203.html 138.80.61.12 19970221164622 text/html 7132HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:16:20 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 6961Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact Sheet 203 Climate Change and the special concerns of coastal areas and small island states A rise in the sea-level caused by climate change would directly threaten low-lying coastal areas and small island states. A number of international meetings have addressed this problem, which is most serious for developing countries. The South Pacific Forum in July 1989 expressed concern over the effects of sea-level rise,1 and the Heads of Government of the Commonwealth declared their support for the efforts of coastal and island states to protect themselves against sea-level rise in their 1989 Langkawi Declaration on Environment.2 None of these statements have legal force, but they influenced the climate treaty negotiations and they are reflected in the 1992 Framework Convention on Climate Change. The 1989 Mal� Small States Conference on Sea Level Rise recognized climate change and its effects as a common concern of mankind . The Conference was held in the Maldives from November 14-18 and issued the Mal� Declaration on Global Warming and Sea Level Rise.3 The Declaration stressed the serious consequences of sea-level rise for low-lying coastal and island states and the special responsibility of industrialized states. The Mal� conference decided to develop a programme of action. This programme would consist initially of an Action Group, including representatives from the Caribbean, South Pacific, Mediterranean, and Indian Ocean regions, to develop and coordinate joint strategies to approach the issues. The conference also called upon all states to adopt immediate measures in the fields of energy efficiency, coastal zone management and protection, and afforestation. It expressed support for the work of the Intergovernmental Panel on Climate Change (IPCC) , urged industrialized countries to give financial and technical assistance to developing countries, and proposed the eventual establishment of a broader monitoring network involving collaboration with the World Meteorological Organization (WMO), the United Nations Environment Programme (UNEP), and the United Nations Educational, Scientific, and Cultural Organization (UNESCO). The 1991 Noumea Ministerial Declaration on Environment and Development addressed the effects of climate change on the islands of the South Pacific. Adopted in July by the representatives of 27 members of the South Pacific Regional Environment Programme (SPREP)4, it highlights the special vulnerability of these states and the importance of appropriate energy-efficient technologies, further scientific research into and monitoring of sea-level rise, and technical and financial assistance to Pacific island states. The ministers also expressed their commitment to control the emission ofgreenhouse gases, manage their natural resources on a sustainable basis, cooperate in international efforts, and implement their own strategies. The United Nations General Assembly also discussed the "possible adverse effects of sea-level rise on islands and coastal areas, particularly low-lying coastal areas."5 The Assembly's December 1989 resolution welcomed the growing international attention the problem was receiving, urged all states to help the countries concerned, and recommended the issue for consideration by the IPCC, the drafters of a future climate treaty, and the UN Conference on Environment and Development (UNCED). In April 1991, the UN Secretary-General submitted to the General Assembly an extensive report on actions taken at the global and regional levels.6 The call for support of low-lying states was reiterated by the Governing Council of UNEP in May 1991, which also asked the UNEP Regional Seas Programme to help these states build up their capacity to respond to the threat of a sea-level rise.7 The threat posed to low-lying coastal and island states was an important issue in the treaty negotiations and is reflected in the Climate Change Convention. The IPCC�s Impacts Working Group took special note of it. At the Second World Climate Conference in 1990, coastal and small island states formed the Alliance of Small Island States (AOSIS) to demand immediate action on climate change and its impacts, in particular sea-level rise.8 The problem was also taken up during the negotiations of the Intergovernmental Negotiating Committee for a Framework Convention on Climate Change (INC), where AOSIS represented the interests of coastal and small island states.9 INC�s Working Group I agreed that the future climate treaty would have to take account of the special needs of small coastal and island states. The 1992 UN Framework Convention on Climate Change does indeed address these concerns. In particular, it calls on parties to give full consideration to the needs of small island and low-lying coastal states when fulfilling their obligations under the treaty (Art. 4, para. 8). Notes: 1 Final Communique of the Twentieth South Pacific Forum, held at Tarawa (Kiribati) 10-11 July 1989, reprinted in UN Document A/44/463. 2 The Langkawi Declaration on Environment, issued by the Commonwealth Heads of Government at Langkawi (Malaysia), 21 October 1989. Reprinted in UN Document A/44/673 and in American University Journal of International Law and Policy, Vol. 5 (1990), p. 589. 3 Reprinted in UN Document A/C.2/44/7, Annex, and in American University Journal of International Law and Policy, Vol. 5 (1990), p. 602. 4 Fourth South Pacific Regional Environment Programme Ministerial Level Intergovernmental Meeting: Ministerial Declaration on Environment and Development, Noumea (New Caledonia), 9 July 1991. Reprinted in UN Document A/CONF.151/PC/87, Annex. 5 UNGA Resolution 44/206, 22 December 1989. (See also fact sheet 205.) 6 "Possible adverse effects of sea-level rise on islands and coastal areas, particularly low-lying coastal areas," Report of the Secretary-General to the General Assembly at its 46th Session, Document A/46/156, 19 April 1991. 7 UNEP Govrning Council Decision 16/27, 31 May 1991. 8 "Environmental Policy and Law", No. 20/6 (1990), p. 1. 9 "Environmental Policy and Law", No. 22/1 (1992), p. 8. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-24IA005-000053-B041-214http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs216.html 138.80.61.12 19970221170109 text/html 3721HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 16:30:24 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 3550Last-modified: Tue, 09 Jul 1996 02:25:57 GMT Climate Change Fact Sheet 216 The Tata Conference on Global Warming and Climate Change The Tata Conference was the first international climate change meeting dedicated to the concerns of developing countries. Held in New Delhi from 21-23 February 1989, the Conference was co-sponsored by the United Nations Environment Programme UNEP) and the US-based World Resources Institute. The Conference Statement1 provides a detailed analysis of climate change and its impact on developing countries. It calls for response measures in very strong and uncompromising terms, stressing the urgency of "aggressive action" by the international community. Among its many recommendations, the Statement advocates using fossil fuels more efficiently, introducing non-fossil fuel technologies, phasing out CFCs by the end of the century and transferring substitute technologies to developing countries on a non-commercial basis, halting deforestation, and stabilizing the world�s population. The Statement reminds industrialized countries that they bear most of the responsibility for climate change. The Statement asserts that, "having caused the major share of the problem and possessing the resources to do something about it, the industrial countries have a special responsibility to assist developing countries in finding and financing appropriate responses." It calls on them to adopt measures for reducing greenhouse gas emissions and to provide technology and funding to enable developing countries to do the same. Developing countries should participate in the international response to climate change. . . The Statement notes that 20% of greenhouse gas emissions currently emanate from developing countries, and that this figure could climb to well over 50% by the middle of the 21st century. Consequently, developing countries are "potential contributors [to the problem] in the future and burden-sharers today." Developing countries should be particularly active in improving energy efficiency, pioneering the use of renewable energy, halting deforestation and moving toward net forest growth, and slowing population growth. but economic development should not be sacrificed. The Conference Statement addresses the potential conflict between development and the proposed climate change measures: "The developing countries� contribution in response to the greenhouse challenge should be carried out in a way that enhances, rather than diminishes, development prospects. Where these are in conflict, priority should be given to development, which brings so many clear and needed benefits..." The Conference Statement emphasizes the importance of research and training in developing countries. It recommends setting up national climate monitoring, research, and management boards to coordinate research and implementation strategies. Notes: 1 "The Tata Conference on Global Warming and Climate Change: Perspectives From Developing Countries", reprinted in American University Journal of International Law and Policy, Vol. 5/2 (1990), p. 554. Last revised 1 May 1993 by the Information Unit on Climate Change (IUCC), UNEP, P.O. Box 356, CH-1219 Ch�telaine, Switzerland. Tel. (41 22) 979 9111. Fax (41 22) 797 3464. E-mail iucc@unep.ch. WT03-B20-25IA006-000057-B011-288http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind/ukhave.html 138.80.61.12 19970221181741 text/html 1601HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:47:37 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1430Last-modified: Tue, 09 Jul 1996 02:26:09 GMT Wind Plants of Cumbria, England Wind Plants of Cumbria, England The British firm Windcluster is attempting to develop wind plants on unused land near harbors or old industrial sites. In their first project, Windcluster installed five Vestas V27 turbines on an abandoned air field. The airport at Haverigg on the Cumbrian coast had not been used since World War II when it was an army air base. Cluster at Haverigg Air Base The Cumbrian Mountains of the Lake District plunge into the Irish Sea just north of the Haverigg air base at Black Combe (background). Photo copyright © by Paul Gipe. All rights reserved. Paul Gipe, 20.08.95, pgipe@igc.apc.org More information on wind energy. Wind Energy Comesof Age Wind Power for Home & Business Workgroup Windturbines WT03-B20-26IA005-000051-B017-87http://lacebark.ntu.edu.au:80/j_mitroy/sid101/acid/envben.html 138.80.61.12 19970221151508 text/html 12498HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:45:27 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 12326Last-modified: Tue, 09 Jul 1996 02:26:05 GMT Environmental Benefits Fact Sheet Acidic deposition, or acid rain as it is commonly known, occurs when emissions of sulfur dioxide (SO2)and oxides of nitrogen (NOx) react in the atmosphere with water, oxygen, and oxidants to form various acidiccompounds. These compounds then fall to the earth in either dry form (such as gas andparticles) or wet form (such as rain, snow, and fog). Prevailing winds transport the compounds, sometimes hundreds of miles,across state and antional borders. Electric utility plants account for about 70 percent of annual SO2 emissions and 30 percentof NOx emissions in the United States. Mobile sources (tranportation) also contribute significantly to NOxemissions. Overall, over 20 million tons of SO2 and NOx are emitted into the atmosphere each year. Acid rain causes acidification of lakes and streams and contributes to damage of trees at high elevations (forexample, red spruce trees above 2,000 feet in elevation). In addition, acid rain accelerates the decay of building materials and paints, includingirreplaceable buildings, statues,and sculptures that are part of our nation's cultural heritage.Prior to falling to the earth, SO2 and NOx gases and their particulate matter derivatives, sulfates and nitrates, contribute to visibility degradation and impact public health. Implementation of the Acid Rain Program under the 1990 Clean Air Act Amendments will confer significant benefits on the nation. By reducing SO2 and NOx, many acidified lakes and streams will improve substantially so that they can onceagain support fish life. Visibility will improve, allowing for increased enjoyment of scenic vistas across our country, particularly in National Parks. Stress to our forests that populate the ridges of mountains from Maine to Georgiawill be reduced. Deterioration of our historic buildings and monuments will be slowed. Finally,reductions in SO2 and NOx will reduce sulfates, nitrates, and ground level ozone (smog), leadingto improvements in public health. Surface Waters Acid rain primarily affects sensitive bodies of water, that is, those that rest atop soil with a limited ability toneutralize acidic compounds (called "buffering capacity"). Many lakes and streams examined in a NationalSurface Water Survey (NSWS) suffer from chronic acidity, a condition in which water has a constant low pHlevel. The survey investigated the effects of acidic deposition in over 1,000 lakes larger than 10 acres and in thousandsof miles of streams believed to be sensitive to acidification. Of the lakes and streams surveyed in theNSWS, acid rain has been determined to cause acidity in 75 percent of the acidic lakes and about 50 percent ofthe acidic streams. Several regions in the U.S. were identified as containing many of the surface waterssensitive to acidification. They include, but are not limited to, the Adirondacks, the mid-Appalachian highlands,the upper Midwest and the high elevation West. In some sensitive lakes and streams, acidification has completely eradicated fish species, such as the brooktrout, leaving these bodies of water barren. In fact, hundreds of the lakes in the Adirondacks surveyed in theNSWS have acidity levels indicative of chemical conditions unsuitable for the survival of sensitive fish species. Emissions from U.S. sources also contribute to acidic deposition in eastern Canada, where the soil is verysimilar to the soil of the Adirondack Mountains, and the lakes are consequently extremely vulnerableto chronic acidification problems. The Canadian government has estimated that 14,000 lakes in eastern Canadaare acidic. Streams flowing over soil with low buffering capacity are equally as susceptible to damage from acid rain aslakes are. Approximately 580 of the streams in the Mid-Atlantic Coastal Plainare acidic primarily due to acidic deposition. The New Jersey Pine Barrens area endures the highest rate of acidic streams in the nationwith over 90 percent of the streams acidic. Over 1,350 of the streams in the Mid-Atlantic Highlands (mid-Appalachia)are acidic, primarily due to acidic deposition. Many streams in that area have alreadyexperienced trout losses due to the rising acidity. Acidification is also a problem in surface water populations that were not surveyed in federal research projects. For example, although lakes smaller than 10acres were not included in the NSWS, there are from one to four times as many of these small lakes as there are larger lakes. In theAdirondacks, the percentage of acidic lakes is significantly higher when it includes smaller lakes (26 percent) than when it includes only the target size lakes (14 percent). The acidification problem in both the United States and Canada grows in magnitude if "episodic acidification" (brief periods of low pHlevels from snowmelt or heavy downpours) is taken into account. Lakes and streams throughout the UnitedStates, including high elevation western lakes, are sensitive to episodic acidification. In the Mid-Appalachians, the Mid-Atlantic Coastal Plain, andthe Adirondack Mountains, many additional lakes and streams become temporarily acidic during storms and snowmelt. Episodicacidification can cause large scale "fish kills." For example, approximately 70 percent of sensitive lakes in the Adirondacks are at risk of episodic acidification.This amount is over three times the amount of chronically acidic lakes. In the mid-Appalachians, approximately 30 percent of sensitive streams are likelyto become acidic during an episode. This level is seven times the number of chronically acidic streams in that area. Acid rain control will produce significant benefits in terms of lowered surface water acidity. If acidic deposition levels were to remainconstant over the next 50 years (the time frame used for projection models), the acidification rate of lakes in the Adirondacks that are largerthan 10 acres would rise by 50 percent or more. Scientists predict, however, that the decrease in SO2 emissions required by the Acid RainProgram will significantly reduce acidification due to atmospheric sulfur. Without the reductions in SO2 emissions, the proprotionjs of acidicaquatic systems in sensitive ecosystems would remain high or dramatically worsen. The impact of nitrogen on surface waters is also critical. Nitrogen plays a significant role in episodic acidification and new research recognizes the importance of nitrogen in long-term chronic acidification as well. Furthermore, the adverse impact of atmospheric nitrogen deposition on estuaries and other large water bodies may be significant. For example, 30 to 40 percent of the nitrogen in the Chesapeake Bay comes from atmosphericdeposition. Nitrogen is an important factor in causing eutrophication (oxygen depletion) of water bodies. Forests Acid rain has been implicated in contributing to forest degradation, especially in high-elevation spruce trees that populate the ridges ofthe Appalachian Mountains from Maine to Georgia, including national park areas such as the Shenandoah andGreat Smokey Mountain national parks. Acidic deposition seems to impair the trees' growth in several ways;for example, acidic cloud water at high elevations may increase the susceptibility of the red spruce to winter injury. There also is a concern about the impact of acid rain on forest soils. There is good reason to believe thatlong-term changes in the chemistry of some sensitive soils may have already occurred as a result of acid rain. As acid rain moves through the soils, it can strip away vital plant nutrients through chemical reactions, thusposing a potential threat to future forest productivity. Visibility Sulfur dioxide emissions lead to the formation of sulfate particles in the atmosphere. Sulfate particles accountfor more than 50 percent of the visibility reduction in the eastern part of the United States, affecting ourenjoyment of national parks, such as the Shenandoah and the Great Smoky Mountains. The Acid Rain Programis expected to improve the visual range in the eastern U.S. by 30 percent. Based on a study of the value national parkvisitors place on visibility, the visual range improvements expected at national parks of the eastern United Statesdue to the Acid Rain Program's SO2 reductions will be worth a billion dollars by the year 2010. In the westernpart of the United States, nitrogen and carbon also play roles, but sulfur has been implicated as an importantsource of visibility impairment in many of the Colorado River Plateau national parks, including the GrandCanyon, Canyonlands, and Bryce Canyon. Materials Acid rain and the dry deposition of acidic particles are known to contribute to the corrosion of metals anddeterioration of stone and paint on buildings, cultural objects, and cars. The corrosion seriously depreciates theobjects' value to society. Dry deposition of acidic compounds can also dirty buildings and other structures,leading to increased maintenance costs. To reduce damage to automotive paint caused by acid rain and acidic drydeposition, some manufacturers use acid-resistant paints, at an average cost of $5 for each new vehicle (or atotal of $61 million per year for all new cars and trucks sold in the U.S.) The Acid Rain Program will reducedamage to materials by limiting SO2 emissions. The benefits of the Acid Rain Program are measured, inpart, by the costs now paid to repair or prevent damage--the costs of repairing buildings, using acid-resistantpaints on new vehicles, plus the value that society places on the details of a statue lost forever to acid rain. Health Based on health concerns, SO2 has historically been regulated under the Clean Air Act. Sulfur dioxideinteracts in the atmosphere to form sulfate aerosols, which may be transported long distances through the air. Most sulfate aerosols are particles that can be inhaled. In the eastern United States, sulfate aerosols make upabout 25 percent of the inhalable particles. According to recent studies at Harvard and New York Universities,higher levels of sulfate aerosols are associated with increased morbidity (sickness) and mortality from lungdisorders, such as asthma and bronchitis. By lowering sulfate aerosol levels, the Acid Rain Program willreduce the incidence and the severity of asthma and bronchitis. When fully implemented by the year 2010,the public health benefits of the Acid RainProgram will be significant, due to decreased mortality, hospital admissions,and emergency room visits. Decreases in nitrogen oxide emissions are also expected to have a beneficial impact on health effects by reducing thenitrate component of inhalable particulates and reducing the nitrogen oxides available to react with volatile organic compounds and form ozone. Ozone impacts on human health include a number of morbidity and mortality risks associated with lung disorders. Clean Air for Better Life By reducing SO2 emissions by such a significant amount, the Clean Air Act promises to confer numerousbenefits on the nation. Scientists project that the 10 million-ton reduction in SO2 emissions should significantlydecrease or slow down the acidification of water bodies and will reduce stress to forests. In addition, visibility will be significantlyimproved due to the reductions, and the lifespan of building materials and structures of cultural importance shouldlengthen. Finally, the reductions in emissions will help to protect public health. Last Modified July 21, 1995 Return To Acid Rain Program's Home PageWT03-B20-27IA006-000057-B011-214http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind/nlwind.html 138.80.61.12 19970221181702 text/html 2091HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:47:20 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1920Last-modified: Tue, 09 Jul 1996 02:26:08 GMT Wind Plants of the Netherlands Wind Plants of the Netherlands For a nation known worldwide for its mastery of the wind in the eighteenth century, modern wind technology got off to a slow start in the Netherlands. Wind energy has finally begun to grow in the Netherlands after being stymied for nearly a decade by mis-guided government programs. Despite the highest population density outside Bangladesh, the Netherlands has one of the worlds' most ambitious wind programs. Officially the Netherlands is striving to install 1,000 MW of wind generating capacity by the year 2000. Unfortunately only 200 MW will be installed by the end of 1995 and it's likely the Dutch will miss their target.. Goliath near Eemshaven The polder mill Goliath frames a modern wind turbine on land reclaimed from the sea near Eemshaven in the Dutch province of Groningen. The 26-meter diameter Micon (250 kW) turbine is the last in a string of 40 identical turbines on an interior dike near the gas-fired power station at Eemshaven. Photo copyright © by Paul Gipe. All rights reserved. Paul Gipe, 20.08.95, pgipe@igc.apc.org More information on wind energy. Wind Energy Comesof Age Wind Power for Home & Business Workgroup Windturbines WT03-B20-28IA006-000057-B011-376http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind/ukelhl.html 138.80.61.12 19970221181846 text/html 1405HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:49:07 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1234Last-modified: Tue, 09 Jul 1996 02:26:09 GMT Wind Plants of Northern Ireland Wind Plants of Northern Ireland There are three wind plants operating in Northern Ireland providing a combined capacity of 15 MW. Vestas V39 on Elliott's Hill B9 energy services operates the ten 39-meter wind turbines on Elliott's Hill. 15 miles (25 kilometers) north of Belfast. The site offers a grand view of the countryside in County Antrim, including Slemish Mountain, a nationally significant topographic feature. (The volcanic plug can be seen in the background at lower right.) Photo copyright © by Paul Gipe. All rights reserved. Paul Gipe, 20.08.95, pgipe@igc.apc.org Wind Energy Comes of Age Wind Power for Home & Business Workgroup Windturbines WT03-B20-29IA006-000057-B011-419http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind/ukroyd.html 138.80.61.12 19970221181910 text/html 2684HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:49:26 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 2513Last-modified: Tue, 09 Jul 1996 02:26:09 GMT Wind Plants of Yorkshire, England Wind Plants of Yorkshire, England Though Britain has the best wind resources in Europe, wind development only began in earnest in the early 1990s when dairyman Peter Edwards installed ten wind turbines on his farm in Cornwall. Subsequently, most development in the United Kingdom has been concentrated in England, Wales, and Cornwall. There is currently 170 MW of installed capacity, but the third tranche of Britain's Non Fossil Fuel Obligation has awarded contracts for 300 MW of new wind power plants in England and Wales. Another 100 MW of contracts were awarded in Scotland. Car Park at Royd Moor Tourists enjoying a beautiful summer evening on Royd Moor. The car park (parking lot) provides a comfortable place for viewing the 13 Bonus 450 kW wind turbines on the site in the southern Pennine chain. The hill top site offers spectacular views of southern Yorkshire. The 37-meter turbines stand in two rows on the paddock. Wind development in Britain is following a far different pattern from that in California or elsewhere in Europe. Unlike northern Europe, where most of the wind turbines are being installed singly or in small clusters, wind turbines have been installed in England and Wales in small wind power plants of 10 to 100 turbines. These wind plants are far smaller than those in California where it's not uncommon to install 300 turbines at one time. British wind plants are models of uncluttered, aesthetically pleasing development. Many, such as this one on Royd Moor, include car parks for visitors, and provide access for ramblers (hikers). Photo copyright © by Paul Gipe. All rights reserved. Paul Gipe, 20.08.95, pgipe@igc.apc.org More information on wind energy. Wind Energy Comesof Age Wind Power for Home & Business Workgroup Windturbines WT03-B20-30IA006-000055-B008-38http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/ch5.html 138.80.61.12 19970221175518 text/html 23442HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:25:36 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 23270Last-modified: Tue, 09 Jul 1996 02:25:45 GMT NEPP Chapter 5 Chapter 5: Invest in Science and Technology Advances (Note: Figures referenced in this chapter have not been included at this time.) Advances in science and technology contribute to the Nation's goals of economic growth, environmental quality, and national security. The Administration is pursuing a broad agenda of scientific research and technology development, effectively using national assets such as the national laboratory system and ensuring that scientific discoveries and technological advances move rapidly into U.S. and global markets. Science and technology provide the essential foundation for economic prosperity. Investments in research and development (R&D) that spawn new technologies increase long-term productivity and high-wage job growth. This is especially true for energy technologies. Basic technologies, such as the electric motor, the internal combustion engine, and incandescent and fluorescent lighting, have had a profound effect on the way we live and work today. Emerging technologies, likefuel cells, photovoltaics, aerogels, and radio-wave lighting, may have an equally profound effect on the way Americans live in the next century. The technologies that we use to transform energy and materials into goods and services affect the amount of natural resources consumed, the types and amount of wastes produced, and the proportion of high-paying, high-tech jobs created in the economy. Because energy is so important to economic progress, energy R&D provides some of the highest national paybacks available for public expenditure. Advances in energy science and technology are critical to achieving the long-term economic, environmental, and national security goals of this Nation. The Administration's science and technology policy emphasizes effective investments in energy R&D that will keep America in the forefront of providing sustainable energy technology for domestic and global markets. The demand for solutions through science and technology will only become more acute as the world population increases and competition for limited resources intensifies. Recent projections suggest that world demand for energy could increase by 50 percent over the next 25 years,5 and energy R&D must respond to these global pressures and opportunities. The Federal Role in Supporting Science and Technology The Federal Government has traditionally supported a balanced portfolio of basic and applied research in a variety of disciplines. That balance is evident in energy R&D, where Federal support ranges from conducting long-term and high-risk energy research to assisting technology development and facilitating rapid diffusion of new technologies into the marketplace. The Administration's energy R&D agenda is part of the larger national policy framework. This policy framework is rooted in a strong belief that basic science provides the essential underpinnings for technology advances and that government can play a critical role in accelerating the development and deployment of technologies that serve the Nation's economic, environmental, and national security needs. Whether conducted by private firms or government, R&D is an investment in the future and must compete with current spending priorities. Industry is the largest supporter of R&D in the Nation, providing about 52 percent of the total national R&D investment. However, it is estimated that more than 95 percent of the $107 billion industry will spend on R&D in 1995 will be for very low-risk projects. While the private sector has always been more focused on near-term R&D, it is increasingly relinquishing its sponsorship of more basic and long-term research. Troublesome trends in private-sector R&D suggest that government must maintain its robust commitment to basic and applied R&D if the Nation is to retain its status as the world leader in energy-related science and technology. The recent Yergin Task Force report concludes that private-sector R&D in energy and energy-related areas has declined substantially over the past 10 years. At the same time, R&D spending by U.S. firms has shifted sharply toward near-term development (See Figure 16). The Government must partner with industry to maintain the long-term research base that will support tomorrow's applied R&D by offsetting ominous trends toward shorter R&D time horizons. Federal support will help the United States remain competitive in the large and growing domestic and international market for sustainable, productivity-enhancing technologies. The cornerstone of the Administration's strategy is expansion and improvement in peer-reviewed competitive research, placing a high priority on cost-shared partnerships at all levels of applied R&D in efficient end-use energy technologies, renewable energy systems, fossil-fuel combustion systems, and nuclear technologies. Stakeholder involvement throughout the planning process provides the link to the marketplace. In an era of constrained Federal budgets, the Administration continues to pursue innovative efforts to enhance and leverage national science and technology assets. Basic Science and Energy Research: Building a Foundation for a Sustainable Energy Future Advances in basic science improve our understanding of real-world phenomena and provide the basis for both revolutionary breakthroughs in technology and informed national policies that anticipate and mitigate the effects of energy choices on society. But such basic advances in knowledge or technology are not adequately rewarded in the market. The Administration continues its firm commitment to basic science and energy research where long-term potential for great scientific discovery compensates for a lack of immediate commercial interest. Under this guiding premise, seven areas of basic research are of particular importance in helping to find new ways to conserve resources, develop new energy sources, control pollution, reduce waste in manufacturing, assess and predict the impacts of global climate change, develop innovative approaches to cleaning up hazardous waste sites, and to define the long-term human health and environmental risks from energy production and use. The seven basic research areas are the following: Materials Science. New discoveries about the nature and properties of materials will result in significant energy savings and productivity gains. Research sponsored by the Department of Energy has resulted in such breakthroughs as metallic glasses; pollution-preventing lead-free solder; materials that offer superior properties for corrosion resistance, welding and joining, and high-rate and superplastic metal forming; solar photovoltaic cells; high-temperature ceramics and ceramic matrix composites; lightweight materials for batteries and fuel cells; and high-temperature alloys and superconducting materials. Geosciences. Research focused on the behavior and properties of the outermost layers of the Earth's crust has contributed significantly to the development of underground imaging, computer modeling, experimental simulation, and remote sensing. These advances are critical to the discovery of new energy resources and the enhanced recovery of existing reserves, ranging from conventional fossil fuels to alternative energy sources such as geothermal and gas hydrides. Energy Biosciences. Energy-related biotechnologies show great promise for improving energy productivity and environmental performance. Developing tomorrow's energy-related biotechnologies requires improving our basic understanding of biological processes. Research focuses on the basic mechanisms affecting plant productivity, conversion of biomass and other organic materials into fuels and chemicals by microbial systems, and the ability of biological systems to replace energy-intensive processes. Chemical Sciences. Critical research into chemical properties and interactions provides insight into the chemistry involved in combustion processes, enabling increased combustion efficiency and improved environmental performance, as well as the development of new catalysts and catalytic processes. Catalytic processes help produce nearly 75 percent of chemical and petroleum products (by dollar value), and they are central to the prevention and remediation of environmental pollution and the conversion of solar energy to clean hydrogen fuels. Biological and Environmental Science. Research on the environmental, physical, and safety risks related to the development and use of energy can help policymakers identify, understand, and anticipate the health and environmental effects of energy use. Fundamental research in physical, chemical, and biological processes contributes to the development of advanced monitoring and analysis systems that provide the foundation for addressing major biological, medical, and environmental problems. Supercomputing and Modeling. Advancing the state of the art in computational techniques, such as the use of massively parallel supercomputing, can help probe basic questions regarding the effects of energy systems on the environment as well as aid research into new techniques and methods of energy production and conversion. These efforts can help solve mysteries both "large" (for example, by analyzing environmental and climatic phenomenon) and "small" (such as by developing and analyzing advanced materials at the molecular and subatomic levels and mapping the human genome). Future Energy Sources. Research into fundamentally new, advanced energy sources such as hydrogen technologies and fusion energy can have substantial future payoffs. Because of ongoing research, hydrogen may serve someday as an energy carrier like electricity or as an end-use fuel like natural gas, or both. Similarly, the Nation's fusion program has made steady progress and last year set a record of producing 10.7 megawatts of power output at a test reactor supported by the Department of Energy. This development has significantly enhanced the prospects for demonstrating the scientific feasibility of fusion power, moving us one step closer to making this energy source available sometime in the next century. Technology Research and Development: Linking Science to Sustainable Energy Goals Advances in science provide a critical foundation for a sustainable energy future, particularly as subsequent technology developments help maximize energy productivity, prevent pollution, and enhance national security. Strengthening the linkage between science and the market is important, not to displace the impetus of private R&D efforts but rather to accelerate the introduction of knowledge and technologies that respond to the Nation's goals and to the needs of markets. Appropriate partnerships with the private sector, and other methods to enhance the linkage between science and the marketplace, can maximize the contribution of Federal investments in science and technology research to attain sustainable energy goals. Many of the specific technologies that are critical for a sustainable energy future are described elsewhere in this document. The overall strategy that motivates the Administration's technology R&D portfolio reflects the goals established in the Energy Policy Act of 1992. Three other operational goals also guide our research agenda: Encourage holistic, systems-level processes and technologies that support integrated resource efficiency, pollution prevention, and industrial ecology. These emerging concepts and techniques require industrywide participation in creative partnerships with government agencies to get beyond the pilot stage. If widely adopted, they have the potential to change the landscape of industrial America. Develop appropriate cost-sharing arrangements with industry to speed the development of specific technologies that increase energy efficiency, increase the use of clean-burning natural gas, and enhance the performance and lower the cost of renewable energy sources and other supply technologies that are smaller, cheaper, and less polluting. These programs focus on technologies with the highest potential payback from the standpoint of multiple national energy goals, where the private sector is less capable of providing the necessary R&D resources and where cost sharing ensures a high probability of market acceptance. Maintain leadership in technology research for future energy resource options that have not reached the commercial stage but hold the most promising long-term economic, environmental, and national security gains. For example, continued research into technologies that could accelerate the adoption of hydrogen energy systems could someday revolutionize the way we produce, distribute, store, and use energy. Enhancing and Leveraging National Assets The U.S. science and technology enterprise is the largest in the world and has been recognized as one of the Nation's premier assets, a treasure that we must sustain and build on for the future. The scope of this scientific establishment is truly impressive, and it includes public and private institutions, individual researchers, and facilities and instrumentation located throughout the Nation. These investments in the capacity to do long-term, fundamental research deliver a steady stream of new knowledge and maintain our global leadership in science and technology. The Administration's policy is to continue strong support for strategic national science and technology assets, so they can serve our needs today and respond to the challenges of tomorrow. In these times of constrained budgets, the Administration is ensuring that the national science and technology enterprise will meet our Nation's future needs by finding and adopting new and better ways to leverage scarce resources. These include pursuing tighter linkages between our public-sector, university-based, and private-sector research; collaborating with other nations on large-scale scientific initiatives; and coordinating and streamlining the Federal research system to maximize efficiency. The National Laboratories The Department of Energy's national laboratories make up the largest research system of its kind in the world and form the backbone of the Nation's scientific and technological enterprise. They offer the expertise of 18,000 engineers and scientists performing world-class research in the pursuit of national security, an affordable, sustainable and productive energy future, and enhanced environmental quality. These laboratories feature centralized, generally large-scale facilities that perform R&D for which there is a strong public purpose and that cannot be maintained by academia or the private sector. More than 15,000 industry, university, and government-sponsored scientists currently conduct unique, cutting-edge experiments at state-of-the-art Federal research facilities, including high energy and nuclear physics accelerators, neutron and light sources, and smaller facilities such as electron microscopy centers. The demand for the use of facilities such as the Department of Energy's accelerators for high energy and nuclear physics research, neutron sources, magnetic fusion facilities' supercomputers, and synchrotron light sources exceeds their availability. The Department's synchrotron light sources, alone, are used by more than 3,000 government, university, and private-sector researchers each year for work in the areas of semiconductors, polymers, alloys, superconductors, magnetic materials, structural biology, and pharmaceuticals. Administration policy encourages increasing access to these highly sought-after resources by providing additional funding to increase significantly and immediately the experimentation time available to researchers. The University Research Infrastructure University capabilities are another important component of the Nation's science and technology assets. Universities are critical to maintaining our science and technology leadership because of their primary role in educating the next generation of scientists and engineers, their ability to provide low-cost, high-quality research, and their relative freedom from national security-related barriers to the flow of knowledge. While the national laboratories excel at "big science" and in areas where the ability to form large interdisciplinary research teams is essential, universities contribute substantially to the Nation's other important science goals. Research and development conducted at U.S. universities accounted for 5.7 percent of the total fiscal year 1993 R&D expenditures of $160.8 billion. The Department of Energy's direct investments in the universities are complemented by its support for the national user facilities at national laboratories. Maintaining Science and Technology Excellence The intellectual resources of our national laboratories and the larger scientific community are far too valuable for future generations to lose. The Administration is committed to preserving national research capabilities through continued support of the research infrastructure. Key strategic areas needing support include robotics, alloys, ceramics, semiconductors, fuel cells, building technologies, bioremediation, batteries, and biomass and other renewable energy resources, in addition to the more traditional fields of high energy physics and nuclear physics. Continued success in meeting our national science, energy, national security, and environmental quality goals also depends on the quality and quantity of our scientific and technical workforce. Thus, science, mathematics, and engineering education is a key component of maintaining U.S. excellence in science and technology and, by extension, international economic and technological competitiveness. Linking, Coordinating, Streamlining. The National Science and Technology Council is the primary vehicle used to coordinate the use of the Nation's R&D infrastructure to meet current needs. Through the use of this structure, for example, the Department of Energy and the Department of Commerce are effectively coordinating their activities with the three major U.S. automakers and their supplier companies in the Partnership for a New Generation of Vehicles, and the Department of Energy coordinates biofuels research with the Department of Agriculture. In addition, advances in information technology are creating new opportunities to link the research capabilities at each of the national laboratories to each other and to the university and private-sector R&D community. Focusing on National Needs, Revisiting Structural Relationships. Changing world and domestic conditions present increasing challenges for successfully leveraging our national science and technology assets. In response to these realities, and with input from a variety of sources, the Administration is moving to strategically align science and technology actions with our national security, environmental quality, and economic productivity goals. This alignment will end unnecessary redundancies within the Federal science and technology enterprise and will yield savings by reforming contract and procurement practices, by reducing layers of management, and by privatizing, delegating, or eliminating assets that are no longer critical to the Nation's needs. International Collaborations. Encouraging international collaboration on a variety of larger scale, high-cost, and experimental facilities is one way to leverage our Nation's research capabilities in selected areas. Although international collaboration on R&D projects risks diffusion of control of the research focus and many user facilities are already overburdened by excess demand, there are nonetheless great opportunities for the United States to share information, costs, and expertise with the larger international community (See Chapter 7). Access and Communication: Capitalizing on Science Investments To be more effective in bringing the results of scientific research and technology development to the market and to help our Nation capitalize on R&D investments, Administration policy emphasizes communication and cooperation among public- and private-sector participants. Building on prior Administrations' efforts to improve technology transfer, the Administration is taking further steps to enhance cooperation among States, local governments, and community resources. The goal is to bring science and technology to successful market application through the formation of a seamless network of information and service providers. This more decentralized approach to leveraging existing infrastructure will ensure that Federal science and technology, coupled with private-sector and university science and technology, will quickly find its way into the marketplace. Specifically, the Administration is working to improve the communication and possible synergy across the continuum of basic science, applied R&D, and commercial development by; Making scientific results and breakthroughs more accessible to applied researchers for conversion to technologies Improving public access to information on new technologies Improving access to and the quality of science, mathematics, and engineering education Providing scientists with feedback on market needs and demands Improving industry access to cooperative technology programs of State and local governments, which are investing close to $400 million per year in these efforts. R&D partnerships are an important vehicle for bringing scientists, engineers, and commercial developers together in productive undertakings. Large partnerships and R&D consortia offer extremely high leverage in bridging the gap between basic science and commercial products to benefit U.S. industry and the Nation. The traditional linear model of the research, development, and demonstration continuum has given way to a more enlightened and realistic model that integrates basic science, applied research, and commercial development into a blended, iterative process with better feedback from consumers and users throughout the process. This system relies on "real time" interactions among the participants, each bringing his or her own unique perspectives to problem solving. Return to Table of Contents WT03-B20-31IA006-000055-B008-196http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/ch7.html 138.80.61.12 19970221175721 text/html 24258HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:26:58 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 24086Last-modified: Tue, 09 Jul 1996 02:25:45 GMT NEPP Chapter 7 Chapter 7: Engage the International Market (Note: Figures referenced in this chapter have not been included at this time.) The fundamental goals of U.S. energy policy are to maximize energy productivity, prevent pollution, and help shield our Nation from global energy risks. Promoting these goals internationally can ensure U.S. economic competitiveness and create new export markets for U.S. energy technologies and services. Global economic development depends on international markets that support reliable, secure, affordable, and clean energy systems. All nations, including the United States, benefit from effective international engagement that promotes a global sustainable energy future. Energy demand worldwide will continue to grow with population and economic activity. This is especially true in the developing countries, where potential increases in energy consumption per capita are large. The Energy Information Administration now expects energy consumption in the developing countries to almost equal that of the Organization for Economic Cooperation and Development (OECD) countries by 2010 (see Figure 17). Other forecasts suggest that energy use in the non-OECD countries could rise to 60 percent or more of total world consumption by 2020.8 Fossil fuels (oil, natural gas, and coal), which currently account for 87 percent of primary world energy consumption, will comprise the bulk of the growth in energy demand, especially in the developing countries. If current patterns of energy production continue over the next several decades, carbon dioxide emissions will grow roughly in line with energy consumption. Finally, the concentration of available world oil resources in the Middle East will lead to a growing dependence upon that volatile region to supply the world's oil needs. The Administration's policy is to protect U.S. security interests, ensure U.S. economic competitiveness, and create new markets for U.S. exports, while working to limit global environmental damage from international energy development. This is implemented through policies of multilateral cooperation to coordinate energy and environmental goals, as well as policies that support the competitive position of U.S. industries abroad. Coordinating International Energy and Environmental Policy A more integrated global economy and the global aspects of many environmental issues increase the value of international cooperation in policy development and implementation. The Administration actively engages other countries, on a bilateral or multilateral basis where appropriate, to strengthen international energy policies and to counter global environmental threats. The Department of Energy conducts the bulk of these activities, interacting with its cabinet- or ministerial-level counterparts in industrialized and developing countries. Strengthening Energy Policies Here and Abroad The Administration promotes global energy markets by encouraging developing countries to adopt policies that will encourage private investment in energy projects, support the free and efficient operation of energy markets, and remove barriers to investment by U.S. firms in these markets. U.S. participation in bilateral and multilateral forums aids these efforts. The United States also provides advice and assistance to countries in the privatization and opening of their energy sectors to investment by U.S. firms. Examples of actions taken by the Administration to further international energy policy goals include the following: The Gore-Chernomyrdin Commission Energy Committee. This body has encouraged the Russian government to create a commercial and legislative framework to attract international participation in oil and gas resource development, which will strengthen the commercial presence of U.S. energy companies in Russia. Russian oil production has declined from its peak of 11.7 million barrels per day in 1987 to about 6.3 million barrels per day in 1994. The Russian Federation is now considering legislation on oil and gas production sharing agreements and amendments to its underground resources law. By actively promoting the adoption of a stable policy environment that will encourage foreign participation, the Administration is helping the Russians to maximize oil and gas recovery in an efficient and environmentally benign way. Financing. U.S. and multilateral lending agencies have made available loans, grants, and insurance totaling roughly $6 billion to Russia, central Europe, and the newly independent states of the former Soviet Union. The Summit of the Americas. At the Summit of the Americas in December 1994, 34 democratically elected leaders of the Western Hemisphere agreed to adopt sustainable energy policies emphasizing energy efficiency and development of indigenous conventional and renewable energy resources. The summit countries and the private sector will initiate cooperative actions to increase access to reliable, clean, and least-cost energy services through activities and projects that meet economic, social, and environmental requirements within the context of national sustainable development goals and national legal frameworks. Preparing for Global Energy Supply Disruptions A recent Commerce Department study concluded that our country's heavy reliance on imported oil threatens to impair our national security. Policies that effectively deal with potential disruptions in international oil markets can reduce economic and security risks. The Administration's current policy relies on market forces to allocate supply in the event of a disruption and would supplement supply through an early drawdown of the Strategic Petroleum Reserve in large volumes and in coordination with our allies and trading partners. The Clinton Administration recently moved to strengthen U.S. energy security policy by reviewing and reaffirming U.S. policy for responding to oil supply disruptions. The effectiveness of any U.S. response to an oil disruption is greatly enhanced by multilateral coordination with our trading partners. Since its creation in 1974, the International Energy Agency (IEA) has developed effective response mechanisms for limiting economic losses from disruptions in world oil supplies. The United States has worked with the IEA to encourage members to establish emergency oil reserves and to coordinate emergency response measures and information dissemination. The Administration recently succeeded in amending IEA policy for addressing oil disruptions by expressing a preference for market-oriented response measures to replace the outdated international oil-allocation system that has been in place for nearly two decades. A key strategy of the U.S. Government is to prepare for energy emergencies by maintaining the Strategic Petroleum Reserve at levels adequate to reduce macroeconomic impacts and price increases that result from oil supply disruptions. The U.S. Government recognizes the global dimensions of energy supplies and works within the IEA framework to encourage crude oil stockpiling so that global markets are more resistant to supply shocks. The current inventory in the Strategic Petroleum Reserve is 592 million barrels of crude oil, equal to 74 days of net imports. The current drawdown capacity is 3.1 million barrels per day, with a goal of 3.9 million barrels by 1998. The final element of the Administration's program of guarding against disruptions is to provide accurate and timely information to enhance the transparency and functioning of markets-a key to energy and economic stability. To meet this need, the United States works with other nations to monitor and analyze domestic and international energy supply events. The goal is to track potential or emerging situations and to ensure widespread exchange and dissemination of information on market situations to combat price fluctuations caused by rumor. Responding to Global Environmental Challenges In response to the increasingly global nature of environmental degradation, governments are cooperating to develop strategies for addressing international environmental issues. In the last decade, global agreements have been reached that address ozone depletion, climate change, biodiversity, and international shipments of hazardous waste. The trend toward international agreements to protect the environment reflects both the transboundary character of the pollution damage and the need to establish environmental protection measures that treat trade competitors fairly. Given the environmental impacts of energy production and use in countries around the world, energy policy plays a key role in international discussions. Where global environmental risks warrant international solutions, the Administration pursues strong, fair, and balanced approaches in cooperation with other nations. Several principles guide the U.S. approach to addressing international environmental problems. First, policies should be based on sound scientific evidence. Second, responses should be based on international cooperation and include the participation of all countries concerned. Third, environmental protection measures taken under international cooperative arrangements should be cost-effective and use market mechanisms. Finally, technology development programs should emphasize products and services that could address environmental problems in the context of market development and export opportunities for U.S. technologies. Framework Convention on Climate Change. The link between global energy use, greenhouse gas emissions, and potential impacts on the Earth's climate requires effective international solutions. The Administration has led international efforts to make the Framework Convention a strong and relevant institution. The Convention entered into force in March 1994, and the first Conference of the Parties to the Convention took place in Berlin in March 1995. The U.S. Government has been an active participant in developing proposals for the operation of the Convention, which requires that parties periodically review the adequacy of existing commitments. While the Administration believes that new steps need to be taken to fulfill the objective of the Convention-the stabilization of atmospheric concentrations of greenhouse gases at levels that would not dangerously interfere with the natural climate system-the U.S. delegation has made it clear that steps should be taken by all parties to the Convention, involve all greenhouse gases, all sources and sinks, as well as all economic sectors. Toward this end, the United States has led the international effort to establish an international "joint implementation" pilot program. Joint implementation is a mechanism by which parties will be able to share emissions reduction activities. For example, a developed country could invest in emissions reductions in a developing country where reductions are far less expensive than in the developed country and where host countries could gain the advantages of the technology transfer. The Department of Energy and the Environmental Protection Agency are comanagers of the U.S. pilot program called the U.S. Initiative on Joint Implementation. This program made its first-round selection of projects for inclusion in the initiative in February 1995. Cooperation on Joint Implementation. The U.S. Government has signed a number of statements of intent to cooperate on joint implementation projects that would reduce net emissions of greenhouse gases, while attracting private investment for sustainable development. Agreements have been signed with India, Pakistan, Costa Rica, and Chile. Negotiations are under way for a regional statement of intent with the Central American countries. Cooperation on Climate Change Through the U.S. Country Studies Program. The U.S. Government is helping other signatories to the Framework Convention to meet their commitments through the U.S. Country Studies Program. This interagency program supports financial and technical assistance for 55 studies of developing countries and countries in transition that do not have the financial and technical resources to develop their own inventories of greenhouse gases, which is required by the Convention. The countries also are encouraged to look at their vulnerabilities to climate change, possible adaptation measures in sectors such as agriculture and forestry, and mitigation actions that could involve fuel switching and increased use of energy efficiency in all sectors of their economies. Convention on the Law of the Sea. The 1982 United Nations Convention on the Law of the Sea, which entered into force in November 1994, has important implications for U.S. energy needs. The continental shelf provisions of the convention serve U.S. interests both in regard to oil and gas development off our coast as well as the interests of U.S. companies operating abroad. The navigation provisions of the convention-innocent passage in the territorial sea, transit passage in straits used for international navigation, and archipelagic sealane passage in legal archipelagoes-are essential to the transportation of oil and gas resources. The Administration led efforts to reform the deep seabed part of the convention in 1994, correcting the fundamental flaws that resulted in nonsignature by the United States in the early 1980s and opening the way to U.S. accession. The convention and the amending deep seabed mining agreement now await Senate advice and consent to accession and ratification by the United States. Exporting Our Success: U.S. Technology for Sustainable Development One of the most important challenges facing the United States over the next decade will be to compete effectively in international markets for energy and related environmental technologies. Energy industries are a vital component of the Administration's export policies. These industries help the Nation to maintain technological leadership through research, development, and demonstration initiatives, and to compete in international markets that support high-technology, high-wage jobs for market winners. Maintaining the U.S. share of international energy markets will require strong, coordinated, effective support from the Federal Government. Not only will these exports create high-paying jobs in the United States, but the technologies most likely to gain international market shares can also strengthen our own economy and increase our standard of living. In working toward sustainable international energy development, the Administration will encourage continued improvement of alternative energy and energy-efficiency technologies; worldwide use of U.S. energy technologies; broad dissemination of environmental technologies; and strong partnerships with energy officials around the world. This strategy will lead to ever-expanding deployment of these highly competitive, efficient, and environmentally sound U.S. technologies. Promoting Sustainable Global Energy Markets As the economies of the developing world industrialize, higher incomes create demand for modern consumer technologies. Because of both industrialization and consumer demands, energy systems in these countries will need to expand rapidly to keep pace with increased energy needs. The estimated international market potential for U.S. energy technologies is more than $200 billion through 2010 . Concern for environmental issues will also affect the competitiveness of energy technologies. Expanding use of alternative energy technologies such as wind, solar, biomass, and other non-fossil fuel technologies is gradually leading to improvements in the technologies and to cost reductions. The international market for these technologies will gradually expand over the next decade, accelerating cost improvements that will benefit both their manufacturers and their domestic users. The deployment of state-of-the-art energy technologies can provide countries with an important boost to diversify their energy consumption and to use resources more efficiently. These technologies can be particularly helpful to developing countries where energy demand is rising rapidly. The U.S. Government works with developing and newly privatizing nations to promote energy strategies that encourage markets for alternative energy supplies, including natural gas and renewables. The United States is a recognized leader in the manufacture of many state-of-the-art energy technologies. Technology projects developed to meet opportunities overseas can also be used back home, thereby increasing the competitiveness of domestic industries. By partnering with domestic industry and foreign countries, the U.S. Government can demonstrate environmentally sound and clean energy technologies abroad, providing information and trade opportunities to advance export markets for U.S. technologies. These partnerships also enable developing countries to provide affordable energy to their citizens and to reduce the environmental degradation that accompanies many energy production and end-use processes. Competing in International Technology Markets Fierce commercial competition is beginning to dominate both domestic and foreign markets. Combined with strong support for their exporting firms, our traditional competitors are expanding their commitment to research, development, and demonstration. The newly industrializing countries of Asia are gaining market shares in many product lines, and countries such as China and Russia soon could become major players in the international marketplace. President Clinton announced the National Export Strategy in October 1993, which provides a roadmap for expanding U.S. exports to foreign markets. The Trade Promotion Coordinating Committee has identified the energy and environmental industrial sectors as two of the priority sectors of the U.S. economy that could provide the impetus for competitive technologies and increased exports. A followup document, Environmental Technologies Exports: Strategic Framework for U.S. Leadership, provides direction and goals for U.S. action in the area of environmental and clean energy technologies. In addition to identifying major export sectors, the Trade Promotion Coordinating Committee also identified the major markets on which the Administration's trade promotion resources are focused. These major export markets, called the big emerging markets, are the Chinese Economic Area, Indonesia, India, South Korea, Mexico, Argentina, Brazil, South Africa, Turkey, and Poland. For the energy sector, countries with substantial energy resources, such as Russia and Venezuela, were also identified as important markets for U.S. exports. Overseas demonstration projects range from photovoltaic electricity technologies in Brazil, Mexico, and Indonesia to clean coal technologies in Poland to waste remediation and pollution prevention technologies in Mexico, Russia, and Eastern Europe. Supporting U.S. Energy Technology The Administration continues to work with industry to expand export opportunities for U.S. energy technologies. These efforts, which have already helped U.S. companies increase exports, provide a strong foundation for lasting commercial relationships. Presidential Trade Missions for Sustainable Development. Presidential trade missions are coordinated by the Administration and give U.S. companies unparalleled access to energy ministries and private markets. For example, Secretary of Energy Hazel O'Leary led three missions during the past year: to India in July 1994, Pakistan in September 1994, and China in February 1995. These three trade missions resulted in agreements for private-sector energy projects exceeding $10 billion. These missions and followup activities promote commercial transactions that will deliver clean, affordable energy to fuel the economic growth of developing countries with U.S. technology, capital, and expertise. Long-term market success is promoted through major conferences sponsored during the missions that disseminate information about technologies available from the United States and the economic, financial, and regulatory issues that support increased trade and investment. The varied menu of technology and expertise offered will enable these countries to develop more diversified, efficient, and secure energy systems to meet their rapidly growing energy needs. Competitive Export Financing. The financial community must be mobilized in support of U.S. firms bidding on major energy and environmental projects abroad. The Federal Government sponsors project-financing conferences and workshops in key markets, bringing project developers, decisionmakers, U.S. suppliers, and financiers together to discuss financing issues, including risk factors such as the legal and regulatory framework that underpins the projects. The Export-Import Bank, the Overseas Private Investment Corporation, and the Trade Development Administration also provide financing opportunities and insurance for U.S. exports to developing and newly privatizing countries. Opening Markets and Removing Barriers to U.S. Exports and Services. The U.S. Government is actively involved in negotiating and monitoring international trade agreements and compliance that are critical to improving the competitiveness of U.S. energy and related industries. The Uruguay Round of the General Agreement on Tariffs and Trade (GATT) and the North American Free Trade Agreement, as well as the GATT Government Procurement Code, the Subsidies Code, and other specialized negotiations, all affect the ability of U.S. energy and environmental technology companies to enter world markets. Technical understanding of various traded technologies is key to effectively negotiating the removal of trade barriers. Joint U.S.-Foreign Energy-Efficiency Centers. Joint energy-efficiency centers have been established in Russia, Ukraine, Poland, Bulgaria, the Czech Republic, and China. Through these institutions, the United States helps other nations develop efficiency legislation and regulations; advances U.S. technologies and methods through training and demonstrations; promotes joint ventures between U.S. companies and foreign industry; and promotes policy reform and the adoption of integrated resource planning and demand-side management. Cost-Sharing Collaborations for Renewable Energy Systems. Several programs help promote rural electrification in countries where access to the electric grid is neither available nor economically feasible. In Brazil, Mexico, India, China, Pakistan, and Indonesia, the Administration facilitates cost-shared collaborations between U.S. companies and the host-country partners to provide cost-effective renewable energy applications for home and village-sized systems. The U.S. companies provide the technology and products, as well as community training for installation and maintenance of the systems. These pilot projects have attracted the attention of independent power producers and multilateral development banks interested in financing cost-effective renewable energy projects. At present, approximately 10 percent of energy loans to Southeast Asia go to finance renewable energy projects. Return to Table of Contents WT03-B20-32IA005-000051-B018-557http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/titlepg.html 138.80.61.12 19970221152255 text/html 8930HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:53:17 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 8759Last-modified: Tue, 09 Jul 1996 02:25:46 GMT NATIONAL ENERGY POLICY PLAN: Table of Contents SUSTAINABLE ENERGY STRATEGY CLEAN AND SECURE ENERGY FOR A COMPETITIVE ECONOMY July 1995 National Energy Policy Plan Pursuant to Section 801 of the Department of Energy Organization Act TABLE OF CONTENTS Preface Executive Summary Chapter 1: Ensure a Competitive, Clean, and Secure Energy Future Energy Policy and Everyday Life Energy and National Progress Energy Fuels a Competitive Economy Energy Affects the Quality of the Environment Energy Affects National Security Goals of Sustainable Energy Policy Delivering on the Goals: Energy Policy for a Sustainable Future Chapter 2: Energy and U.S. Economic Productivity, Environmental Quality, and National Security U.S. Energy Production and Use Energy and Economic Productivity Energy and the Environment Oil Use, World Markets, and Energy Security Alternative Energy Futures Chapter 3: Increase the Efficiency of Energy Use The Federal Role in Improving Energy Efficiency and Flexibility Enhancing Market Forces Accelerating Technology Development and Adoption Transportation: Improving Efficiency and Introducing Alternatives Promoting Near-Term Efficiency Improvements Developing Markets for Alternative Fuels Reducing the Demand for Travel Developing Technologies for a New Generation of Vehicles Remaining Challenges and Opportunities Buildings: Overcoming the Barriers to Investments in Efficiency Enhancing the Effectiveness of Markets for Efficiency Investments Developing New Technologies To Improve Building Efficiency Cooperating With State and Community Programs That Encourage Building Efficiency Establishing Minimum Efficiency Standards for New Buildings and Appliances Supporting Efficiency Investments Remaining Challenges and Opportunities Industry: Using Energy-Efficient Technologies To Improve Productivity and Reduce Waste Partnering With Industry in Developing New Technologies Accelerating Deployment of Energy-Efficiency Technologies Remaining Challenges and Opportunities Chapter 4: Develop a Balanced Domestic Energy Resource Portfolio The Federal Role in Helping To Develop a Domestic Energy Resource Portfolio Oil: Enhancing Domestic Production Capability Partnering With Industry To Develop Advanced Supply Technologies Providing Appropriate Incentives To Increase Domestic Exploration and Production Reforming Regulations To Reduce Uncertainties and Costs Removing Artificial Market Barriers Supporting Exploration and Production of New Supplies in Environmentally Sensitive Ways Reforming the Regulatory Structure Governing Refining Activities Natural Gas: Increasing Efficient Utilization Partnering With Industry To Develop Advanced Natural Gas Technologies Providing Appropriate Incentives To Increase Exploration and Production Stimulating Markets for Natural Gas Use Reforming Regulations To Reduce Costs and Increase Efficiency Electricity: Promoting Competition in the National Interest Restructuring the Industry To Link Competition with National Goals Examining the Statutory Framework Renewable Energy: Increasing Long-Term Investments Partnering With Industry To Develop Lower Cost Renewable Resource Technologies Stimulating Markets for Renewable Resources Coal: Reducing Environmental Impacts Supporting Clean Coal Technology Development Improving Environmental Outcomes From Mining Operations Nuclear Energy: Increasing Safety and Preserving Options Working With Industry To Maintain Safety Levels and Preserve Future Options Resolving the Problem of Spent Fuel Disposal Chapter 5: Invest in Science and Technology Advances The Federal Role in Supporting Science and Technology Basic Science and Energy Research: Building a Foundation for a Sustainable Energy Future Technology Research and Development: Linking Science to Sustainable Energy Goals Enhancing and Leveraging National Assets The National Laboratories The University Research Infrastructure Maintaining Science and Technology Excellence Access and Communication: Capitalizing on Science Investments Chapter 6: Reinvent Environmental Protection Environmental Protection: Challenges and Opportunities for Reform A New Approach to Environmental and Energy Policy Reinventing Environmental Regulation Encouraging Pollution Prevention in the Energy Sector Fostering Technology Development Chapter 7: Engage the International Market Coordinating International Energy and Environmental Policy Strengthening Energy Policies Here and Abroad Preparing for Global Energy Supply Disruptions Responding to Global Environmental Challenges Exporting Our Success: U.S. Technology for Sustainable Development Promoting Sustainable Global Energy Markets Competing in International Technology Markets Supporting U.S. Energy Technology Notes OBTAINING COPIES OF THE NATIONAL ENERGY POLICY PLAN To order original copies of the National Energy Policy Plan, please contact; By Mail: Willie Bruce Printing Operation Branch, HR-841 U.S. Department of Energy 1000 Independence, Ave., S.W. Washington, D.C. 20585 By Telephone or Fax: Telephone: (202) 586-9642 Facsimile: (202) 586-0753 WT03-B20-33IA006-000055-B006-136http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/preface.html 138.80.61.12 19970221175017 text/html 4866HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:20:16 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 4695Last-modified: Tue, 09 Jul 1996 02:25:45 GMT NEPP Preface Sustainable Energy Strategy: Clean and Secure Energy for a Competitive Economy Energy is a primary mover of industrial economies, high living standards, environmental challenges, and geopolitical struggles. While clearly important in crisis situations, energy policy remains a national priority even when no immediate crisis focuses national attention. As the Nation looks toward the 21st century, a new era is taking shape an era in which dynamic energy markets and advanced energy technologies have the potential to alleviate many of the economic, environmental, and national security concerns that have driven energy issues over the past decades. We are unlikely to realize the full potential of this new era, however, without a national energy policy that continually guides our markets and technologies toward the goals of affordable, clean, reliable, and secure energy. National policy that maximizes energy productivity, prevents pollution, and helps shield our Nation from global energy risks can improve our standard of living today and enhance our quality of life tomorrow. This document presents the Clinton Administration's national energy policy. The policy is built upon the bipartisan consensus that has emerged in the U.S. Congress, most recently embodied in the Energy Policy Act of 1992. The policy is implemented by the Department of Energy and other Federal agencies, and it actively engages State and local government and the private sector, where much of the dynamism of our society resides. In the United States, the private sector determines most features of energy production and use. This is appropriate, given the poor record of direct government intervention to set prices or to allocate resources in energy markets. The Administration's energy policy emphasizes and reinforces the dominant role of the private sector and supports expanding the role of markets in several key areas. However, there remains an important role for the Government in preventing abuse of the environment, providing some national insurance against import disruption, and conducting research and development efforts in partnership with the private sector. The Administration pursues a rational and focused energy policy informed by past experiences both positive and negative. By prioritizing Federal support, we bolster research and development into new energy technologies that have the potential to transform our economy and restore the environment, while cutting back funding for mature technologies and collaborating with industry to avoid "picking winners." We support privatization and extending competition in energy markets where government no longer needs to play a prominent role, but we stand behind our energy producers and technology vendors as they meet fierce competition abroad. We are reinventing Federal environmental regulation but are firmly opposing efforts to roll back a generation of progress on protecting the environment. The Administration's energy policy reflects a common sense approach that helps both energy consumers and producers, gives a high return to the taxpayer, respects our environment, and looks toward the future. In developing its energy policy, the Administration conducted numerous public meetings across the country to listen to the concerns and ideas of individuals and organizations most involved with energy issues, as well as citizens concerned about the energy future. A separate volume summarizes public input. In addition to public meetings, the Departments of Energy, Commerce, State, and Agriculture and the Environmental Protection Agency have actively participated in the deliberations of the President's Council on Sustainable Development. This advisory body consists of Cabinet officials, business leaders, and the heads of environmental, labor, and community groups. Although the Council will submit final recommendations to the President later this year, the Administration's energy policy reflects much of the Council's efforts over the past 2 years. The Clinton Administration's energy policy responds to the concerns of today and the challenges of tomorrow. It recognizes the interrelationship between energy, the economy, and the environment and the importance of the international marketplace to our economic future. The Administration will continue to work with Congress and the people to attain our economic, environmental, and national security goals. Return to Table of Contents WT03-B20-34IA006-000055-B006-200http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/execsum.html 138.80.61.12 19970221175049 text/html 6450HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:20:48 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 6279Last-modified: Tue, 09 Jul 1996 02:25:45 GMT NEPP Executive Summary Executive Summary Our Nation's progress depends on our ability to use our energy resources in ways that strengthen our competitive economy, protect our environment, and keep our Nation secure. The Clinton Administration has embarked upon a sustainable energy policy that improves our standard of living today and expands our Nation's opportunities for tomorrow. Although Americans tend to focus on energy issues only in periods of energy crises, as a Nation we should be concerned about energy at all times. Energy's continuing importance is grounded in three central facts: Energy Fuels a Competitive Economy. Our standard of living and industrial productivity require reliable and competitively priced energy supplies. U.S. businesses and consumers directly spend more than $500 billion per year on energy. Energy Affects the Quality of the Environment. The United States has made significant strides in reducing the environmental impacts of energy production and use. These impacts include air and water pollution; nuclear, toxic, and other waste disposal; disruption of wilderness and natural ecosystems; and greenhouse gas emissions. But to make additional progress in reducing environmental risks, we need new, more cost-effective approaches to address the problems we have already tackled, along with innovative methods to counter emerging risks. Energy Affects Our National Security. Disruptions in global oil markets and energy price shocks have been followed by three recessions in the past 20 years. Energy policy can help reduce the economic and national security risks of relying on oil produced in unstable regions of the world. The concept of "sustainable development"-development that meets the needs of today without compromising the ability of future generations to meet their own needs-guides the formulation of the Administration's energy policy and motivates three strategic goals: Maximize energy productivity to strengthen our economy and improve living standards. Prevent pollution to reduce the adverse environmental impacts associated with energy production, delivery, and use. Keep America secure by reducing our vulnerability to global energy market shocks. Sustainable energy policy pursues all three goals, because actions that further one goal at the expense of the others can increase the economic, environmental, or security risks borne by future generations. When markets do not maximize energy productivity, adequately protect the environment, or reflect national security considerations, Administration policy aligns market forces and technology development with the goals of sustainable development. These efforts yield immediate and significant benefits for consumers and firms and lay the foundation for sustained progress. The Administration pursues a wide range of programs to attain the goals of national energy policy. The following are the strategic components of sustainable energy policy: Increase the Efficiency of Energy Use (Chapter 3). A fundamental tenet of sustainable development is to use resources efficiently. The Administration pursues market and technology programs that help our citizens use energy more efficiently and maximize energy productivity and value. These programs help improve energy efficiency in all critical market segments-commercial and residential buildings, transportation, industry, utilities, and government. Develop a Balanced Domestic Energy Resource Portfolio (Chapter 4). The Administration is committed to enhancing the competitiveness of domestic oil producers, expanding the role of clean, efficient, and domestically produced natural gas, encouraging the continued development of renewable energy resources, reducing the environmental impacts of coal, and maintaining the safe contribution of nuclear energy. Invest in Science and Technology Advances (Chapter 5). Technological progress enables us to expand our resources, improve efficiency, and reduce the environmental impacts of producing, transporting, and consuming energy. The Administration's science and technology policy complements private sector research and development efforts with public investments in areas where the long-term economic, environmental, or social objectives are not adequately or immediately rewarded in the marketplace. Reinvent Environmental Protection (Chapter 6). Americans solidly support a cleaner and healthier environment, and as a Nation, we have enjoyed substantial progress in protecting our environment and decreasing public health risks over the past two decades. By combining market forces and technology, the Administration has undertaken many initiatives to increase energy efficiency, prevent pollution, reduce costs, enhance flexibility, and strengthen accountability. Taken as a whole, these initiatives are the first steps in reinventing environmental protection to achieve a more sustainable system. Engage the International Market (Chapter 7). The development of international markets for energy and technology both poses risks and presents opportunities for our Nation. Through international technology and development programs, participation in multilateral activities, support for privatization and competitive markets abroad, and a strong commitment to U.S. companies that export the energy technologies that will enable sustainable development, the Administration can expand high-wage jobs in the United States and help other nations develop in ways that will ease the burden of energy development on the global environment. Return to Table of Contents WT03-B20-35IA006-000055-B006-226http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/ch1.html 138.80.61.12 19970221175102 text/html 21604HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:21:21 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 21432Last-modified: Tue, 09 Jul 1996 02:25:44 GMT NEPP Chapter 1 Chapter 1: Ensure a Competitive, Clean, and Secure Energy Future Our Nation's progress depends on our ability to use energy resources in ways that help the economy grow, protect the environment, and keep our Nation secure. A sustainable energy policy improves our standard of living today and expands future opportunities by maximizing energy productivity, preventing pollution, and reducing our vulnerability to global energy disruptions. Each day, Americans depend on the benefits of energy, usually without considering the role that energy plays in our quality of life. Absent an energy crisis-lines at the gas pump or sudden blackouts-few Americans think about how national energy policy helps to improve our economy, our environment, or our national security. But energy is fundamental to progress, and energy policy is crucial for three main reasons: energy fuels a competitive economy, energy utilization affects the environment, and energy use may affect our national security. The need for a rational and forward-looking energy policy does not disappear when our attention to energy wanes. Indeed, a time of relative stability offers a significant opportunity to build a sustainable national energy policy-one that recognizes the links between the economy and the environment, considers the future impacts of today's decisions, and promotes investment to enhance future opportunities. As our experience with several energy crises and our ongoing concerns for the environment attest, the future does not always take care of itself. Smart policy decisions can make a difference in how well we realize our national goals. Since January 1993, the Clinton Administration has been developing and implementing a sustainable energy policy-one that delivers real benefits to our citizens today and that expands opportunities for future generations. The Administration's policy is informed by the successes and the failures of 20 years of Federal efforts to respond to immediate energy crises. But in contrast to past approaches, the Administration has adopted a longer run perspective to fashion policy that addresses the economic, environmental, and security risks that our Nation will continue to face in the next century. Energy Policy and Everyday Life National energy policy affects each American every day. Consider four examples from everyday life: turning on the lights at home, filling up the tank at the gas station, adjusting a thermostat in the office, and starting a motor on the assembly line. How does energy policy affect us in these ordinary activities? Although residential customers turning on the lights need not know whether the electricity is produced from coal, nuclear power, natural gas, or windpower, the rates they pay and the quality of the air they breathe reflect policies that influence the electric utility's choice of fuels, generating technologies, and investments in pollution controls. Sometime in the future, their bills could change as a result of proposed market reforms that could allow customers to choose their electricity supplier in the way that they choose long-distance telephone service-but analysts disagree about whether residential electric rates will rise as a result or whether competition will help keep prices down. A motorist filling the tank may remember waiting in gas lines during the 1970s and may recall the shock of having to pay sharply rising prices for gasoline. But many people do not realize that in real terms (compared with the prices of other goods) gasoline prices now are at their lowest level since World War II, due in part to government actions to decontrol oil prices that began in the late 1970s. The gasoline the motorist is pumping was likely derived from imported crude oil, given that imports now constitute roughly half of domestic oil consumption.1,2 A decade of low world oil prices has benefited consumers but has hurt domestic oil producers and halved employment in this industry. At the same time, government regulation has spawned impressive technological advances: to drive a mile, the average motorist now uses about 60 percent of the gasoline he or she used 20 years ago, and that mile of travel now emits a minute fraction of the tailpipe emissions produced then. However, increased driving has kept gasoline consumption-and oil imports-steadily growing and has limited progress in reducing overall air pollution. An office worker probably is unaware that many commercial building owners have invested heavily in energy-efficient heating, air-conditioning, and lighting systems that have improved working conditions while drastically lowering energy bills. Many of the new technologies-such as more thermally efficient windows-resulted from federally funded research and development efforts. Much of this investment has occurred because some regulated utilities were allowed to make a profit by helping building owners reduce energy consumption, often aided by government programs that helped owners and tenants gauge the economic benefits of energy-efficient investments. Finally, the assembly line worker might not know that industrial motors use more than 20 percent of the electricity generated in the United States, even though current electric motors are vastly more efficient than those used just a decade ago. The company might be considering entering into a partnership with the Department of Energy to install more efficient equipment, both to save money and to reduce greenhouse gas emissions that contribute to global warming. Maybe the firm also is considering generating its own electricity with process waste heat-but has shelved the plan until new rules of the market grow out of regulatory reform efforts. Or perhaps the company is examining new pollution prevention ideas, hoping to save money on environmental compliance costs if environmental rules grant it more flexibility in choosing the means of reaching pollution reduction goals. Energy and National Progress The preceding examples illustrate the intrinsic importance of energy in everyday life and, when multiplied to the scale of the entire economy, how energy policy can affect our national well-being. They also reveal some of the dynamic changes that are occurring below the surface in many energy markets, some of which may accelerate in the coming years. And they vividly illuminate the three basic reasons why Americans need to be concerned about energy: our economy depends on energy, energy production and use can damage the environment, and current patterns of energy use may affect national security. Energy Fuels a Competitive Economy Our standard of living and industrial productivity require reliable and competitively priced energy. In 1993, American businesses and consumers directly spent $505 billion per year on energy, or about 8 percent of the gross domestic product. Industrial facilities purchased $121 billion worth of energy every year, while the average family spent more than $2,000 per year on utility, transportation, and other fuels. At $56 billion per year, energy imports accounted for almost 10 percent of total product imports. On the other side of the ledger, U.S. firms export billions of dollars worth of energy technology, sustaining thousands of high-paying domestic jobs. Several aspects of our energy use pose risks to our economy. Some energy markets do not perform efficiently-leading consumers and firms to spend too much money and waste energy resources. Low world oil prices have led not only to a surge in oil imports that could make some sectors vulnerable to oil price increases, but also to a major decline in the domestic oil industry. Low energy prices also have blunted the incentive to conduct long-term research to develop new, more efficient technologies that will improve the economics of energy use in the future. Energy Affects the Quality of the Environment The United States has made significant strides in reducing the environmental impact of energy use, relying on a regulatory system that focuses on specific energy technologies such as powerplants and automobiles. For example, although the use of fossil fuels increased 13 percent from 1975 to 1993, emissions of sulfur dioxide decreased by 22 percent and emissions of lead decreased by 97 percent. The Clean Air Act Amendments of 1990 will further reduce many air pollutants from fossil fuel use, such as nitrogen oxides and sulfur dioxide, which contribute to acid rain. Despite the progress made in reducing some energy-related emissions, other pollutants have remained difficult and expensive to address under the existing regulatory structure and with current technology. Many of our large urban areas continue to experience regional haze and smog as a result of nitrogen oxides and volatile organic compounds emitted from energy use. Increasing energy use threatens to erode the progress made under the Nation's current environmental control system, while new environmental risks are emerging, such as the potential for global energy use to contribute to climate change. Increased atmospheric concentrations of greenhouse gases due to rising carbon dioxide and methane emissions from energy use are likely to alter the Earth's climate system, although scientists do not agree on the timing and nature of potential climate changes or on the scope and severity of the problems associated with a changing climate system. To make additional progress on reducing environmental risks, the Nation needs new, more cost-effective approaches to address the problems we have already tackled, as well as innovative methods to counter emerging risks. We must develop new technologies for preventing pollution and must harness market forces more effectively in an effort to improve the U.S. and global environment. Energy Affects National Security Because the United States relies on oil for more than 40 percent of its energy needs, disruptions in world oil markets can have serious economic and national security implications. Energy policy has reduced, but not eliminated, the risks from relying heavily on global oil supplies concentrated in potentially unstable regions of the world. In 1994, about 50 percent of the oil consumed in the United States was imported, a figure that could rise to more than 60 percent by 2010 according to the Energy Information Administration. By 2010, two-thirds of the oil traded on the international market will originate in the Persian Gulf, an area that has experienced conflict in recent years. The experience of the past 20 years suggests that the international oil market is prone to disruptions and price shocks and that macroeconomic policies may aggravate the adverse effects on the U.S. economy. Even though domestic and international energy markets operate much more efficiently than was the case 20 years ago, the United States still faces economic and security risks from current patterns of oil consumption and production. Goals of Sustainable Energy Policy Although everyone has a stake in the energy future, energy policy tends to receive national attention only in crisis situations. But policies fashioned in response to a crisis tend to focus narrowly on the immediate predicament, while failing to recognize and deal effectively with the underlying cause of the crisis itself. Certainly energy policy should aim to avoid crises. But energy policy should also address risks that threaten to undercut progress over the long term. Because energy is so deeply intertwined with everyday life, energy policy is a tool that can help our economy grow, improve our environment, and enhance national security today, while protecting us from future threats. This is the basis for a sustainable energy policy. Sustainable development has been defined as development that meets the needs of today without compromising the ability of future generations to meet their own needs. In some cases, competitive markets meet the criteria of sustainability, while in other cases markets fall short. Where energy markets fall short of sustainability-where they fail to adequately protect the environment, where they do not generate sufficient progress in science and technology, where they do not recognize fully the economic and security risks of certain elements in the overall energy picture-energy policy can help ensure sustainable economic progress. Sustainable development policy also reflects a long-term perspective that helps balance the short-term orientation of private markets. We currently enjoy the benefits of actions taken decades ago-investments in science, technology development, environmental protection, and national security-and we have an obligation to provide future generations with a foundation to realize greater progress. To pursue the overall objective of sustainable development, the Clinton Administration's energy policy has three strategic goals: Maximize Energy Productivity to strengthen our economy and improve living standards. Getting more out of the energy we use will keep costs of energy services such as light, heat, and mobility at levels that our citizens can afford and at which our businesses can thrive. And as we develop new technologies to increase energy productivity, we can seize global market opportunities in technology exports, creating high-paying jobs and improving the Nation's balance of trade. Prevent Pollution to reduce the adverse environmental impacts associated with energy production, delivery, and use. Americans cherish a clean environment, and we have made tremendous strides in improving the quality of our land, air, and water. However, as U.S. and global energy needs grow, we must continue to press for more cost-effective and less polluting ways to produce and use energy and explore new approaches to reduce environmental risks, keep our economy strong, and maintain our global leadership in protecting the environment. Keep America Secure by reducing our exposure to events beyond our control. The United States depends on reliable and competitively priced energy supplies to fuel stable economic growth. However, our economy relies on oil for 40 percent of our energy needs, which are being met increasingly by potentially unstable sources of world oil supply. While existing energy policy and improved macroeconomic policies can help reduce the economic harm from supply disruptions, our economy continues to be vulnerable to oil price shocks. Sustainable energy policy pursues all three goals, because actions that further one goal at the expense of the others can increase the economic, environmental, or security risks borne by future generations. These goals have guided the Administration in developing a sustainable energy policy. Delivering on the Goals: Energy Policy for a Sustainable Future Although market limitations warrant policy action in some cases, sustainable energy policy should not displace market forces or impair well-functioning markets. In fact, market forces can be powerful tools to promote sustainable development. Where markets are not maximizing energy productivity, adequately protecting the environment, or reflecting national security considerations, Administration policy tries to align market forces with the goals of sustainable development. These efforts yield immediate and significant benefits for consumers and firms and lay the foundation for sustained progress. The Administration's long-term energy strategy focuses on advancing technologies to promote a transition toward more sustainable energy systems. Over time, the state of technology exerts enormous leverage on how energy affects the Nation's economy, environment, and security. In the coming decades, our quality of life will depend significantly on the scientific and technological advances supported by today's energy policies. Government can play a positive role in improving markets and stimulating technological advances in a variety of energy sectors. Specific policies must be tailored to the unique circumstances of the markets-whether they be the markets that supply energy, the economic sectors that use energy, or the markets that provide the technologies of energy production and use. Policies must also reflect lessons learned from previous failures, whether they were counterproductive regulatory approaches or ill-advised technology development projects. Finally, policies must take advantage of the unique capabilities of State and local governments, communities, and the private sector. Although national energy policy requires an active Federal role, many of the programs and activities are conducted at the State and local level or with private firms that generate immediate and local benefits. The Administration has initiated a wide range of programs to attain the goals of sustainable national energy policy. These programs, described in subsequent chapters, reflect five key strategies: Increase the Efficiency of Energy Use (Chapter 3). Efficient resource use is a fundamental tenet of sustainable development. The economic, environmental, and national security implications of inefficient energy use motivate market and technology programs that help energy consumers find the most efficient ways to use energy and to maximize energy productivity and value. The Administration is committed to improving energy efficiency in all critical market segments-commercial and residential buildings, transportation, industry, utilities, and government. Develop a Balanced Domestic Energy Resource Portfolio (Chapter 4). Markets that provide critical energy resources for the Nation must also attain our national economic, environmental, and security goals. The Administration is committed to enhancing the competitiveness of domestic oil producers, expanding the role of clean, efficient, and domestically produced natural gas, encouraging the continued development of renewable energy resources, reducing the environmental impacts of coal, and maintaining the safe contribution of nuclear energy. Invest in Science and Technology Advances (Chapter 5). Technological progress enables us to improve the efficiency and reduce the environmental impacts of producing, transporting, and consuming energy. Markets tend to underprovide the resources for science and technology development in sectors where immediate returns are not evident or where benefits are not captured fully by those who invest in research or development. The Administration's science and technology policy complements the private sector with public investments in areas where the long-term economic, environmental, or social objectives are not adequately or immediately rewarded in the marketplace. Reinvent Environmental Protection (Chapter 6). Americans solidly support a cleaner and healthier environment, and as a Nation, we have enjoyed substantial progress in protecting our environment and decreasing public health risks over the past two decades. However, the current system of environmental protection can be reformed to increase benefits and decrease costs-while still reflecting the fundamental environmental values of our citizens. By combining market forces and technology, the Administration has embarked on many initiatives to increase energy efficiency, prevent pollution, reduce costs, enhance flexibility, and strengthen accountability. Taken as a whole, these initiatives are the first steps in transforming environmental protection into a more sustainable system. Engage the International Market (Chapter 7). While the development of international markets for energy and technology poses risks for our Nation, it also presents tremendous opportunities. Through international technology and development programs, participation in multilateral activities, and strong commitment to U.S. companies that export the energy technologies that enable sustainable development, the Administration is working to expand high-wage jobs in the United States and help other nations develop in ways that will ease the burden of energy development on the global environment. Chapter 2 presents a brief overview of our national and global energy picture. This overview highlights the trends that shape our energy use, as well as the challenges and opportunities that define the scope and intent of national energy policy. Return to Table of Contents WT03-B20-36IA006-000055-B006-265http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/ch2.html 138.80.61.12 19970221175133 text/html 16768HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:21:36 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 16596Last-modified: Tue, 09 Jul 1996 02:25:44 GMT NEPP Chapter 2 Chapter 2: Energy and U.S. Economic Productivity, Environmental Quality, and National Security (Note: Figures referenced in this chapter have not been included at this time.) Energy policy is motivated by the economic, environmental, and national security implications of energy production and use. An analysis of recent trends indicates that our Nation has made substantial progress in several key areas, while in other areas, energy-related risks have grown. How well is our energy system performing in terms of meeting the goals of economic productivity, environmental quality, and national security? Recent data show that the Nation has made significant progress in several areas, but some trends continue to work against the attainment of national goals. This chapter describes the main forces that shape our energy picture, discusses economic, environmental, and national security implications of the current path of energy development, and explains how risks and opportunities define the scope and intent of the Administration's energy policy. U.S. Energy Production and Use An overall description of the U.S. energy system includes all of the purchases of fuel and power by consumers, business, and industry (that is, energy demand quantified in dollars or energy units) and all of the sales of fuel and power by market providers (that is, energy supply quantified in dollars or energy units). By definition, demand equals supply in both dollar and energy terms. Energy demand depends on the energy service requirements of consumers and firms and on the technologies they use to turn fuel and power into heat, light, mobility, and other services. Energy supply is determined by the natural resources (renewable or nonrenewable) that provide energy and the technologies used to extract, convert, and distribute energy products to end-users. As shown in Figure 1, total U.S. energy consumption increased more than 25 percent in the past two decades, with expanded use of electricity in residential and commercial buildings contributing most to this growth. The energy sources that fueled the growing demand for electricity were mostly nuclear power and coal, as shown in Figure 2. Oil imports also increased more than one-third to meet the increasing need for fuel in the transportation sector and to offset declining domestic production. The overall growth in energy demand, however, was outstripped by the 65-percent growth of the U.S. economy over the same period. Energy and Economic Productivity Reliable and competitively priced energy supply has played a critical role in supporting U.S. economic growth and the high standard of living now enjoyed by most Americans. In 1993, the United States produced and imported raw energy resources worth roughly $159 billion-primary energy that was mostly in the form of fossil fuel extracted from the earth (see Figure 3). Energy producers converted these resources to end-use energy worth $505 billion, split between $307 billion of solid, liquid, or gaseous fuels and $198 billion worth of electric power. Domestic energy production and distribution represent about 8 percent of total gross domestic product (GDP). In addition to the $505 billion spent directly on fuels and power in 1993, energy users spent hundreds of billions of dollars annually on equipment, vehicles, buildings, and other energy-using goods. Energy is also a major component of our international balance of trade. In 1994, net U.S. oil imports of $46 billion and net imports of natural gas worth $4.5 billion represented 7.5 percent of total domestic imports. The impact on the balance of trade was partly offset by the net export of $2.5 billion worth of coal and billions of dollars worth of exports of energy technology and related products. To put these numbers in perspective, the trade deficit in 1994 was $151 billion. Energy supply markets and demand markets have changed considerably over the past 25 years. On the supply side, greater competition has led to expanded production of coal, natural gas, and renewable energy under conditions of stable or even falling energy prices. On the demand side, as technology has become more efficient and the U.S. economy has shifted away from energy-intensive industry, the amount of energy that we use to produce a dollar's worth of GDP-the energy intensity of the economy-has declined, as it has in most other industrialized economies. This trend of U.S. GDP growth outpacing energy use is expected to continue for at least the next 15 years (see Figure 4). The increased demand for energy services has been partially offset by more efficient use of energy in all three major end-use sectors. The net result of this mix of trends has been relatively stable or slowly growing energy demand in most sectors. In 1992, the United States accounted for nearly one-quarter of world energy consumption; however, its share of world energy demand has been declining. In the next several decades, the United States is likely to face increasing competition for the world's energy resources, particularly from the rapidly expanding economies of East Asia. The Energy Information Administration's International Energy Outlook 1995 forecasts that, by 2010, energy use in the Organization for Economic Cooperation and Development (OECD) countries will be about 26 percent above 1992 levels, while the non-OECD countries will increase their energy use by almost 48 percent; other projections indicate even more rapid growth in demand in developing countries. Increased global energy demand could lead to higher real energy prices in the United States as a result of our trade in energy, especially oil. Compared with other developed countries, the United States and Canada remain the most energy-intensive, in part because of dispersed geographic development (increasing transportation distances) and more severe or varied climates (requiring both heating and air-conditioning in many regions). These indigenous factors are reinforced by relatively low U.S. energy prices that reflect natural abundance, a mature energy infrastructure, and low energy taxes. Energy prices in the United States are considerably lower than those of most other industrialized countries, which has helped energy-intensive industries but has muted the incentives for greater reductions in our economy's energy intensity. For example, as shown in Figure 5, gasoline prices in the United States are lower than in other industrialized nations, contributing to high rates of motor-vehicle use. Energy and the Environment Energy transformation and consumption are the primary sources of most harmful air pollutants and greenhouse gases. Although considerable progress has been made over the past two decades in limiting or reducing air pollutants (see Figure 6), significant air-quality problems persist in some areas. Because our environmental regulatory system tends to focus on individual technologies, we have made more progress reducing the pollution produced per unit of energy use (or per unit of energy service delivered) compared with the slower pace of reducing overall levels of pollution (see Figure 7). The global climate changes likely to result from increasing concentrations of greenhouse gases pose another potentially serious environmental problem related to the production and use of energy. Unlike the case with several other energy-related air pollutants, emissions of carbon dioxide (the principal greenhouse gas) from energy use have been generally rising over the past 10 years and are expected to continue to rise (see Figure 8).3 Although many uncertainties remain regarding the magnitude, timing, and regional effects of such global climate changes, there is mounting evidence that climate changes could begin to occur early in the next century and that, if current trends continue, these changes could ultimately have significant harmful effects on economic growth, human health, and the stability of vital ecosystems. Energy production and use also have other important impacts on the environment, such as energy-related water pollution, including oil spills; nuclear, toxic, and other waste disposal problems; and disruption of sensitive land and natural ecosystems. Existing environmental laws and regulations help reduce the adverse effects of many of these energy-related environmental impacts. However, some pollutants are not controlled, and progress toward a cleaner environment has been accompanied by increasing costs. Because these high costs could limit further progress, the Administration is committed to the development of new technologies and new approaches to environmental regulation that will help minimize the costs of pollution reductions. In particular, the Administration has initiated a shift toward more performance-based regulation that can achieve environmental goals at a lower cost by giving industries more flexibility in selecting compliance strategies. Oil Use, World Markets, and Energy Security Petroleum has provided the largest share of energy for the United States since 1950, although its share of total primary energy consumption has declined from a peak of 49 percent in 1977 to about 40 percent in 1994. Since the United States became a net importer in 1948, U.S. oil imports rose to nearly 50 percent of domestic requirements in 1977, fell to roughly 30 percent of domestic needs in 1985, and have risen steadily in the past decade to once again reach the 50 percent level (see Figure 9). Meanwhile, U.S. domestic oil production has been declining since 1970. Today, proved U.S. oil reserves represent only about 2 percent of the world total, while the United States consumes more than one-fourth of world oil production. As shown in Figure 10, almost two-thirds of world oil reserves are located in the Middle East (nearly 20 percent of world oil reserves are located in Iran and Iraq alone), and another 12 percent are located in Africa and the former Soviet Union. These regions recently have experienced considerable social and political turmoil. According to the Energy Information Administration, the Persian Gulf will become the dominant source for oil traded on the world market, accounting for two-thirds of global oil trade within a decade (see Figure 11). This concentration could increase the sensitivity of world oil prices to the political and social situation in the Middle East, raising the prospects for future oil market disruptions and price shocks. The last three sharp increases in the world oil price (1974, 1979, and 1990) were each followed by periods of negative economic growth (see Figure 12). Beyond the direct economic impact of consumers and producers paying a higher price for an important commodity, the resulting increases in the overall price level and perceived risk of inflation have challenged monetary and fiscal policy authorities to formulate appropriate responses to world oil price increases. Shortcomings in previous economic policy reactions have amplified the negative impacts of oil price increases and helped to trigger recessions. Because fiscal and monetary policies may not respond appropriately to future sharp increases in world oil prices, sudden and pronounced increases in oil prices could contribute to recession in the United States. Is the United States more or less secure compared to the situation in the 1970s? Greater diversification of import sources, the existence of the Strategic Petroleum Reserve, a more efficient global market, the emergence of futures markets, and the removal of harmful price and allocation controls suggest that the United States is less vulnerable to the economic damages of supply disruptions than was the case 20 years ago. However, other indicators-the increasing concentration of global oil from potentially unstable regions, domestic consumption patterns, and production trends-imply that unstable global energy markets may still compromise our economic and national security goals. Alternative Energy Futures The trends that are shaping the Nation's energy future suggest that we face substantial challenges in meeting our economic, environmental, and national security goals. These trends also indicate that the Nation has found ways to improve energy productivity, prevent pollution, and enhance national security. The Administration's energy policy is not predicated on a single projection of our energy future. Instead, it is based on an appraisal of risks and opportunities evident in current patterns of energy development, and it focuses on areas where the opportunities to reduce risks are greatest. The major risks that threaten the attainment of our national goals include rapid growth in global energy demand, unstable world oil supplies, growing recognition of international environmental threats, and a slowdown in investments to develop and deploy new energy technologies. These trends could lead to the following scenarios: Growth in U.S. and world energy demand could strain the capacity of energy suppliers to expand production, resulting in sustained energy price increases. Midrange projections of growth in world energy demand generally assume that economic growth continues at about 2 percent annually in industrialized countries, with nonindustrialized countries growing about 3 to 4 percent annually. But growth during 1993 and 1994 exceeded these rates in most parts of the world. If such growth persists, world energy supplies could become tight within the next decade. International conflict or social turmoil in unstable oil-producing regions could disrupt global oil markets and rapidly increase oil prices. Continuing political and social upheaval in the Middle East, Central Asia, and Africa could lead to significant and prolonged disruptions in the ability or willingness of the countries in these regions to supply oil to the international market. Resulting high oil prices, combined with inadequate monetary and fiscal policy responses in industrialized nations, could impair U.S. and global economic growth. Clear evidence of significant global climate changes or other energy-related environmental problems could precipitate widespread public demand for more stringent measures to reduce greenhouse gas emissions or other environmental risks from energy production and use. Many scientists believe that stronger evidence could emerge in the next decade or two indicating that human-induced climate changes would result in large adverse impacts. Although it is difficult to forecast how the international community would respond, nations that are less dependent on carbon-intensive fuels or that have developed and begun to deploy the technologies needed to reduce such dependence are likely to have an advantage. Reduced investments in developing and deploying advanced energy technologies could place the United States at a competitive disadvantage in global energy technology markets. Such a trend could result from the relatively low overall rate of private savings and investment in the United States or from less research and development (R&D) investment in the energy sector. These risks are evident in current trends, but are not forecasts of future events. Plausible scenarios of energy futures bracket a range of possibilities. Scenarios can be envisioned in which energy risks threaten our economy, environment, and national security, while equally plausible scenarios emphasize the positive trends that could reduce risks over time. The four scenarios describe the dominant risks evident in current trends, which provide the fundamental motivations for Administration energy policy. By focusing national energy policy on the most prevalent risks, the Administration's energy policy enhances the prospects for a clean, competitive, affordable, and secure energy future. Return to Table of Contents WT03-B20-37IA006-000055-B006-314http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/ch3a.html 138.80.61.12 19970221175202 text/html 22237HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:22:09 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 22065Last-modified: Tue, 09 Jul 1996 02:25:45 GMT NEPP Chapter 3 (part 1 of 3) Chapter 3: Increase the Efficiency of Energy Use (Note: Figures referenced in this chapter have not been included at this time.) Increased energy efficiency has provided the Nation with significant economic, environmental, andsecurity benefits over the past 20 years. To make further progress toward a sustainable energyfuture, Administration policy encourages investments in energy efficiency and fuel flexibility inkey economic sectors. By focusing on market barriers that inhibit economic investments inefficient technologies and practices, these programs help market forces continually improve theefficiency of our homes, our transportation systems, our offices, and our factories. In the past 20 years, the U.S. economy has grown by almost 65 percent. But the energy resourcethat has satisfied the lion's share of our growing need for energy services was neither pumped normined from the earth but, instead, was found in our homes, factories, and vehicles. The increasedefficiency of energy use-better insulated homes, more efficient manufacturing processes, andbetter mileage in cars-has allowed us to reap the benefits of energy while using far less energythan we otherwise would have. Fuel diversity has also increased over the past 20 years, as oil usehas fallen dramatically in the industrial, buildings, and electric utility sectors through efficiency,fuel substitution, and increased use of fuel-flexible systems. Along the way, the Nation hasreduced its cumulative energy bill by more than $2 trillion and substantially reduced theenvironmental burdens and security risks associated with using energy. Figure 13 compares the sectoral changes in energy intensity that occurred from 1975 to 1991, andit breaks the transportation and buildings sectors into their two major components. With theexception of freight transportation, all parts of the economy showed declines in energy intensityranging from 14 to 27 percent. Freight transportation showed a 5-percent increase in energyintensity largely because of declining load factors that more than offset the gains in vehicleefficiency. Much of this efficiency resource was tapped because people and companies often found that it wascheaper to save energy than to buy it. Investing in efficient technologies allowed them to reducewasteful energy use and get the energy services they needed at less overall cost. Market forceshave provided incentives to use energy more efficiently. However, many of the efficiency gains realized over the past 20 years were the result ofgovernment policies to encourage, facilitate, and, in some instances, require investments in cost-effective efficiency technologies. By addressing barriers to economically desirable investments inenergy efficiency, these policies continue to produce significant economic payoffs for allAmericans. The opportunities for further gains are abundant. Studies have shown that efficient deployment ofexisting technologies could save a substantial portion of the energy we currently use, whilelowering overall costs. Continued development of new technologies would compound potentialsavings in the future. Not only would lowered energy use directly benefit the environment, butsuch efficiency gains could also obviate the need for more expensive controls that would otherwisebe required to attain environmental goals. Major opportunities still exist for substituting alternative fuels for oil in the transportation sectorand for introducing fuel-flexible technologies. These developments would create fuel competitionin the transportation sector and help diversify the energy sources that serve our transportationneeds. The Federal Role in Improving Energy Efficiency and Flexibility Despite the progress already made and the many remaining opportunities, the pace of overallefficiency gains has slowed over the past decade. Gains by alternative fuels are also slow; forexample, nonpetroleum-based fuels still represent only 3 percent of transportation energy use. Lowenergy prices have contributed to the reduced pace of efficiency gains; conversely, efficiency gainshave deterred suppliers from raising energy prices. But these low prices have also limited ourattentiveness to cost-effective efficiency opportunities and have blunted the incentive for pursuingthem. In this era of low prices, the Federal Government and the private sector can maximize energyproductivity by identifying and removing market barriers that inhibit cost-effective investments inefficiency and flexibility. These barriers exist in varying degrees in all energy-usemarkets-residential and commercial buildings, transportation, and industry-and remain a primarytarget of energy policy. They include inadequate market information and incentives, lack ofaccessible financing, inappropriately regulated markets, and fragmented technology and servicemarkets. By effectively addressing these barriers, energy policy can boost energy efficiency,increase economic productivity, create jobs, and contribute to a cleaner and healthier environment. Government and the private sector also can increase energy efficiency in the long run by investingin research and development (R&D) efforts. Government support is necessary because of thehigh-risk nature of such technology development, the inability of private manufacturers to capturethe full benefits of such research, and the benefits to the general public (such as reduced pollutionand enhanced national security) that accrue from the application of these technologies. Energy efficiency and fuel flexibility are key objectives on the path to a sustainable energy future,and barriers to their achievement threaten sustainability. The Administration supports overcomingbarriers to energy efficiency and fuel flexibility by enhancing market forces and acceleratingtechnology development. Enhancing Market Forces The ability of market forces to motivate economically desirable energy-efficiency investments canbe impaired by poor information, certain government regulations, the conflicting motivations ofowners and renters, inadequate financing, inaccessible accounting of external benefits or costs, andmany other factors. Energy policies that focus on specific "market imperfections" can improve theeffectiveness of market forces and lead to lasting improvements in energy and economicefficiency. These procompetition policies can be as simple as providing better information toenergy users, and as complex as restructuring the electric utility industry. Accelerating Technology Development and Adoption Continual improvements in the efficiency and fuel flexibility of energy-using technologies helpsenable sustainable development. But industry support of technology development is often hinderedby a focus on short-term profitability, a lack of resources, the inability of individual firms tocapture the full benefits of specific technology improvements, or the general underinvestment inresearch that benefits the common good more than the corporate bottom line. And widespread useof new technology is often prevented or delayed by persistent market or institutional barriers toinvestment. Consequently, the second main strategy for improving energy efficiency and flexibilityentails support for the accelerated research, development, and deployment of new technologies.This includes direct support of R&D as well as other policies to encourage or require the adoptionof new technologies. Combinations of these strategies have proven highly effective in all energy markets. The remainderof this chapter describes energy policy in each major market sector. Transportation: Improving Efficiency and Introducing Alternatives Efficient transportation services sustain a vibrant economy and ensure the personal mobility of thegeneral population. Increasing transportation efficiency and fuel flexibility are cornerstones of theAdministration's transportation sector energy policy. These policies are critical to achieving theAdministration's goals of improving U.S. energy security and environmental quality, while alsoreducing the economic risks posed by this sector's heavy dependence on oil. Transportation-related goods and services make up approximately 17 percent of U.S. grossdomestic product (GDP), and our entire economy relies heavily on low-cost, highly flexible, andrapid mobility of goods and services. U.S. transportation fuels are taxed at lower levels and havethe lowest delivered prices in the developed world (see Figure 5 in Chapter 2). In 1993, the average Federal and State gasoline tax was $0.37 per gallon (split about equally between Federal and State taxes). The average U.S. pump price of gasoline in 1994, at $1.17 per gallon, was its lowest in real terms since before World War II. The transportation sector accounts for almost two-thirds of total U.S. petroleum consumption,with cars and trucks alone accounting for more than 20 percent of total U.S. energy use. Last yearthe United States used 38 percent more oil in transportation than was produced in U.S. oil fields.Energy use in transportation has risen slowly over the past 15 years, as increased vehicle-milestraveled (VMT) have more than offset increased average vehicle fuel economy. On average, everyAmerican travels nearly 8,800 miles per year in a light-duty vehicle, and per capita VMT isgrowing at a rate of more than 2 percent per year. Rapid growth in heavy-truck and airline travelhas also increased the demand for transportation energy. Increased use of heavy truckssubstantially outpaced modest efficiency gains, while in the airline industry, a rapid surge inpassenger-miles traveled offset a substantial increase in per-passenger fuel economy. Mostimportantly, the transportation sector has not altered its virtually total dependence on petroleum. Transportation energy use is the Nation's largest source of air pollution, with highway vehiclesalone accounting for 26 percent of U.S. emissions of volatile organic compounds, 32 percent of emissions of oxides of nitrogen (the two principal precursors of ozone pollution inurban areas), and 62 percent of total carbon monoxide emissions. The transportation sectoraccounts for 32 percent of U.S. greenhouse gas emissions. To meet near-term objectives, the Administration's transportation energy policy encouragesincreased efficiency in the existing stock of vehicles, promotes the expanded use of efficienttechnologies in new vehicles, targets opportunities to reduce travel demand, and spurs the marketdevelopment of alternative transportation fuels. To ensure that the transportation system meets theneeds of the United States over the long term, the Administration has focused on developingvehicle and fuel technologies that will substantially improve vehicle efficiency and enhance fuelflexibility in this sector. The Administration continues to develop policies to reduce the environmental impact oftransportation energy use. In implementing the Climate Change Action Plan, the Administrationestablished a Federal Advisory Committee to make recommendations by the fall of 1995 onpolicies that cost-effectively reduce greenhouse gas emissions from personal motor vehicles.While the committee focuses on policies that reduce greenhouse gases, these same policies arelikely to address other strategic goals such as energy security, improved performance of the U.S.transportation system, and environmental concerns other than climate change. Promoting Near-Term Efficiency Improvements The Administration supports measures to promote fuel-economy improvements and to maintainfuel-economy levels in the face of declining real fuel prices. These include the corporate averagefuel economy (CAFE) standard and the "gas guzzler tax," which applies to cars getting less than21 miles per gallon. The current CAFE standard is 27.5 miles per gallon for automobiles and 20.5miles per gallon for light trucks. The National Highway Traffic Safety Administration (NHTSA)has initiated a rulemaking to set the light-truck standard in the future at its maximum feasible level.Supporting these policies are education programs designed to ensure a well-informed marketplace.For example, NHTSA is also developing rules to require fuel-economy labels for tires as an aid toconsumers, and the Federal Government's Gas Mileage Guide continues to assist consumers incomparing the fuel efficiency of new cars and trucks. Developing Markets For Alternative Fuels The Administration supports the development of markets for nonpetroleum-based alternativefuels, including encouraging infrastructure investment and alternative-fuel vehicle technologies.Alternative fuels include compressed natural gas, propane, electricity, methanol, and ethanol. These fuels are used in vehicles today, and some are used as nonpetroleum inputs to gasoline. Theenvironmental and energy security benefits of this market support will increase through time asmore advanced vehicle and fuel technologies become commercially available and as the refuelinginfrastructure expands. Current policies support an orderly market evolution under challenging conditions that includeperceived high risks, uncertain costs, and limited infrastructure for the distribution and use ofalternative transportation fuels. The Alternative Motor Fuels Act of 1988, the Clean Air ActAmendments of 1990, and the Energy Policy Act of 1992 established a number of programs andpolicy goals that are being implemented by the Department of Energy, the Department ofTransportation, and the Environmental Protection Agency. These programs have beensupplemented by new initiatives to further increase Federal agency purchases of alternative-fuelvehicles and to support local efforts to bolster the use of alternative fuels through coordinatedFederal, local, and private efforts. Current Federal programs and market developments are expected to lead to 2.5 million alternative-fuel vehicles on the road by 2010. Reducing the Demand for Travel The Administration supports increased efficiency in transportation systems to reduce traveldemand. Even with successful development of high-efficiency and alternative-fuel vehicletechnologies, stemming the rapid growth in annual VMT may be necessary to further reduce oiluse and air pollution over the long run. If recent growth trends are maintained into the future,Americans will be driving twice as many miles in 2015 as we do today. Even given expectedreductions in the annual growth rates, the Energy Information Administration projects that VMTwill be more than 50 percent higher in 2010 than it was in 1990. A number of Administrationinitiatives are designed to improve the efficiency of travel and to provide Americans with effectivemodes of alternative travel. The Climate Change Action Plan includes new efforts to promotetelecommuting and to reform the tax code to allow commuters to choose between employer-subsidized parking and cash. The Environmental Protection Agency has established the voluntaryTransportation Partners program, which assists localities in implementing and receiving credit fortransportation efficiency measures that reduce emissions of carbon dioxide and other air pollutantsby reducing demand for vehicle travel. The Administration is supporting considerable research todevelop "intelligent transportation systems," and experimental programs, such as the Departmentof Transportation's Congestion Pricing Pilot Program, to improve understanding of the feasibility,costs, and benefits of an array of strategies for increasing transportation system efficiency. Developing Technologies for a New Generation of Vehicles Transportation technologies are key in the Administration's strategy for realizing energy security,economic, and environmental goals. The Administration supports a policy of long-term, highlyfocused R&D to improve vehicle fuel economy and alternative-fuel technologies and to make these technologies available to manufacturers and, ultimately, consumers. On the vehicle side, the Administration's long-term development efforts include programs like thegovernment-industry Partnership for a New Generation of Vehicles, which targets breakthrough technologies capable of increasing vehicle fuel efficiency by as much as three times current levels. To help achieve these potentials, the Government supports efforts such as the development of advanced materials, improved electric and hybrid vehicles, advanced battery concepts, fuel-cell vehicles, and lightweight vehicle body designs. On the fuel side, Administration policy focuses on reducing the feedstock and production costs ofrenewable fuels, such as hydrogen and biomass-based ethanol, and increasing the efficiency andperformance of alternative-fuel vehicle technologies. Hydrogen is used today as an input to avariety of chemical processes and is generally produced from natural gas. Hydrogen can also beextracted from water through electrolysis, using electricity generated from renewable resources,biomass, and coal. The costs of producing, transporting, and storing hydrogen on vehicles mustdecline before this clean-burning alternative can gain a share of the vehicle market. Ethanol from various biomass sources is another promising long-term opportunity. Research isunder way to develop commercially competitive ethanol from low-energy-input crops and wasteswith desirable environmental attributes. The Department of Energy and the Department ofAgriculture coordinate research to lower the cost of producing starch-derived ethanol andagricultural oil-based biodiesel fuels. Initially, these technologies would be commercialized byusing fiber and agricultural waste sidestreams from current production as feedstocks, but theycould ultimately evolve into independent domestic energy industries. In addition to addressing energy security concerns, biofuels production creates new alternatives foragricultural crops. Expanded biofuel production will add value to crops, increase farm income, andbuild economic opportunity for rural communities. In addition to increasing on-farm economicactivity, biofuel production would also stimulate "upstream" industries that supply inputs tofarmers and would increase production in "downstream" sectors that process agricultural productsinto final goods. Remaining Challenges and Opportunities The transportation-related programs and policy initiatives outlined above represent acomprehensive effort to capitalize on a range of opportunities to achieve national energy policygoals. Nonetheless, there remain some further opportunities, issues not fully addressed, andsignificant challenges to making the programs successful. The Administration continues toexamine new ways to increase efficiency in transportation, encourage the use of domesticalternative fuels, and work with State and local governments to help manage the growing demandfor travel in ways that will enhance the overall economic and environmental performance of theU.S. transportation system. Buildings: Overcoming the Barriers to Investments in Efficiency Energy use in buildings accounts for 35 percent of total primary energy demand, 42 percent oftotal energy costs, and 35 percent of all U.S. carbon emissions. Because utility bills are asubstantial part of family budgets, residential building energy use affects what kind of housing wecan afford and how comfortable and healthy we are at home. Energy use in the commercial sectorrepresents a cost to business and can have a substantial bearing on employee productivity. Recentstudies have concluded that the overall potential for reducing energy use in buildings through cost-effective investments is on the order of 30 percent by 2015, with an overall enhancement of indoorair quality, employee satisfaction, and comfort. The Administration supports a strong, multifaceted effort to encourage cost-effective investmentsin energy efficiency and to speed the development and introduction of new, efficient technologiesfor buildings. Extensive barriers stand in the way of economically desirable energy-efficiencyinvestments in buildings. By enhancing the effectiveness of markets, developing new technologies, coordinating with State and local governments, strengthening codes and standards, and providing direct investment support to selected market segments, the Administration's energy policy substantially decreases building energy use and related costs. Current Federal programs for building efficiency are estimated to result in $31.5 billion in annualenergy savings for consumers by 2010. Continue Chapter Three Return to Table of Contents WT03-B20-38IA006-000055-B007-18http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/ch3b.html 138.80.61.12 19970221175228 text/html 22851HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:22:46 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 22679Last-modified: Tue, 09 Jul 1996 02:25:45 GMT NEPP Chapter 3 (part 2 of 3) Chapter 3: Increase the Efficiency of Energy Use - continued (Note: Figures referenced in this chapter have not been included at this time.) Enhancing the Effectiveness of Markets for Efficiency Investments Markets remain the primary vehicle for energy decisions. However, reforms in both regulated andunregulated markets can significantly improve the performance by which markets deliverconvenient, reliable, and affordable energy to the buildings sector. The Administration supportsmarket-oriented efforts to promote cost-effective investments in energy efficiency. Recent changes in Federal law and State regulation have initiated substantial changes that will leadto increasingly competitive and less regulated markets for the supply of electricity and natural gas.The Administration strongly supports these developments where they are likely to improve theoverall productivity and efficiency of the utility sector and provide energy users with better pricesignals and more types of utility service from which to choose. The Administration expects thatappropriate changes in the utility sector will lead to the further evolution of utility and independentefforts to increase end-use efficiency and load management as a means of meeting the consumerdemand for reliable and affordable energy services. Utility Integrated Resource Planning. Regulated electric and natural gas utilities provide more than86 percent of the energy used in residential and commercial buildings. However, retail utility ratesoften differ markedly from both the marginal costs of supplying electricity and natural gas and themarginal social costs of using energy. Therefore, consumers often do not receive accurate energyprice signals. Utility demand-side management programs, which exist in a number of States toencourage efficiency investments, may partly compensate for the distortions created by regulatedprices in this sector. In the 5-year period ending in 1993, with the regulatory and technical supportof Federal and State government agencies, utilities spent almost $9 billion on residential andcommercial demand-side management programs. These programs will save consumers severalbillion dollars annually over the next decade. To bolster the effectiveness of utility ratepayer funds spent on the development of residential andcommercial technologies, the Federal Government has supported the establishment of aconsortium of utilities willing to coordinate their efforts in this area. The first project of thisconsortium was the support of a "Golden Carrot" competition among manufacturers ofrefrigerators, which resulted in the design and production of refrigerators that are 30 percent moreefficient than the current standard for comparably sized conventional units. Similar approaches arebeing used for products such as high-efficiency gas and geothermal heat pumps in the Departmentof Energy's Technology Introduction Partnerships. Residential Energy-Efficiency Financing. Information on the energy efficiency of a new orexisting home is generally not available to prospective buyers, and home prices and mortgagefinancing options generally are not differentiated to reflect the lower energy costs of energy-efficient housing. The Department of Energy, together with the Department of Housing and UrbanDevelopment, private lenders, builders, and others, has helped develop and apply home energy-rating systems that provide buyers and sellers of residential real estate with information regardingthe energy performance of homes and the economics of potential improvements. This effort iscoupled with programs to expand the availability of mortgages that explicitly recognize the linkbetween lower energy bills and a homeowner's ability to afford monthly mortgage payments. Efficiency Investments in Commercial Buildings. Energy costs in commercial buildings typicallyrepresent less than 2 percent of the total costs of the businesses or public institutions that occupythem, which limits sustained management attention to energy-efficiency opportunities. Lackingstandardized methods for measuring and verifying the resulting energy savings, vendors ofenergy-efficiency technology and energy-performance contractors are only slowly gaining marketshares in the commercial building sector. Combined with limited monitoring by buildingmanagers and little market information on investment options, many commercial buildings harborprofitable but yet unrealized efficiency gains. Beginning with the Environmental Protection Agency's Energy Star programs (including GreenLights and Energy Star Buildings) that target efficient commercial lighting investments and otherprofitable building energy-efficiency upgrades, and continuing with the Rebuild America initiativeand other Department of Energy programs, the Federal Government has been extremely effectiveat encouraging commercial building owners and tenants to lower energy consumption and bills.Voluntary agreements to examine operations, combined with public recognition of theenvironmental benefits of lowering energy use, have proven to be a powerful extra incentive forprofitable investment in energy-efficient technologies for commercial building. These programsare coordinated with utility and State efforts to maximize the impact of modest Federal resources. Energy-Efficiency Product Labeling. Building energy users, especially individual consumers,often lack the basic price and performance information necessary to make sound energyinvestment and use decisions. Since the late 1970s, the Federal Government has requiredperformance labels on major energy-using products or building materials to help consumers makemore informed decisions. Major appliances and other home equipment have had such labels sincethe program's inception, but efficiency labels for windows have only recently become widelyavailable. A variety of labeling and recognition programs are currently under development with thebuilding products industry. Although commercial building managers and equipment purchasers tend to have access to moreinformation than do individual consumers, there continue to be significant gaps in this information.For example, office equipment is a major source of increasing energy demand in this sector, butthere is still very little information available on the comparative energy efficiency and use ofdifferent products. To fill this void the Environmental Protection Agency developed the voluntaryEnergy Star office equipment labeling program. The personal computer industry has widelyadopted this program, which recognizes and highlights products with superior energyperformance. Developing New Technologies To Improve Building Efficiency The Administration supports efforts to develop new technologies that can improve energy efficiency in buildings. The building construction industry is highly fragmented, with more than 157,000 home builders, 342,000 home improvement contractors, and tens of thousands of commercial building contractors. As a result, the industry does not support the R&D or training necessary to develop and rapidly adopt new technologies. R&D in the construction industry is just 0.5 percent of revenues, compared with 3.4 percent of revenues for other industries, on average. Since the mid-1970s, the Federal Government has supported the successful development of a number of building energy technologies that are now in widespread use or beginning to enter the marketplace. This support remains a key element in reducing energy demand from the building sector. The major technologies now under development in private industry and national laboratories withthe support of the Department of Energy include the gas-fired absorption (GAX) heat pump,electrochromic window glazings, the sulfur lamp, and the 30/30 roof. Compared with other high-efficiency heating and cooling technologies, the GAX heat pump has several significantadvantages. The system is approximately 40 percent more efficient than current technology and isexpected to reduce primary energy use for space heating, space cooling, and water heating in U.S.residences by 0.3 quad by 2020, while reducing emissions of carbon dioxide and otherenvironmental pollutants by more than 40 percent. Powered by natural gas, the GAX heat pumpcould also reduce the electric utility peaks associated with summer cooling loads by about 18gigawatts. Electrochromic window glazings that switch between opacity and transparency when an electricalcharge is applied may be used to control the level of direct solar radiation admitted to a building'sinterior. Such windows could offer significant energy savings and may become commonplace. The recently developed sulfur lamp can replace conventional high-intensity discharge (HID) lampsin many commercial applications. National annual energy savings from achieving a 10-percentshare of the U.S. HID lamp market by the year 2000 would amount to 8.7 billion kilowatthours,worth $600 million to U.S. industry and other consumers of HID lamps. Another example is the30/30 roof, so named for its R-30 insulation rating and expected life of 30 years. Widespreadinstallation of such roofs would save a minimum of 20 percent of the energy used for heating andcooling commercial buildings across the Nation, with many buildings achieving savings of 50percent or more. To date, a total Federal investment of $600 million in buildings research has resulted in $23 billionin energy savings. Cooperating With State and Community Programs That Encourage Building Efficiency The Administration supports cooperative programs that effectively encourage or require improvedefficiency in buildings by linking Federal efforts with the considerable capabilities and uniqueauthorities of State, regional, and local government agencies. State Programs. The Department of Energy helps fund State programs that support energyefficiency in residential markets, and works in partnerships with State energy offices in criticalareas such as updating and enforcing building energy codes, supporting home energy ratingsystems, and establishing related financing programs. Direct State support for energy-efficiencyprojects has decreased considerably as a result of the gradual expenditure of oil overcharge moniesthat have been used for years as a funding source for States. However, Federal financial assistancecontinues to enable States to implement a broad range of unique and effective conservationprograms. Community Programs. In several areas, communities play a key role in leading or coordinatingthe implementation of important building energy-efficiency measures. For example, the heat-absorbing nature of many buildings and pavement surfaces can raise outdoor temperatures by 5 to10 degrees Fahrenheit in many communities. Reducing such temperature increases can improveoutdoor comfort, reduce smog and related health effects (heat is a major contributor to smogformation), and reduce the energy required for cooling. Without community-level action, there islittle incentive for individual building owners to invest in the lighter surfaces and strategic treeplanting that would benefit the entire community. The Administration, through its CoolCommunities initiative, provides the technical information and support needed by localgovernments to promote the use of "cool" or reflective building and paving surfaces throughplanning or zoning actions. Urban development or redevelopment can also create opportunities for coordinating heating,cooling, and electric loads in ways that substantially reduce overall energy use. District heating andcooling systems, sometimes called community energy systems, use the "waste" heat fromelectricity generation to heat or cool nearby buildings. Recently, the Department of Agriculture, the Department of Energy, and other agencies haveassisted communities that have been devastated by natural disasters, such as the Midwesternfloods of 1993, to rebuild in more sustainable ways. This technical assistance helps communitiesrelocate in less vulnerable areas and build infrastructure and housing that is more energy-efficientand makes greater use of renewable resources. This new program helps communities inimmediate distress to apply their rebuilding efforts toward a long-term vision of a more secure andprosperous life. Establishing Minimum Efficiency Standards for New Buildings and Appliances Although the Administration recognizes that policies that use market forces or market-basedincentives are preferable in most circumstances, appropriate regulatory intervention can achieveefficiency gains that will benefit consumers, businesses, and the Nation. This strategy is employedwhere market barriers are particularly severe and where durable investments in equipment orbuildings will dictate energy performance over long periods of time. Building Codes and Standards. Since the mid-1970s, most State and many local building codeshave imposed significant energy-efficiency requirements on new homes and commercialbuildings, and model codes have been upgraded several times. However, despite considerableefforts by professional organizations, national building code organizations, and Federal agencies, aconsiderable gap remains between what is considered to be economically desirable buildingconstruction and actual practice. The Energy Policy Act of 1992 requires States to ensure that newnonresidential buildings meet or exceed the efficiency standards recommended by the AmericanSociety of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE), and it requiresStates to consider requiring new homes to meet or exceed the Model Energy Code of the Councilof American Building Officials (CABO). The Department of Energy assists States and local codejurisdictions in upgrading their building standards and improving their implementation andenforcement. Effectively implementing updated building codes will produce substantial long-termbenefits. Implementation of ASHRAE commercial standards is expected to reduce energy bills by $2.1billion annually by 2010. Effective October 1994, the Department of Housing and Urban Development established thermalperformance requirements for manufactured housing, such as trailer homes. The Rural Housingand Community Development Service of the Department of Agriculture is reviewing theserequirements for conformity with Department of Agriculture standards for such housing in aneffort to reduce potentially duplicative regulatory requirements on the manufactured housingindustry while maintaining adequate thermal design and construction. Appliance and Equipment Efficiency Standards. During the late 1970s and early 1980s, a numberof States established efficiency standards for major home appliances, and appliance manufacturerspetitioned the Federal Government to establish uniform preemptive national standards in theNational Appliance Energy Conservation Act in 1987. There are now national efficiency standardsfor most categories of home appliances and equipment, and the Department of Energy periodicallyreviews and updates these standards. The efficiency standards are established at levels wherehigher purchase prices are fully offset by energy cost savings to consumers in a few years ofoperation. Supporting Efficiency Investments Despite Federal and other efforts to improve markets focused on the energy efficiency ofbuildings, major barriers to the rapid and widespread use of economically desirable efficiencytechnologies in some market segments warrant direct support. The Administration is using Federalfacilities to demonstrate leadership in energy efficiency. In addition, where appropriate, theAdministration assists low-income households in private and public buildings to weatherize and,in some cases, to help defray high energy bills. Federal Energy Management. The Federal Government is the largest user of energy in the UnitedStates and probably the single largest purchaser of energy-efficient products and services. Federalfacilities ranging from offices to military apartments to hospitals use about 2.5 percent of allenergy used in commercial and residential buildings. The Federal Government has theresponsibility to set an excellent example in its own facilities and transfer lessons learned toindustry, other governments, and communities. To "reinvent" Federal energy use, the Administration has begun to implement programs to reduceFederal energy consumption in 2005 by 30 percent per square foot in Federal buildings, relative toan agency's 1985 energy use. The specific actions for realizing these savings include changes inprocurement practices for everything from computers to new buildings, new ways to fundefficiency investments, and incentives and training for energy managers. These initiatives are being coordinated at the Federal level by the Department of Energy's FederalEnergy Management Program. About $250 million per year is being invested to improve theenergy efficiency of existing Federal buildings, which will save taxpayers billions of dollars infuture government costs. In addition to these direct investments, agencies may now use special"energy savings performance contracts" that allow private companies to make the up-frontinvestments and be paid over time through the resulting cost savings. Federal agencies are alsobeing encouraged to participate in utility-sponsored efficiency programs. The energy code for newFederal building construction is being revised to conform to the model codes that are familiar toprivate contractors. Finally, a "procurement challenge" for Federal purchases of everything fromcomputers to light bulbs will encourage further energy and costs savings. On average, every dollar invested in efficiency improvements through the Federal Energy Management Program results in $3 in savings. Assistance to Low-Income Households. Most low-income households do not have access to thefinancing necessary to support cost-effective home efficiency improvements. There areapproximately 27 million low-income households, which use about 24 percent of total energy inthe residential sector. These homes tend to be older, in worse states of repair, and substantially lessenergy efficient than the rest of the housing stock. Furthermore, low-income households areespecially vulnerable to rapid price increases or severe weather that forces up utility costs. Utilityand fuel bills account for 14 percent of low-income household expenditures, compared with under5 percent for all U.S. households. To help low-income households achieve lasting reductions in their energy bills, the FederalGovernment has supported the direct weatherization of low-income households since the mid-1970s. Funds provided by Congress through the Department of Energy have been supplementedby funds from the Low Income Home Energy Assistance Program, State oil-overcharge monies,and utilities. Through 1994, it is estimated that more than 4 million homes have been weatherized.Ninety percent of the households served by the Department of Energy's Weatherization AssistanceProgram have annual incomes under $15,000. For two-thirds of the households, income is under$8,000. In 1992, the Department of Energy completed a careful evaluation of the actual energy andcost savings resulting from the weatherization of low-income homes. Results indicated that the netbenefits of the program significantly exceeded its costs. Energy costs for public and assisted housing, paid by the Federal Government through theDepartment of Housing and Urban Development, are about $4 billion per year. These units aresubstantially less energy efficient than similar market-rate dwellings and the energy costs representa sizable portion of their Department of Housing and Urban Development operating subsidies. Toincrease the quality and affordability of low-income housing, the Department of Energy and theDepartment of Housing and Urban Development have recently joined in Energy Partnerships forAffordable Homes with the goal of 30 percent energy-efficiency gains in more than 1 millionDepartment of Housing and Urban Development-assisted housing units by the year 2000. Additionally, since 1979 Congress has provided funds to the States through the Department ofHealth and Human Services to assist low-income households pay energy bills, especially bills foressential energy services, such as heating. The program has enabled hundreds of thousands ofhouseholds to avoid cutoffs of energy supplies and has enabled many others to afford minimallevels of comfort in their homes. The program has been especially vital during periods of rapidlyrising energy costs, such as the steep rise in fuel oil costs during the 1990-91 winter, or the severecold of the 1993-94 winter. Currently, the Federal Government provides about $1.3 billion peryear for this purpose, which is supplemented by many utilities and some States. Assistance for Schools, Hospitals, and Other Public Buildings. The Administration supportsefficiency investments in public and nonprofit schools and hospitals through a matching grantprogram for these institutions. During the late 1980s and early 1990s, these funds weresupplemented substantially by oil-overcharge funds available to States. This program hassupported efficiency investments of $1.8 billion since 1979, and the affected institutions reportsavings of $5.6 billion through fiscal year 1994. Return Previous Section Continue Chapter Three Return to Table of Contents WT03-B20-39IA006-000055-B007-95http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/ch3c.html 138.80.61.12 19970221175313 text/html 13831HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:23:31 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 13659Last-modified: Tue, 09 Jul 1996 02:25:45 GMT NEPP Chapter 3 (part 3 of 3) Chapter 3: Increase the Efficiency of Energy Use - continued (Note: Figures referenced in this chapter have not been included at this time.) Remaining Challenges and Opportunities Utility sector restructuring and improved technologies for monitoring and controlling buildingenergy use will likely lead to major changes in building energy markets over the next decade. Theoverall effects of these changes are impossible to predict at this time, but utility support ofdemand-side management programs has already been reduced or refocused in some areas andmay undergo major near-term changes as utilities compete more effectively in less regulatedmarkets. A key challenge for Federal and State policymakers will be determining how toencourage the continuation of effective utility demand-side management efforts-or comparableefforts-in the context of these more competitive markets. The barriers to the entry of radically new technologies, such as compact fluorescent light bulbs andheat-pump water heaters, can be especially difficult for manufacturers to overcome and representanother key challenge. Federal and State policymakers must strike an appropriate balance betweenregulatory mandates and voluntary programs that accelerate the market entry of such technology,especially in areas where major technology changes could result in very large efficiency andeconomic gains for most consumers and the Nation. Industry: Using Energy-Efficient Technologies To Improve Productivity and Reduce Waste U.S. industries form the backbone of our Nation's economic strength. The industrial sectorencompasses more than 360,000 manufacturing, forestry, agricultural, construction, and miningfacilities. These facilities employ more than 28 million people, or about one-quarter of the totalU.S. work force and produce more than $900 billion in goods and services. In producing ourmaterial goods in an increasingly competitive global market and providing high-wage jobs,industry consumes vast amounts of energy and creates a substantial public burden in the form ofpollution. Industry faces the ongoing challenges of improving productivity, enhancing productquality, and complying with environmental requirements. Through strategic R&D, technologydemonstrations, information and education, industry recognition, and market facilitation, theAdministration supports industry efforts to improve productivity while cutting energy use, emissions, and waste generation. Industry uses about 38 percent of the Nation's primary energy for fuel and feedstocks, for whichthe manufacturing sector alone pays about $100 billion per year. This energy is consumed inroughly equal amounts in electricity generation and in the direct use of natural gas and petroleum-based fuels. Industry accounts for more than one-quarter of total U.S. petroleum consumption,approximately one-third of which is for petrochemical feedstocks. The industrial sector also has a major impact on the environment. Each year, industry producesmore than 14 billion tons of hazardous and nonhazardous waste products. Direct consumption offossil fuels generated about 290 million metric tons of carbon in the form of carbon dioxideemissions in 1993, accounting for one-fifth of the Nation's total carbon dioxide emissions fromhuman activities. Including industry's share of emissions from electricity generation, industrialenergy use accounted for one-third of the carbon dioxide emissions. Industry's fuel and electricityuse also generated about 7.5 million metric tons of sulfur dioxide (38 percent of the U.S. total) and5.3 million metric tons of nitrogen oxides (25 percent of the total). Between 1972 and 1991, industry reduced the amount of energy required for every dollar of output(energy intensity) by 30 percent, with two-thirds of these savings from improvements in energyefficiency. Much of this decline in energy intensity occurred in response to higher energy pricesand increased concern about the reliability of energy supplies and markets. When energy pricesbegan to fall in the mid-1980s, however, the rate of efficiency improvement also slowed; between1970 and 1980, energy intensity declined by 2.5 percent per year, but the rate of decline hasdropped to below 1 percent in recent years. Partnering With Industry in Developing New Technologies A number of industries facing market challenges are curtailing R&D at a time when, in fact, thereis a strong need for U.S. industry to pursue the development and adoption of innovative, cost-effective, environmentally superior technologies that will help the industry remain competitive inglobal markets. Aside from the ordinary risk of conducting R&D, firms that would be potentialbuyers of new technologies are often reluctant to invest in first-of-a-kind process equipment, evenif pilot-tested and determined to be technically sound and cost-effective. Administration policyfocuses Federal support to help sustain R&D efforts and reduce investment risks for energytechnologies that can boost overall productivity, cut end-of-pipe cleanup costs, and promotepollution prevention. Government-Industry Consortia Research and Commercialization. An example of government-industry partnership is the joint effort by the Department of Energy and the American Iron andSteel Institute to develop an energy-efficient, economically competitive, continuous process toreplace conventional steelmaking operations involving coke ovens, blast furnaces, and basicoxygen furnaces. Another is the development of advanced fluid catalytic cracking units for theU.S. refining industry that can reduce oil consumption by better tailoring product yields to meetreformulated gasoline formulas and to achieve higher yields per barrel processed. Industries of the Future. The Industries of the Future program aligns Federal investment intechnology research, development, and deployment with the needs and expectations of technologyusers in the private sector. Industries of the Future focuses on seven industries that are vital to themanufacturing sector and the health of the U.S. economy-steel, aluminum, chemicals, petroleumrefining, glass, forest and paper products, and metal casting. These industries account for about 80percent of the energy consumed in U.S. manufacturing and more than 90 percent of U.S. Industrial waste, and they represent the biggest opportunities to increase energy efficiency and reduce the generation of waste. Industries of the Future establishes partnerships between industry and Government to aid in thedevelopment and application of energy efficient, environmentally beneficial technologies while alsoimproving industrial competitiveness. Industry participants first outline a vision that reflects thedynamic impact of market, business, social, and regulatory drivers in their industries. The visiondefines the context within which technology development priorities are set. The industry thendevelops a technology road map toward achieving the vision, which the Department of Energydraws upon to shape an R&D program that is also aligned with the Department's mission. Theresult is a Government R&D agenda that is well matched to the technology needs of our vitalprocess industries. Accelerating Deployment of Energy-Efficiency Technologies Performance and cost information on efficient technologies is not readily accessible to allparticipants in the marketplace who need it, especially small- and medium-sized companies. Andbecause the information needs of individual industries are so diverse and fragmented, availableinformation is often confusing and inconsistent and does not get to the decisionmakers who needit. The Administration supports programs that help transfer the results of R&D efforts to industryand encourage the adoption of state-of-the-art technology. These programs increase awareness ofopportunities to save energy, lower costs, and reduce wastes, particularly for smaller firms; andthey help create markets for new technology and services. Specific ongoing programs arehighlighted below. National Industrial Competitiveness through Energy, Environment, and Economics (NICE3).NICE3 is a cost-shared grant program with businesses, States, and Federal agencies (theDepartment of Energy and the Environmental Protection Agency) to advance competitiveness bydemonstrating technology applications that increase productivity and energy efficiency and reducewastes. Projects are selected on the basis of economic profitability, potential energy and wastesavings, cost-efficiency, capability, degree of innovation, and resources leveraged. Twenty-nineprojects have already been funded through the NICE3 program; nine have already producedcommercially viable technologies that are currently operating in industry to reduce costs, energyuse, and industrial waste. By the year 2000, the current NICE3 projects are projected to result in savings to industry of about$8 billion per year. This results from a Federal investment of only $11.4 million through fiscalyear 1995. Energy Analysis and Diagnostic Centers/Industrial Assessment Centers. This program uses theexpertise and resources of engineering faculty and students at major U.S. universities to provideno-cost energy efficiency and waste reduction assessments for the small- and medium-sizedmanufacturers that make up about 64 percent of the manufacturing sector. These firms often donot have in-house expertise or staff to address energy efficiency or waste minimizationimprovements. The average energy-efficiency audit recommends improvements that can lead toenergy savings of about 4 billion Btu, with annual dollar savings of about $40,000. Since 1976,these university-based centers have conducted more than 5,100 industrial energy audits orassessments, providing approximately 1,000 university students with energy managementexperience. The investments made as a result of these recommendations have saved industry morethan $500 million in energy bills and reduced carbon dioxide emissions by about 300,000 metrictons of carbon-equivalent. By the close of 1994, Industrial Assessment Centers had conducted more than 5,000 assessments,helping manufacturers save $517 million and 94 trillion Btu worth of energy and eliminating200,000 metric tons of greenhouse gases (carbon equivalent)-all for a total Federal investment ofonly $27 million. Motor Challenge. There are substantial opportunities for improving the efficiency of electric motoruse, beyond improving the motor itself, through improved integration of motors, drives, anddriven equipment to optimize performance of the overall "motor system." Strengthening theexisting market capability to provide higher performance, systems-oriented products and servicesis central to capturing these opportunities for greater efficiency improvements in the electric motorsystem and other industrial equipment markets. The Motor Challenge program will increase market penetration of energy-efficient industrialelectric motor systems and encourage industry to adopt a systems approach in developing,purchasing, and managing equipment. The Motor Challenge engages industry, Government, andutility partners in information exchange, conferences, seminars, and training activities. Showcasedemonstrations are sponsored with industry partners to demonstrate existing or new applicationsof efficient electric motor systems. The Motor Challenge is also developing an electric motorsystems data base that will contain performance data on motor systems in practical applications,and it is working with industry to develop market transformation strategies to stimulate marketdemand for motor system equipment with improved efficiency. By the year 2000, the Motor Challenge program will save 25 billion kilowatts per year. This willreduce emissions by the equivalent of 6 million metric tons of carbon per year. Remaining Challenges and Opportunities The different timeframes between R&D outcomes and regulatory requirements represent asignificant challenge for Federal programs designed to help industry develop and deploy energy-efficient pollution prevention technologies. Innovative technologies can take a decade or more todevelop into commercial products, while the time given industry to comply with certain regulatoryrequirements is often shorter. The Administration is coordinating activities that support technologydevelopment for energy efficiency and pollution prevention with the evolving system ofenvironmental regulation, described more fully in Chapter 6. By supporting efficient and cleantechnologies that would contribute to a more flexible and cost-effective environmentalmanagement system, the Administration's programs will ensure that a cleaner and morecompetitive industrial sector will emerge in the 21st century. Return to Table of Contents WT03-B20-40IA006-000055-B007-133http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/ch4a.html 138.80.61.12 19970221175347 text/html 22351HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:23:46 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 22179Last-modified: Tue, 09 Jul 1996 02:25:45 GMT NEPP Chapter 4 (part 1 of 3) Chapter 4: Develop a Balanced Domestic Energy Resource Portfolio (Note: Figures referenced in this chapter have not been included at this time.) To achieve the goals of increased energy productivity, pollution prevention, and enhanced nationalsecurity, the Nation must make the most efficient use of a diverse portfolio of domestic energyresources that will allow us to meet our energy needs today, tomorrow, and well into the 21stcentury. The Administration continues to promote the economically beneficial and environmentallysound expansion of domestic energy resources. Our domestic energy resources include coal, petroleum, natural gas, nuclear energy, and renewableenergy (primarily hydropower, biomass, geothermal, and wind). About 85 percent of the energywe consume comes from fossil fuels such as coal, oil, and natural gas (see Figure 14). In the year2010, we undoubtedly will use greater amounts of renewable energy resources, but still will relyprimarily on the energy resources that we use today. Further into the 21st century, we could relymuch more heavily on solar and other renewable resources, and our energy portfolio may includehydrogen and fusion energy. Sustainable development of energy resources means that we must efficiently use our existingdomestic resources today and augment the resources that are available for the future. Somedomestic resources, such as natural gas and oil, must be developed in ways that ensure theseresources are not abandoned and are economically accessible. Developing and deploying advancedenergy technologies today will ensure that we will have the necessary means to produce and useour energy resources when we need them. Maximizing the cost-effective use of our relatively cleanenergy resources, such as renewables and natural gas, will ensure that future generations willinherit a cleaner environment. The Federal Role in Helping To Develop a Domestic Energy Resource Portfolio The Federal Government has a vital role to play in the development of energy technology and inthe stewardship of our domestic natural resources. The private sector alone may not provide all ofthe technology that we will need to find, produce, and use our domestic energy resources and tocompete in world energy markets. Even the largest energy companies often are unwilling to makesubstantial investments in long-term and high-risk research and development (R&D). Throughpartnership with industry, the Federal Government can help develop and demonstrate promising,but still unproven, advanced energy supply and utilization technologies-especially thosetechnologies that have substantial public benefits such as improved environmental performance.To the extent that the private sector cannot or will not do so, the Federal Government invests at allstages of technology development to ensure that all Americans will have the clean and affordableenergy they will need in the future. In the near term, Federal policy helps ensure that our energy industries remain viable, providingjobs for Americans and energy to the Nation. By supporting our energy industries throughappropriate tax treatment, sound regulatory decisionmaking, and R&D programs that help reducecosts, the Federal Government can help ensure that U.S. industries will continue to meet thecompetitive challenges of the global economy. To maintain economic growth and enhance national security, we must diversify our energyresources and increase the efficiency of energy use. To ensure a cleaner environment, we mustincrease our use of cleaner energy resources, such as renewables and natural gas, and we mustdevelop technologies that will allow us to produce and use existing energy resources in ways thatwill result in fewer harmful effects on the environment. To achieve our national goals, the Administration is working to promote the efficient developmentand use of a domestic energy resource portfolio by- Partnering with industry to research, develop, and deploy advanced technologies that willallow industry to explore for, produce, develop, and use our domestic energy resources moreefficiently, more safely, with fewer harmful effects on the environment, and in more applications Providing appropriate tax and other fiscal incentives to support our domestic energyresource industries Stimulating markets for clean energy technologies, such as natural gas and renewableenergy Reforming and streamlining regulations to make energy markets more efficient, to allowour domestic energy resources to compete in energy markets, and to better weigh costs againstrisks Promoting U.S. energy resources and advanced energy technologies abroad As discussed below, the Administration's policy is to apply the appropriate tools to each sector,and to make the best use of taxpayer dollars by aligning budgetary priorities with national goals. Oil: Enhancing Domestic Production Capability The growing level of U.S. oil consumption raises potential economic and national securityconcerns. In addition to emphasizing efficient use of oil products and enhancing fuel flexibility,national energy policy must address declining domestic production levels with minimuminterference with market forces. The Administration's policy is to improve the economics ofdomestic oil production by reducing costs, in order to lessen the impact on this industry of low andvolatile prices. Petroleum products account for nearly 40 percent of the energy consumed in the United States, andthis share is expected by most analysts to remain constant for the next decade. Investment in oilexploration and production activity in the United States is related directly to the price of oil ininternational markets as well as to costs of domestic production. Exploration and productionspending by major U.S. oil companies is increasing overseas, rather than in the United States, andindependent producers increasingly are unable to respond alone to the technically demanding andhighly volatile price environment that characterizes the oil industry. U.S. crude oil production decreased for most of the 1970s, stabilized or increased slightly in theearly 1980s (mostly because of production in Alaska), and has been decreasing since 1986. Oilfields are being abandoned at an increasingly high rate, resulting in the loss of billions of barrels ofoil reserves each year. At the same time, industry cost-cutting since 1986 has yielded domestic finding costs similar toforeign finding costs, and technical innovation is increasing production efficiency. Still, thedomestic oil industry is characterized mainly by marginal, high-cost production. The few areas ofhigh production at lower per-barrel costs-parts of Alaska, the Gulf of Mexico, and deepformations in the Rockies and elsewhere-have not offset the decline in the domestic resource base.With declining industry profitability caused by low prices and high price volatility, long-termindustry R&D spending dropped by $400 million per year between 1985 and 1992, and thedecline is continuing. The consequences of this decline in private R&D not only are being felttoday, but will be especially severe after the turn of the century if the introduction of newexploration and production technologies effectively stops. The price of oil is set by the world market; the U.S. Government cannot exert much influence onthis critical component of industry revenues. Consequently, the best opportunity for the FederalGovernment to improve the economics of domestic oil production is to help reduce costs.Throughout the Federal Government, the Administration is pursuing various ways of loweringcosts, including R&D support, and targeted tax and regulatory reform. Partnering With Industry To Develop Advanced Supply Technologies Continued development and deployment of advanced technology can ameliorate the decline in domestic oil production through enhanced recovery in existing fields and increased ability to find new sources. Advanced technologies reduce operating costs, improve process reliability, develop flexible processes, and meet environmental requirements. The Administration supports government- industry cost-sharing partnerships to meet high-priority needs. The Department of Energy's oil R&D program addresses these needs, which include maintaining access to valuable reserves threatened by shut-in, recovering more of the oil resource in the ground, finding oil in geologically difficult areas, and complying with environmental requirements. By 2010, it is estimated that domestic oil production linked to the deployment of advancedexploration and production technologies supported by the Administration will be 1 million barrelsper day. Through the Department of Energy's Advanced Computation Technology Initiative (ACTI), theFederal Government also is making available to domestic oil and gas producers extraordinarycomputational and modeling skills developed by the Department's weapons laboratories as part oftheir national security mission. These cutting-edge computer technologies can aid in the search fornew resources and can optimize extraction from existing reservoirs. Providing Appropriate Incentives To Increase Domestic Exploration and Production Although limited by the need to reduce the Federal budget deficit, financial incentives can providesupport and reduce barriers to selected promising activities that the private sector deems too riskyto develop or that are at risk of premature abandonment during periods of depressed market prices.Financial incentives can take many forms, including reductions in Federal income taxes, reducedroyalty rates for natural gas or oil production on Federal lands, and assistance to the industry inimproving access to capital markets. One of our best opportunities for adding large new oil reserves can be found in the central andwestern Gulf of Mexico, particularly in deeper water. Royalty relief can be a key to timely accessto this important resource. The Administration supports targeted royalty relief to encourage theproduction of domestic oil and natural gas resources in deep water in the Gulf of Mexico. This stepwill help to unlock the estimated 15 billion barrels of oil-equivalent in the deepwater Gulf ofMexico, providing new energy supplies for the future, spurring the development of newtechnologies, and supporting thousands of jobs in the gas and oil industry and affiliated industries. The Administration also supports a royalty rate reduction for onshore heavy-oil wells to improveultimate recovery and increase the domestic oil reserve base. The Department of the Interior'sBureau of Land Management (BLM) has proposed a rulemaking that would provide royalty relieffor heavy-oil wells. This relief would reduce the abandonment rate for marginally economicheavy-oil wells and would increase reserves by up to 40 million barrels of oil. Reforming Regulations To Reduce Uncertainties and Costs Throughout the Federal Government, the Administration is working to streamline regulationsgoverning all industries and, in particular, the oil industry. These reforms will have significantbenefits for natural gas producers as well. Virtually all aspects of the oil industry are governed byregulations concerning environmental issues, exploration and production activities, royalties, andtaxes. The current Federal and State regulatory framework lacks incentives for cost-effectivepollution control and innovation and adoption of promising technologies. These regulations areoften complex, duplicative, and inflexible. Industry faces increased regulatory uncertainty and highcompliance costs, while Federal, State, and local governments also spend more than necessary onimplementing and enforcing regulations. The Department of the Interior's Minerals Management Service (MMS), which manages theleasing of Federal outer continental shelf (OCS) petroleum and natural gas resources, is reviewingits regulations in the following areas: (1) deepwater production rules governing subsea installationsand the flaring or venting of small amounts of gas; (2) blowout, safety, and pollution preventionequipment; (3) conservation and diligent development requirements; (4) management ofdirectional survey drilling; (5) requirements regarding construction and removal of platforms; (6)daily pollution inspection requirements; and (7) training. Regulatory changes may be appropriatefor all of these areas, and MMS is studying proposals to address these issues. MMS has completed a negotiated rulemaking process for gas valuation on Federal leases andinitiated similar efforts for Indian leases. MMS issued final regulations facilitating companyrecoupment of certain payments associated with OCS leases and is examining regulations toreduce or eliminate certain reporting requirements and to make it easier and cheaper for companiesto properly report and pay royalties. MMS is looking at changes to lease terms and royalty ratesthat would encourage more domestic production. The Royalty Management Program, which governs the collection of royalty revenues from Federalonshore and offshore lands, is being reviewed in the following areas: (1) valuation of natural gasfrom unitized properties, and the responsibilities of payers and lessees; (2) procedures forobtaining refunds and credits for excess royalty payments from OCS production; (3) penalties andassessments for improper reporting; and (4) streamlining of the MMS administrative appealsprocess. The BLM has conducted a sweeping review of its oil and gas leasing program to make its onshoreleasing program more efficient and responsive. In April 1995, the review team published itsreport, which contained a list of proposals for improving the onshore oil and gas leasing program.Examples of these proposals include the following: simplifying unitization and communicationsagreements; simplifying requirements related to oil sampling and to gas flaring and venting;eliminating duplicate bonding requirements; developing a pilot program for environmentalcompliance self-certification; and selectively increasing bonding amounts for problem-proneoperations. The Department of Energy also is funding the Interstate Oil and Gas Compact Commission in aneffort to streamline regulations governing the exploration and production of oil and natural gas onFederal lands. The Commission has convened a working group to examine State and Federalrequirements on Federal lands and to recommend ways to reduce duplication, share informationand resources, and reduce costs. This 3-year effort focuses on the States with the greatest amountof production from Federal land: California, Colorado, New Mexico, and Wyoming. Removing Artificial Market Barriers The Administration supports lifting the ban on the export of Alaska North Slope (ANS) crude oil.ANS crude oil is geographically convenient to only 20 percent of U.S. refining capacity. Since theNorth Slope reached full production in the mid-1970s, ANS crude oil that cannot be used on theWest Coast has had to be shipped more than 13,000 miles to Gulf Coast markets. Confronted withthis expensive option, ANS producers have been forced to sell ANS crude on the West Coast atprices considerably lower than the oil's value to refiners. Historically, this discount also has helpedto depress California crude oil prices. Removing the ban on exporting ANS crude oil will allow the marginal supply of ANS crude toseek its true value in world markets. It will create additional employment, raise royalty collectionfor the Federal Government and revenues for the States of Alaska and California, and lift domesticoil production by roughly 100,000 barrels per day. ANS exports will contribute to energyproduction and economic growth, without employment dislocation, environmental degradation, orsignificant increases in retail petroleum product prices. In the event of an oil emergency, thePresident could terminate the exports. Supporting Exploration and Production of New Supplies in Environmentally Sensitive Ways The Administration supports exploration for and production of oil and natural gas in areas that arenot environmentally sensitive. For example, through technology R&D and appropriate financialincentives, the Administration is supporting exploration and production of oil and natural gas in deepwater Gulf of Mexico. More than 40 million acres of public lands are covered by oil and gas leases, and BLM has issued competitive oil and gas leases on more than 4 million acres of public lands and has approved more than 4,300 drilling permits in the past 2 years. The Administration continues to oppose opening the Arctic National Wildlife Refuge (ANWR)and certain Outer Continental Shelf areas for oil and natural gas development. ANWR is just onepart of the Alaska North Slope oil exploration and development picture. There are extensive Stateand Federal lands that can be developed without the need for development in ANWR, and theAlaska Department of Natural Resources' Oil and Gas Division estimates that about 6.2 billionbarrels of oil remain to be developed on the North Slope outside of ANWR. Reforming the Regulatory Structure Governing Refining Activities The Administration's policy is to simplify the complex regulatory structure governing refiningactivities and capital investment. This will speed investment in clean, efficient, and competitiverefining technologies. U.S. refining capacity increased by almost 50 percent during the 1970s, but fell during the 1980sand has remained in the range of 15 million to 16 million barrels per day so far in the 1990s. Theconfiguration of the industry has changed substantially over this period. While U.S. refiningcapacity today is close to what it was in the mid-1970s, the number of operable refineries hasdropped by one-third, reflecting the trend away from smaller, more inefficient refineries, and thecapacity to process heavier, high-sulfur crude oils has increased significantly. On a capacity basis,the ability to remove sulfur from petroleum products has doubled. The net result is that U.S.refineries are able to process heavier, higher sulfur crude oils and make a wider range of lightpetroleum products than they could in the 1970s or even in the 1980s. Today's refineries are cleaner, and the products they manufacture are less damaging to theenvironment. Lead has been phased out of gasoline; and, as a result of the lowering of its vaporpressure, gasoline is less likely to emit toxic substances such as benzene. Sulfur content in dieselfuel and residual oils is substantially lower. Virtually all of these improvements have resulted from Federal and State environmental regulation,but such regulations have become expensive. The National Petroleum Council estimated thatcapital expenditures of $37 billion (1990 dollars) will be needed in the refining industry between1991 and the year 2000 for product quality and stationary source regulatory compliance. The challenge of the 1990s for the petroleum refining industry is to find ways to become moreefficient and clean and to reduce the costs of environmental compliance. If the regulatory burdenon domestic refiners drives the industry offshore, energy security concerns will intensify. Addressing these issues, the Environmental Protection Agency's Common Sense Initiative isdesigned to identify "cheaper, cleaner, smarter" ways of achieving our environmental objectives.The initiative brings together industry, the States, the Federal Government, and public interestgroups to identify and implement both short- and long-term opportunities to simplifyenvironmental regulations and policies. In the oil refinery sector, the initiative will identify thepermit, regulatory, and statutory areas of flexibility that will increase the refining industry'sflexibility and lower its costs, without sacrificing environmental performance. Additionally, the Department of Energy's Refinery of the Future initiative is assisting the refiningindustry to hasten the transition from responding to command and control environmentalregulation to developing and adopting pollution-avoidance and resource conservation technologies.The Department will assist industry in developing and deploying novel or improved high-priorityprocess technologies and an advanced technology base that will (1) maintain or enhance industrycompetitiveness, (2) foster the evolution of more economic approaches for reducing effluenthazardous and nonhazardous waste streams, (3) improve overall refinery energy efficiency, and(4) promote greater flexibility in utilization of alternative energy, feedstock, and raw materialinputs. Continue Chapter four Return to Table of Contents WT03-B20-41IA006-000055-B007-219http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/ch4b.html 138.80.61.12 19970221175442 text/html 26327HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:24:29 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 26155Last-modified: Tue, 09 Jul 1996 02:25:45 GMT NEPP Chapter 4 (part 2 of 3) Chapter 4: Develop a Balanced Domestic Energy Resource Portfolio - continued (Note: Figures referenced in this chapter have not been included at this time.) Natural Gas: Increasing Efficient Utilization The Administration's policy is to maximize the economic, environmental, and national securitybenefits of increased use of natural gas. To implement this policy, the Administration focuses oncollaboration with industry to develop advanced natural gas technologies, provide incentives forincreased exploration and production, stimulate end-use markets, and continue regulatory reformsthat reduce costs and increase efficiency. Natural gas provides approximately 24 percent of the energy consumed in the United States,heating 57 percent of U.S. homes and serving 175 million consumers. Approximately 45 percentof natural gas delivered to end-users is used to provide energy in the industrial sector, 14 percent isused to generate electricity, and 16 percent is used in the commercial sector. In 1993, more than300,000 Americans were employed in the search for and production of crude oil and natural gas,while an additional 196,000 Americans were employed by U.S. natural gas utilities. The United States has a vast recoverable natural gas resource base. In its 1992 study, the National Petroleum Council estimates a technically recoverable gas resource base for the lower 48 States of 1,295 trillion cubic feet (tcf), sufficient gas to meet U.S. demand at current levels for about 60 years. The U.S. Geological Survey projects that technically recoverable onshore natural gas reserves in the United States are about 1,074 tcf, 11 percent higher than their 1989 estimate. Moreover, our North American Free Trade Agreement partners, Canada and Mexico, also have large supplies of natural gas that can help us to reduce the growth of oil imports from unstablesources. Natural gas has fewer harmful effects on the environment than other fossil fuels. Natural gas hasvirtually no sulfur emissions, lower emissions of nitrogen oxides (NOx), and extremely lowparticulate emissions. Natural gas has approximately 30 percent lower carbon dioxide emissionsthan oil and 45 percent lower carbon dioxide emissions than coal on an energy-equivalent basis.Finally, natural gas combustion does not generate solid waste. Increased use of natural gas is acornerstone of the Administration's Climate Change Action Plan, as one of the measures used toreduce greenhouse gas emissions to 1990 levels by the year 2000. Over the past two decades, natural gas markets have become increasingly competitive, largelybecause of Federal legislation and regulatory reforms. The sale of natural gas at the wellhead hasbeen deregulated, and the interstate transportation of natural gas has been opened to third parties.These reforms have provided natural gas consumers with the ability to shop for the mosteconomic gas supplies. Partnering With Industry To Develop Advanced Natural Gas Technologies Through the Department of Energy's natural gas supply R&D program, the Administration isinvesting in a new generation of technologies to reduce the cost, ensure long-term supply, andincrease the use of natural gas. In the 21st century, natural gas supply will depend increasingly on drilling and producing fromreservoirs that are more difficult to develop because of their depth and size, hardness of the rock,lack of permeability, or the quality of the natural gas they contain. New advanced drilling systemtechnology to induce fractures or to locate and drill through natural fractures is needed to convertsuch natural gas resources into proved reserves. These technologies will address both the technicalbarriers in such areas as 3-D seismology, horizontal drilling, and fracture detection, as well as thecost savings associated with drilling faster and with less damage to the reservoir. By 2010, it is estimated that domestic natural gas production linked to deployment of advancedexploration and production technologies supported by the Administration will be 2 trillion cubicfeet per year, or about 10 percent of current U.S. consumption The Department of Energy's natural gas R&D program focuses increasingly on the developmentand deployment of the best and most efficient natural gas end-use technologies, such as naturalgas-fired space air-conditioning equipment for residential and commercial applications; naturalgas-fueled vehicles that are more reliable, produce lower emissions, and have longer cruisingranges than the current generation vehicles; high-efficiency, industrial gas turbines with lownitrogen oxide emissions to meet future electricity generation needs; and fuel cells that chemicallyconvert natural gas to electricity at efficiencies of 50 to 60 percent. Providing Appropriate Incentives To Increase Exploration and Production The Administration supports providing appropriate revenue-neutral incentives to increase naturalgas exploration and production. The Administration is exploring a royalty rate reduction foronshore marginal natural gas wells, similar to the "Stripper Program" rule currently in effect foroil, that will conserve resources, improve ultimate recovery of finite resources, increase thedomestic natural gas reserve base, and promote the production of natural gas. Stimulating Markets for Natural Gas Use The two markets with the largest growth potential for natural gas are transportation vehicles andelectricity generation. The Administration is working to stimulate these markets by purchasingnatural gas vehicles for Federal fleets, by providing tax incentives, and by encouraging the use ofnatural gas for electric power generation. As a result of the Energy Policy Act of 1992, the Federal Government is providing leadership inpurchasing alternative-fuel vehicles (AFVs), including natural gas vehicles, for Federal fleets. In1993, by executive order, the President increased the AFV targets. By the end of fiscal year 1994,the Federal Government had purchased 6,700 natural gas vehicles. Using fiscal year 1995 funds,the Federal Government plans to purchase an additional 9,250 natural gas vehicles. Thesepurchases represent 59 percent of planned Federal AFV stock, and 20 percent of natural gasvehicles nationwide. To promote further the penetration of natural gas vehicles into the transportation sector, the FederalGovernment provides tax deductions for AFV purchases, including natural gas vehicles, of up to$2,000 per vehicle, and for AFV refueling facilities of up to $100,000 per location. It is likely that at least half of all new electricity generation capacity to be added in the United Statesover the next 20 years will be natural gas-fired. New gas turbines and combined-cycle technologyare making gas the fuel of choice for many utilities, and these technologies have been improvedthrough Federal R&D efforts. However, a principal impediment to increasing utility marketpenetration by natural gas is electric utilities' concern about reliable, long-term natural gas supplies.The Administration, through the Department of Energy and the Energy InformationAdministration, is working to allay these concerns by providing timely data on the availability anddeliverability of domestic natural gas supplies. By 2010, commercial deployment of advanced natural gas power technologies supported by theAdministration is projected to attain 16 gigawatts of electric capacity investment per year indomestic and export markets. Reforming Regulations To Reduce Costs and Increase Efficiency The Administration supports continued regulatory reform throughout the Federal Government toreduce the cost of natural gas exploration and production and to increase efficiency in natural gasmarkets. The regulatory reforms described in the Oil section of this chapter apply to theexploration and development of natural gas as well. In addition, the Administration has exploredregulatory reforms that could expand the role of natural gas in end-use markets. Sales of natural gas occur in markets where regulation influences consumers' decisions regardingthe commercial attractiveness of the fuel. The Administration will continue to promote acompetitive natural gas industry that offers more and better choices to energy consumers. To thisend, the Administration supports regulatory reforms to ensure a more robust secondary market forinterstate natural gas pipeline transportation capacity, to promote efficient and fair pricing of naturalgas transportation service, and to expedite the construction of new facilities to ensure reliablenatural gas delivery. The Department of Energy continues to support, on a collaborative basis,States that are examining their regulations to bring the benefits of competition to retail natural gasmarkets. Electricity: Promoting Competition in the National Interest The electric power sector is the largest direct energy consumer in the United States, using 36percent of all primary energy consumed in 1994, while providing power worth approximately$200 billion annually to residential, commercial, and industrial users. The electric powersector-including investor-owned, municipal and cooperatively owned, independent, and federallyowned power-has a combined capital investment of several hundred billion dollars, making thissector one of the largest and most important economic sectors in the United States. Over the past two decades, U.S. demand for electricity grew by about 3 percent per year, but thisgrowth in demand is expected to be slower between now and 2010, averaging slightly more than 1percent per year. To meet the growing demand for electricity between 1975 and 1994, electricutilities increased their annual consumption of primary energy by 10.5 quads. As shown in Figure15, greater use of coal and nuclear power accounted for all of the increase, while also displacingmore than 2 quads of oil use per year. Currently, more than half the Nation's electricity is generated from coal, a share that most analysts expect to remain essentially steady for the next decade or two. Nuclear power supplies 22 percent of total generation. Natural gas and renewables (including hydroelectric power) each account for about 10 percent, and the shares of both natural gas and nonhydroelectric renewables are expected to increase steadily over the next decade. The U.S. electricity industry is in a period of fundamental change. The next several years will see atransformation of the industry, affecting its organizational structure, the products and services itdelivers to customers, and the Federal and State institutions that regulate it. The transformation ofthe industry and its regulatory institutions is being driven by a combination of forces, including thefollowing: Broad recognition that the electricity-generation sector no longer is a natural monopoly and could be substantially deregulated Statutory changes, such as the Energy Policy Act of 1992, that promote increasedcompetition in bulk power markets Large disparities in electric rates from utility to utility, which encourage customers to seekaccess to lower cost suppliers New low-cost generation technologies, which offer cheaper power and reduce theeconomic value of existing traditional generation equipment The generally successful overall experience in recent years with reduced regulation in otherindustries (for example, telecommunications and natural gas) Restructuring the Industry To Link Competition with National Goals The Administration believes that prudent restructuring of the electricity industry should increasecompetition, reduce consumers' electricity-related costs, maintain environmental protection, andenhance economic productivity. Although complete deregulation of the industry is not possiblebecause transmission and distribution networks are still natural monopolies, there are significantopportunities to increase competition in the industry. Many proposals for increasing competitionare now being considered at the State level. Meanwhile, the Federal Energy RegulatoryCommission (FERC) has begun to consider appropriate changes in Federal regulations toaccommodate greater competition under the existing statutory framework. On March 29, 1995, theFERC issued a notice of proposed rulemaking that addresses key issues involved in movingtoward a more competitive wholesale electricity marketplace. The Administration supports the scope and direction of the Commission's proposed rulemaking asa constructive, even essential, starting point for enabling greater competition. Significant nationalissues must be addressed under the general rubric of competition and restructuring as individualState and Federal regulatory reform efforts evolve. These fall into two main categories: the near-term transitional issues of stranded assets and the long-term design issues of stranded benefits. Stranded Assets. Stranded assets (sometimes called stranded costs) refer to prudent investmentsincurred or contracts entered under the existing regulatory framework whose costs are unlikely tobe recovered under more competitive market conditions. Examples include high-cost generatingunits and some above-market contracts for wholesale power and fuels. These one-time transitioncosts could be borne by investors or consumers, or both, depending on the restructuring policypursued. The extent to which the owners of these assets are forced to absorb the loss in economicvalue or are able to recover costs in electricity rates during a transition period will partly determinethe structure of the emerging competitive industry. Because resolving the debate over stranded costrecovery will accelerate the introduction of competition in the electricity industry, theAdministration supports restructuring proposals that address stranded cost issues in a fair andequitable manner. State and Federal regulators exploring competition have solicited ideas and suggestions regardingappropriate mechanisms for potential recovery of stranded assets. One potential national issue isthe performance of nuclear power stations in competitive markets and the subsequent implicationsfor civilian radioactive waste disposal policy in the near term. The Administration will continue toevaluate moves toward competition in terms of their impact on the civilian waste disposalprogram. Stranded Benefits. Stranded benefits refer to systemwide and social objectives currently served bythe regulated market that may be vulnerable under more competitive electricity markets. Theselong-term design goals include ensuring system reliability, promoting end-use efficiency andrenewable energy investments, providing sufficient electricity-related R&D, encouragingreductions of greenhouse gas emissions, incorporating environmental considerations into resourceselection, and making minimum levels of service available to low-income consumers. TheAdministration will support proposals for restructuring that promote the continued achievement ofthese important electricity-related public policy objectives. Many analysts question the ability of a competitive industry to provide a high level of efficiencyinvestments and services currently supported by utilities' integrated resource planning and demand-side management efforts. Under some scenarios of market evolution, service providers vying fordifferentiated market niches could promote aggressive efficiency measures by offering innovativeenergy service contracts that would overcome traditional market barriers to demand-sideinvestments. Under other scenarios, however, intense commodity competition for the cheapestelectric power would deter innovative service providers, and market barriers would continue toinhibit cost-effective energy-efficiency investments. Until more information emerges from State-level competitive experiments, the Administration will support only those restructuring effortsdesigned to enable high levels of cost-effective energy-efficiency investments. The same commitment also extends to ensuring a strong market for renewable energy investmentsand an appropriate level of electricity-related R&D conducted by a more competitive sector. TheAdministration will continue to work with States, the FERC, Congress, the industry, andconsumer and environmental interests to ensure that greater competition in the electricitymarketplace serves the needs of consumers, investors, and the environment now and into thefuture. Examining the Statutory Framework The Energy Policy Act of 1992 established the statutory framework that enables the market tomove toward greater competition. Recently, some utilities and other stakeholders have proposedabolishing two key statutes that circumscribe and, detractors claim, unnecessarily hinder morerapid movement toward a competitive market. The first is the Public Utility Holding Company Actof 1935 (PUHCA), which significantly constrains the investments, financing mechanisms, andcorporate structures available to electric utility companies. The second is Section 210 of the PublicUtility Regulatory Policies Act (PURPA), which encourages cogeneration and renewable energyby requiring utilities to purchase electricity from designated generating sources at rates establishedby State regulators under Federal guidelines. Both statutes have played important roles in limiting anticompetitive behavior by utilitymonopolies. Although market conditions have changed substantially since their inception, thesestatutes continue to provide important safeguards and incentives that benefit consumers during thepresent phase of industry evolution. As a clearer picture of industry structure and competitiveoperation emerges from State actions and the FERC's ongoing regulatory reforms, however,modifications of these statutes or their implementing regulations may become warranted. Giventhe current status of industry restructuring proposals and the significant potential for greatercompetition under the existing statutory framework, the Administration opposes the repeal ofSection 210 of PURPA at this time. However, the Administration will continue to assess theprospects for changing regulatory implementation or undertaking statutory reforms of PURPAand PUHCA in view of the national objectives of reliable, clean, and economic energy resources. Renewable Energy: Increasing Long-Term Investments The Administration's energy policy promotes the development and deployment of renewableenergy resources and technologies in the United States and abroad. By helping to reduce costs andby stimulating market adoption of U.S. renewable energy technologies, energy policy canaccelerate the environmental, economic, and security benefits of increased use of renewableresources. The United States has an abundance of renewable energy resources. Renewable energy-whichincludes photovoltaic, solar thermal, biomass (wood, wood waste, refuse, agriculturalcommodities, both traditional and new, and agricultural waste), wind, geothermal, and hydropower technologies-accounts for approximately 10 percent of the electricity generated in the United States. Renewable resources also can be used as feedstocks from which to derive alternative transportation fuels, such as those derived from biomass. In addition, solar energy systems for space heating, water heating, and process heat augment energy supply by reducing demand for conventional energy resources. During the last 15 years, intensive work by industry and the Department of Energy's nationallaboratories has steadily increased the reliability of renewable energy systems while dramaticallylowering their costs. These systems are gradually becoming commercially competitive withconventional power sources. Today, wind-generated electricity is nearly competitive withconventional electric power in several regions; the United States has regained its lead in worldphotovoltaic shipments; the geothermal heat pump industry is striving for 12 percent of the U.S.space-conditioning market; the bioenergy industry is working to become a major new source ofelectric power and transportation fuels (and a major new source of income for U.S. farmers and animpetus for rural economic development); and hydrogen energy is being developed as a majorenergy carrier for the future. One indication of the global potential for renewable energy can be found in recent energy scenariosdeveloped by the Shell International Petroleum Company. In the "Sustainable Growth" scenario,emerging renewable technologies progress along their learning curve, first capturing niche marketsand then gradually expanding to become broadly competitive during the early decades of the nextcentury. In this scenario, renewable energy resources would contribute 20 percent of commercialworldwide energy by 2020, a contribution that could grow to more than 50 percent in the later partof the 21st century as technologies mature and diffuse into the marketplace. While such a scenario represents only a possibility-and not a prediction-it does suggest that greatpotential exists for renewable energy resources. The primary challenge to expanding the role ofrenewable energy resources in the domestic energy portfolio is the need to further reduce costs toensure that competitive renewable energy technology is available in expanding domestic andinternational energy markets. Programs supporting renewable electric supply will contribute 0.6 quad of primary energy in theyear 2000, saving $4 billion in annual fuel costs and reducing 7 million metric tons of carbon-equivalent emissions. Partnering With Industry To Develop Lower Cost Renewable Resource Technologies The Administration supports fundamental and applied research that helps the renewable industrydevelop technologically advanced products. The Department of Energy's renewable energy R&Dprograms support fundamental research in areas such as aerodynamics, solid-state physics,reservoir engineering, combustion processes, and fermentation. Such research has contributed tomajor advances in wind energy, photovoltaics, geothermal energy, biomass utilization, and liquidfuels production. Applied research into thin reflective membrane deposition, airfoil design, andsolar module fabrication has reduced costs and increased productivity from solar thermal powerplants, wind turbines, and flat plate photovoltaic arrays. In addition to renewable electric systems, technology programs are expected to produce nonelectricenergy contributions equivalent to 60 million barrels of imported oil per year in the year 2000 andtwice that amount in 2010. For example, the Department of Energy is advancing the developmentof alternative transportation fuels. Through the Biofuels Program, the Department is working withindustry to develop transportation fuels from domestic biomass resources, to displace imports andenhance rural economic development. The largest research program in this area focuses onproducing ethanol from agricultural and forestry residues, waste paper, and low-value industrialwastes. The goal of the program is the commercial production of 600 million gallons of biomassfuels by the year 2000, rising to 16.8 billion gallons by 2020, displacing more than 320 millionbarrels of imported oil each year. The Department of Energy's Hydrogen Research Program works with industry and a scientificadvisory panel to develop safe, practical, and competitive hydrogen technologies to meet energyneeds. In fiscal year 1996, the program will operate a hydrogen delivery system using metalhydride storage for mobile applications and scale up a laboratory-sized hydrogen storage andproduction plant that will be suitable for export to developing countries in remote areas. The Department of Energy's Geothermal Electric RD&D program will reduce the cost of high-temperature geothermal electricity by 20 percent in the years ahead, and by up to 40 percent formoderate-temperature resources. It will help U.S. industry expand its domestic generating capacityto 4,000 megawatts by the year 2000, while capturing 1,000 megawatts annually in internationalmarkets. The movement of technologies into the marketplace requires additional investment inapplied research, development, and deployment of products to facilitate rapid return of embeddedinvestment costs to the point where industry and markets can take over. The Department of Energy's Photovoltaic Systems Program supports private sector research todevelop roofing materials and windows that incorporate photovoltaics and can produce electricity.These efforts are pursued in close collaboration with industry in programs with the Utility PVGroup. Return to Previous Section Continue Chapter four Return to Table of Contents WT03-B20-42IA006-000055-B008-1http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/ch4c.html 138.80.61.12 19970221175459 text/html 18375HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:25:17 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 18203Last-modified: Tue, 09 Jul 1996 02:25:45 GMT NEPP Chapter 4 (part 3 of 3) Chapter 4: Develop a Balanced Domestic Energy Resource Portfolio - continued (Note: Figures referenced in this chapter have not been included at this time.) Stimulating Markets for Renewable Resources The Administration is working throughout the Federal Government to identify and overcomemarket barriers and accelerate market acceptance of cost-effective renewable energy technologies.Integrated Resource Planning programs assist States in evaluating the contribution that renewablescan make to energy supply and environmental protection. Other Federal programs focus thepurchasing power of Federal agencies and help aggregate markets to provide market pull foradvanced technologies such as alternatively fueled vehicles, photovoltaic systems, and geothermalheat pumps. One important example is the Solar Enterprise Zone (SEZ) that has been proposed for theDepartment of Energy's Nevada Test Site. Formerly used for weapons tests, this facility has anexcellent solar resource and efforts are under way to convert it to a solar power generation andequipment manufacturing site. The Enron and Amoco corporations have proposed a joint 100-megawatt photovoltaic manufacturing and generation project for the site. The collaborative Biomass Power for Rural Development Initiative between the Department ofEnergy and the Department of Agriculture is designed to demonstrate the economic andenvironmental viability and sustainability of integrated feedstock production and power conversiontechnologies through partnerships with farm communities to produce reliable supplies of biomassfeedstocks and cost-competitive electric power. A request for proposals has already garnered morethan 350 applications. The Department of Energy's Wind Energy Program provides incentives to utilities, manufacturers,and third-party funders to develop multimegawatt wind energy farms using state-of-the-artmachines. In fiscal year 1996, the program will stimulate up to 150 megawatts of new installedcapacity, make six new advanced wind turbines available, and participate in up to 20 new cost-shared partnerships. The Department of Energy also is helping U.S. renewable energy equipment and services firmscompete internationally by conducting trade missions and sustainable development demonstrationsin large, newly emerging privatized markets to stimulate market pull for these technologies.Markets for photovoltaic, wind, and geothermal electric systems are emerging as especiallyimportant. Special emphasis is placed on markets in Latin America, the Caribbean, Asia, Africa,and Eastern Europe. Through partnerships with industry and international financial institutions such as the World Bank,the Department of Energy has been able to leverage small commercialization investments inrenewable energy technologies to expanding overseas markets. The Department's SolarInternational Program uses a variety of strategies to secure a greater U.S. market share byaddressing trade and other market barriers that continue to impede the acceptance of renewabletechnologies abroad. Coal: Reducing Environmental Impacts The Administration's policy is to help industry develop cleaner and more efficient uses for coal aspart of the U.S. energy portfolio. Because the primary risks of coal use are environmental ratherthan economic or security-related, technology development and deployment that reduceenvironmental impacts are critical to sustaining coal's contribution to U.S. energy needs. The United States relies on coal for 22 percent of its total energy consumption. Fifty-five percentof U.S. electricity is generated from coal, and EIA projects that coal will hold this market sharethrough at least 2010. Domestic coal resources are sufficiently large to sustain economicproduction at high levels for several hundred years. In fact, the United States has more provedreserves of coal on an energy-equivalent basis than the rest of the world has oil. In terms of marketprice for delivered energy, coal is also our least expensive energy source, providing economicalenergy for electric power and industrial needs. Internationally, coal will continue to be a major source of fuel for electric power generation. Manydeveloping countries, such as China and India, will continue to use inexpensive, abundant,indigenous coal to meet growing domestic needs. This trend could greatly increase the use of coalworldwide as economies in these countries continue to expand. In recent years, the major, long-term environmental concern about coal use has changed from acidrain to greenhouse gas emissions primarily carbon dioxide from combustion and, to a lesserextent, methane from mining operations. Coal accounts for more than one-third of U.S. human-made carbon dioxide emissions, or about 4 percent of worldwide greenhouse gas emissions. Supporting Clean Coal Technology Development Technological improvements over the past two decades have reduced significantly theenvironmental impacts of coal combustion, and that trend is expected to continue for many of theconventional regulated air pollutants. Today's technology can capture more than 95 percent ofsulfur dioxide emissions and 99.5 percent of particulates released from coal combustion. Nitrogenoxide emissions, which are an important contributor to ozone (smog) formation, can be reducedsubstantially by new burner technology and other emission controls. Many newer coaltechnologies produce a benign solid waste or, in some cases, commercially marketablebyproducts. While no economic technologies currently exist to remove carbon dioxide from stackgases, high-efficiency technology can cost-effectively reduce carbon dioxide emissions per unit ofenergy supplied. Thus, while major environmental improvements have been made in our ability to use coal, thebiggest impediment to using coal in a sustainable energy future, particularly in developingcountries, is the cost of pollution control. Success in developing cleaner, more efficient and cost-effective technology, coupled with persuading countries to implement environmental regulatorysystems that are responsive to economic and health constraints, will determine coal's ultimate role. The long-term risk of investing in new technologies that have not been demonstrated in multiplecommercial applications inhibits the new technology market to such an extent that Federalresources have been needed to help fund commercial demonstration efforts. The Department ofEnergy's cost-shared Clean Coal Technology Demonstration Program, to which industry hascontributed $2 for every $1 of Federal money invested, has evolved from an early focus onemission control systems to an emphasis in its later rounds on highly efficient, environmentallysuperior advanced power systems. With thermal efficiencies that will approach 50 percent as thesetechnologies mature, compared with 35 percent for current state-of-the-art technologies, these cleancoal technologies can have a measurable effect on reducing carbon emissions. Emissions of acidrain precursors from these advanced technologies are also well below those required for newsources under the Clean Air Act and are especially valuable under the sulfur dioxide emission capsestablished under the Clean Air Act Amendments of 1990. The emission-control technologies of the early rounds of Clean Coal Technology are now beingcommercialized, and the advanced power systems of the later rounds also will reach commercialstatus within a decade. Recent studies by the National Academy of Sciences and the National CoalCouncil conclude that continued Federal support is needed to ensure that these advancedtechnologies will be available. In the future, the Department of Energy will focus its coal R&Dprogram on advanced electric power technologies with efficiencies of 60 percent and 10 to 20percent lower generating costs, making a new generation of power systems available throughgovernment-industry cost-shared investments. By 2010, commercial deployment of advanced coal power systems supported by theAdministration is projected to account for 56 gigawatts of capacity per year in both domestic andexport markets. The Administration supports the efforts of U.S. vendors of clean coal technologies to exportequipment and services, because one key to commercial success would be expanding foreignmarkets for these U.S. equipment makers. The use of clean and efficient U.S. generationequipment, including clean coal technology, will help reduce the global environmental impactsfrom expanded coal-fired generation and create U.S. jobs to develop and build these advancedpower systems. Improving Environmental Outcomes From Mining Operations If not done properly, the mining of coal can also have severe impacts on the environment. The1977 Surface Mining Control and Reclamation Act established the Federal Office of SurfaceMining and Enforcement (OSM) within the Department of the Interior to develop and implementdeferral reclamation standards for all coal surface mines in the United States. The Act alsoprovided States with the authority and funding to implement State programs that meet the minimalFederal requirements. Although controversy has plagued the implementation of the Surface Mining Control andReclamation Act since its inception, much progress has been made during the last 15 years incleaning up abandoned mines and in implementing standards for the environmentally safe miningof coal. To improve the relationship between State and Federal regulators, OSM has developed itsShared Commitment policy with States and tribes to move its regulatory program to a higher levelof success. The goal is to ensure that OSM, the States, and tribes will work together in anenvironmentally safe manner and that land and water will be restored after mining ceases. OSM has also initiated its Appalachian Clean Streams Initiative. The initiative is a cooperativeeffort between several Federal, State, and private agencies to restore streams in the Appalachianregion that have been plagued for decades by severe acid mine drainage from both abandoned andcurrent coal mining operations. Nuclear Energy: Increasing Safety and Preserving Options The Administration's policy is to maintain the safe operation of existing nuclear plants in theUnited States and abroad and to preserve the option to construct the next generation of nuclearenergy plants. The policy is implemented by working with industry to enhance safety and bycontinuing to press for safe storage of spent nuclear fuel. Nuclear energy supplies more than 20 percent of the Nation's electricity supply. Measured levels of safety at U.S. nuclear plants continue to improve, and production costs continue to decrease, asplants increase their efficiency and production capability. Many of these plants are expected to seekrenewed licenses from the Nuclear Regulatory Commission (NRC) for an additional 20 years ofoperation. However, some of them will close at, or even before, the end of their current licenseterms. The continued use of nuclear energy as a part of our energy resource portfolio depends on ensuringthe safe operation of the Nation's nuclear plants and providing an acceptable means of spent fueldisposal. In addition, industry must acquire sufficient experiential data on long-term materialsdegradation during plant lifetimes, as well as experience in implementing NRC requirements forrelicensing U.S. nuclear energy plants, to avoid premature shutdowns, fewer license renewals, and possible delays in orders for advanced nuclear energy plants. Globally, a significant threat to nuclear safety is the continued operation of older Soviet-designedreactors in Central and Eastern Europe and the former Soviet Union. Many Soviet-designed plantslack the high standards of safety demonstrated by the design, construction, and operation ofWestern nuclear energy plants. Working With Industry To Maintain Safety Levels and Preserve Future Options The Administration's policy is to increase operational safety at existing nuclear plants. TheDepartment of Energy will continue to assist utilities in extending the safe, efficient, economical,and reliable operational lifetimes of existing nuclear energy plants in the United States. All suchactivities will be cost-shared with industry on at least a 50-50 basis. The Department also willassist industry in preserving the nuclear option for future generations by participating in thedevelopment of improved, standardized, and certified advanced light-water reactor designs thatoffer improved active and passive safety systems, simplified construction and operation, andgreater economy. In May 1995, the NRC issued a new procedure for applying to extend the license of a nuclearenergy plant, now set at 40 years. The NRC's rule streamlines and simplifies the operating licenserenewal process by eliminating unnecessary requirements, reducing the number of plant systemsand components subject to review, and focusing attention on significant safety issues. The Administration has made achieving the greatest possible degree of global nuclear safety a toppriority. The Department of Energy will lead U.S. participation in international efforts to enhancenuclear safety in Russia, Ukraine, and the countries of Central and Eastern Europe to improve thesafety of Soviet-designed reactors. Current efforts are focused on improving operational safety,reducing near-term risks, enhancing regulatory regimes, and establishing an effective nuclearsafety culture in the workplace. The Department of Energy will also encourage two-waytechnology transfer by facilitating commercial ventures between U.S. companies and countrieswith Soviet-designed reactors to upgrade the knowledge and operational experience of nuclearplant operators in the United States and abroad. The Administration also promotes improving the safety of nuclear energy abroad by activelysupporting the U.S. nuclear industry's pursuit of the multibillion dollar international nuclear energy export market. Because of the demonstrated quality of U.S. nuclear technologies and expertise, substantial opportunities to export U.S. nuclear energy equipment and services exist in many parts of the world, particularly the Pacific Rim, Central and Eastern Europe, and the newly independent states of the former Soviet Union. These technologies and services would include orders for upgrades of nuclear safety systems and other plant components, emergency-related equipment, personnel training programs, environmental cleanup at nuclear facilities, and nuclear waste management. By 1999, the Department of Energy's cost-shared nuclear energy program will produce four NRC-certified advanced light-water reactor designs with passive safety systems and simplifiedconstruction and operation available for the marketplace. Resolving the Problem of Spent Fuel Disposal The accumulation of high-level radioactive wastes from commercial nuclear power installations inthe United States could severely limit the contribution of nuclear energy to the Nation's energyportfolio. The Administration's policy is to expedite the characterization of a geological repositoryas a safe method for high-level waste disposal, and if determined to be safe, to build a geologicalrepository to accept commercial nuclear waste. Ten countries that use nuclear energy to provide a substantial fraction of their energy needs haveselected deep geological repositories for final spent fuel disposal. A consensus appears to haveemerged in the international community that this is the method of choice. By the end of 1998, theDepartment of Energy's Civilian Radioactive Waste Management program will have assessed thetechnical suitability of the Yucca Mountain candidate site as a geologic repository and, if the site issuitable, will license and operate a repository by 2010. Currently, utilities are relying on existing orincremental spent-fuel storage capacity. To provide a standard technology for the safe storage,transportation, and disposal of commercial spent nuclear fuel and to minimize fuel handling, theDepartment of Energy is developing multipurpose canisters for potential use by utilities on theirsites and, subsequently, at storage and disposal facilities. Steady and sustained progress on the U.S. repository effort will reinforce the worldwidefundamental agreement on geologic disposal and help strengthen other nations' resolve to pursuegeologic disposal as the preferred long-term radioactive waste management solution. Our Nation'scommitment to geologic disposal is essential to maintaining confidence in the integrity of ourdomestic once-through fuel cycle, which supports the Administration's opposition to internationalreprocessing alternatives. In addition to providing geologic disposal for spent fuel from civilianreactors, the repository will be designed to accept other high-level radioactive wastes, consistentwith the Administration's nonproliferation goals and its opposition to the reprocessing of spentnuclear fuel. Return to Previous Section Return to Beginning of Chapter 4 Return to Table of Contents WT03-B20-43IA006-000055-B008-100http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/ch6.html 138.80.61.12 19970221175612 text/html 19440HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:26:02 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 19268Last-modified: Tue, 09 Jul 1996 02:25:45 GMT NEPP Chapter 6 Chapter 6: Reinvent Environmental Protection Energy production and use pose significant environmental challenges. Policy approaches must align energy and environmental issues to ensure that economic growth and environmental protection are achieved together. The Administration is reinventing environmental protection, creating regulatory systems that are more flexible and accountable, emphasizing pollution prevention over "end-of-pipe" cleanup, and fostering the development of new energy-efficient technologies to meet both economic and environmental goals. Over the past two decades, energy policy and environmental policy evolved along essentially separate tracks, but growing recognition of the links between energy use and environmental impact has prompted a convergence of energy and environmental policy in the past several years. For example, the Clean Air Act Amendments of 1990 have exerted a profound influence on electric utilities and transportation fuels, while the Energy Policy Act of 1992 emphasized the environmental benefits of increasing energy efficiency and expanding the use of renewable energy. The Administration has accelerated this integration of energy and environmental policy because of the potential economic, environmental, and national security benefits that can flow from adopting a more systemic perspective. The ultimate goal is to create a 21st century America in which economic incentives, environmental objectives, and technological innovation are aligned so that economic growth improves-rather than diminishes-environmental quality. Environmental Protection: Challenges and Opportunities for Reform Much of the present environmental protection system still relies on the command-and-control approach that has evolved over two decades. This approach has resulted in tremendous improvements in air and water quality, but at increasingly high costs as the era of "end-of-pipe" controls begins to yield diminishing economic returns for many key pollutants. This does not mean that such controls are unimportant or cannot be cost-effective. However, there is a recognition that additional environmental protection can often be obtained far more cheaply by examining pollution-creating processes and activities for abatement opportunities and by focusing on preventing pollution rather than on cleaning it up. The current regulatory system sometimes undermines energy policy goals. For example, environmental regulations sometimes require technologies that increase operating energy demands while disregarding the environmental implications of the extra power generated to run the equipment. Or the regulations may simply shift pollution from one medium to another, necessitating additional cleanup. Efficient energy technologies are sometimes discriminated against under environmental regulations, even though they reduce energy use and waste. New technologies offer the best prospects for simultaneous environmental and economic gains-but markets and regulatory systems need to encourage their development and adoption. The specificity and rigidity of many of our Nation's environmental statutes and regulations have blocked the adoption of more creative environmental solutions, have driven up compliance costs unnecessarily, and have stifled innovation in technologies and practices. The Administration recognizes the need to move away from the one-size-fits-all approach to regulation and to engage industry in deciding how to most efficiently and flexibly reach environmental goals. A New Approach to Environmental and Energy Policy The Administration supports an innovative slate of policies to further environmental and energy goals, including reinventing the regulatory system, encouraging cost-effective pollution prevention, and fostering the development of new energy-efficient technologies. The principles at the heart of the Administration's approach to environmental protection are common sense, cost-effectiveness, and a firm commitment to strong environmental goals combined with flexibility in how we reach those goals. The three components of the Administration approach are the following: Reinvent the regulatory system to increase flexibility and adopt performance-based approaches to augment or replace command-and-control measures. Additional flexibility can improve environmental performance and cut costs to industry. This must require new accountability mechanisms to ensure that goals are attained. Encourage cost-effective pollution prevention through partnerships with industry that identify innovative methods for improving environmental performance and provide incentives for pursuing new approaches. An array of voluntary programs that encourage energy efficiency, renewable energy resources, emissions offsets, and other investments has gained enthusiastic industry support and is producing demonstrable benefits in cost-effective pollution reduction. Foster the development of technologies that will increase environmental protection as well as provide U.S. industry with new technologies for export into the global market. Because flexible approaches to pollution prevention tend to blur the traditional distinctions between environmental and energy technologies, Federal research and development programs and partnerships focus heavily on technologies that simultaneously increase productivity, reduce pollution or waste, and cut energy use. Reinventing Environmental Regulation Administration policy increases the flexibility with which we achieve our environmental objectives, without sacrificing accountability or environmental quality. The Reinventing Environmental Regulation initiative announced by President Clinton in March 1995 outlined a new approach to regulatory action, one that recognizes that pollution is often a sign of economic waste and that focuses efforts oncost-effective pollution prevention. Regulations that provide flexibility-but require accountability-can provide greater protection at a lower cost. The initiative also emphasizes collaborative processes over adversarial ones for decisionmaking and compliance. The initiative includes 25 high-priority actions directed both at improving the regulatory system and at generating the building blocks for the regulatory system of the future. These actions, which are being applied across many industries, will have a direct impact on energy production and use and will result in more competitive industries and lower energy costs. Improvements in the Current System. The Administration is undertaking a number of initiatives to make the present regulatory system more flexible and effective. Efforts such as increased open-market air-emissions trading and one-stop emission reports can reduce industry costs while maintaining existing levels of environmental compliance. Reexamining existing regulations to maximize environmental quality while eliminating excessive burdens on industry has led the Administration to refocus the implementation of the Resource Conservation and Recovery Act on high-risk wastes, to consolidate Federal air-quality rules, and to pursue regulatory negotiation and consensus-based rulemaking. In addition, priorities for enforcement of regulations will now be based on risk. These actions will directly affect the energy-related industries by decreasing costs and making environmental compliance simpler through increased flexibility. At the same time, environmental quality will be ensured and efforts will be focused on the highest areas of risk. Building Blocks for a New System. Setting performance standards and allowing the regulated community to find the best way to meet those standards can get results more cheaply, quickly, and cleanly than mandating design standards or specific technologies. This promotes both lower cost environmental protection and innovation in pollution prevention and control technology. Performance standards can also reward energy-efficiency investments that also reduce pollution. Accountability and responsibility must accompany this increased flexibility to ensure that our Nation's environmental goals are being met. Where environmental and economic goals can be best reconciled by giving maximum flexibility to industry, the most flexible tool is the concept of performance agreements between business and regulators. The Environmental Protection Agency has developed a coordinated series of demonstration projects designed to provide facilities, industrial sectors, communities, and Federal agencies with opportunities to implement alternative management strategies. Project XL. Through Project XL (Excellence and Leadership), the Federal Government, working in cooperation with States and selected firms, will test a new approach to environmental management. The goal of this initiative is to improve the environmental performance of U.S. industries by providing the option of following an alternative path to existing regulatory-driven compliance. The pilot program will require reductions in discharges below current regulatory standards in exchange for greater flexibility in achieving environmental objectives. The key elements of this initiative are performance, flexibility, accountability, and partnerships. The components of the process are: (1) an environmental review of the facility; (2) development of an alternative strategy to achieve emissions reductions beyond current standards; (3) community involvement throughout the strategic planning and goal-setting process; (4) Environmental Protection Agency approval, whereupon the plan replaces many existing regulatory requirements; (5) establishment of alternative monitoring and reporting mechanisms to allow auditing and regular public reporting of the facility's environmental performance. Alternative strategies for sectors. Through the use of industry covenants and other enforceable agreements, the Environmental Protection Agency and several industries will demonstrate how adjustments and modifications in environmental regulatory requirements can achieve more cost-effective environmental results. The industries involved in the Common Sense Initiative will provide the first opportunities to test this approach. Coordination with petroleum refiners through this initiative to reduce the cost of compliance and increase productivity assists the industry, ensures environmental quality, and helps keep energy prices affordable. In addition to performance-based standards, the Administration is examining new tools for both government and industry to use in implementing new environmental management systems. Tools such as third-party audits for industry compliance and multimedia permitting will streamline compliance procedures. Encouraging Pollution Prevention in the Energy Sector The Administration is exploring ways that the Federal Government can work with industry and market forces to encourage environmental improvement while stimulating markets. In some areas, market forces can simultaneously contribute to environmental protection, increased productivity, and continuous improvement in technologies and practices. Cost-effective energy-efficiency investments, for example, can save money, reduce pollution, and reduce the need for expensive additional pollution controls. Likewise, some renewable energy sources can displace the need for fossil fuels that have greater environmental impact. Unfortunately, markets often do not sufficiently encourage these investments, primarily because of two factors. First, the market price of energy does not always reflect the full costs of use, including environmental impacts, which leads to underuse of efficient technologies. Second, information on the latest technologies is not always readily available, preventing market participants from making the best decisions. Where markets do not fully recognize the environmental benefits of increased energy efficiency, voluntary programs can help create incentives for technology development, stimulate energy-efficiency investments, and disseminate information. Voluntary programs have emerged as a key tool in addressing environmental problems without statutory mandates. For example, carbon dioxide is the dominant greenhouse gas, but is not currently subject to regulatory requirements. The Administration has pioneered a voluntary market approach to reduce greenhouse gas emissions cost-effectively. Climate Change Action Plan. Climate change raises complex environmental and energy issues. It also offers an opportunity for new approaches, because few Federal regulations limit greenhouse gas emissions. The Climate Change Action Plan initiated a host of voluntary programs, including Climate Challenge and Climate Wise, that will reduce greenhouse gas emissions and boost productivity and profits. Climate Challenge. Under this voluntary program, which was developed jointly by the Department of Energy and the electric utility industry, utilities agree to reduce, avoid, or sequester greenhouse gas emissions using flexible individualized plans to achieve the reductions in the most cost-effective way. The program recognizes that utilities are in the best position to determine how to reduce emissions cost-effectively in their own systems. To date, 450 utilities have joined the Climate Challenge program, pledging to substantially reduce greenhouse gas emissions by the year 2000. Climate Wise. Climate Wise, which is being carried out by the Environmental Protection Agency and the Department of Energy, is designed to stimulate industrial emission reductions. Industrial companies representing almost 4 percent of U.S. industrial energy use have become partners in Climate Wise and have pledged to reduce emissions significantly over the next several years. Information programs also play a key role in encouraging the reduction of greenhouse gas emissions. Under the authority contained in Section 1605 of the Energy Policy Act of 1992, the Department of Energy has issued guidelines for anyone, including participants in Climate Challenge, Climate Wise, and other voluntary programs, to record the results of emission-reduction actions. This mechanism allows industry partners to demonstrate that they have attained the performance goals to which they have agreed, and it provides a way for any business or organization to record the greenhouse gas emissions from their operations and consider efforts to reduce emissions. Environmental benefits are also realized through other Administration programs to enhance efficiency and increase productivity while reducing environmental impacts. Examples of voluntary programs established by the Department of Energy and the Environmental Protection Agency are the Motor Challenge and Green Lights programs (see Chapter 3), which increase energy efficiency while decreasing the environmental impacts of excess energy use. The Climate Change Action Plan was constructed to meet the U.S. goal of returning net greenhouse gas emissions to 1990 levels by the year 2000, using economic growth, energy price, and Federal budget assumptions based on 1993 projections. Because these projections have shifted, and since the Plan relies heavily on the performance of voluntary programs whose outcomes are subject to market uncertainty, the Administration will conduct an assessment of the Plan's progress in the fall of 1995. Clean Cities Program. An example of partnerships at the community level is the Clean Cities program. This program leverages the Federal purchase of alternative-fuel vehicles into a coordinated effort to overcome infrastructure barriers for alternative-fuel vehicles while assisting local governments with environmental mandates. In this voluntary program, cities that are not in compliance with the Clean Air Act are encouraged to partner with the Federal Government to establish fleets of alternative-fuel vehicles. While this program was designed to provide the market pull for private-sector infrastructure investment in alternative fuels (see Chapter 3), it is targeted to assist cities that are working to comply with Federal air pollution standards. Fostering Technology Development The Administration supports research and development on energy technologies that improve environmental performance, and assists in commercializing these technologies to help industry increase competitiveness and decrease pollution. An environmental technology strategy that helps industry shift from waste management toward pollution prevention, more efficient resource use, and industrial ecology models will increase long-term economic growth and improve and sustain the environment. The Administration's environmental technology strategy helps companies become more competitive by lowering their energy and resource needs while reducing or eliminating their waste cleanup or disposal costs. Nationally, the strategy will spur economic growth by capturing the rapidly growing market for clean technologies and shifting money from consumption of resources to investment in new plants and equipment. Globally, it will help developing countries to use sustainable technologies in many industrial and service sectors more rapidly. As energy and environmental policies converge, technology is one clear bridge between the two. A wide array of technologies can improve environmental performance, not just conventional pollution-abatement technology. Technologies that increase the efficiency of energy use or minimize waste in energy production invariably reduce the environmental impacts associated with those activities. These include technologies that increase energy efficiency, reduce materials consumption, improve product quality and durability, and provide substitutes for energy-intensive products and processes. Where market forces are slow to stimulate these developments, the Administration has embarked upon a National Environmental Technology Strategy to accelerate the adoption of environmentally superior technology. The National Environmental Technology Strategy outlined in Bridge to a Sustainable Future is a set of policies and programs that will establish a new course for the development and use of environmental technologies into the next century. The strategy expands the environmental tool kit, replacing those instruments that are no longer effective with a new set of tools designed to meet today's challenges and tomorrow's needs. Return to Table of Contents WT03-B20-44IA006-000055-B009-52http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/notes.html 138.80.61.12 19970221175821 text/html 4852HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:27:59 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 4681Last-modified: Tue, 09 Jul 1996 02:25:45 GMT NEPP Notes Notes 1. Except where another source is directly or indirectly identified, the statistics used in Chapters 1 and 2 are from published Energy Information Administration (EIA) reports or from the data bases used to develop those reports. The primary sources of data are the following EIA reports: Annual Energy Outlook 1995 [DOE/EIA-0383(95)], January 1995; International Energy Annual 1993 [DOE/EIA-0219(93)], May 1995; Annual Energy Review 1994 [DOE/EIA-0384(94)], July 1995; International Energy Outlook 1995 [DOE/EIA-0484(95)], June 1995. EIA is an independent statistical agency within the Department of Energy; other than providing its most recent data, EIA played no role in the formulation of value judgments or development of policies presented in this document. 2. Different crude oils and petroleum products have different energy (or heat) content. For example, 1 gallon of distillate fuel oil contains almost 11 percent more energy than 1 gallon of motor gasoline, and 1 barrel of imported crude oil contains about 2.7 percent more energy than 1 barrel of domestically produced crude oil. Therefore, computations of petroleum import dependence differ depending on whether the measure is volume or energy content. In terms of volume, the United States imported 8.9 million barrels per day of crude oil and petroleum products in 1994, and we exported 0.9 million barrels per day. Net petroleum imports were thus 8.0 million barrels per day. Also during 1994, U.S. consumption of gasoline, fuel oil, and other petroleum products totaled 17.7 million barrels per day. Therefore, on the basis of volume, net imports accounted for 45 percent of consumption. In terms of energy content, however, import reliance was more than 49 percent. This is because the 8.0 million barrels per day of net petroleum imports over the year contained 17.1 quadrillion Btu (quads) of energy, and the 17.7 million barrels per day of consumed petroleum products contained 34.7 quads. 3. Energy-related CO2 emissions published in the Annual Energy Review 1995 are 92 million metric tons higher than those estimated in the Climate Change Action Plan published in October 1993. Most of this difference is attributed to differences in assessments regarding the impact of the Plan. EIA prorates the impact of the Plan based on the funding received in fiscal year 1995, which was substantially less than the Administration's request. Other factors contributing to EIA's projection of higher CO2 emissions include higher economic growth and lower energy price assumptions than those used in 1993. An update of the Plan is scheduled for the fall of 1995. 4. For an analysis of gains in energy savings since 1972, see U.S. Department of Energy, Office of Policy, Energy Conservation Trends: Understanding the Factors Affecting Energy Conservation Gains and Their Implications for Policy Development [DOE/PO-0034], April 1995. That study concluded that the U.S. economy used 30.6 quads less in 1991 than if the overall energy intensity of the U.S. economy had remained at its 1972 level. The annual primary energy savings in 1991 were worth approximately $250 billion, and the cumulative energy savings over the 1972-91 period were worth approximately $2.5 trillion (in 1991 dollars). 5. See, for example, World Energy Council (WEC) Commission, Energy for Tomorrow's World-the Realities, the Real Options and the Agenda for Achievement (New York: St. Martin's Press, 1993), which analyzes four energy cases "believed to cover the broad range of likely outcomes for global energy demand and supply to 2020." 6. This framework is described in key Administration policy documents such as Science in the National Interest, Technology for America's Economic Growth, and Technology for a Sustainable Future. 7. These sources include (1) Secretary of Energy Advisory Task Force on Alternative Futures for the Department of Energy National Laboratories (Robert Galvin, chairman), Alternative Futures for the Department of Energy National Laboratories, February 1995, and (2) Secretary of Energy Advisory Board Task Force on Strategic Energy Research and Development (Daniel Yergin, chairman), Energy R&D: Shaping Our Nation's Future in a Competitive World, June 1995. 8. See, for example, World Energy Council (WEC) Commission, Energy for Tomorrow's World-the Realities, the Real Options and the Agenda for Achievement. Return to Table of Contents WT03-B20-45IA006-000057-B017-375http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/ch3bx3.html 138.80.61.12 19970221185623 text/html 1741HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:26:22 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1570Last-modified: Tue, 09 Jul 1996 02:25:46 GMT Innovations in Buildings Technology Innovations in Buildings Technology During the 1970s, the Department of Energy developed the low-E window coating, which remains transparent to visible light but reduces heat gains. By 1993, about one-third of residential and one-fifth of commercial building window sales employed low-E coatings, resulting in energy savings to U.S. building owners of more than $1.8 billion. The flame retention head oil burner, which results in a 10-percent reduction in consumption of oil in residential furnaces and boilers, was developed with Department of Energy support. This technology has captured virtually 100 percent of the oil heating market in the United States. An advanced supermarket refrigeration system developed by the Department of Energy is now marketed by all major manufacturers, has a 50-percent market share, and offers a 20-percent energy savings over the conventional system. A transpired solar collector, developed with Department of Energy support, preheats incoming building ventilation air, and reduces heating loads to conventional equipment. The panel collectors transfer 75 percent of the heat of incident solar energy. The panels sell for $10 per square foot and can pay for themselves in as little as 3 years. Return to Chapter Return to Table of Contents WT03-B20-46IA006-000057-B017-428http://lacebark.ntu.edu.au:80/j_mitroy/sid101/sustain/ch3bx4.html 138.80.61.12 19970221185720 text/html 996HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:27:14 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 826Last-modified: Tue, 09 Jul 1996 02:25:46 GMT Energy and Business Productivity Energy and Business Productivity Energy is second in cost only to personnel in many government and business operations. Reducing energy use goes straight to the bottom line. If employee productivity is increased simultaneously, the benefits can more than double. At the U.S. Post Office in Reno, Nevada, improvements in lighting reduced energy costs and, more significantly, directly improved mail sorting efficiency by 6 percent. This is one of numerous examples of increased productivity as a direct result of energy efficiency. Return to Chapter Return to Table of Contents WT03-B20-47IA006-000057-B018-116http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind/tehfaq.html 138.80.61.12 19970221185836 text/html 9803HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:28:54 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 9632Last-modified: Tue, 09 Jul 1996 02:26:10 GMT Answers to Questions about Tehachapi Wind Turbines Answers to Common Questions about Wind Turbines in the Tehachapi-Mojave Wind Resource Area Copyright by Paul Gipe. All rights reserved. The Tehachapi-Mojave Wind Resource Area is in the Tehachapi Mountains of Southern California about 100 miles north of Los Angeles. The following questions and their answers are from a brochure distributed to visitors of the Tehachapi Pass. Why Are They Here? Because the Tehachapi Pass is one of the windiest areas in the world. Wind turbines have also been installed in the San Gorgonio Pass southeast of Los Angeles, and in the Altamont Pass east of San Francisco. How Many Wind Turbine Are There? Of the nearly 16,000 wind turbines in the state more than 5,000 are in the Tehachapi Pass. What Do They Do? Collectively the wind turbines generate electricity much like a conventional power plant except that their fuel--the wind--is both free and inexhaustible. Since they are open-air power plants the term "wind power plant" or "wind power station" is more descriptive than the term, "wind farm," sometimes used. When is the Best Time To See Them Turning? The wind normally peaks during the afternoon when heating on the nearby Mojave desert is greatest. (During the early morning hours winds are usually light.) Spring and early summer are the windiest seasons when winds can reach gale force for several days on end. What Kind of Electricity Do They Produce? The wind turbines generate electricity identical to that produced by the local electric utility. The electricity is carried from Tehachapi on the utility's transmission lines. Who Uses The Electricity? Customers of Southern California Edison Company. The wind power plants sell their electricity to SCE. The utility then distributes the electricity as it would that from any of its own power plants. On a moderately windy day Tehachapi's wind turbines will meet the needs of the entire Antelope Valley and export electricity to Los Angeles. How Much Energy Do They Produce? Tehachapi's wind power plants generate more than 1 Terawatt-hours (1,000 million kilowatt-hours) yearly--enough electricity to meet the needs of more than 500,000 Southern Californians. How Many Kinds Are There? There are more than two dozen different kinds of wind turbines operating in California today. Most use a propeller that spins about a horizontal axis. The hoop-shaped Darrieus or "Eggbeater" turbines account for 5% of the state's wind machines. Who Makes Them? California's wind turbines were made by firms in the U.S., Canada, Denmark, Germany, Great Britain, Japan, and the Netherlands. Who Paid For Them? Private and institutional investors. (The utility only buys the electricity.) Some wind turbines have been installed by the wind companies themselves with conventional financing from banks and leasing companies. What Do They Cost To Operate?Studies by electric utilities have found that it costs about 1 cent per kilowatt-hour to operate and maintain modern wind power plants. For comparison, it costs twice as much to operate, maintain and fuel nuclear-powered and coal-fired plants. Are Wind Turbines Used Anywhere Else? Wind power plants are in regular commercial service in California, Canada, Denmark, Germany, Great Britain, Ireland, India, and the Netherlands. Wind turbines in California produce about 40% of the world's wind-generated electricity. Do All the Wind Turbines Operate at the Same Time? Seldom. It's unlikely that all of the machines will be operating at the same time because of variations in the wind and differences between turbines. Most turbines in the Tehachapi Pass will be spinning when there are winds between 15 and 50 mph (7 m/s and 25 m/s). Often there will be turbines out of service for routine maintenance. How Strong is the Wind in the Tehachapi Pass? The winds through the pass average from 14 to 20 miles per hour (7 m/s to 9 m/s) from one year to the next. Wind speeds vary with the terrain, season, and time of day. Do the Turbines Operate Automatically? Yes. Each wind turbine has its own electronic "brain" and thinks for itself. They turn themselves on when conditions are right, and they turn themselves off when the winds are too strong, or when they sense a problem. Most turbines change directions as the wind changes. The Eggbeater or Darrieus turbines accept the wind from all directions. Are They Computer Controlled? Though all wind turbines are capable of "thinking" for themselves, some at the larger, more modern wind power plants are connected to a central computer. An operator at the console is then able to turn the turbines on and off at will, similar to that at most modern power plants. (There must be wind before the turbines will spin.) How Long Do They Last? No one knows for sure. The turbines are designed to operate 20 years. During that time the blades, transmissions, generators, and other moving parts will be rebuilt or replaced. Because of California's dry climate the metal structure supporting the turbines should last indefinitely. Is Wind Energy Economic? Yes. The technology has advanced dramatically since 1981, when the first wind turbines were installed in the state. The cost of wind turbines has dropped from about $4,000 per kilowatt to about $1,100 per kilowatt today. Meanwhile their productivity has increased by nearly three times. Do the Wind Turbines Lower Electric Rates? Not directly. Instead they help hold off higher rates by stabilizing electricity costs and by providing fuel diversity. Electric utility systems insure low costs and reliability by using a mix of generating sources to maximize the advantages of each technology. Wind power plants are now an integral part of this mix. Do Wind Turbines Help Clean the Air? Yes. By producing electricity cleanly California's wind power plants reduce pollutant gases that contribute to smog, acid rain, and global warming.Each year Tehachapi's wind power plants prevent the emission of more than 3 billion (3,000 million) pounds of sulfur oxides, nitrogen oxides, particulates, and carbon dioxide that would otherwise be produced. What are the Air Quality Benefits Worth? The air quality benefits alone are worth from 1 cent/kWh to 14 cents/kWh. These benefits are not incorporated into the price paid for wind-generated electricity. (Nor are these costs charged to conventional sources.) How Does Wind Rank Among the California's Power Plants? Wind power plants produce about 1% of the state's electricity. As a state with 10% of the nation's population and the world's seventh largest economy, California uses a staggering amount of electricity. California emits nearly as much global warming gases as the entire nation of France. How Much Energy Could Wind Turbines Provide? The California Energy Commission estimates that wind energy could provide 10% of the state's electricity. To reach this goal properly-spaced wind turbines would need to occupy about 0.1% of the state. They "use" even less land. Do Wind Turbines Cause Urban Growth? No. They help preserve open space. A study by the University of California's Department of Environmental Design found that wind power plants, because they represent long-term capital investments in the rural economy, may preclude the "parceling out" of land for large-scale real-estate development Can the Land Still Be Used For Other Purposes? Yes. Grazing, farming, and recreational land uses can coexist with wind power plants. Cattle grazing remains an important land use among the 6,000 wind turbines in the Altamont Pass. In Tehachapi the Pacific Crest Trail winds its way among the wind turbines atop Cameron Ridge. And within some wind power plants in Northern Europe, where land is at a premium, the land is tilled to the base of each tower. In Denmark farmers and homeowners install wind turbines, like those used near Tehachapi, in their back yards to meet their own needs. Prepared for the Kern Wind Energy Assocation P.O. Box 277 Tehachapi, CA 93581 ph: 805 822 7956 fax: 805 822 8452 Linda White, Executive Director kweawhite@aol.com KWEA represents the wind industry in the Tehachapi-Mojave Wind Resource Area. Guide More information on wind energy. Wind Energy Comes of Age Wind Power for Home & Business Workgroup Windturbines pgipe@igc.apc.org The URL for this site is http://keynes.fb12.tu-berlin.de/luftraum/konst/windfarm/tehqna.htmlWT03-B20-48IA005-000051-B017-387http://lacebark.ntu.edu.au:80/j_mitroy/sid101/gh1/ghblues.html 138.80.61.12 19970221151725 text/html 56103HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:47:38 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 55931Last-modified: Tue, 09 Jul 1996 02:25:43 GMT The Greenhouse Blues                GREENHOUSE BLUES               "A permanent modern scenario: apocalypse looms,               and it doesn't occur... Apocalypse has become an               event that is happening, and not happening.  It               may be that some of the most feared events, like               those involving the irreparable ruin of the               environment, have already happened.  But we don't               know it yet, because the standards have changed.                Or because we do not have the right indexes for               measuring the catastrophe.  Or simply because this               is a catastrophe in slow motion.                                                    -- Susan Sontag [1]     The 1990 edition of the Worldwatch Institute's annual State ofthe World [2] claims we have about 40 years to establish a sustainableeconomy -- or be engulfed in a self-perpetuating spiral of ecologicaland economic decline.  It's an odd, precise figure, the sort usuallydiscounted as a product of apocalyptic excess.  These days, though, itseems to warrant extra consideration.  For one thing, the elements ofthe projected catastrophe -- from pollution, extinction, andpopulation growth to deforestation and the ecological costs of energyproduction -- have become depressingly familiar.  For another, thepower, inertia, and sheer dilapidation of the global economy havebegun to sink in, as have the depth and power of institutionalizedmilitarism.     Twenty years after the first Earth Day, we are then perhaps only40 years from the abyss.  If the greenhouse theorists are right -- andthe bulk of the evidence suggests that they are -- local ecologicalcrises must be seen in the light of, and indeed as elements of, theeven more threatening catastrophe of climatic destabilization.  Thisaggregation offers the perverse satisfaction of rendering equivocationand half-measures absurd, but it may also set the stage for a tragicdenouement that has been centuries in the making.     Death, to misquote Samuel Johnson, concentrates the mindwonderfully, and it's for just this reason that global warming makesso potent a subject of reflection.  The frames within which it ispresented reveal, with rare clarity, the presumptions and prejudicesof the framers.  This is true in the media battle between those whoemphasize the near-consensus among scientists on the warming and thosewhose stories abet equivocation and business-as-usual.  And it's truein the environmental movement, where the warming, though unproven, istaken dead seriously, and the debate centers on its political andcultural significance.THE SCOPE OF THE THREAT     The major consequences of a rapid greenhouse warming have beenenvisioned in detail: dying reefs, withering forests, seas of refugees(many of them leaving the United States), extinction, drought, andcrushingly violent storms.  Still, and despite the clarity of suchforesight, uncertain and prospective events are difficult to forceonto the policy agenda -- especially in the United States, where muchof the ruling elite regards the costs of prevention as a threat moredire than the warming itself.         The bulk of the greenhouse warming is caused by carbon dioxide(CO2), chlorofluorocarbons (CFCs) and methane, gases which have beencalled "nontraditional" pollutants because they take their toll not bydespoiling isolated regions but by accumulating globally anddestabilizing the biosphere as a whole.  CO2, which because of itssheer abundance is the most damaging of the greenhouse gases, is anunavoidable product of fossil-fuel consumption -- and the cheap, easyenergy available from fossil fuels has played a major and largelyunappreciated role in the history of industrial capitalism.  Now, itseems imperative to drastically reduce the use not only of methane andCFCs, but of the fossil-fuels as well, even though this reductionimplies a radical break with the energy economy that has underlaincapitalism from its very earliest days.  As the Gulf War has onceagain shown, this break will not come easily.     There is -- and this is crucial -- much that can be done, thoughnot an indefinite time to do it in.  This is clear from the gapbetween "bad case" and "good case" scenarios, both supplied by theWorld Resources Institute [3].  In the bad case, no substantive effortis made to slow C02 emissions, increase energy efficiency, or speedsolar development, although CFC production and use is brought in linewith the Montreal Protocol.  The outcome?  By 2020, an average globaltemperature of from 1.8 to 5.4 degrees Fahrenheit greater than it wasa century ago, and by 2075 a rise of 5.8 to 17.5 degrees!  If coal useincreases as well -- a real possibility, particularly in China -- thewarming could soon reach a truly staggering magnitude.  (Even thisisn't the worst case scenario, since there's a possibility of a "supergreenhouse effect" set off by a relatively small warming and a seriesof "positive feedbacks" on it.  Melting permafrost ice, for example,could release massive amounts of methane, an extremely powerfulgreenhouse gas.)     In the "good case" scenarios, such as they are, strong globalefforts to reduce greenhouse-gas emissions allow the atmosphere tostabilize.  Oil, gas, and coal prices are sharply increased, percapita energy use declines in industrialized countries, and supportfor solar development is radically increased.  Tropical countries notonly stop decimating their rain forests but begin large-scalereforestation efforts.  And so on.  The World Resources Institutemodel indicates that, even if all these heroic efforts had begun in1988, there would still be a warming of 3 to 7 degrees Fahrenheit by2075, when the temperature would level off.  Such a temperatureincrease, over such a short period of time, is unprecedented in humanhistory, and could well be large enough to cause massive ecologicaland social disruptions.  According to the First Assessment Report [4]of the scientific committee of the Intergovernmental Panel on ClimateChange (IPCC), the U.N. agency chartered with the task of coordinatingglobal warming policy, the "state of scientific knowledge" indicatesthat such a "stabilization" is the best that can be expected, andwould require "an immediate reduction in global man-made [CO2] emis-sions by 60 to 80%."       So, if greenhouse theory is valid, the whole global economy mustbe restructured, fundamentally and soon.  The longer marketeers pushtheir phony optimism, the longer politicians dispute, equivocate, andlie, the longer technicians strain for faith in the viability of halfmeasures, the worse the odds will get.  Not that these are the lastdays -- humans are adaptable and will likely survive (which is morethan can be said for lots of other species).  But each moment wastedis another step along a path to an ugly and inauspicious future.     As the predictions of greenhouse theory have become familiar,they -- like ecology in general -- have emerged as key social metrics.We now know, for example, that ours is not a "sustainable" society,and as the ecological crisis worsens this charge becomes damning in amanner that somehow exceeds all prior accusations.  It's no wonder,then, that ecological and climactic science are clotted with politics,or that the media frames imposed on greenhouse theory often emphasize"uncertainty," cost, and the putative dangers of "over-reaction."  Theissues here are difficult, for science resists reduction to ideology,even as it can offer ideology its firmest ground.  Witness theinfluence of John Sununu, architect of U.S. greenhouse policy andprobably the first high U.S. official to hold a PhD in fluid dynamics,a PhD he uses to disguise a corrupt and technocratic optimism aslegitimate scientific skepticism.     The outlines of at least the immediate future are clear.  TheBush Administration has distinguished itself by the vehemence of itsopposition to meaningful anti-greenhouse policies, while every otherwealthy industrial country has taken at least tentative steps towardstabilizing or reducing CO2 emissions by the year 2000.  Germany, NewZealand, Denmark, and Australia have committed themselves to emissionreductions of at least 20 percent by 2005, and though this is causefor hope, global negotiations have been all but paralyzed by the U.S.delegation. (The first U.N. session, held February 1991 in ChantillyVirginia, was spent, by U.S. insistence, on procedural matters.  Allsubstantive proposals were deferred.)  Meanwhile, in the Third World,particularly India and China, efforts to engage the greenhouse issuehave been caught up in charges of "environmental colonialism."  And inthe old Soviet bloc, particularly the Soviet Union itself, rapidprogress does not appear likely.  All in all, it's fair to say thatthe politics of the greenhouse are inseparable from the politics ofthe larger world system -- which suffers its fossil-fuel dependency asonly one element in a rich variety of debilitating legacies.ENVIRONMENTAL OPTIMISM, ECOLOGICAL DESPAIR     The most compelling of the new greenhouse books are probablyStephen Schneider's Global Warming: Are We Entering the GreenhouseCentury?, Michael Oppenheimer and Robert Boyle's Dead Heat: The RaceAgainst the Greenhouse Effect and Bill McKibben's The End of Nature[5].  Taken together, they form a grim syllabus, not only about globalwarming but also about the depth of the environmental movement's breakwith systemic political analysis.  Each chooses, in its own manner andfor its own reasons, to present the erosion of climatic stability bythe fossil-fuel economy -- coal and oil! -- with barely a nod to thepolitical, economic, or ideological dynamics of the society withinwhich it developed.     Stephen Schneider, head of Interdisciplinary Climate Systems atthe National Center for Atmospheric Research, is among the most famousof the greenhouse scientists, and serves well to reveal the contoursof mainstream environmentalism.  Schneider cuts an interesting figure-- the liberal public scientist as statesman of the greenhouse age. His world is big science and big government, and though he spends agood deal of time debating Reaganesque bureaucrats and politicians, hedoesn't allow himself to lose hope.  Trained in the habits ofrationalism, he sees an emergency and thinks the need for acooperative global response will soon be undeniable.  He is apragmatist, as he believes the times require.     In Scientific American's special "Managing Planet Earth" issue,Schneider argued that conversion to a post-greenhouse economy wouldcost "hundreds of billions of dollars every year for many decades,both at home and in financial and technical assistance to developingnations," yet he was confident that this was a realistic possibility. Anticipating a pessimistic reading, he argued that pessimism was tooeasy, that not long ago "a massive disengagement of NATO and WarsawPact forces in Europe also seemed inconceivable." [6]  And so it did,to many.  But today, with the flush of post-Cold War euphoria longpast, the case for pessimism is again obvious.  It is well known thatworld military expenditures, at almost a trillion dollars per year,could feed and educate the entire human race.  They could also build apost-greenhouse economy.  The question is whether they'll ever bespent doing so.       Optimism can be dangerous.  When the Worldwatch Institute's ten-hour series, The Race to Save the Planet -- which took five years and$7 million to produce -- was recently broadcast in the San FranciscoBay Area, one local critic remarked that it devoted so much time toupbeat segments (designed, presumably, to avoid spreading doom andgloom) that it left the impression that the situation wasn't that grimafter all.  The producers had stumbled in the great, precarious tug-of-war of ecologic politics, which, despite widespread grassrootsactivism and an increasingly coherent analysis, is torn betweendespair on the one side and liberal optimism on the other.       Here's an extreme dose of inspirational liberal optimism, fromDead Heat, written by Michael Oppenheimer, a senior scientist at theEnvironmental Defense Fund, and Robert Boyle.  This text was reprinted(as "Techno-Hope") in the 1990 Earth Day issue of Mother Jones:     The change began in 1992 when the nations of the world, under the     guidance of the United Nations Environmental Program, signed an     agreement to limit greenhouse gases.  Cynics scoffed at a     "toothless" accord, but they failed to notice the underlying     currents.  Battered by foreign economic invasion and constrained     by a limited budget, the United States canceled the Super     Collider, the Space Station, and Star Wars and diverted billions     to research and education.  By the mid-1990s, it had concluded a     series of treaties with the Soviet Union, which led gradually to     a mutual reduction of forces.  But the expected blow to the     defense contractors never materialized because the government     poured some of the billions of dollars saved into procurement of     solar energy sources, and several large companies quickly     converted to renewable-energy development.  The trend accelerated     when the Great Drought of the late nineties struck and industry     began the total phaseout of fossil fuels.  Now rich nations     compete to supply smaller countries with solar cells rather than     weapons.          One must ask the purpose of such prose.  Should it be excused asbad exhortatory fiction, or is it true to the dreams of Washingtonenvironmentalists?  If this the hope they can imagine, it's no wonderthat "radical environmentalism," as it has emerged in the last decade,has chosen pessimism.       Examples of radical pessimism are easy to come by, but BillMcKibben's The End of Nature is distinguished by its vivid evocationof greenhouse crisis and by its relentless despair.  McKibben is anapostle of deep-ecological radicalism.  Not being a scientist, he neednot posture at objectivity -- yet neither does he accept political andeconomic views.  To even name this society as "capitalist" and insistthat the distinctions made thereby are significant strikes him asabsurd.  The warming is only the last in a long series of attacks onthe environment, and politics too small a thing to place against theend of wilderness.  We can and should resist -- with direct action, byconsuming less, wearing sweaters around the house, not having children-- but it won't be enough, not really.  Soon we'll find ourselvesalmost alone, with only the plants and animals we've chosen to save,on a desiccated and lonely world.     It's a view so tragic as to be almost absurd, were it not for itsdesire to face the whole grim truth.  With the U.S. Administrationbent on resisting any real greenhouse treaty, there's a strongtemptation to see the measures being taken elsewhere -- like Germany'scommitment to a 25 percent cut in CO2 production by the year 2005 --as substantial alternatives.  But are they?  The Third World is nowwell along on the road of fossil-fuel industrialism -- will theEuropeans pay to help it go solar instead?  In the next 40 years? McKibben knows of the calls for massive military cuts and a crashcampaign of conversion to a post-greenhouse economy; he just doesn'tthink it's going to happen."UNCERTAINTY" AS IDEOLOGY     In February 1990, when President Bush addressed the UN's climaticchange panel, many environmentalists were bitterly disappointed.  Theyhad allowed campaign rhetoric about a "White House effect" to seducethem into hope for emergency initiatives, and now Bush, instead ofgiving voice to any spirit of emergency, had insisted that greenhousepolicy "be consistent with economic growth and free-marketprinciples." [7]               Just days before Bush delivered his speech, revisions hadtransformed it from one that emphasized the seriousness of globalwarming to one that emphasized scientific uncertainties and made norecommendations for action.  In the preceding months, the media hadtaken the same turn, buzzing with reports that scientists (except fora few greenhouse radicals) in fact doubted the warming predictions. On December 13, 1989, for example, The New York Times ran a coverfeature on the "greenhouse skeptics," and Forbes -- which subsequentlybegan advertising itself as "The magazine that's not afraid to take alittle heat" -- ran an expose on "The Global Warming Panic" (in its1989 Christmas issue.)      The backstage maneuvering had been going on for some time, withboth greenhouse theorists and skeptics pressing their arguments inWashington as the time for Bush's speech approached.  In September1989, for example, Richard S. Lindzen of M.I.T and Jerome Namais ofthe Scripps Institute of Oceanography wrote the White House, arguingthat current global warming forecasts "are so inaccurate and fraughtwith uncertainty as to be useless to policy makers."  Sununu and Bush,according to press reports, "welcomed the input." [8]     Lindzen and Namais are both noted meteorologists and members ofthe National Academy of Sciences.  Lindzen, who is perhaps the leaderof the anti-greenhouse scientists, has made major contributions toatmospheric theory.  Their views carry weight.  Yet Lindzen's theoriesare, by his own words, of a "theological or philosophical" nature [9],and reduce to a faith in the existence of atmospheric dynamics, yetundiscovered, of sufficient magnitude to counter the known effects ofthe greenhouse gases.  That such faith can be used to justify U.S.intransigence in U.N. greenhouse negotiations indicates how strong thepolitical currents running below the surface of "objective" sciencecan be.  More specifically, it demonstrates the ideological uses, notonly here but in a variety of debates over cancer and toxicity, of the"uncertainty" of scientific data.  (Incidentally, one of Lindzen'schief claims -- that water vapor may diminish rather than amplifyglobal warming -- was refuted by research published only a month afterBush's speech. [10])     Uncertainty plays such a key role in scientifically loaded policydebates because it converts the question "What is to be done?" into"Should anything be done at all?"  Witness Warren Brookes' Forbesarticle, which asks whether greenhouse warming is "the 1990s versionof earlier scares: nuclear winter, cancer-causing cranberries and $100oil?" and goes on to assert that "just as Marxism is giving way tomarkets, the political `greens' seem determined to put the worldeconomy back into the red, using the greenhouse effect to stopunfettered market-based expansion." [11]  Further, Brookes says, eventhough "60% of the public is convinced [global warming] will worsen,"in reality "the evidence of that alleged trend is under increasinglysharp and solid attack."     Is it indeed?  This claim, common in the popular press, directlycontradicts the scientific consensus and the conclusions of the U.N.IPCC's scientific committee.  (The United States has its greatest vetopower in the policy committee).  Yet because climatic dynamics areboth complex and poorly understood, uncertainty can always be made toseem plausible simply by citing data selectively.  Brookes, forexample, is careful to focus on U.S. statistics, rather than theglobal statistics that would contradict his chosen view.     All scientists acknowledge uncertainties in the atmosphericmodels.  The real debate turns on the meaning of that uncertainty, andits ideological uses.  These uses are both conscious and deliberate,as demonstrated by a White House memo to the American IPCC delegation,which was leaked to the press in April 1990.  The memo's list of"debates to avoid" included whether "there is or is not" globalwarming and how much warming could be expected.  "In the eyes of thepublic," it went on, "we will lose this debate.  A better approach isto raise the many uncertainties that need to be understood on thisissue." [12]     "Uncertainty" has a different meaning in the laboratory than inthe White House press room.  Though scientists argue over the details,the scientific community accepts the thrust of greenhouse theory for the simple reason that it dovetails with routine laboratory physics. Carbon dioxide, methane and the CFCs do trap heat; that's why they'recalled greenhouse gases.  The rising atmospheric concentrations ofthese gases are not in dispute; nor is the geological record, whichindicates that increases in CO2 correlate with increased temperatures. And though the earth's metabolism incorporates many buffers -- icesheets, forests, oceans, clouds -- there's no scientific reason tobelieve that these buffers will neutralize the vast seas of CO2 thatwill be spewed into the air in the coming century.       The summer of 1988, with its wilting heat wave and correspondingspike of greenhouse fear, was ages ago in ideological time.  (Thoughglobally 1990 was the hottest year on the recent record books.)  Theanti-greenhouse counter-revolution began in 1989, and today, as oil,recession, and militarism push hope, ecology and Eastern Europe offthe front page, it's wise to brace for a wave of denial and lies. Even before the Gulf War, with ecology still in the media foreground,the greenhouse debate had begun to thicken with equivocation.  Now,stonewalling is official U.S. policy.       The near future of the debate is reasonably predictable.  It isonly necessary to project current trends, and to review the history ofanother atmospheric crisis -- the destruction of the ozone layer bychlorinated chemicals.  Here, too, laboratory science predicted thethreat.  Here, too, the difficulty of proving atmospheric damage madeit possible for interested powers to frame a scientific near-consensusas controversy -- even though it was already clear in 1973 that CFCsdamaged the ozone layer.  Only the discovery of the Antarctic ozonehole finally turned the tide.  (If indeed it has been turned at all. The 1990 revision to the Montreal Protocol allows half again the CFCsproduced in the past 50 years to be produced in the next ten! [13])     Sherwood Rowland, the scientist whose lab discovered the ozone-shredding properties of CFCs, once commented "It is quite common onthe scientific side of industry to believe that there aren't any realenvironmental problems, that there are just public relationsproblems." [14]  It's an astute remark.  Public relations, notphysics, is the paradigm science of the modern age, and it's difficultto imagine it counting for more than it does today, when 76 percent ofthe American people describe themselves as environmentalists.      Why, finally, is prudence so difficult, even in the face of amore-than-plausible catastrophe?  There are many answers, but risingabove the fog is the insistence that the economy be left unfetteredunless it can be "proved" that the ecological costs of inaction areabsolutely intolerable.  Meanwhile, 1990 was the hottest year to date,the year that dying coral reefs (global warming is the chief suspect)became big news, and the fourth straight record year for coalproduction.FOSSIL-FUEL DEPENDENCY     The fossil-fuel economy is expensive, very expensive.  In theUnited States, subsidies to the automobile alone are about as large asthe military budget -- amounting to $2,200 per car per year, or $400billion a year! [15]  (The U.S. truck and auto fleet now exceeds 180million vehicles, up from 108 million in 1970.  That's one for every1.7 Americans, up from one for every 2.4 per in 1970).  In fact, thereal auto subsidy is much larger: although the $400 billion figureincludes highway and road construction and repair, as well as otherdirect support services like police protection and paramedical aid, itdoes not include indirect costs like the estimated $47 billion theUnited States has annually spent patrolling the Persian Gulf -- afigure that has recently been substantially increased.  And then thereare the billions that go to Big Oil each year as tax shelters anddepletion allowances, the health costs associated with the burning offossil fuels, and the costs of the strategic petroleum reserve andoil-spill cleanups.  The cost of this last item alone vastly exceedsthe entire solar-energy budget.  As for the ultimate costs of oil andcoal dependence, these defy calculation.     The fossil fuels benefit from fantastic, almost unaccountablesubsidies -- paid in dependence, demoralization, and catastrophe, aswell as in cash.  In the 1970s, it seemed that a scarcity of oil mightforce a transition to a new energy economy, but in the last few yearsmore fossil fuel has been discovered than has been extracted from theground.  This trend leaves state regulation standing alone, withoutincreased energy prices to buttress environmental policies -- asituation that will likely remain for the indefinite future.  Justbefore the war OPEC held a meeting that focused on Western Europeanmoves to reduce pollution by curbing the use of oil.  Calling this "aserious challenge," a Saudi official said that OPEC economists wouldstudy the environmental concerns of Western Europe and the effectsthey might have on the use of oil. [16]  Now, after the war, pricestability -- at a level advantageous to the winners -- is the order ofthe day, and there's little chance that the cost of oil will go highenough to force a real energy transition.     The New York Times Magazine recently claimed that "many people,throughout the world, believe that the implications -- scientific,economic, medical, social, technological and political -- of LA'senvironmental confrontation foreshadow the showdown awaiting theentire world." [17]  It's a plausible claim.  If even Los Angeles, oneof the world's richest cities, can't control its air pollution, canthere be any hope in the larger global "showdown"?  At issue, ofcourse, is the auto-industrial economy, or rather, the car.  Cars meanindividual freedom, and given the social and physical structure of themodern world, and especially of its cities, it's not hard to see why. Roads jam and commutes grow ever longer, but instead of real masstransit (which is hard to build for the low-density cities of theautomobile age) we get "auto environments" -- quieter cars, CDstereos, recorded novels, cellular phones, and even microwave ovens.       Even technical fixes, long scorned by eco-radicals as means ofavoiding substantive changes, are considered threats -- to the pointwhere it's hard to imagine a technology as revolutionary as, say,insulation, being used to anything approaching its potential.  In late1988 a mild EPA report on greenhouse policy became instantlycontroversial, recommending as it did auto efficiency standards of 40miles per gallon, plantation-grown wood as a fossil-fuel substitute,and the aggressive pursuit of solar power.  Such initiatives pose nothreat to capitalism, at least not in the abstract, though they seemto threaten this particular capitalism, with all its ties -- inhistory, ideology and infrastructure -- to cheap fossil fuels.       If there is to be a conversion to a post-greenhouse economy, thepower of the fossil-fuel sector must be broken.  This, obviously, is atall order, for we inhabit a world in which "cheap" fossil fuels faceus on every side, objectified as cities, factories, ruling elites.  Itwill take work to get out of here, and luck, and planning.  But, atleast in the United States, free-market ideology is so strong, andplanning held in such ill esteem, that even government research anddevelopment funds for computer and networking technologies arethreatened.  One need be no friend of the high-tech path to see that,if the dogmatic allure of minimal government can endanger developmentssuch as these, solar power and light rail are far from anyrenaissance.PREVENTION VERSUS ADAPTATION     In the greenhouse literature, conversion strategies aimed atreducing the production of greenhouse gases (i.e. carbon taxes aimedto phase out fossil-fuels) go by the name of "prevention," whilemuddling along as the climate changes (i.e. building dikes againstrising seas) is known as "adaptation."  Environmentalists supportprevention, while mainstream economists, following their own failinggod, usually advocate "freeing" the market to reshape society (andnature) to the new climate.  Usually, the debate is restrained, andthe likely specifics of adaptation -- political chaos, for example, orferocious storms -- disappear in a cloud of genteel generalization. And though everyone knows that some adaptation will be inevitable,environmentalists don't like to admit it, justifiably fearing thatsuch an admission will help force prevention from the agenda.  It'seasier, after all, to breed drought-resistant crops (the Israelis arealready doing so) than to convert to a sustainable economy; easier toinsure against a possible rise in sea level by raising the height ofoffshore drilling platforms (Shell Oil is already doing so) than toreduce oil consumption.     Stephen Schneider has weighed into the prevention-vs-adaptationdebate with the notion of "active adaptation," in which preventivemeasures are used to slow the warming and a planned post-greenhouseinfrastructure is phased in as the existing infrastructure wears out. It's a fine idea, if a bit abstract, and he tries to make it practicalas well by arguing that the adaptations necessary to stabilize theatmosphere -- conservation, reforestation, and all the rest -- wouldbe economically beneficial in themselves, regardless of any threat ofglobal warming.     The U.S. economics establishment, for its part, is wasting notime in combatting moves to make prevention strategies appearreasonable.  The New York Times, in a front-page story by economicswriter Peter Passell, entitled "Cure for Greenhouse Effect: The CostsWill Be Staggering," [18] sketched the drift of official opinion. "Crude initial estimates," Passell opines, indicate that for at leastthe next 50 years it will be cheaper to adapt to greenhouse warmingthan to attack its causes.  He then presents a number of economists,all regally unconcerned by the "externalities" ignored by their models-- storms, starvation, extinction -- and suffering little strainextending their traditional views to emerging conditions.  Harvard'sThomas Shelling argues that increasing costs for irrigation and floodcontrol will raise the price of food by only 20 percent, that thequality of life 100 years from now will depend as much on technologyand capital as on the amount of CO2 in the air, and that "if money tocontain carbon dioxide emissions comes out of other investment, futurecivilizations could be the losers."  In another article, Passell iseven more sanguine.  Citing Department of Agriculture estimates that adoubling of atmospheric CO2 would cost $170 billion annually inreduced agricultural productivity, he concluded, "That's a lot ofmoney.  But it amounts to just 1 percent of current world income.  As[Bush's Council of Economic Advisors] argues, it is in the sameballpark as the costs of government regulations that now distort foodproduction and reduce its total value.  To put it another way, freermarkets in agriculture, the Council suggests, could more than offsetgreenhouse-effect losses." [19]     There are more examples, equally appalling.  The anti-preventioneconomists tend to the political right, and generally see the economyas conforming to some idealized notion of free-market efficiency.  Inreality, the economy is anything but efficient, as is shown by thework of physicist and alternative energy analyst Amory Lovins.A "LEAST COST" ENERGY ECONOMY?     Lovins, best known for his Soft Energy Paths [20], takes aposition so far from the traditional standard that it's hard tobelieve he's describing the same world.  While many economists arguethat conversion to a post-greenhouse world is unaffordable, especiallyin the Third World, Lovins insists that it would be far cheaper to cutgreenhouse gas emissions by increasing energy efficiency than tocontinue along the present path.  Further, he sees energy efficiencyas a prerequisite for sustainable development, and believes that "farfrom being costly, abating global warming should, on the whole, beimmensely profitable.  Improving energy productivity can save theworld upwards of a trillion dollars per year -- as much as the globalmilitary budget."       Lovins isn't crazy.  He's simply worked out the "inefficiencies"in world energy markets, in detail, and concluded that the world canbe saved by being rationally frugal.  Talking about a 1981 book, LeastCost Energy: Solving the CO2 Problem, which he coauthored, Lovinssays,  "We began by documenting the potential to save about three-fourths of the energy used in 1973 to run the West German economy...We then imagined, a century hence, an entire world industrialized tothat level: a world of eight billion people, with a fivefold grossincrease in the World Product and a tenfold increase in developingcountries' economic activity.  Yet if, in such a world, energy wereused in ways that saved money with 1980 prices and technologies, totalenergy use would fall to a third of today's level... Of course, aworld with eight billion people industrialized to a West Germanstandard may be unrealistic or undesirable for other reasons.  Ourpoint was that an energy policy built on efficiency would enable theEarth to support a prosperous civilization that was not plagued byacid rain, global warming, urban smog, nuclear proliferation anddeforestation." [21]     And what of Lovins' view that the market makes a good vehicle forsuch discipline?  It makes his work interesting in two respects. First, by showing just how far today's economy diverges from one basedon "least cost" energy, he proves the ideological nature of analysesthat treat the existing economy as the product of "efficient" marketforces.  Second, his faith in the market prompts a key question: Why,after all, is our present society so monumentally inefficient?  Is it,as Lovins believes, because powerful institutional forces prevent themarket from imposing economic discipline?  Or is it, as green radicalshave long claimed, the very success of the market -- in particular itssuccess in "externalizing" social and ecological costs -- that is toblame?       Certainly the captains of industry skip out on the bill wheneverthey can, but this doesn't prove the market compels them to do so, fortoday's economy is structured as much by militarism and state subsidyas by direct market force.  What part, then, does the market play?  Isthis even an important question, or is it enough to know thatcountries and corporations alike find profit and comparative advantagein wasted energy?  If some new energy policy could change the rules ofthe game, making it more profitable to pursue a solar transition thanto continue along the present disastrous path, would that policy standa chance of being implemented?  Lovins believes that it would, andperhaps he's right.  But is such an energy policy possible, given thedegree to which the rich and powerful have bound their interests tothe fossil fuels?  Is it possible soon enough?THE POLITICS OF EMERGENCY     Like ecological crisis in general, the greenhouse crisis compelsus to ask, first of all, what will work.  Unfortunately, this isn't asimple question, even in the best of cases -- and judging by the rushto parlay the greenhouse crisis into a rebirth for nuclear power, itdoesn't appear that much rational, clear-headed pragmatism will comeof this emergency.  Just how much room remains for cheap maneuveringis clear from the wave of articles we've suffered in the last fewyears on a "new generation" of "passively safe" reactors.  The anti-nuclear battle, it seems, will have to be fought once again, and thistime the battle lines will not be clearly drawn.  Senator Wirth caughtthe spin of greenhouse nuclear boosterism just right when he argued, afew years back, that it's time for the country to get over the"nuclear measles."  "The environmentalists will come around," headded.  "They can't help but come around." [22]     And some environmentalists have come around.  William Reilly,director of the Environmental Protection Agency, is one of them, andso is Schneider, who finds himself compelled to advocate "inherentlysafe" nukes -- in a book published by the Sierra Club! [23]  Thus,environmentalists take up the spirit of emergency, but sometimes onlydeepen the fogs of false necessity that obscure the real choices. Lovins, with his insistence on simple economics, is rather clearer,noting that "investing in nuclear power rather than in far cheaperenergy-efficient technologies will make global warming worse." [24] It's a point that alone justifies the end of the nuclear industry.  Italso shows how, amidst apocalyptic storms, the "least cost" argumentcan be a welcome tonic -- the more you spend on nukes, the less youhave for fluorescent bulbs, weatherstripping, solar research and masstransit.       It's from the strength of such simple economics, ultimately, that"Third Wave Environmentalism" draws its charms.  The Third Wavers,from the Worldwatch Institute to the Environmental Defense Fund to theEPA, seek to yoke the market to the goal of ecological reform -- andto avoid stickier political issues -- by arguing that the best hopelies in modulating existing markets with new taxes (like carbon taxes)and in creating new markets in abstract goods like tradable "pollutionrights."  With such markets in place, both companies and countriescould profit by controlling pollution, for any unused space in theirpollution allocations could be sold to the highest bidder.  The goalof such devices is to "internalize" ecological costs into the economiccalculus, and to thereby avoid the absurdities that occur whenbureaucracies attempt to micromanage economies.  In other words, ThirdWavers want to use the market to force the larger economy to adapt tonatural limits.     The historical background here is crucial -- traditionalpollution-control measures have failed, and just about everyone admitsit.  Further, in the wake of the Eastern Bloc catastrophe, state-centered "planning" is held in poor repute indeed.  The Third Wavers,without faith in democratic forms of planning (if they have even heardof them) see the market as the sole alternative to bureaucraticcommand-and-control.  These days, there are even radical Third-Wavers, dreaming of markets that, though still capitalist, have become bothecologically efficient and democratically accountable. [25]  It's adesperate dream, to be sure, for it relegates popular opposition tothe margins of a market-oriented politics.  "Substantive democracy" isjust not in the cards.  Democracy may be possible, even necessary, butecological realism demands that the old dream of liberation yield tothe "socialization of the market."        Can Third-Wave environmentalism work?  The test must come in thereal social world.  Could a global market in CO2-emissions rights, asBush has half-seriously proposed, really help?  The first problem isdetermining just how much CO2 each country would have the "right" togenerate in the first place.  If any matter was ever complicated, thisis it, but it's fair to say that, in general, the issue is splitbetween the North and the South, with the North preferring quotasbased on GNP and the South preferring quotas based on population. This is a rather basic disagreement, and it's hard to imagine it beingovercome without social changes that go far beyond those imagined byThird Wave reformers.     In fact, it's difficult to be optimistic about the prospects forany kind of regulation, direct or market-mediated, as long as the coreinstitutions of society -- the market, militarism, property and wagerelations, the fossil-fuel economy -- are taken as surface phenomenaamenable to easy manipulation by bureaucratic agencies.  Consider LosAngeles again, or any traffic-clotted U.S. city.  What institutionswere central to its construction?  Well, there's the auto-industrialcomplex, for starters, but also the real-estate market.  How shallthey be regulated?  Fossil Fuels Policy Action, a small and notparticularly Third Wave organization, proposes a "paving moratorium."[26]  It's a simple, lovely idea, and we may perhaps measure ourcondition by checking to see if we can take it seriously. THE THIRD WORLD     The greenhouse debate has split the planet between theindustrialized nations and countries like China and India that claimthat, since they currently generate only a fraction of the greenhousegases, they should be substantially aided for cooperating in ways thatwill impede their development.  As things stand, their claim is morethan reasonable.  The habits of the average North American releasefive tons of CO2 into the atmosphere each year, while the globalaverage, an average that includes North Americans, is only half a ton.       From an ecological perspective, everything depends on the ThirdWorld following a post-greenhouse path.  If it doesn't, conservationgains in the industrialized countries will be swamped by deforestationand increased greenhouse gas production in the Third World -- wherethe rate of growth of energy production, much of it based on burningdirty coal, is twice the global average.  Unfortunately, helping theThird World carve out a radical new development path is no more than arhetorical priority in the North, where all the real effort is goinginto the post-Cold War scramble for position.     At least environmentalists now understand this situation,something that was not true as recently as five years ago.  In fact,both the left and the liberal wings of the environmental movement nowsee Third World debt as an ecological issue -- along with pollution,population, energy production, agriculture, and the rest of it. Today, all serious reviews of the greenhouse crisis agree there's noreal solution that doesn't define a new kind of development, oneattractive enough to derail the considerable momentum of heavy-metalindustrialism.  All of which serves to complicate life for liberalpoliticians like Senate Majority Leader George Mitchell, who recentlybecame the first U.S. Senator to write (with coauthor Jack Waugh) agreenhouse book, World on Fire: Saving an Endangered Earth. [27]  Itsmost interesting moment comes when Mitchell tries to wedge"sustainable development" into the political agenda.  He makes themain point, that "even dramatic improvements in energy efficiency willnot be sufficient to protect the environment -- if they are confinedto the industrialized world," then goes on to call for "retiring thedebt of developing countries" and for "technology-transfer, subsidiesand loans" designed to "take developing counties beyond the levels ofefficiency justified on the basis of free market prices."  Bravewords, these, from a Senator who toured the Persian Gulf before thewar, posing with Bush and adding a much-needed bipartisan gloss to histhreats of war.     Public relations, as always, is part of the problem.  The"development community" -- the World Bank and its kin -- is abuzz withtalk of "sustainable development," but thus far the reality has beenconsiderably less inspiring than the rhetoric.  It's the same storywith the debt crisis.  Certainly the Brady Plan for "debt relief" aimsnot to lift the debt now crushing the Third World, but only to reduceit enough to avoid open revolt, and thus to continue the agonizingprocess by which the debt crisis is indefinitely, and profitably,protracted.  The workings of the debt-management machinery arefamiliar enough -- new loans and a new market, this time a market indiscounted debt.  The discounts reflect domestic austerity andinternational "confidence," and the debt market that sets them seemsnatural enough in a market-fixated society.  Will it, however, easethe burden enough to allow humane development paths?  This is analtogether different matter.  Much of the Third World is sinking intoecological and social chaos, yet the international response isconstrained by the prerogatives of the banks.  The banks!  This willchange only when, in the words of one New York Times commentary, aWashington "consensus" develops that "the problems afflicting [the]debtors pose a threat to regional political and economic stability."[28]  How, under such conditions, can the Third World be asked tosacrifice for the common good?      The bottom line is that the Third World is profoundly constrainedby economic and political dependency on the cores of internationalpower.  The industrial nations, with their control of capital andtechnology, hold the keys to a global post-greenhouse economy.  Ifecological disaster is to be avoided, they must act soon, decisivelyand in a way that doesn't simply seek to modernize the terms of ThirdWorld dependency.  It's difficult to see this happening without asuccessful and radical campaign for ecological conversion here athome.     And where is that campaign?  Stalled as usual, though conversionactivists remain optimistic about "the long run."  There are, to besure, grounds for optimism -- if the US economy continues its decline,as is likely, pressure for a green "New Deal" of some sort will build. But will it be green enough to stop the warming?  Think concretely. What authority would enforce the depreciation of oil company stocks? If this is politically impossible, then what politics and what marketstructure could motivate the redeployment of the capital now tied upin fossil fuels?  Over what time period?  What about the overallinfrastructure, so much of which wouldn't fit into a post-greenhouseworld?  Who would take the loss?  The government?  Is the financing ofa solar transition in the Third World to be left to the World Bank? What, finally, will it take to get something more than fatal half-measures? ECOLOGY AFTER THE COLD WAR     Earth Day 1990 brought a spasm of liberal self-congratulation, a"Good Earthkeeping Seal of Approval," and a victory in the dolphinbattle -- but little by way of structural reform.  Still, 1990 willremain as a milestone, marking the passing of the Cold War as clearlyas it marks a watershed in environmental history.  The next decade maynot be a happy one, but it will see the emergence of a new kind ofecological politics.     Not that there isn't already a radical green politics, or that ithasn't already marked the political landscape.  But, far too often,the radicals place their faith in abstract ideals like "nature" and"personal responsibility," and are innocent of any coherent, sensible explanation for ecological crisis.  Fortunately, a mature red/greenpolitics is also, finally, emerging -- "fortunately" because the needfor it is acute.  Why do we go to war for oil?  Why is economic growthso chaotic and compulsive?  Why does technology always seem to betrayits promise?  These questions simply find no good answers in the moraland biocentric catagories of traditional environmentalism.     The ecology movement has been deeply marked by the silences andfears of Cold War culture, and it will take time for it to learn thesignificance (and limits) of the socialist tradition.  Many if notmost ecological radicals still see "politics" as synonymous with self-serving compromise, and "capitalism" as merely an ideological categoryof a justly moribund left.  Their view -- that communism andcapitalism are only variations on a system of exterministindustrialism -- is not without merit, but it's hard to see it as thebasis of an adequate politics, if only because its name for the beast,"industrialism," suggests inadequate correctives.     It is a measure of the times that, in the last few years, an essay called The End of History [29] and a book called The End ofNature both took proud places in the march of literary events.  In thefirst, a State Department functionary named Francis Fukuyama invokedHegel's philosophy of history to claim the "triumph of liberaldemocracy" as an event of such significance that now "there will beneither art nor philosophy, just the perpetual caretaking of themuseum of human history."  In the second, a young man described thelikely outcomes of greenhouse warming, then went on to dismiss allpolitics as insignificant, and to compose an ode to despair.          McKibben tells the tale of environmental radicals throughout thedeveloped world, who can squeeze neither their fears nor their hopesinto the old moulds.  That they reject "socialism" along with officialenvironmentalism is not altogether bad news, for the "real" socialismwe have known has no just claim to our loyalty.  But neither is itgood news, for environmentalism, radical or otherwise, has a distanceto go before it can take up the challenges of the ecological crisis.       The distinctive culture and ideas of the greens -- deep ecology,romantic naturalism, the claim to be "neither right nor left, but outfront," animal rights, direct action, the too-simple Luddism that seesonly evil in advanced technology -- are easy to ridicule.  But recallthat the ecology movement evolved in a world where socialism, and leftradicalism in general, were lost in the shadows of the Eastern Bloc,and that by virtue of its independence it has been able to claim keycultural and technological problems as its own, and to develop its ownincreasingly radical language.  Left greens should remember this, evenas they call upon their traditions to study the economic and politicalaspects of the crisis, deepen the notion of radical democracy, andmark the currents in these all pervasive and almost invisiblecapitalist waters.     The coming devastation will breed a vast hatred.  It may even bethat the ideas of the green hard core -- ecological misanthropy mostnotable among them -- are poised for a breakout into larger domains. Fortunately, this is not the only possibility, and radical outrage isnot likely to remain eternally constrained within the anti-communistframeworks of Cold War analysis.  The ecology movement is full ofthose just now discovering the pleasures of romanticism, and believingit the essence of true revolution.  Far more important than theirillusions, though, is the probability that capitalism -- like theatmosphere -- may soon cease to seem a part of a natural, eternalworld.       It's hard to be realistic about the ecological crisis withoutyielding to the formidable logic of a very grim situation.  Still, itis wrong to follow McKibben in attributing the tragedy of the times tosome inescapable trajectory towards an ecological holocaust.  It is,rather, because our trajectory is not inescapable, because there is somuch that could be done, and because so little of it is being donethat this is such a dark time.  This paralysis is the real tragedy.I'd like to thank Bill McKibben, who's book pissed me off so much Ihad to write this, and the San Francisco Socialist Review Collective,which really bent over backwards to help me get it right.NOTES1) Susan Sontag, "AIDS and Its Metaphors," New York Review of Books,October 27, 1988.2) Lester Brown, et. al., State of the World, 1990: A WorldwatchInstitute Report on Progress Toward a Sustainable Society.  (New York:W.W. Norton, 1990.  Page 174.)3) Francesca Lyman, et al., The Greenhouse Trap: What We're Doing tothe Atmosphere and How We can Slow Global Warming.  (Boston: BeaconPress, World Resources Institute, 1990.  Page 159-162).4) "Team of Scientists Sees Substantial Warming of Earth," New YorkTimes, April 16, 1990.5) Stephen H. Schneider, Global Warming: Are We Entering the Green-house Century? (San Francisco: Sierra Club, 1989).  MichaelOppenheimer and Robert Boyle, Dead Heat: The Race Against theGreenhouse Effect (New York: Basic Books, 1990).  Bill McKibben, TheEnd of Nature. 6) Stephen H. Schneider, "The Changing Climate," Scientific AmericanSeptember 1989.  Page 79.7) "Bush Asks Cautious Response To Threat of Global Warming," New YorkTimes, Feb 6, 1990.  Page 1.8) "Skeptics are Challenging Dire 'Greenhouse' Views," New York Times,December 13, 1989.  Page 1. 9) "Greenhouse Skeptic Out in the Cold," Science, December 1, 1989. Page 1118.10) "Vapor Trail: Observations Support a Key Aspect of WarmingForecasts, Scientific American, March 1990.  Page 24.11) Warren T. Brookes, "The Global Warming Panic," Forbes, December25, 1989. 12) "Bush Denies Delaying Action On Averting Shift in Climate,"  NewYork Times, April 19, 1990. 13) See 50% More Production: The Failure of the 1990 MontrealProtocol, a report of the Greenpeace Atmosphere and Energy campaign.14) Dennis Hayes, "Highest Disregard," Mother Jones, December 1989. 15) See, "The Clean Air Act Won't Clean the Air," Greenpeace, November12, 1989, "The True Cost of Oil," Earth Island Journal, Summer 1988,page 22., and "Stagnant Politics, Dirty Air: Autos and theEnvironment," In These Times, December 12, 1989.16) "Controlling Oil's Two Great Threats," New York Times, December14, 1990.17) "LA Fights for Breath," New York Times Magazine, July 30, 1989. 18) "Cure for Greenhouse Effect: The Costs Will Be Staggering," NewYork Times, November 19, 1989. 19) "Global Warming: Look or Leap?," New York Times, February 14,1990. 20) Amory B. Lovins, Soft Energy Paths: Toward a Durable Peace,(Cambridge: Ballinger, Friends of the Earth, 1977). 21) Amory Lovins, et al., Least-Cost Energy.  Reprinted by the RockyMountain Institute, 1739 Snowmass Creek Road, Snowmass CO) 81654-9199.22) Fighting the Greenhouse Effect," New York Times, August 28, 1988. For a more recent, and very unabashed version of the same tale, see"Nuclear Power: Do We Have a Choice?," the cover story of the April29, 1991 issue of Time.23) Reilly's support of nukes is a matter of public record, but seefor example "U.S. Reported Speeding Talks For Global Warming Accord,"in the November 21, 1989 issue of the New York Times.  For Schneider'sposition, see Global Warming, page 245.24) See Least Cost Energy; in fact, see the Rocky Mountain Institute'scatalog.25) See Amory Lovins, "Making Markets in Resource Efficiency," RockyMountain Institute, 1989.  For an entry into the left discussion, seeDiane Elton's "The Socialization of Markets," in issue 172 of New LeftReview (Nov/Dec 88, pages 3-44), or look up "environmental protection"in the index of Alec Nove's The Economics of Feasible Socialism,(Boston: George Allen & Unwin, 1983)26) Alliance for a Paving Moratorium, Federal Square-E. PO Box 8558Fredericksburg VA 22404.27) George Mitchell with Jack Waugh, World on Fire: Saving anEndangered Earth (New York: Scribner's Sons, 1991).28) "U.S. Efforts to Aid Debtor Nations Bring 'ProfoundDisappointment'", New York Times, July 24, 1989.  Page 1. 29) "The End of History" was first printed in the Summer 1989 issue ofThe National Interest.BIO BLURBTom Athanasiou runs an online publishing group at Sun Microsystems. He is currently developing a book on ecological politics after theCold War.WT03-B20-49IA005-000051-B018-316http://lacebark.ntu.edu.au:80/j_mitroy/sid101/nuke1/nip22.html 138.80.61.12 19970221152048 text/html 13703HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:51:02 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 13531Last-modified: Tue, 09 Jul 1996 02:25:43 GMT Chernobyl and Soviet Reactors Chernobyl and Soviet Reactors Nuclear Issues Briefing Paper 22 December 1995 The Chernobyl accident was the result of a flawed reactor design thatwas operated outside authorised procedures. The resulting steam explosion and fire released 3-4 per cent ofthe reactor core into the atmosphere. 31 people were killed, and there have since been ten deaths fromthyroid cancer due to the accident. Other similar reactors continue to operate, at the Chernobyl site and alsoin Russia and Lithuania. The April 1986 disaster at the Chernobyl nuclear power plant in theUkraine was the product of a very flawed Soviet reactor designcoupled with serious mistakes made by the plant operators who during a test,violated procedures intended to ensure safe operation of the plant. The accident destroyed the reactor, killed 31 people almost immediately (28from radiation exposure), caused radiation sickness in a further 200-300, and contaminated large areas of Belarus, Ukraine, Russia and beyond. About 130,000 people received significant radiation doses (ie aboveinternationally accepted ICRP limits) and are being closely monitored. A much larger number received some radiation exposure from the accident. About ten deaths from thyroid cancer have occurred, and further deaths fromleukaemia and other cancers are expected. The Chernobyl accident was aunique event and the only time in the history of commercial nuclear power thatradiation-related fatalities occurred. The Soviet-designed RBMK-type reactor of 950 MWe unit uses a large mass ofgraphite to moderate the reaction and water flowing through channelsholding the fuel elements to cool it. There is no containment structure onthis kind of reactor. WHAT HAPPENED On 25 April the reactor crew at Chernobyl 4 beganpreparing for a test to determine how long turbines would spin and supplypower following a complete shut down of the reactor. Similar tests hadalready been carried out successfully at Chernobyl and other plants. A series of operator mistakes including the disabling of automatic shutdownmechanisms preceded the attempted test early on 26 April. As water flow diminished, power output increased. When the operator moved to shut down the reactor from its unstable conditionarising from his previous errors, a peculiarity of the design caused the powerto surge dramatically. The fuel elements ruptured and the resultant steam lifted off the cover plate of the reactor, releasing fission products to the atmosphere. A second explosion threw out fragments of burning fuel and graphite from thecore and allowed air to rush in, causing the graphite to burst into flames. The graphite burned for 9 days, causing the main release of radioactivityinto the environment. Some 5000 tonnes of boron, dolomite, sand, clay and lead were dropped on tothe burning core by helicopter in an effort to extinguish the blaze and limit the release of radioactive particles. IMMEDIATE IMPACT It is estimated that about 3 to 4% of the total radioactive materialin the Chernobyl 4 reactor core was released from the plant. Most of this was deposited as dust close by, the rest was carried by wind over the Ukraine and Europe. The main casualties were among the firefighters whoattended the initial small fires on the roof of the turbine building, which were all put out in a few hours. The next task was cleaning up the radioactivity at the site so that the remaining three reactors could be restarted, and the damaged reactor shielded more permanently. According to Soviet estimates 600,000 people involved in the recovery and clean up immediately after the accident received high doses of radiation. Some children in the surrounding areas were exposed to radiation doses sufficient to lead to thyroid cancers, (which themselves are not usually fatal if diagnosed and treated early). Earlyradiation exposure was due to iodine-131; later, caesium-137 was the mainhazard. Initially some 45 000 residents were evacuated from a 10 km radius of theplant. In May 1986, all those living within an 30 kilometre radius of theplant - a further 100 000 people, had been relocated. ENVIRONMENTAL AND HEALTH EFFECTS Several organisations have reported on the impacts of the Chernobylaccident, but all have found difficulties in assessing the significance of what they have observed because of the paucity of reliable statistical information on health matters prior to 1986. In 1989 the World Health Organisation (WHO) raised concerns that localmedical scientists were not well versed in radiation effects and hadincorrectly attributed various biological and health effects to radiation exposure. A Red Cross team in 1990 raised similar concernsregarding thyroid problems encountered in the population. An IAEA study involving more than 100 experts in 1991 was more substantial, and in the absence of pre 1986 data used a control population to compare those exposed to radiation. They found significant health disorders in both control and exposed groups, but at that stage none was radiation related. Studies in the Ukraine, Russia and Belarus since have confirmed a risingincidence of thyroid cancer among exposed children. Late in 1995, WHO linked nearly 700 cases of thyroid cancer among children andadolescents to the Chernobyl accident, and among these some ten deaths areattributed to radiation from it. Psycho-social effects among thoseaffected by the accident are emerging as a major concern, and are similar tothose arising from other major disasters such as earthquakes, fires amdfloods. CHERNOBYL TODAY The Chernobyl unit 4 is now enclosed in a concrete sarcophagus which allows continuing operation of the other reactors at the plant. However, the structure has been weakened byradioactivity. A feasibility study is examining remedial work includingpossible replacement of the whole structure. Numerous safety deficiencies remain in the other reactors at Chernobyl.However, worsening energy shortages necessitate the continued operation of those reactors. The Ukraine is dependent upon, and deeply in debt to, Russia for energy supplies, particularly oil and gas, and these have been drastically reduced. Continued operation of its nuclear power stations, which supply about 30% of its electricity, has become even more important and the country is pursuing the development of nuclear fuel cycle facilities which will increase its energy independence. SOVIET-DESIGNED REACTORS According to the International Atomic Energy Agency (IAEA) the safety of nuclear power plants in the former Eastern bloc is a problem of continuing concern. The current energy demand in these countries is such that there is little flexibility to allow for closing even those plants which are of most concern. There are currently 63 nuclear plants in operation in the former Eastern bloc countries and others under construction. They include: 11 first generation VVER-440/230 pressurised water reactors which have serious design deficiencies (4 in Bulgaria, 4 in Russia, 2 in Slovakia, 1 in Armenia). Four such units in eastern Germany have been permanently shut down. 14 second generation VVER-440/213 pressurised water reactors with some major design deficiencies which have been partly remedied. (Two more in Finland have been greatly modified.) 20 third generation VVER-1000 pressurised water reactors with a full containment structure. These have some instrumentation and controlsystem deficiencies. 15 RBMK light water graphite reactors such as at Chernobyl, in operation in (and unique to) Russia, Ukraine and Lithuania. The five oldest of these were commissioned in the 1970s at Kursk (2), Leningrad (2) and Chernobyl (1), and are of most concern. 4 small GBWR reactors in eastern Siberia, constructed in the 1970s. Significant assistance has been initiated by the OECD, IAEA and Commission of the European Communities to bring these reactors up to western safety standards, or at least to effect significant improvements. This involves plant modifications, identification and remedying of safety deficiencies, training of staff and audits of the status of plant components. The national, bilateral and international safety initiatives being undertaken are concentrating on the first generation of Soviet-builtVVER-440/230 pressurised water reactors and the RBMK reactors, which have the most serious deficiencies. Modifications have already been made to overcome deficiencies in the 15 RBMK reactors still operating. In these, originally the nuclear chain reaction and power output would increase if cooling water were lost, in contrast to Western designs. It was this effect which caused the uncontrolled power surge that led to the destruction of Chernobyl unit 4. Automated inspection equipment has also been installed in these reactors. The other class of reactors which has been the focus of international attention for safety upgrades is the VVER-440/230 reactors. One of these, in Armenia, was restarted late in 1995 after a six-year shutdown. However it had been substantially upgraded and an IAE A inspection had then approved it. SOURCES Nuclear Energy Institute 1995, Info Bank briefing sheets and Source Book,3rd edn. IAEA, 1991,The International Chernobyl Project - An Overview. V G Snell and J Q Howieson, 1991, Chernobyl - A Canadian Perspective, AECL Canada. ENS NucNet background # 18/95 THE CHERNOBYL ACCIDENT control rods withdrawn/important safety systems switched off major power surge began fuel elements rupture causing steam explosion top of reactor blew off releasing radioactive products to the atmosphere graphite moderator burns Appendix Chernobyl Accident Health Effects - WHO Facts & Figures The following is the text of a briefing paper issued in May 1995 to officials of the World Health Organisation on the known health effects of the 1986 Chernobyl accident. THE CHERNOBYL ACCIDENT 26 April 1986 In connection with the anniversary of the Chernobyl accident I have received a number of requests for information about the health effects that have followed the accident. The following is an extract from a recentreport giving details of the health effects that have been verified so far and those that might be expected in the future, which may prove useful to members of WHO's European Centre for Environment and Health. "In the accident about 200 persons among the operating staff and the fire fighting crew suffered acute effects of radiation. About 115,000residents were moved from the 30 km radius exclusion zone surrounding the reactor, and there is the possibility that an additional 200,000 or more will be relocated in the future. Some 650,000 persons involved in the clean up of the plant site and the 30 km zone were exposed. Very extensive areas ofthe former Soviet Union and beyond its frontiers were affected by radioactive fallout. "At the present time, nine years later, many of the expected potential health effects of exposure to radiation have not become apparent because of the latencies for some radiation-induced cancers. The full impact of the accident will become apparent only after severaldecades. So far the health consequences are as follows: acute radiation sickness and burns to the skin from beta radioactivity to some 200 persons, causing 28 deaths from acute radiation syndrome; childhood thyroid cancer in children living in Belarus,the northern districts of the Ukraine and parts of the border of the Russian Federation with Belarus and the Ukraine. So far nearly 500 cases of childhood thyroid cancer have been detected in a population of about 3 million children particularly at risk; psycho-social effects from stress-related conditions, through lifestyle changes, to near complete social disintegration of communities. Some 10 million persons live in the most affected regions. On the basis of past experience of exposure to radiation further effects may be observed. The tissue most sensitive to radiation exposure, in addition to the thyroid and bone marrow, is the breast of young women. Populations within 100 km would be particularly at risk. The isotopes of iodine could contribute to breast dose at certain stages of development or during and after pregnancy. "The effects of beta-emitting hot particles on the induction skin and lung cancers is also a matter for concern. Such hot particles were observed as far afield as Finland and animal experiments have shown them to be capable of inducing lung tumours." Due to the size of the exposed population, hitherto unrecognised effects of radiation resulting from the incorporation of the less well studied fission products may well become evident in due course. Source: Dr Keith Baverstock, WHO office, Rome. Distributed 10 May 1995 by NucNet, as Background #9/95, European Nuclear Society, Berne, Switzerland WT03-B20-50IA005-000051-B018-254http://lacebark.ntu.edu.au:80/j_mitroy/sid101/nuke1/nip19.html 138.80.61.12 19970221152021 text/html 12669HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:50:31 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 12497Last-modified: Tue, 09 Jul 1996 02:25:43 GMT Plans For New Reactors Worldwide Plans For New Reactors Worldwide Nuclear Issues Briefing Paper 19 February 1995 Nuclear power capacity worldwide is increasing steadily but not dramatically, with 36 reactors under construction in 12 countries. Most planned reactors are in the Asian region. Further capacity is being created by plant upgrading. Plant life extension programs are receiving serious attention. Although some countries, notably Japan and the Republic of Korea,intend to continue major nuclear power construction programs,the rate of growth of installed nuclear generating capacity overthe next ten to fifteen years is expected to be low. One country,Italy, has completely abandoned its nuclear power program andothers have cancelled or indefinitely suspended partially constructedand planned power reactors and/or imposed a moratorium on anynew nuclear construction. UNDER CONSTRUCTION The International Atomic Energy (IAEA) Agency estimates that bythe year 2000 worldwide installed nuclear capacity will have increasedby 9.5 to 12%, leading to a total installed capacity of some 370-380GWe. These IAEA estimates, and other recent industry estimatesof nuclear power growth over the near term, are based only onfirm orders and known plans for plant expansion, life extensionor closure. They do not involve any extrapolation of theoreticalmodels, so are generally conservative and not subject to the largeuncertainties that resulted in speculative projections by thenuclear industry in past decades. Some 36 power reactors are currently being constructed in 12 countriesin the world (Table, p4), notably France, the Republic of Korea,Japan and eastern Europe. Two of these, Monju and Kakrapar 2,have achieved initial criticality. Construction is well-advancedon many of them and, based on reported progress and allowing fordelays in countries such as India, 31 with a total net capacityof 25,000 MWe are expected to be in operation before the year2000. This excludes the third and fourth Mochovce reactors inSlovakia and three of the five reactors at Romania's first nuclearpower station. In addition, if the necessary financing is available, three almost-completed950 MWe PWRs in the Ukraine could be commissioned within a fewyears, replacing some of the existing nuclear capacity there andArmenia has begun to return to service its two closed 400 MWereactors, with one unit planned to be restarted in 1995. INCREASED CAPACITY Increased nuclear capacity in some countries is resulting fromthe uprating of existing plants. Power reactors in Belgium, Swedenand Germany, for example, have had their generating capacity increased. In Switzerland, where there is a ten-year moratorium on nuclearpower construction, a program is being undertaken to increasethe capacity of its five reactors by 10%. Finland plans to boost the capacity of the Olkiluoto plant, whichhas two 710 MWe Swedish BWRs commissioned in 1978 and 1980, by250 MWe (17.6%) to 1670 MWe. The engineering program will costA$ 260 million. LONGER TERM PLANS After the year 2000 and until orders are placed or constructioncommenced, forecasts of installed nuclear capacity become muchless certain. When the present construction programs are completed,most significant nuclear power growth is expected to continueonly in the Asian region. The International Energy Agency (IEA)forecasts that the growth rate of electricity output from nuclearpower will be 1.3% per year for the period 1991 to 2010. TheIEA predicts that the nuclear share of world electricity outputwill decrease from 17.5% in 1991 to 13.2% in 2010. ANSTO's nuclear power reactor data files show that seven countrieswith existing nuclear power programs (the UK, Russia, China, India,Japan, South Korea and Taiwan) and four countries without anynuclear capacity at present (Indonesia, Egypt, Turkey and Iran)have announced firm plans to build new power reactors. This doesnot include North Korea, where the declared intention to introduceLWR technology is presently under international negotiation. In all, 73 power reactors with a total net capacity of 60,700MWe are planned, none in North or South America. In addition,orders have been placed in 1995 for two reactors in South Koreaand one in Iran. In western Europe, Nuclear Electric, the UK nuclear utility, hasannounced plans for Sizewell C (2 x 1,288 MWe, PWRs) and HinkleyPoint C (1,110 MWe, PWR). However, the intention of the UK Governemntto privatise Nuclear Electric makes implementation of these plansdependent on private sector financing. In France, where furtherreactor orders had been expected in the later half of the decade,the national utility announced in June that it will order no newgenerating capacity, nuclear or conventional, until at least theyear 2000. Sites have, however, been designated for new powerreactors and new reactor construction is expected to be resumedsome time next decade. In eastern Europe, the Russian government in December 1992 approveda nuclear power construction program up to the year 2010. Inaddition to the two reactors presently being completed, a further12, with a total capacity of 9,200 MWe, are planned to be operatingby 2010. Most are at existing sites. Several of the oldest Russianreactors are expected to be retired next decade and it is Russia'sannounced intention to replace retired nuclear capacity by newconstruction at the same site, to optimise the use of establishedinfrastructure and personnel. Obtaining the approval of localand regional authorities, which have been responsible for thesuspension of a number of nuclear power plant projects in recentyears, may be difficult. Most planned reactors are in the Asian region. Nuclear poweris intended to play a major role in the future electricity supplymix in both South Korea and Japan. In addition to the seven reactors under construction, South Koreaplans to bring a further seven, with a total capacity of 6,700MWe, into operation by the year 2006. Two 950 MWe reactorswere ordered early in 1995; these are expected to be the 17thand 18th to go on line, in 2001-2. Japan has plans and, in mostcases, designated sites and announced timetables for a further19 power reactors, totalling 22,000 MWe, but only two of thesehave completed the governmental approval process. Fulfillingthe necessary conditions for approval can take over a decade. India has announced plans for twelve power reactors (4,400 MWe)but financing difficulties are expected to continue to cause considerabledelays. China has begun the next phase of its nuclear power program. Civil construction work has started at its third nuclear powerstation (2 x 600 MWe, PWRs), and an agreement with Russia fortwo 953 MWe PWRs has been signed. In 1995 China signed an agreementwith France to construct and a second nuclear power station (2 x 900 MWe, PWRs) in Guangdong Province. Power reactors inother Chinese provinces are under active consideration. Indonesia has recently announced that the feasibility study forits first power reactor has been completed and that constructionof the 600 MWe unit could begin as early as 1996. Egypt and Turkey have for decades included a nuclear power plantin their electricity plans. A site has been selected in eachcountry and a number of feasibility and other studies carriedout. Construction remains unlikely. Nuclear power plant construction in Iran was suspended in 1979but in 1995 Iran signed an agreement with Russia to complete a1000 MWe PWR at Bushehr. Iran also has an earlier agreement withRussia for the supply of two 410 MWe power reactors and one withChina for the supply of a nuclear power station with two 300 MWePWRs. PLANT LIFE EXTENSION Most nuclear power plants have a nominal design lifetime of 20to 40 years, but it is now believed they can operate longer andmajor programs exploring plant life extension are being undertaken. When the oldest commercial nuclear power stations in the world,Calder Hall and Chapelcross in the UK, were built in the 1950s,it was assumed that they would have a useful lifetime of 20 years. They are now authorised to operate for 40 years and the operatoris seeking to extend their lives even further. The technical and economic feasibility of replacing major reactorcomponents, such as steam generators in PWRs and pressure tubesin Candu HWRs, has been demonstrated. The possibilities of componentreplacement and licence renewals extending the lifetimes of existingplants are very attractive to utilities, especially in view ofthe public acceptance difficulties involved in constructing replacementnuclear capacity. On the other hand, economic, regulatory and political considerationshave led to the premature closure of some power reactors, particularlyin the United States. The OECD Nuclear Energy Agency has predictedthat 19.3 GWe of nuclear capacity (35 units) will be taken outof service in OECD/NEA member countries between 1994 and 2010. Sources: Nuclear Services Section, External Affairs, ANSTO. 9/94, updated 5/95 NucNet News #191/95 POWER REACTORS UNDER CONSTRUCTION YEAR*       COUNTRY             REACTOR             TYPE        MWe         1995        Japan               Monju               FBR         250         1995        India               Kakrapar 2          HWR         202         1995        Romania             Cernavoda 1         HWR         620         1995        Russia              Kursk 5             BWR         925         1995        USA                 Watts Bar 1         PWR         1177        1997        Argentina           Atucha 2            HWR         692         1996        France              Chooz B1            PWR         1455        1996        France              Chooz B2            PWR         1455        1996        India               Kaiga 1             HWR         202         1996        Japan               Kashiwazaki 6       BWR         1315        1996        Korea RO            Yonggwang 4         PWR         950         1996        Russia              Kalinin 3           PWR         950         1997        Slovak Republic     Mochovce 1          PWR         388         1997        Czech Republic      Temelin 1           PWR         892         1997        France              Civaux 1            PWR         1450        1997        India               Kaiga 2             HWR         202         1997        India               Rajasthan 3         HWR         202         1997        India               Rajasthan 4         HWR         202         1997        Japan               Genkai 4            PWR         1127        1997        Japan               Kashiwazaki 7       BWR         1315        1997        Korea RO            Wolsong 2           HWR         650         1997        Romania             Cernavoda 2         HWR         620         1998        Slovak Republic     Mochovce 2          PWR         388         1998        Czech Republic      Temelin 2           PWR         892         1998        France              Civaux 2            PWR         1450        1998        Korea RO            Ulchin 3            PWR         950         1998        Korea RO            Wolsong 3           HWR         650         1999        Brazil              Angra 2             PWR         1245        1999        Slovak Republic     Mochovce 3          PWR         388         1999        Korea RO            Ulchin 4            PWR         950         1999        Korea RO            Wolsong 4           HWR         650         1999        Pakistan            Chashma 1           PWR         300         2000        Slovak Republic     Mochovce 4          PWR         388         2001        Romania             Cernavoda 3         HWR         620         2002        Romania             Cernavoda 4         HWR         620         2002        Romania             Cernavoda 5         HWR         620         * Latest announced year of commercial operation. Return to Index WT03-B20-51IA005-000051-B017-426http://lacebark.ntu.edu.au:80/j_mitroy/sid101/ipcc/ipcc92.html 138.80.61.12 19970221151808 text/html 17583HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:48:14 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 17411Last-modified: Tue, 09 Jul 1996 02:25:59 GMT IPCC92 Supplement      1992 IPCC Supplement  -  Scientific Assessment, SummaryThe 1992 Supplement is an interim report, it is not aswell-structured as the 1990 Assessment, and there are almostno references.  I tried to select what I presumed to be generallyinteresting points, this is a personal judgment.  Informationpertaining to the same topic often appears at two or three placesin the Supplement, I tried to collect it.  I tried to retain themeaning and often the wording of the original but, alas, Englishis not my first language. Any errors or misconceptions are mine.Jan Schloerer             May 29, 1992------------------------------------------------------------------   IPCC 1992 SUPPLEMENT  -  SUMMARY OF THE SCIENTIFIC ASSESSMENT        [1]  Major Conclusions        [2]  Gases and Aerosols   (includes radiative forcing)        [3]  Climate Observations        [4]  Modelling and Scenarios        [5]  Key Uncertainties and Further Work Required        [6]  References[1]  Major ConclusionsFindings of scientific research since 1990 either confirm or donot justify alteration of the major conclusions of the 1990 IPCCScientific Assessment, in particular the following :o  Emissions resulting from human activities are substantiallyincreasing the atmospheric concentrations of carbon dioxide (CO2),methane (CH4), chlorofluorocarbons (CFCs), and nitrous oxide(N2O).o  The sensitivity of global mean surface temperature to doublingCO2 is unlikely to lie outside the range 1.5 to 4.5 oC,  with abest estimate, based on model results and taking into account theobserved climate record, of 2.5 oC.o  Due to our incomplete understanding, there are manyuncertainties in the predictions, particularly with regard to thetiming, magnitude and regional patterns of climate change.o  Global mean surface air temperature has increased by 0.3 to 0.6oC over the last 100 years.o  The size of this warming is broadly consistent with predictionsof climate models, but it is also of the same magnitude as naturalclimate variability. Thus the observed increase could be largelydue to this natural variability; alternatively this variabilityand other human factors could have offset a still largerhuman-induced greenhouse warming.o  The unequivocal detection of the enhanced greenhouse effectfrom observations is not likely for a decade or more.[2]  Gases and Aerosols   (includes radiative forcing)o  CO2:  The best estimate for global fossil fuel emissions in1989 and 1990 is 6.0 (5.5-6.5) GtC  (gigatonnes of carbon = 10^9tonnes of C), compared to 5.7 (5.2-6.2) GtC in 1987.  Despite newinformationregarding rates of deforestation, there is no strongreason to revise the IPCC 1990 estimate of annual average net fluxto the atmosphere of 1.6 (0.6-2.6) GtC from land-use change duringthe decade of the 1980s.  The estimated total release of carbon inthe form of CO2 from oil well fires in Kuwait during 1991 was0.065 GtC, about one percent of total annual anthropogenicemissions.  Over the last decade, the fraction of CO2 emissionswhich remains in the atmosphere is estimated to have been  46 %(39-53 %).  The imbalance (of order 1-2 GtC/year) between sourcesand sinks, that is, the "missing CO2 sink", has not yet beenresolved.o  Sulphur:  The cooling effect of sulphate aerosols (particles,not to be confused with propellants used in 'aerosol sprays')resulting from sulphur emissions may have offset a significantpart of the greenhouse warming in the Northern Hemisphere duringthe past several decades.  For clear-sky conditions alone, thecooling caused by current rates of emissions has been estimated tobe about 1 W/m^2  (averaged over the Northern Hemisphere), whichshould be compared with the estimate of 2.5 W/m^2 for the heatingdue to anthropogenic greenhouse gas emissions up to the present. In addition, sulphate aerosols may affect the radiation budgetthrough changes in cloud optical properties.There are large regional variations in the effects of theaerosols.  The effect is negligible in the Southern Hemisphere. The globally averaged magnitude of the effect of sulphate aerosolshas not yet been calculated accurately and further work is needed. The aerosols are very short-lived in the atmosphere, their effecton global warming rapidly adjusts to increases or decreases inemissions.  While partially offsetting the greenhouse warming, thesulphur emissions are also responsible for acid rain and otherenvironmental effects.o  Stratospheric ozone and CFCs:  Depletion of O3 in the lowerstrato-sphere during the 1980s has caused a reduction in radiativeforcing which is believed to be comparable in magnitude  to theincrease in radiative forcing due to CFCs (globally-averaged) overthe last decade or so.  The effect at high latitudes isparticularly pronounced.  The worldwide consumption of CFCs 11,12, and 113 is now 40 % below 1986 levels.  Further reductions aremandated by the 1990 London Amendments to the Montreal Protocol. Even if the 1990 London Amendments were to be implemented by allnations, the abundance of stratospheric chlorine and bromine willincrease over the next several years.o  Tropospheric ozone:  There is evidence to indicate that O3levels in the troposphere up to 10km altitude above the fewexisting ozonesonde stations in Europe have increased by up to 10% per decade over the past decades.  There is not an adequateglobal set of observations to quantify the magnitude of theincrease in radiative forcing.  However, it has been calculatedthat a 10 % uniform global increase in tropospheric O3 wouldincrease radiative forcing by about 0.1 W/m^2. Little newinformation is available regarding the tropospheric O3 precursors  (CO, non-methane hydrocarbons, NOx), their budgets remainuncertain.o  Methane:  Whilst the rates of increase in the atmosphericconcentrations of many greenhouse gases have continued to grow orremain steady, those of methane and some halogen compounds haveslowed. The rate for methane has declined from about 20 ppbv/yearin the late 1970s to possibly as low as 10 ppbv/year in 1989.  Nocompletely satisfactory hypothesis has yet been forwarded toexplain these observations.  There has been some progressregarding the sources of the methane emissions, e.g., some dataindicate that global emissions from rice paddies may amount toless than previously estimated.  But there are still manyuncertainties in accurately quantifying the magnitude of emissionsfrom individual sources.  The latest estimate of the atmosphericlifetime of methane is about 11 years.o  Nitrous oxide (N2O):  Nylon production, nitric acid productionand automobiles with three-way catalysts have been identified aspossibly significant anthropogenic global sources of N2O. However, the sum of all known anthropogenic and natural sources isstill barely sufficient to balance the calculated atmospheric sinkor to explain the observed increase in the atmospheric abundanceof N2O.o  Global Warming Potentials:  Gases can exert a radiative forcingboth  directly and indirectly.  Direct forcing occurs when the gasitself is a greenhouse gas; indirect forcing occurs when chemicaltransformation of the original gas produces or destroys a gas orgases which themselves are greenhouse gases.  The concept of theGlobal Warming Potential (GWP) has been developed as a measure ofthe possible warming effect of each gas relative to CO2.  For manygases, the practical utility of the GWP depends on adequatequantification of the indirect effects as well as the direct. Thedirect components of the GWPs have been recalculated, taking intoaccount revised estimated lifetimes, for a set of time horizonsranging from 20 to 500 years, with CO2 as a reference gas. Because of incomplete understanding of chemical processes, most ofthe indirect GWPs reported in IPCC (1990) are likely to be insubstantial error.  Working Group I is not yet in a position torecommend revised numerical values.  For methane, it is known thatthe indirect GWP is positive and could be comparable in magnitudeto its direct value.  Recalculated GWP values for selected keygases for the 100 year time horizon  (note that GWPs depend on thetime horizon) :                            Direct GWP   Sign of the indirect GWP      Carbon dioxide              1          none      Methane (CH4)              11          positive      Nitrous oxide (N2O)       270          uncertain      CFC-11                   3400          negative      CFC-12                   7100          negative      HCFC-22                  1600          negative      HFC-134a                 1200          none[3]  Climate Observationso  The anomalously high global mean surface temperatures of thelate 1980s have continued into 1990 and 1991 which are the warmestyears in the record.   o  Average warming over parts of the Northern Hemispheremid-latitude continents has been found to be largely characterizedby increases in minimum (night-time) rather than maximum (daytime)temperatures.  These changes appear to be partly related toincreases in cloudiness but other factors cannot be excluded suchas a direct cooling effect of aerosols on maximum temperatures insunny weather, an influence of increasing concentrations ofgreenhouse gases and some residual influence of urbanisation onminimum temperatures.  Only 25 % of the gobal land area has beenanalysed, a more complete study is needed.o  There has been considerable interest in mid-tropospherictemperature observations made since 1979 from the MicrowaveSounding Unit (MSU) aboard the TIROS-N satellites.  The MSU datahave a truly global coverage but there is only a short record (13years) of measurements; the surface and the radiosonde data areless spatially complete but have much longer records (over 130 andnear 30 years respectively).  Globally-averaged trends in MSU,radiosonde and surface data sets between 1979 and 1991 differsomewhat (0.06, 0.17, and 0.18 oC per decade, respectively),although the differences are not statistically significant. Satellite sounders, radiosonde and surface instruments all havedifferent measurement characteristics; in addition, geographicaland temporal variations in mid-tropospheric and surfacetemperatures are not expected to be identical.  Despite this,correlations between global annual values of the three data setsare quite high.o  The volcanic eruption of Mount Pinatubo in 1991 is expected tolead to transitory stratospheric warming.  With less certainty,because of other natural influences, surface and troposhericcooling may occur during the next few years.  Individual volcaniceruptions, such as that of El Chichon (1982) or Mount Pinatubo,should have negligible effect on the long-term trend.o  Some influence of solar variations on time-scales associatedwith several sunspot cycles remains unproven but is a possibility. The existence of strong correlations between characteristics ofthe solar activity cycle and global mean temperatures has beenreported.  The only immediately plausible physical explanation ofthese correlations involves variability of the sun's totalirradiance on time-scales longer than that of the 11-year activitycycle.  Since precise measurements of the irradiance are onlyavailable for the last decade, no firm conclusions regarding theinfluence of solar variability on climate change can be drawn.   [4]  Modelling and Scenarioso  The consistency between observations of global temperaturechanges over the past century  and model simulations of thewarming due to greenhouse gases over the same period  is expectedto improve  if allowance is made for the increasing evidence of acooling effect due to sulphate aerosols and stratospheric ozonedepletion.  This has not yet been done.o  There continues to be slow improvement in the ability of modelsto simulate present climate, although further improvement in themodel resolution and the parametrization of physical processes areneeded.  Since the last report, further evidence has accumulatedthat atmospheric models are capable of reproducing a range ofaspects of atmospheric variability.  Confidence in regionalclimate patterns based directly on GCM (general circulationmodels) output remains low and there is no consistent evidenceregarding changes in variability or storminess.  Given the presentincomplete knowledge of climate, the possibility of surprisescannot be ruled out.o  Transient (time-dependent) simulations with coupled ocean-atmosphere models (CGCMs), in which neither aerosols nor ozonechanges have been included, suggest a rate of global warming thatis consistent, within the range of uncertainties, with the 0.3 oCper decade warming rate quoted by IPCC (1990) for Scenario A(Business-as-Usual) of greenhouse gas emissions.  The simulatedrate of change of sea level  _due to oceanic thermal expansiononly_ ranges from 2 to 4 cm per decade, again consistent to theprevious report.  CGCMs produce variability on decadal time-scalessimilar in some respects to that observed.  Much furtherdevelopment and validation of coupled models is required.o  A set of updated scenarios have been developed for use inmodelling studies which describe a wide range of possible futureemissions in the absence of coordinated policy change to climatechange.Some of their assumptions are :      Scenario   Population   Economic Growth   Economic Growth                  by 2100       1990-2025         1990-2100      IS92a         11.3          2.9 %             2.3 %      IS92b         11.3          2.9 %             2.3 %      IS92c          6.4          2.0 %             1.2 %      IS92d          6.4          2.7 %             2.0 %      IS92e         11.3          3.5 %             3.0 %      IS92f         17.6          2.9 %             2.3 %In addition, each scenario includes assumptions on energysupplies, emission controls, sources and sinks of greenhousegases, forest biomass and deforestation.  IS92e and IS92f resultin higher CO2 emissions than SA90  (Scenario A, Business as Usual,from IPCC 1990). IS92a and IS92b result in slightly lower,  IS92dand IS92c in much lower CO2 emissions than SA90.  The revisedscenarios for CFCs are much lower than in SA90.  Scenarios must beused with great caution, scenario outputs are not predictions ofthe future.  Population and economic growth, structural changes ineconomies, technological advance, or developments such as those inthe republics of the former  Soviet Union are among the factorswhich could exert major influence on future levels of CO2emisssions.[5]  Key Uncertainties and Further Work RequiredPrediction of the timing, magnitude and regional patterns ofclimate change is affected by uncertainties which continue to berooted in the inadequate understanding of :o  Sources and sinks of greenhouse gases and aerosols (resultingfrom sulphur emissions), including their indirect effects onglobal warming.o  Clouds  (their feedback effect on greenhouse gas-induced globalwarming, also the effect of aerosols on clouds and their radiativeproperties)  and other elements of the atmospheric water budget. o  Oceans, which through their thermal inertia and possiblechanges in circulation, influence the timing and pattern ofclimate change.o  Polar ice sheets, whose response to climate change also affectspredictions of sea level rise.o  Land surface processes and feedbacks, including hydrologicaland ecological processes which couple regional and globalclimates.Reduction of these uncertainties requires, among others, improvedobservations of climate-forcing variables and of the relevantvariables of the climate system, better understanding ofclimate-related processes, more detailed knowledge of past climatechanges, and improved international exchange of climate data. Scenarios depend not only on factors which can be addressed by thenatural sciences but also on factors such as population andeconomic growth and energy policy where there is much uncertaintyand which are the concern of the social sciences.  Natural andsocial scientists need to cooperate closely in the development ofscenarios of future emissions.[6]  ReferencesIntergovernmental Panel on Climate Change  (WMO, UNEP)   1992 IPCC Supplement.   February, 1992.   IPCC Secretariat, World Meteorological Organization   Case Postale 2300,  CH-1211  Geneve  2,  SwitzerlandClimate Change  -  The IPCC Scientific Assessment   Report Prepared for IPCC by Working Group I   Houghton, J.T.,  G.J. Jenkins,  J.J. Ephraums  (eds.)   Cambridge Univ. Press, Cambridge, UK 1990   ISBN 0-521-40720-6  paperback  (approx.  US:$35, D:DM60)Climate Change  -  The IPCC Impacts Assessment   Report prepared for IPCC by Working Group II   Tegart, W.J.McG.,  G.W.Sheldon,  D.C.Griffiths  (eds.)   Australian Government Publishing Service, Canberra/New York(?)1990   ISBN 0-644-13497-6  paperback  (approx.  US:$25, D:DM55 bysurface)Climate Change  -  The IPCC Response Strategies   [ anonymous, prepared by Working Group III ]   Island Press, Washington, D.C. 1991   ISBN 0-55963-102-3  paperback  (approx.  US: ? , D:DM70 by air)WT03-B20-52IA006-000057-B011-89http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind/dkwind.html 138.80.61.12 19970221181542 text/html 2216HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:45:57 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 2045Last-modified: Tue, 09 Jul 1996 02:26:07 GMT Wind Plants of Denmark Wind Plants of Denmark The Scandinavian country of Denmark contains the largest concentration of wind turbines outside California and Germany. Nearly one percent of the nation's 5 millioninhabitants own a wind turbine or own a share in a wind turbine. Denmark's 3,700 wind turbines generate more than 1 Terawatt-hour (1,000,000,000 kWh) electricity per year, about 3.5% of national consumption. In some provinces, such as thaton the west coast of Jutland, wind turbines now provide as much as 7% of theregion's electricity. Unlike California, where all the wind turbines are installed in massive wind power plants75% of the wind power capacity in Denmark is installed in single units or in small clusters. Most of the wind turbines in Denmark are owned cooperatively. Tourists Visiting Velling Mærsk-Tændpibe Until the early 1990s Velling Mærsk-Tændpibe was Europe's largest wind powerplant. The 100 turbines overlooking Ringkøbing fjord on the west coast of the Jutland Peninsulastand in a geometric array on an old lake bed. Farmers till the soil to the base ofthe tubular towers. Photo copyright © by Paul Gipe. All rights reserved. Paul Gipe, 20.08.95, pgipe@igc.apc.org More information on wind energy. Wind Energy Comesof Age Wind Power for Home & Business Workgroup Windturbines WT03-B20-53IA006-000055-B013-445http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind/inwind.html 138.80.61.12 19970221181459 text/html 1499HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:45:21 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1328Last-modified: Tue, 09 Jul 1996 02:26:08 GMT Wind Plants of India Wind Plants of India India is surpassed only by Germany as one of the world's fastest growing markets for wind energy. By the mid 1990s the subcontinent was installing more wind generating capacity than North America, Denmark, Britian, and the Netherlands. Vestas V15s, Okha, Gujarat The ten machines near Okha in the province of Gujarat were some of the first wind turbines installed in India. These 15-meter Vestas wind turbines overlook the Arabian Sea. Photo copyright © by Paul Gipe. All rights reserved. More information on wind energy. Wind Energy Comes of Age Wind Power for Home & Business Workgroup Windturbines Paul Gipe, 20.08.95, pgipe@igc.apc.orgWT03-B20-54IA005-000051-B018-365http://lacebark.ntu.edu.au:80/j_mitroy/sid101/nuke1/nip30.html 138.80.61.12 19970221152105 text/html 13077HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:51:20 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 12905Last-modified: Tue, 09 Jul 1996 02:25:43 GMT Glossary Glossary Nuclear Issues Briefing Paper 30 May 1995 The following is a list of terms which are commonly usedin discussion of the uranium industry and the nuclear fuel cycle. Activation product: A radioactive isotope of an element(eg in the steel of a reactor core) which has been created byneutron bombardment. Alpha particle: A positively-charged particle from thenucleus of an atom, emitted during radioactive decay. Atom: A particle of matter which cannot be broken up bychemical means. Atoms have a nucleus consisting of positively-chargedprotons and uncharged neutrons of the same mass. The positivecharges on the protons are balanced by a number of negatively-chargedelectrons in motion around the nucleus. Atomic bomb: An explosive device whose energy comes fromthe fission of heavy elements such as uranium or plutonium. Beta particle: A particle emitted from an atom duringradioactive decay. Biological shield: A mass of absorbing material placedaround a reactor or radioactive material to reduce the radiation(especially neutrons and gamma rays respectively) to a level safefor humans. Boiling water reactor (BWR): A common type of light waterreactor (LWR), where water is allowed to boil in the core thusgenerating steam directly in the reactor vessel. Breed: To form fissile nuclei, usually as a result ofneutron capture, possibly followed by radioactive decay. Breeder reactor: see Fast Breeder Reactor and Fast NeutronReactor. Chain reaction: A reaction that stimulates its own repetition,in particular where the neutrons originating from nuclear fissioncause an ongoing series of fission reactions. Control rods: Devices to absorb neutrons so that the chainreaction in a reactor core may be slowed or stopped. Conversion: Chemical process turning U308 into UF6 preparatoryto enrichment. Core: The central part of a nuclear reactor containingthe fuel elements and any moderator. Critical mass: The smallest mass of fissile material thatwill support a self-sustaining chain reaction under specifiedconditions. Decay: The radioactive disintegration of an atomic nucleusresulting in the release of alpha or beta particles or gamma radiation. Decommissioning: Removal of a facility (eg reactor) fromservice and making the site available for unrestricted use. Depleted uranium: Uranium having less than the natural0.7% U-235. As a by-product of enrichment in the fuel cycle itgenerally has 0.20-0.25% U-235, the rest being U-238. Can beblended with highly-enriched uranium (eg from weapons) to makereactor fuel. Deuterium: "Heavy hydrogen", an isotope havingone proton and one neutron in the nucleus. It occurs in natureas 1 atom to 6500 atoms of normal hydrogen, (Hydrogen atoms containone proton and no neutrons). Element: A chemical substance that cannot be divided intosimple substances by chemical means; atomic species with samenumber of protons. Enriched uranium: Uranium in which the proportion of U-235(to U-238) has been increased above the natural 0.7%. Reactor-gradeuranium is usually enriched to about 3% U-235, weapons-grade uraniumis usually more than 90% U-235. Enrichment: Physical process of increasing the proportionof U-235 to U238. Fast breeder reactor (FBR): A fast neutron reactor (qv)configured to produce more fissile material than it consumes,using fertile material such as depleted uranium. Fast neutron reactor: A reactor with no moderator andhence utilising fast neutrons and able to utilise fertile materialsuch as depleted uranium. Fertile (of an isotope): Capable of becoming fissile,by capturing one or more neutrons, possibly followed by radioactivedecay. U-238 is an example. Fissile (of an isotope): Capable of capturing a neutronand undergoing nuclear fission, e.g. U-235, Pu-239. Fission: The splitting of a heavy nucleus into two, accompaniedby the release of a relatively large amount of heat and generallyone or more neutrons. It may be spontaneous but usually is dueto nuclear absorption of a neutron. Fission products: Daughter nuclei resulting either fromthe fission of heavy elements such as uranium, or the radioactivedecay of those primary daughters. Usually highly radioactive. Fossil fuel: A fuel based on carbon presumed to be originallyfrom living matter, eg coal, oil, gas. Burned with oxygen toyield energy. Fuel fabrication: Making reactor fuel elements, usuallyfrom UO2. Gamma rays: High energy electro-magnetic radiation. Genetic mutation: Sudden change in the chromosomal DNAof an individual gene. lt may produce inherited changes in descendants. Mutation can be made more frequent by irradiation. Graphite: A form of carbon used in very pure form as amoderator, principally in gas-cooled reactors, but also in Soviet-designedRBMK reactors. Greenhouse gases: Radiative gases in the earth's atmospherewhich absorb long-wave heat radiation from the earth's surfaceand re-radiate it, there-by warming the earth. Carbon dioxideand water vapour are the main ones. Half-life: The period required for half of the atoms ofa particular radioactive isotope to decay and become an isotopeof another element. Heavy water: Water containing an elevated concentrationof molecules with deuterium ("heavy hydrogen") atoms. Heavy water reactor (HWR): A reactor which uses heavywater as its moderator, eg Canadian CANDU. High-level wastes: Extremely radioactive fission productsand transuranic elements (usually other than plutonium) separatedas a result of reprocessing spent nuclear fuel. Highly-enriched uranium: Uranium enriched to more than20% U-235. That in weapons is about 90% U235. Ionising radiation: Radiation (including alpha particles)capable of breaking chemical bonds, thus causing ionisation ofthe matter through which it passes and damage to living tissue. Isotope: An atomic form of an element having a particularnumber of neutrons. Different isotopes of an element have thesame number of protons but different numbers of neutrons and hencedifferent atomic masses, e.g. U-235, U238. Light water: Ordinary water (H20) as distinct from heavywater. Light water reactor (LWR): A common nuclear reactor cooledand usually moderated by ordinary water. Megawatt (MW): A unit of power, = 106 watts. MWerefers to electric output from a generator, MWt to thermal outputfrom a reactor or heat source (eg the gross output of a reactoritself). Metal fuels: Natural uranium metal as used in a gas-cooledreactor. Micron: One thousandth of a millimetre (10-6m). Mixed oxide fuel (MOX): Reactor fuel which consists ofboth uranium and plutonium oxides, usually with about 5% Pu. Moderator: A material such as water or graphite used ina reactor to slow down fast neutrons so as to expedite furtherthermal fission. Natural uranium: Uranium with an isotopic compositionas found in nature, containing 99.3% U-238, 0.7% U-235 and a traceof U-234. Neutron: An uncharged elementary particle found in thenucleus of every atom except hydrogen. Solitary mobile neutronstravelling at various speeds originate from fission reactions. Slow neutrons can in turn readily cause fission in atoms of someisotopes, e.g. U-235, and fast neutrons can readily cause fissionin atoms of others, e.g. Pu-239. Sometimes atomic nuclei simplycapture neutrons. Nuclear reactor: A device in which a nuclear fission chainreaction occurs under controlled conditions so that the heat yieldcan be harnessed or the neutron beams utilised. All commercialreactors are thermal reactors, using a moderator to slow downthe neutrons. Oxide fuels: Enriched or natural uranium in the form ofthe oxide U02, used in many types of reactor. Plutonium: A transuranic element, formed in a nuclearreactor by neutron capture. It has several isotopes, some ofwhich are fissile and some of which undergo spontaneous fission,releasing neutrons. Weapons-grade plutonium is produced with>90% Pu-239, reactor-grade plutonium contains about 30% non-fissileisotopes. Pressurised water reactor (PWR): The most common typeof light water reactor (LWR). Radiation: The emission and propagation of energy by meansof electromagnetic waves or particles. Radioactivity: The spontaneous decay of an unstable atomicnucleus, giving rise to the emission of radiation. Radionuclide: A radioactive isotope of an element. Radiotoxicity: The adverse health effect of a radionuclidedue to its radioactivity. Radium: An element often found in uranium ore. It hasseveral radioactive isotopes. Radium-226 decays to radon222. Radon (Rn): A heavy radioactive gas given off by rockscontaining radium (or thorium). Radon daughters: Decay products of radon-222. Reactor: see Nuclear Reactor. Reprocessing: Separation of uranium and/or plutonium fromused reactor fuel and the production of a much reduced quantityof high-level waste. Separative Work Unit (SWU): This is a complex unit which is afunction of the amount of uranium processed and the degree to which it isenriched, ie the extent of increase in the concentration of the U-235isotope relative to the remainder. The unit is strictly: KilogramSeparative Work Unit, and it measures the quantity of separative work(indicative of energy used in enrichment) when feed and product quantitiesare expressed in kilograms. For instance, to produce one kilogram of uranium enriched to 3% U-235requires 3.8 SWU if the plant is operated at a tails assay 0.25%, or 5.0SWU if the tails assay is 0.15% (thereby requiring only 5.1 kg instead of6.0 kg of natural U feed). About 100-120,000 SWU is required to enrich the annual fuel loading for atypical 1000 MWe light water reactor. Enrichment costs are related toelectrical energy used. The gaseous diffusion process consumes some 2400kWh per SWU, while gas centrifuge plants require only 100-200 kWh/SWU. Sievert (Sv): Unit indicating the biological damage causedby radiation. One Joule of beta or gamma radiation absorbed perkilogram of tissue has 1 Sv of biological effect; 1 J/kg of alpharadiation has 20 Sv effect and 1 J/kg of neutrons has 10 Sv effect. Stable: Incapable of spontaneous radioactive decay. Tailings: Ground rock remaining after particular ore minerals(e.g. uranium oxides) are extracted. Tails: Depleted uranium (cf. enriched uranium), with about0.3% U-235. Thermal reactor: A reactor in which the fission chainreaction is sustained primarily by slow neutrons (as distinctfrom Fast Neutron Reactor). Transuranic element: A very heavy element formed artificiallyby neutron capture and subsequent beta decay(s). Has a higheratomic number than uranium (92). All are radioactive. Neptunium,plutonium and americium are the best-known. Uranium: A mildly radioactive element with two isotopeswhich are fissile (U235 and U-233) and two which are fertile (U-238and U-234). Uranium is the basic raw material of nuclear energy. Uranium hexafluoride (UF6): A compound of uranium whichis a gas above 56oC and is thus a suitable form in which to enrichthe uranium. Uranium oxide concentrate (U308): The mixture of uraniumoxides produced after milling uranium ore from a mine. Sometimesloosely called yellowcake. It is khaki in colour and is usuallyrepresented by the empirical formula U308. Uranium is exportedfrom Australia in this form. Vitrification: The incorporation of high-level wastesinto borosilicate glass, to make up about 14% of the product bymass. Waste: Low-level waste is mildly radioactive materialusually disposed of by incineration and burial. High-level waste is highly radioactive material arisingfrom nuclear fission. It requires very careful handling, storageand disposal. Yellowcake: Ammonium diuranate, the penultimate uraniumcompound in U308 production, but the form in which mine productwas sold until about 1970. See also Uranium oxide. Return to Index WT03-B20-55IA005-000051-B018-152http://lacebark.ntu.edu.au:80/j_mitroy/sid101/ipcc/sumwg1.html 138.80.61.12 19970221151919 text/html 20382HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:49:38 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 20210Last-modified: Tue, 09 Jul 1996 02:26:01 GMT IPCC Working Group I 1995 Summary for Policymakers Considerable progress has been made in the understanding of climatechange science since 1990 and new data and analyses have becomeavailable. Greenhouse gas concentrations have continued to increase Increases in greenhouse gas concentrations since pre-industrialtimes (i.e. since about 1750) have led to a positive radiativeforcing of climate, tending to warm the surface and to produceother changes of climate. ° The atmospheric concentrations of greenhouse gases,inter alia carbon dioxide (CO2), methane (CH4) and nitrousoxide (N2O) have grown significantly: by about 30%, 145%, and15%, respectively (values for 1992). These trends can be attributedlargely to human activities, mostly fossil fuel use, land-usechange and agriculture. ° The growth rates of CO2, CH4 and N2O concentrations werelow during the early 1990s. While this apparently natural variationis not yet fully explained, recent data indicate that the growthrates are currently comparable to those averaged over the 1980s. ° The direct radiative forcing of the long-lived greenhousegases (2.45 Wm-2) is due primarily to increases in the concentrationsof CO2 (1.56 Wm-2), CH4 (0.47 Wm-2) and N2O (0.14 Wm-2) (valuesfor 1992). ° Many greenhouse gases remain in the atmosphere for a longtime (for CO2 and N2O, many decades to centuries), hence theyaffect radiative forcing on long time-scales. ° The direct radiative forcing due to the CFCs and HCFCscombined is 0.25 Wm-2. However, their net radiative forcingis reduced by about 0.1 Wm-2 because they have caused stratosphericozone depletion which gives rise to a negative radiative forcing. ° Growth in the concentration of CFCs, but not HCFCs, hasslowed to about zero. The concentrations of both CFCs and HCFCs,and their consequent ozone depletion, are expected to decreasesubstantially by 2050 through implementation of the Montreal Protocoland its Adjustments and Amendments. ° At present some long-lived greenhouse gases (particularlyHFCs (a CFC substitute), PFCs and SF6) contribute little to radiativeforcing but their projected growth could contribute several percent to radiative forcing during the 21st century. °If carbon dioxide emissions were maintained at near current(1994) levels, they would lead to a nearly constant rate of increasein atmospheric concentrations for at least two centuries, reachingabout 500 ppmv (approaching twice the pre-industrial concentrationof 280 ppmv) by the end of the 21st century. ° A range of carbon cycle models indicates that stabilisationof atmospheric CO2 concentrations at 450, 650 or 1000 ppmv couldbe achieved only if global anthropogenic CO2 emissions drop to1990 levels by, respectively, approximately 40, 110 or 240 yearsfrom now, and drop substantially below 1990 levels subsequently. ° Any eventual stabilised concentration is governed moreby the accumulated anthropogenic CO2 emissions from now untilthe time of stabilisation, than by the way those emissions changeover the period. This means that, for a given stabilised concentrationvalue, higher emissions in early decades require lower emissionslater on. Among the range of stabilisation cases studied, forstabilisation at 450, 650 or 1000 ppmv accumulated anthropogenicemissions over the period 1991 to 2100 are 630 GtC, 1030 GtC,and 1410 GtC respectively (± approximately 15% in each case).For comparison the corresponding accumulated emissions for IPCCIS92 emission scenarios range from 770 to 2190 GtC. ° Stabilisation of CH4 and N2O concentrations at today'slevels would involve reductions in anthropogenic emissions of8% and more than 50% respectively. ° There is evidence that tropospheric ozone concentrationsin the Northern Hemisphere have increased since pre-industrialtimes because of human activity and that this has resulted ina positive radiative forcing. This forcing is not yet well characterised,but it is estimated to be about 0.4 Wm-2 (15% of that from thelong-lived greenhouse gases). However the observations of themost recent decade show that the upward trend has slowed significantlyor stopped. Anthropogenic aerosols tend to produce negative radiative forcings ° Tropospheric aerosols (microscopic airborne particles)resulting from combustion of fossil fuels, biomass burning andother sources have led to a negative direct forcing of about 0.5Wm-2, as a global average, and possibly also to a negative indirectforcing of a similar magnitude. While the negative forcing isfocused in particular regions and subcontinental areas, it canhave continental to hemispheric scale effects on climate patterns. ° Locally, the aerosol forcing can be large enough to morethan offset the positive forcing due to greenhouse gases. ° In contrast to the long-lived greenhouse gases, anthropogenicaerosols are very short-lived in the atmosphere, hence their radiativeforcing adjusts rapidly to increases or decreases in emissions. Climate has changed over the past century At any one location year-to-year variations in weather can belarge, but analyses of meteorological and other data over largeareas and over periods of decades or more have provided evidencefor some important systematic changes. ° Global mean surface air temperature has increased by betweenabout 0.3 and 0.6lC since the late 19th century ; the additionaldata available since 1990 and the re-analyses since then havenot significantly changed this range of estimated increase. ° Recent years have been among the warmest since 1860, i.e.,in the period of instrumental record, despite the cooling effectof the 1991 Mt. Pinatubo volcanic eruption. ° Night-time temperatures over land have generally increasedmore than daytime temperatures. ° Regional changes are also evident. For example, the recentwarming has been greatest over the mid-latitude continents inwinter and spring, with a few areas of cooling, such as the NorthAtlantic ocean. Precipitation has increased over land in highlatitudes of the Northern Hemisphere, especially during the coldseason. ° Global sea level has risen by between 10 and 25 cm overthe past 100 years and much of the rise may be related to theincrease in global mean temperature. ° There are inadequate data to determine whether consistentglobal changes in climate variability or weather extremes haveoccurred over the 20th Century. On regional scales there is clearevidence of changes in some extremes and climate variability indicators(e.g., fewer frosts in several widespread areas; an increase inthe proportion of rainfall from extreme events over the contiguousstates of the USA). Some of these changes have been toward greatervariability; some have been toward lower variability. ° The 1990 to mid-1995 persistent warm-phase of the El Niño-Southern Oscillation (which causes droughts and floods in manyareas) was unusual in the context of the last 120 years. The balance of evidence suggests a discernible human influenceon global climate Any human-induced effect on climate will be superimposed on thebackground "noise" of natural climate variability, whichresults both from internal fluctuations and from external causessuch as solar variability or volcanic eruptions. Detection andattribution studies attempt to distinguish between anthropogenicand natural influences. "Detection of change" is theprocess of demonstrating that an observed change in climate ishighly unusual in a statistical sense, but does not provide areason for the change. "Attribution" is the processof establishing cause and effect relations, including the testingof competing hypotheses. Since the 1990 IPCC Report, considerable progress has been madein attempts to distinguish between natural and anthropogenic influenceson climate. This progress has been achieved by including effectsof sulphate aerosols in addition to greenhouse gases, thus leadingto more realistic estimates of human-induced radiative forcing.These have then been used in climate models to provide more completesimulations of the human-induced climate-change 'signal'. In addition,new simulations with coupled atmosphere-ocean models have providedimportant information about decade to century time-scale naturalinternal climate variability. A further major area of progressis the shift of focus from studies of global-mean changes to comparisonsof modelled and observed spatial and temporal patterns of climatechange. The most important results related to the issues of detectionand attribution are: ° The limited available evidence from proxy climate indicatorssuggests that the 20th century global mean temperature is at leastas warm as any other century since at least 1400 AD. Data priorto 1400 are too sparse to allow the reliable estimation of globalmean temperature. ° Assessments of the statistical significance of the observedglobal mean surface air temperature trend over the last centuryhave used a variety of new estimates of natural internal and externally-forcedvariability. These are derived from instrumental data, palaeodata,simple and complex climate models, and statistical models fittedto observations. Most of these studies have detected a significantchange and show that the observed warming trend is unlikely tobe entirely natural in origin. ° More convincing recent evidence for the attribution ofa human effect on climate is emerging from pattern-based studies,in which the modelled climate response to combined forcing bygreenhouse gases and anthropogenic sulphate aerosols is comparedwith observed geographical, seasonal and vertical patterns ofatmospheric temperature change. These studies show that such patterncorrespondences increase with time, as one would expect as ananthropogenic signal increases in strength. Furthermore, the probabilityis very low that these correspondences could occur by chance asa result of natural internal variability only. The vertical patternsof change are also inconsistent with those expected for solarand volcanic forcing. ° Our ability to quantify the human influence on global climateis currently limited because the expected signal is still emergingfrom the noise of natural variability, and because there are uncertaintiesin key factors. These include the magnitude and patterns of longterm natural variability and the time-evolving pattern of forcingby, and response to, changes in concentrations of greenhouse gasesand aerosols, and land surface changes. Nevertheless, the balanceof evidence suggests that there is a discernible human influenceon global climate. Climate is expected to continue to change in the future The IPCC has developed a range of scenarios, IS92a-f, of futuregreenhouse gas and aerosol precursor emissions based on assumptionsconcerning population and economic growth, land-use, technologicalchanges, energy availability and fuel mix during the period 1990to 2100. Through understanding of the global carbon cycle andof atmospheric chemistry, these emissions can be used to projectatmospheric concentrations of greenhouse gases and aerosols andthe perturbation of natural radiative forcing. Climate modelscan then be used to develop projections of future climate. ° The increasing realism of simulations of current and pastclimate by coupled atmosphere-ocean climate models has increasedour confidence in their use for projection of future climate change.Important uncertainties remain, but these have been taken intoaccount in the full range of projections of global mean temperatureand sea level change. ° For the mid-range IPCC emission scenario, IS92a, assumingthe "best estimate" value of climate sensitivity andincluding the effects of future increases in aerosol, models projectan increase in global mean surface air temperature relative to1990 of about 2lC by 2100. This estimate is approximately onethird lower than the "best estimate" in 1990. This isdue primarily to lower emission scenarios (particularly for CO2and the CFCs), the inclusion of the cooling effect of sulphateaerosols, and improvements in the treatment of the carbon cycle.Combining the lowest IPCC emission scenario (IS92c) with a "low"value of climate sensitivity and including the effects of futurechanges in aerosol concentrations leads to a projected increaseof about 1lC by 2100. The corresponding projection for the highestIPCC scenario (IS92e) combined with a "high" value ofclimate sensitivity gives a warming of about 3.5lC. In all casesthe average rate of warming would probably be greater than anyseen in the last 10,000 years, but the actual annual to decadalchanges would include considerable natural variability. Regionaltemperature changes could differ substantially from the globalmean value. Because of the thermal inertia of the oceans, only50-90% of the eventual equilibrium temperature change would havebeen realised by 2100 and temperature would continue to increasebeyond 2100, even if concentrations of greenhouse gases were stabilisedby that time. ° Average sea level is expected to rise as a result of thermalexpansion of the oceans and melting of glaciers and ice-sheets.For the IS92a scenario, assuming the "best estimate"values of climate sensitivity and of ice melts ensitivi ty towarming, and including the effects of future changes in aerosol,models project an increase in sea level of about 50 cm from thepresent to 2100. This estimate is approximately 25% lower thanthe "best estimate" in 1990 due to the lower temperatureprojection, but also reflecting improvements in the climate andice melt models. Combining the lowest emission scenario (IS92c)with the "low" climate and ice melt sensitivities andincluding aerosol effects gives a projected sea level rise ofabout 15 cm from the present to 2100. The corresponding projectionfor the highest emission scenario (IS92e) combined with "high"climate and ice-melt sensitivities gives a sea level rise of about95 cm from the present to 2100. Sea level would continue to riseat a similar rate in future centuries beyond 2100, even if concentrationsof greenhouse gases were stabilised by that time, and would continueto do so even beyond the time of stabilisation of global meantemperature. Regional sea level changes may differ from the globalmean value owing to land movement and ocean current changes. ° Confidence is higher in the hemispheric-to-continentalscale projections of coupled atmosphere-ocean climate models thanin the regional projections, where confidence remains low. Thereis more confidence in temperature projections than hydrologicalchanges. ° All model simulations, whether they were forced with increasedconcentrations of greenhouse gases and aerosols or with increasedconcentrations of greenhouse gases alone, show the following features:greater surface warming of the land than of the sea in winter;a maximum surface warming in high northern latitudes in winter,little surface warming over the Arctic in summer; an enhancedglobal mean hydrological cycle, and increased precipitation andsoil moisture in high latitudes in winter. All these changes areassociated with identifiable physical mechanisms. ° In addition, most simulations show a reduction in the strengthof the north Atlantic thermohaline circulation and a widespreadreduction in diurnal range of temperature. These features toocan be explained in terms of identifiable physical mechanisms. ° The direct and indirect effects of anthropogenic aerosolshave an important effect on the projections. Generally, the magnitudesof the temperature and precipitation changes are smaller whenaerosol effects are represented, especially in northern mid-latitudes.Note that the cooling effect of aerosols is not a simple offsetto the warming effect of greenhouse gases, but significantly affectssome of the continental scale patterns of climate change, mostnoticeably in the summer hemisphere. For example, models thatconsider only the effects of greenhouse gases generally projectan increase in precipitation and soil moisture in the Asian summermonsoon region, whereas models that include , in addition, someof the effects of aerosols suggest that monsoon precipitationmay decrease. The spatial and temporal distribution of aerosolsgreatly influence regional projections, which are therefore moreuncertain. ° A general warming is expected to lead to an increase inthe occurrence of extremely hot days and a decrease in the occurrenceof extremely cold days. ° Warmer temperatures will lead to a more vigorous hydrologicalcycle; this translates into prospects for more severe droughtsand/or floods in some places and less severe droughts and/or floodsin other places. Several models indicate an increase in precipitationintensity, suggesting a possibility for more extreme rainfallevents. Knowledge is currently insufficient to say whether therewill be any changes in the occurrence or geographical distributionof severe storms, e.g., tropical cyclones. ° Sustained rapid climate change could shift the competitivebalance among species and even lead to forest dieback, alteringthe terrestrial uptake and release of carbon. The magnitude isuncertain, but could be between zero and 200 GtC over the nextone to two centuries, depending on the rate of climate change. There are still many uncertainties Many factors currently limit our ability to project and detectfuture climate change. In particular, to reduce uncertaintiesfurther work is needed on the following priority topics: ° estimation of future emissions and biogeochemical cycling(including sources and sinks) of greenhouse gases, aerosols andaerosol precursors and projections of future concentrations andradiative properties. ° representation of climate processes in models, especiallyfeedbacks associated with clouds, oceans, sea ice and vegetation,in order to improve projections of rates and regional patternsof climate change. ° systematic collection of long-term instrumental and proxyobservations of climate system variables (e.g., solar output,atmospheric energy balance components, hydrological cycles, oceancharacteristics and ecosystem changes) for the purposes of modeltesting, assessment of temporal and regional variability and fordetection and attribution studies. Future unexpected, large and rapid climate system changes (ashave occurred in the past) are, by their nature difficult to predict.This implies that future climate changes may also involve "surprises".In particular these arise from the non-linear nature of the climatesystem. When rapidly forced, non-linear systems are especiallysubject to unexpected behaviour. Progress can be made by investigatingnon-linear processes and sub-components of the climatic system.Examples of such non-linear behaviour include rapid circulationchanges in the North Atlantic and feedbacks associated with terrestrialecosystem changes. WT03-B20-56IA006-000055-B013-340http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind/altcal.html 138.80.61.12 19970221181409 text/html 1863HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:44:27 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1692Last-modified: Tue, 09 Jul 1996 02:26:07 GMT Wind Plants of California's Altamont Pass Wind Plants of California's Altamont Pass Though the Altamont Pass still contains the world's largest concentration of wind turbines, it has been surpassed by wind turbines in the Tehachapi-Mojave area which generate more electricity. The 6,000 wind turbines in the Altmaont pass generate 1 to 1.2 TWh (1,000,000,000-1,200,000,000 kWh) per year. The Altamont Pass is one hour east of San Francisco in a series of low hills separating the bay area from the hot interior of the San Joaquin Valley. Kenetech Windpower's 56-100 Kenetech operates more than 3,500 model 56-100 wind turbines in the Altamont Pass. Unlike most other wind turbines, these machines use a rotor downwind of the tower. The model 56-100 uses a rotor 17.8 meters (58 feet) in diameter. Photo copyright © by Paul Gipe. All rights reserved. Paul Gipe, 20.08.95, pgipe@igc.apc.org More information on wind energy. Wind Energy Comesof Age Wind Power for Home & Business Workgroup Windturbines WT03-B20-57IA005-000051-B018-209http://lacebark.ntu.edu.au:80/j_mitroy/sid101/ipcc/sumwg3.html 138.80.61.12 19970221151941 text/html 68751HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:49:50 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 68579Last-modified: Tue, 09 Jul 1996 02:26:01 GMT SUMMARY FOR POLICYMAKERS SECOND ASSESSMENT REPORT, WORKING GROUP III Contents 1. Introduction 2. Scope of the Assessment 3. Decision Making Frameworks for Addressing Climate Change 4. Equity and Social Considerations 5. Intertemporal Equity and Discounting 6. Applicability of Cost and Benefit Assessments 7. The Social Costs of Climate Change: Benefits of Limiting GreenhouseGas Emissions and Enhancing Sinks 8. Generic Assessment of Response Strategies 9. Costs of Response Options 10. Integrated Assessment 11. An Economic Assessment of Policy Instruments for CombatingClimate Change SUMMARY FOR POLICYMAKERS SECOND ASSESSMENT REPORT, WORKING GROUP III 1. INTRODUCTION Working Group III of the Intergovernmental Panel on Climate Change(IPCC) was restructured in November 1992 and charged with conducting"technical assessments of the socioeconomics of impacts,adaptation, and mitigation of climate change over both the shortand long term and at the regional and global levels." WorkingGroup III responded to this charge by further stipulating in itswork plan that it would place the socioeconomic perspectives inthe context of sustainable development and, in accordance withthe Framework Convention on Climate Change (FCCC), provide comprehensivetreatment of both mitigation and adaptation options while coveringall economic sectors and all relevant sources of greenhouse gasesand sinks. This report assesses a large part of the existing literature onthe socioeconomics of climate change and identifies areas in whicha consensus has emerged on key issues and areas where differencesexist. The chapters have been arranged so that they cover severalkey issues. First, frameworks for socioeconomic assessment ofcosts and benefits of action and inaction are described. Particularattention is given to the applicability of cost-benefit analysis,the incorporation of equity and social considerations, and considerationof intergenerational equity issues. Second, the economic and socialbenefits of limiting greenhouse gas emissions and enhancing sinksare reviewed. Third, the economic, social, and environmental costsof mitigating greenhouse gas emissions are assessed. Next, genericmitigation and adaptation response options are reviewed, methodsfor assessing the costs and effectiveness of different responseoptions are summarized, and integrated assessment techniques arediscussed. Finally, the report provides an economic assessmentof policy instruments to combat climate change. In accordance with the approved work plan, this assessment ofthe socioeconomic literature related to climate change focuseson economic studies; material from other social sciences is foundmostly in the chapter on equity and social considerations. Thereport is an assessment of the state of knowledge - what we knowand do not know - and not a prescription for policy implementation.Countries can use the information in this report to help takedecisions they believe are most appropriate for their specificcircumstances. 2. SCOPE OF THE ASSESSMENT Climate change presents the decision maker with a set of formidablecomplications: a considerable number of remaining uncertainties(which are inherent in the complexity of the problem), the potentialfor irreversible damages or costs, a very long planning horizon,long time lags between emissions and effects, wide regional variationin causes and effects, the irreducibly global scope of the problem,and the need to consider multiple greenhouse gases and aerosols.Yet another complication arises from the fact that effective protectionof the climate system requires global cooperation. Still, a number of insights that may be useful to policymakerscan be drawn from the literature: · Analyses indicate that a prudent way to deal with climatechange is through a portfolio of actions aimed at mitigation,adaptation, and improvement of knowledge. The appropriate portfoliowill differ for each country. The challenge is not to find thebest policy today for the next 100 years, but to select a prudentstrategy and to adjust it over time in the light of new information. · Earlier mitigation action may increase flexibility in movingtoward stabilization of atmospheric concentrations of greenhousegases (U.N. Framework Convention on Climate Change, Article 2).The choice of abatement paths involves balancing the economicrisks of rapid abatement now (that premature capital stock retirementwill later be proved unnecessary) against the corresponding riskof delay (that more rapid reduction will then be required, necessitatingpremature retirement of future capital stock). · The literature indicates that significant "no-regrets"opportunities are available in most countries and that the riskof aggregate net damage due to climate change, considerationsof risk aversion, and application of the precautionary principleprovide rationales for action beyond no regrets. · The value of better information about climate change processesand impacts and society's responses to them is likely to be great.In particular, the literature accords high value to informationabout climate sensitivity to greenhouse gases and aerosols, climatechange damage functions, and variables such as determinants ofeconomic growth and rates of energy efficiency improvements. Betterinformation about the costs and benefits of mitigation and adaptationmeasures and how they might change in coming decades also hasa high value. · Analysis of economic and social issues related to climatechange, especially in developing countries where little work ofthis nature has been carried out, is a high priority for research.More generally, research is needed on integrated assessment andanalysis of decision making related to climate change. Further,research advancing the economic understanding of nonlinearitiesand new theories of economic growth is also needed. Research anddevelopment related to energy efficiency technologies and nonfossilenergy options also offer high potential value. In addition, thereis also a need for research on the development of sustainableconsumption patterns. A portfolio of possible actions that policymakers could consider,in accordance with applicable international agreements, to implementlow-cost and/or cost-effective measures to reduce emissions ofgreenhouse gases and adapt to climate change can include: · implementing energy efficiency measures, including theremoval of institutional barriers to energy efficiency improvements; · phasing out existing distortionary policies and practicesthat increase greenhouse gas emissions, such as some subsidiesand regulations, non-internalization of environmental costs, anddistortions in transport pricing; · implementing cost-effective fuel switching measures frommore to less carbon-intensive fuels and to carbon-free fuels suchas renewables; · implementing measures to enhance sinks or reservoirs ofgreenhouse gases, such as improving forest management and landuse practices; · implementing measures and developing new techniques forreducing methane, nitrous oxide, and other greenhouse gas emissions; · encouraging forms of international cooperation to limitgreenhouse gas emissions, such as implementing coordinated carbon/energytaxes, activities implemented jointly, and tradable quotas; · promoting the development and implementation of nationaland international energy efficiency standards; · promoting voluntary actions to reduce greenhouse gas emissions; · promoting education and training, implementing informationand advisory measures for sustainable development and consumptionpatterns that will facilitate climate change mitigation and adaptation; · planning and implementing measures to adapt to the consequencesof climate change; · undertaking research aimed at better understanding of thecauses and impacts of climate change and facilitating more effectiveadaptation to it; · conducting technological research aimed at minimizing emissionsof greenhouse gases from continued use of fossil fuels and developingcommercial nonfossil energy sources; · developing improved institutional mechanisms, such as improvedinsurance arrangements, to share the risks of damages due to climatechange. Contribution of Economics · Estimates of the costs and benefits of stabilizing greenhousegas concentrations are sensitive, inter alia, tothe ultimate target concentration, the emission path toward thislevel, the discount rate, and assumptions concerning the costsand availability of technologies and practices. · Despite its widespread use in economic policy evaluation,Gross Domestic Product is widely recognized to be an imperfectmeasure of society's well-being, largely because it fails to accountfor degradation of the environment and natural systems. Othermethodologies exist that try to take these nonmarket values andsocial and ecological sustainability into account. Such methodologieswould provide a more complete indication of how climate changemight affect society's well-being. · Given the interrelated nature of the global economic system,attempts to mitigate climate change through actions in one regionor sector may have offsetting economic effects that risk increasingthe emissions of other regions and sectors (so-called leakages).These emission leakages can be lessened through coordinated actionsof groups of countries. · The literature suggests that flexible, cost-effective policiesrelying on economic incentives and instruments, as well as coordinatedinstruments, can considerably reduce mitigation or adaptationcosts or increase the cost-effectiveness of emission reductionmeasures. Equity Considerations In considering equity principles and issues related to greenhousegas emissions, it is important for policy consideration to takeinto account in particular Articles 3, 4.2a, and 11.2 of the FrameworkConvention on Climate Change, Principle 2 of the Rio Declaration,and general principles of international law. Scientific analyses cannot prescribe how equity should be appliedin implementing the Framework Convention on Climate Change, butanalysis can clarify the implications of alternative choices andtheir ethical basis. · Developing countries require support for institutionaland endogenous capacity building, so that they may effectivelyparticipate in climate change decision making. · It is important that both efficiency and equity concernsbe considered during the analysis of mitigation and adaptationmeasures. For the purposes of analysis, it is possible to separateefficiency from equity. This analytical separation presupposesthat (and is valid, for policy purposes, only if) effective institutionsexist or can be created for appropriate redistribution of climatechange costs. It may be worthwhile to conduct analyses of theequity implications of particular measures for achieving efficiency,including their social considerations and impacts. 3. DECISION MAKING FRAMEWORKS FOR ADDRESSING CLIMATE CHANGE Since climate change is a global issue, comprehensive analysesof mitigation, adaptation, and research measures are needed toidentify the most efficient and appropriate strategy to addressclimate change. International decision making related to climatechange, as established by the FCCC, is a collective process inwhich a variety of concerns, such as equity, ecological protection,economics, ethics, and poverty-related issues, are of specialsignificance for present and future generations. Treatments ofdecision making under uncertainty, risk aversion, technology developmentand diffusion processes, and distributional considerations areat present relatively poorly developed in international environmentaleconomics, and especially in the climate change literature. Decision making related to climate change must take into accountthe unique characteristics of the "problem":large uncertainties (scientific and economic), possible nonlinearitiesand irreversibilities, asymmetric distribution of impacts geographicallyand temporally, the very long time horizon, and the global natureof climate change with the associated potential for free riding.Beyond scientific uncertainties (discussed in Volume 1)) and impactuncertainties (Volume 2), socioeconomic uncertainties relateto estimates of how these changes will affect human society (includingdirect economic and broader welfare impacts) and to the socioeconomicimplications of emission abatement. The other dimension that magnifies uncertainties and complicatesdecision making is geographical: climate change is a globalproblem encompassing an incredibly diverse mix of human societies,with differing histories, circumstances, and capabilities. Manydeveloping countries are in relatively hot climates, depend moreheavily on agriculture, and have less well developed infrastructureand social structures; thus, they may suffer more than average,perhaps much more. In developed countries, there may also be largeclimate change impacts. The literature also emphasizes that delaying responses is itselfa decision involving costs. Some studies suggest that the costof delay is small; others emphasize that the costs could includeimposition of risks on all parties (particularly the most vulnerable),greater utilization of limited atmospheric capacity, and potentialdeferral of desirable technical development. No consensus is reflectedin the literature. The global nature of the problem - necessitating collective actionby sovereign states - and the large differences in the circumstancesof different parties raise consequential as well as proceduralissues. Consequential issues relate to outcomes while proceduralissues relate to how decisions are made. In relation to climatechange, the existence of an agreed legal framework involves acollective process within a negotiated framework (the FCCC). Accordinglydecision making can be considered within three different categoriesof frameworks, each with different implications and with distinctfoci: global optimization (trying to find the globally optimalresult), procedural decision making (establishing and refiningrules of procedure), and collective decision making (dealing withdistributional issues and processes involving the interactionof numerous independent decision makers). Application of the literature on decision making to climate changeprovides elements that can be used in building collective and/ormarket-oriented strategies for sharing risks and realizing mutualbenefits. It suggests that actions be sequential (temporally distributed),that countries implement a portfolio of mitigation, adaptation,and research measures, and that they adjust this portfolio continuouslyin response to new knowledge. The potential for transfers of financialresources and technology to developing countries may be consideredas a part of any comprehensive analytical framework. Elements of a market-related strategy concern insurance andmarkets for risk. Pooling risk does not change the risk, butit can improve economic efficiency and welfare. Although insurancecapable of sharing climate change risks on a global basis currentlydoes not exist, one of the important potential gains from cooperatingin a collective framework, such as the Framework Convention onClimate Change, is that of risk sharing. Creating an insurancesystem to cover the risks of climate change is difficult, andthe international community has not yet established such sophisticatedinstruments. This, however, does not preclude future internationalaction to establish insurance markets sufficient for some internationalneeds. 4. EQUITY AND SOCIAL CONSIDERATIONS Equity considerations are an important aspect of climate changepolicy and of the Convention. In common language equity means"the quality of being impartial" or "somethingthat is fair and just." The FCCC, including the referencesto equity and equitable in Articles 3.1, 4.2.a, and 11.2, providesthe context for efforts to apply equity in meeting the purposesand the objective of the Convention. International law, includingrelevant decisions of the International Court of Justice, mayalso provide guidance. A variety of ethical principles, including the importance of meetingpeople's basic needs, may be relevant to addressing climate change,but the application to relations among states of principles originallydeveloped to guide individual behaviour is complex and not straightforward.Climate change policies should not aggravate existing disparitiesbetween one region and another nor attempt to redress all equityissues. Equity involves procedural as well as consequential issues. Proceduralissues relate to how decisions are made while consequential issuesrelate to outcomes. To be effective and to promote cooperation,agreements must be regarded as legitimate, and equity is an importantelement in gaining legitimacy. Procedural equity encompasses process and participation issues.It requires that all parties be able to participate effectivelyin international negotiations related to climate change. Appropriatemeasures to enable developing country parties to participate effectivelyin negotiations increase the prospects for achieving effective,lasting, and equitable agreements on how best to address the threatof climate change. Concern about equity and social impacts pointsto the need to build endogenous capabilities and strengthen institutionalcapacities, particularly in developing countries, to make andimplement collective decisions in a legitimate and equitable manner. Consequential equity has two components: the distribution of thecosts of damages or adaptation and of measures to mitigate climatechange. Because countries differ substantially in vulnerability,wealth, capacity, resource endowments, and other factors listedbelow, the costs of the damages, adaptation, and mitigation maybe borne inequitably, unless the distribution of these costs isaddressed explicitly. Climate change is likely to impose costs on future generationsand on regions where damages occur, including regions with lowgreenhouse gas emissions. Climate change impacts will be distributedunevenly. The Convention recognizes in Article 3.1 the principle of commonbut differentiated responsibilities and respective capabilities.Actions beyond "no-regrets" measures impose costs onthe present generation. Mitigation policies unavoidably raiseissues about how to share the costs. The initial emission limitationintentions of Annex I parties represent an agreed collective firststep of those parties in addressing climate change. Equity arguments can support a variety of proposals to distributemitigation costs. Most of them seem to cluster around two mainapproaches: equal per capita emission allocations and allocationsbased on incremental departures from national baseline emissions(current or projected). Some proposals combine these approachesin an effort to incorporate equity concerns not addressed by relyingexclusively on one or the other approach. The IPCC can clarifyscientifically the implications of different approaches and proposals,but the choice of particular proposals is a policy judgment. There are substantial variations both among developed and developingcountries that are relevant to the application of equity principlesto mitigation. These include variations in historical and cumulativeemissions, current total and per capita emissions, emission intensitiesand economic output, and factors such as wealth, energy structures,and resource endowments. The literature is weak on the equityimplications of these variations both among developed and developingcountries. In addition, the implications of climate change for developingcountries are different from those for developed countries. Theformer often have different urgent priorities, weaker institutions,and are generally more vulnerable to climate change. However,it is likely that developing countries' share of emissions willgrow further to meet their social and developmental needs. Greenhousegas emissions are likely to become increasingly global, even whilstsubstantial per capita disparities are likely to remain. It is important that both efficiency and equity concerns shouldbe considered during the analysis of mitigation and adaptationmeasures. It may be worthwhile to conduct analyses of the equityimplications of particular measures for achieving efficiency,including their social considerations and impacts. 5. INTERTEMPORAL EQUITY AND DISCOUNTING Climate policy, like many other policy issues, raises particularquestions of equity among generations, because future generationsare not able to influence directly the policies being chosen todaythat could affect their wellbeing and because it might not bepossible to compensate future generations for consequent reductionsin their well-being. Sustainable development is one approach to intergenerational equity.Sustainable development meets "the needs of the present withoutcompromising the ability of future generations to meet their ownneeds." A consensus exists among economists that this doesnot imply that future generations should inherit a world withat least as much of every resource. Nevertheless, sustainabledevelopment would require that use of exhaustible natural resourcesand environmental degradation are appropriately offset - for example,by an increase in productive assets sufficient to enable futuregenerations to obtain at least the same standard of living asthose alive today. There are different views in the literatureon the extent to which infrastructure and knowledge, on the onehand, and natural resources, such as a healthy environment, onthe other hand, are substitutes. This is crucial to applying theseconcepts. Some analysts stress that there are exhaustible resourcesthat are unique and cannot be substituted for. Others believethat current generations can compensate future generations fordecreases in the quality or quantity of environmental resourcesby increases in other resources. Discounting is the principal analytical tool economists use tocompare economic effects that occur at different points in time.The choice of discount rate is of crucial technical importancefor analyses of climate change policy, because the time horizonis extremely long, and mitigation costs tend to come much earlierthan the benefits of avoided damages. The higher the discountrate, the less future benefits and the more current costs matterin the analysis. Selection of a social discount rate is also a question of valuessince it inherently relates the costs of present measures, topossible damages suffered by future generations if no action istaken. How best to choose a discount rate is, and will likelyremain, an unresolved question in economics. Partly as a consequence,different discount rates are used in different countries. Analyststypically conduct sensitivity studies using various discount rates.It should also be recognized that the social discount rate presupposesthat all effects are transformed to their equivalent in consumption.This makes it difficult to apply to those nonmarket impacts ofclimate change which for ethical reasons might not be, or forpractical reasons cannot be, converted into consumption units. The literature on the appropriate social discount rate for climatechange analysis can be grouped into two broad categories. Oneapproach discounts consumption by different generations usingthe "social rate of time preference," which is the sumof the rate of "pure time preference" (impatience) andthe rate of increase of welfare derived from higher per capitaincomes in the future. Depending upon the values taken for thedifferent parameters, the discount rate tends to fall between0.5% and 3.0% per year on a global basis - using this approach.However, wide variations in regional discount rates exist, butthese may still be consistent with a particular global average. The second approach to the discount rate considers market returnsto investment, which range between 3% and 6% in real terms forlong-term, risk-free public investments. Conceptually, funds couldbe invested in projects that earn such returns, with the proceedsbeing used to increase the consumption for future generations. The choice of the social discount rate for public investment projectsis a matter of policy preference but has a major impact on theeconomic evaluation of climate change actions. For example, intoday's dollars, $1,000 of damage 100 years from now would bevalued at $370 using a 1% discount rate (near the low end of therange for the first approach) but would be valued at $7.60 usinga 5% discount rate (near the upper end of the range for the secondapproach). However, in cost-effectiveness analyses of policiesover short time horizons, the impact of using different discountrates is much smaller. In all areas analysts should specify thediscount rate(s) they use to facilitate comparison and aggregationof results. 6. APPLICABILITY OF COST AND BENEFIT ASSESSMENTS Many factors need to be taken into account in the evaluation ofprojects and public policy issues related to climate change, includingthe analysis of possible costs and benefits. Although costs andbenefits cannot all be measured in monetary terms, various techniquesexist which offer a useful framework for organizing informationabout the consequences of alternative actions for addressing climatechange. The family of analytical techniques for examining economic environmentalpolicies and decisions includes traditional project level cost-benefitanalysis, cost-effectiveness analysis, multicriteria analysis,and decision analysis. Traditional cost-benefit analysis attemptsto compare all costs and benefits expressed in terms of a commonmonetary unit. Cost-effectiveness analysis seeks to find the lowest-costoption to achieve an objective specified using other criteria.Multicriteria analysis is designed to deal with problems wheresome benefits and/or costs are measured in nonmonetary units.Decision analysis focuses specifically on making decisions underuncertainty. In principle, this group of techniques can contribute to improvingpublic policy decisions concerning the desirable extent of actionsto mitigate global climate change, the timing of such actions,and the methods to be employed. Traditional cost-benefit analysis is based on the concept thatthe level of emission control at each point in time is determinedsuch that marginal costs equal marginal benefits. However, bothcosts and benefits may be hard, sometimes impossible, to assess.This may be due to large uncertainties, possible catastropheswith very small probabilities, or simply because there is no availableconsistent methodology for monetizing the effects. In some ofthese cases, it may be possible to apply multicriteria analysis.This provides policy makers with a broader set of information,including evaluation of relevant costs and benefits, estimatedwithin a common framework. Practical application of traditional cost-benefit analysis tothe problem of climate change is therefore difficult because ofthe global, regional, and intergenerational nature of the problem.Estimates of the costs of mitigation options also vary widely.Furthermore, estimates of potential physical damages due to climatechange also vary widely. In addition, confidence in monetary estimatesfor important consequences (especially nonmarket consequences)is low. These uncertainties, and the resolution of uncertaintyover time may be decisive for the choice of strategies to combatclimate change. The objective of decision analysis is to dealwith such problems. Furthermore, for some categories of ecological,cultural, and human health impacts, widely accepted economic conceptsof value are not available. To the extent that some impacts andmeasures cannot be valued in monetary terms, economists augmentthe traditional cost-benefit analysis approach with such techniquesas multicriteria analysis, permitting some quantitative expressionof the trade-offs to be made. These techniques do not resolvequestions involving equity - for example, determining who shouldbear the costs. However, they provide important information onthe incidence of damage, mitigation, and adaptation costs andon where cost-effective action might be taken. Despite their many imperfections, these techniques provide a valuableframework for identifying essential questions that policymakersmust face when dealing with climate change, namely: · By how much should emissions of greenhouse gases be reduced? · When should emissions be reduced? · How should emissions be reduced? These analytical techniques assist decision makers in comparingthe consequences of alternative actions, including that of noaction, on a quantitative basis - and can certainly make a contributionto resolution of these questions. 7. THE SOCIAL COSTS OF ANTHROPOGENIC CLIMATE CHANGE: DAMAGESOF INCREASED GREENHOUSE GAS EMISSIONS The literature on the subject in this section is controversialand mainly based on research done on developed countries, oftenextrapolated to developing countries. There is no consensus abouthow to value statistical lives or how to aggregate statisticallives across countries. Monetary valuation should not obscurethe human consequences of anthropogenic climate change damages,because the value of life has meaning beyond monetary valuation.It should be noted that the Rio Declaration and Agenda 21 callfor human beings to remain at the centre of sustainable development.The approach taken to this valuation might affect the scale ofdamage reduction strategies. It may be noted that in virtuallyall of the literature discussed in this section the developingcountry statistical lives have not been equally valued at thedeveloped country value, nor are other damages in developing countriesequally valued at the developed country value. Because nationalcircumstances, including opportunity costs, differ, economistssometimes evaluate certain kinds of impacts differently amongstcountries. The benefits of limiting greenhouse gas emissions and enhancingsinks are (a) the climate change damages avoided and (b) the secondarybenefits associated with the relevant policies. Secondary benefitsinclude reductions in other pollutants jointly produced with greenhousegases and the conservation of biological diversity. Net climatechange damages include both market and nonmarket impacts as faras they can be quantified at present and, in some cases, adaptationcosts. Damages are expressed in net terms to account for the factthat there are some beneficial impacts of global warming as well,which are, however, dominated by the damage costs. Nonmarket impacts,such as human health, risk of human mortality, and damage to ecosystems,form an important component of available estimates of the socialcosts of climate change. The literature on monetary valuationof such nonmarket effects reflects a number of divergent viewsand approaches. The estimates of nonmarket damages, however, arehighly speculative and not comprehensive. Nonmarket damage estimates are a source of major uncertainty inassessing the implications of global climate change for humanwelfare. While some regard monetary valuation of such impactsas essential to sound decision making, others reject monetaryvaluation of some impacts, such as risk of human mortality, onethical grounds. Additionally, there is a danger that entire uniquecultures may be obliterated. This is not something that can beconsidered in monetary terms, but becomes a question of loss ofhuman diversity, for which we have no indicators to measure economicvalue. The assessed literature contains only a few estimates of the monetizeddamages associated with doubled CO2 equivalent concentration scenarios.These estimates are aggregated to a global scale and illustratethe potential impacts of climate change under selected scenarios.Aggregating individual monetized damages to obtain total socialwelfare impacts implies difficult decisions about equity amongstcountries. Global estimates are based upon an aggregation of monetarydamages across countries (damages which are themselves implicitaggregations across individuals) that reflects intercountry differencesin wealth and income - this fundamentally influences the monetaryvaluation of damages. Taking income differences as given impliesthat an equivalent impact in two countries (such as an equal increasein human mortality) would receive very different weights in thecalculations of global damages. To enable choices between different ways of promoting human welfareto be made on a consistent basis, economists have for many yearssought to express a wide range of human and environmental impactsin terms of monetary equivalents, using various techniques. Themost commonly used of those techniques is an approach based onthe observed willingness to pay for various nonmarket benefits.This is the approach that has been taken in most of the assessedliterature. Human life is an element outside the market, and societies maywant to preserve it in an equal way. An approach that includesequal valuation of impacts on human life wherever they occur mayyield different global aggregate estimates than those reportedbelow. For example, equalizing the value of a statistical lifeat a global average could leave total global damage unchangedbut would increase markedly the share of these damages borne bythe developing world. Equalizing the value at the level typicalin developed countries would increase monetized damages severaltimes, and would further increase the share of the developingcountries in the total damage estimate. Other aggregation methods can be used to adjust for differencesin the wealth or incomes of countries in calculations of monetarydamages. Because estimates of monetary damage tend to be a higherpercentage of national GDP for low-income countries than for high-incomecountries, aggregation schemes that adjust for wealth or incomeeffects are expected to yield higher estimates of global damagesthan those presented in this report. The assessed literature quantifying total damages from 2-3°Cwarming provides a wide range of point estimates for damages,given the presumed change in atmospheric greenhouse gas concentrations.The aggregate estimates tend to be a few percent of world GDP,with, in general, considerably higher estimates of damage to developingcountries as a share of their GDP. The aggregate estimates aresubject to considerable uncertainty, but the range of uncertaintycannot be gauged from the literature. The range of estimates cannotbe interpreted as a confidence interval, given the widely differingassumptions and methodologies in the studies. As noted above,aggregation is likely to mask even greater uncertainties aboutdamage components. Regional or sectoral approaches to estimating the consequencesof climate change include a much wider range of estimates of thenet economic effects. For some areas, damages are estimated tobe significantly greater and could negatively affect economicdevelopment. For others, climate change is estimated to increaseeconomic production and present opportunities for economic development.For countries generally having a diversified, industrial economyand an educated and flexible labour force, the limited set ofpublished estimates of damages are of the order one to a few percentof GDP. For countries generally having a specialized and naturalresource-based economy (e.g., heavily emphasizing agricultureor forestry), and a poorly developed and land-tied labour force,estimates of damages from the few studies available are severaltimes larger. Small islands and low-lying coastal areas are particularlyvulnerable. Damages from possible large-scale catastrophes, suchas major changes in ocean circulation, are not reflected in theseestimates. There is little agreement across studies about theexact magnitude of each category of damages or relative rankingof the damage categories. Climate changes of this magnitude arenot expected to be realized for several decades, and damages inthe interim could be smaller. Damages over a longer period oftime might be greater. IPCC does not endorse any particular range of values for the marginaldamage of CO2 emissions, but published estimates range between$5 and $125 (1990 U.S.) per tonne of carbon emitted now. Thisrange of estimates does not represent the full range of uncertainty.The estimates are also based on models that remain simplisticand are limited representations of the actual climate processesin being and are based on earlier IPCC scientific reports. Thewide range of damage estimates reflects variations in model scenarios,discount rates, and other assumptions. It must be emphasized thatthe social cost estimates have a wide range of uncertainty becauseof limited knowledge of impacts, uncertain future technologicaland socioeconomic developments, and the possibility of catastrophicevents or surprises. 8. GENERIC ASSESSMENT OF RESPONSE STRATEGIES A wide range of technologies and practices is available for mitigatingemissions of carbon dioxide, methane, nitrous oxide and othergreenhouse gases. There are also many adaptation measures availablefor responding to the impacts of climate change. All these technologies,practices, and measures have financial and environmental costsas well as benefits. This section surveys the range of optionscurrently available or discussed in the literature. The optimalmix of response options will vary by country and over time aslocal conditions and costs change. A review of CO2 mitigation options suggests that: · A large potential for cost-effective energy conservationand efficiency improvements in energy supply and energy useexists in many sectors. These options offer economic and environmentalbenefits in addition to reducing emissions of greenhouse gases.Various of these options can be deployed rapidly due to smallunit size, modular design characteristics, and low lifetime costs. The options for CO2 mitigation in energy use includealternative methods and efficiency improvements, among othersin the construction, residential, commercial, agriculture, andindustry sectors. Not all cost-effective strategies are basedon new technology; some may rely on improved information disseminationand public education, managerial strategies, pricing policies,and institutional reforms. · Estimates of the technical potential for switching toless carbon-intensive fuels vary regionally and with the typeof measure and the economic availability of reserves of fossiland alternative fuels. These estimates also have to take accountof potential methane emissions from leakage of natural gas duringproduction and distribution. · Renewable energy technologies (e.g., solar, hydroelectric,wind, traditional and modern biomass, and ocean thermal energyconversion) have achieved different levels of technical development,economic maturity, and commercial readiness. The potential ofthese energy sources is not fully realized. Cost estimates forthese technologies are sensitive to site-specific characteristics,resource variability, and the form of final energy delivered.These cost estimates vary widely. · Nuclear energy is a technology that has been deployedfor several decades in many countries. However, a number of factorshave slowed the expansion of nuclear power, including: (a) warypublic perceptions resulting from nuclear accidents, (b) not yetfully resolved issues concerning reactor safety, proliferationof fissile material, power plant decommissioning, and long-termdisposal of nuclear waste, as well as, in some instances, lower-than-anticipatedlevels of demand for electricity. Regulatory and siting difficultieshave increased construction lead times, leading to higher capitalcosts for this option in some countries. If these issues, includinginter alia the social, political, and environmental aspectsmentioned above, can be resolved, nuclear energy has the potentialto increase its present share in worldwide energy production. · CO2 capture and disposal may be ultimately limitedfor technical and environmental reasons, because not all formsof disposal ensure prevention of carbon reentering the atmosphere. · Forestry options, in some circumstances, offer largepotential, modest costs, low risk, and other benefits. Further,the potential modern use of biomass as a source of fuels and electricitycould become attractive. Halting or slowing deforestation andincreasing reforestation through increased silvicultural productivityand sustainable management programmes that increase agriculturalproductivity, the expansion of forest reserves, and promotionof ecotourism are among the cost-effective options for slowingthe atmospheric build-up of CO2. Forestry programmes raise importantequity considerations. There is also a wide range of available technologies and practicesfor reducing emissions of methane from such sources asnatural gas systems, coal mines, waste dumps, and farms. However,the issue of reduction of emissions related to food supply mayimply trade-offs with rates of food production. These trade-offsmust be carefully assessed, as they may affect the provision ofbasic needs in some countries, particularly in developing countries. Most nitrous oxide emissions come from diffuse sourcesrelated to agriculture and forestry. These emissions are difficultto reduce rapidly. Industrial emissions of nitrous oxide andhalogenated compounds tend to be concentrated in a few keysectors and tend to be easier to control. Measures to limit suchemissions may be attractive for many countries. The slow implementation of many of the technologically attractiveand cost-effective options listed above has many possible explanations,with both actual and perceived costs being a major factor. Amongother factors, capital availability, information gaps, institutionalobstacles, and market imperfections affect the rate of diffusionfor these technologies. Identifying the reasons specific to aparticular country is a precondition to devising sound and efficientpolicies to encourage their broader adoption. Education and training as well as information and advisory measuresare important aspects of various response options. Many of the emission-reducing technologies and practices describedabove also provide other benefits to society. These additionalbenefits include improved air quality, better protection of surfaceand underground waters, enhanced animal productivity, reducedrisk of explosions and fire, and improved use of energy resources. Many options are also available for adapting to the impactsof climate change and thus reducing the damages to national economiesand natural ecosystems. Adaptive options are available in manysectors, ranging from agriculture and energy to health, coastalzone management, offshore fisheries, and recreation. Some of theseprovide enhanced ability to cope with the current impacts of climatevariability. However, possible trade-offs between implementationof mitigation and adaptation measures are important to considerin future research. A summary of sectoral options for adaptationis presented in Volume 2. The optimal response strategy for each country will depend onthe special circumstances and conditions which that country mustface. Nonetheless, many recent studies and empirical observationssuggest that some of the most cost-effective options can be mostsuccessfully implemented on a joint or cooperative basis amongnations. 9. COSTS OF RESPONSE OPTIONS It must be emphasized that the text in this section is an assessmentof the technical literature and does not make recommendationson policy matters. The available literature is primarily fromdeveloped countries. Cost Concepts From the perspective of this section on assessing mitigation oradaptation costs, what matters is the net cost (total cost lesssecondary benefits and costs). These net costs exclude the socialcosts of climate change, which are discussed in Section 7. Theassessed literature yields a very wide range of estimates of thecosts of response options. The wide range largely reflects significantdifferences in assumptions about the efficiency of energy andother markets, and about the ability of government institutionsto address perceived market failures or imperfections. Measures to reduce greenhouse gas emissions may yield additionaleconomic impacts (for example, through technological externalitiesassociated with fostering research and development programmes)and/or environmental impacts (such as reduced emissions of acidrain and urban smog precursors). Studies suggest that the secondaryenvironmental benefits may be substantial but are likely to differfrom country to country. Specific Results Estimates of the cost of greenhouse gas emission reduction dependcritically upon assumptions about the levels of energy efficiencyimprovements in the baseline scenario (that is, in the absenceof climate policy) and upon a wide range of factors such as consumptionpatterns, resource and technology availability, the desired leveland timing of abatement, and the choice of policy instruments.Policymakers should not place too much confidence in the specificnumerical results from any one analysis. For example, mitigationcost analyses reveal the costs of mitigation relative to a givenbaseline, but neither the baseline nor the intervention scenariosshould be interpreted as representing likely future conditions.The focus should be on the general insights regarding the underlyingdeterminants of costs. The costs of stabilizing atmospheric concentrations of greenhousegases at levels and within a time frame that will prevent dangerousanthropogenic interference with the climate system (the ultimateobjective of the FCCC) will be critically dependent on the choiceof emission timepath. The cost of the abatement programme willbe influenced by the rate of capital replacement, the discountrate, and the effect of research and development. Failure to adopt policies as early as possible to encourage efficientreplacement investments at the end of the economic life of a plantand equipment (i.e., at the point of capital stock turnover) imposesan economic cost to society. Implementing emission reductionsat rates that can be absorbed in the course of normal stock turnoveris likely to be cheaper than enforcing premature retirement now. The choice of abatement paths thus involves balancing the economicrisks of rapid abatement now (that premature capital stock retirementwill later be proved unnecessary) against the corresponding riskof delay (that more rapid reduction will then be required, necessitatingpremature retirement of future capital stock.) Appropriate long-run signals are required to allow producers andconsumers to adapt cost-effectively to constraints on greenhousegas emissions and to encourage research and development. Benefitsassociated with the implementation of any "no-regret"policies will offset, at least in part, the costs of a full portfolioof mitigation measures. This will also increase the time availableto learn about climate risks and to bring new technologies intothe market place. Despite significant differences in views, there is agreement thatenergy efficiency gains of perhaps 10% to 30% below baseline trendsover the next two to three decades can be realized at negativeto zero net cost. (Negative net cost means an economic benefit).With longer time horizons, which allow a more complete turnoverof capital stocks, and which give research and development andmarket transformation policies a chance to impact multiple replacementcycles, this potential is much higher. The magnitude of such "no-regret"potentials depends upon the existence of substantial market orinstitutional imperfections that prevent cost-effective emissionreduction measures from occurring. The key question is then theextent to which such imperfections and barriers can be removedcost-effectively by policy initiatives such as efficiency standards,incentives, removal of subsidies, information programmes, andfunding of technology transfer. Progress has been made in a number of countries in cost-effectivelyreducing imperfections and institutional barriers in markets throughpolicy instruments based on voluntary agreements, energy efficiencyincentives, product efficiency standards, and energy efficiencyprocurement programmes involving manufacturers, as well as utilityregulatory reforms. Where empirical evaluations have been made,many have found the benefit-cost ratio of increasing energy efficiencyto be favourable, suggesting the practical feasibility of realizing"no-regret" potentials at negative net cost. More informationis needed on similar and improved programmes in a wider rangeof countries. Infrastructure decisions are critical in determining long-termemissions and abatement costs because they canenhance or restrict the number and type of future options. Infrastructuredecisions determine development patterns in transportation, urbansettlement, and land use and influence energy system developmentand deforestation patterns. This issue is of particular importanceto developing countries and many economies in transition wheremajor infrastructure decisions will be made in the near term. If a carbon or carbon-energy tax is used as a policy instrumentfor reducing emissions, the taxes could raise substantial revenues,and how the revenues are distributed could dramatically affectthe cost of mitigation. If the revenues are distributed by reducingdistortionary taxes in the existing system, they will help reducethe excess burden of the existing tax system, potentially yieldingan additional economic benefit (double dividend). For example,those European studies which are more optimistic regarding thepotential for tax recycling show lower and, in some instances,slightly negative costs. Conversely, inefficient recycling ofthe tax revenues could increase costs. For example, if the taxrevenues are used to finance government programmes that yielda lower return than the private sector investments foregone becauseof the tax, then overall costs will increase. There are large differences in the costs of reducing greenhousegas emissions among countries because of their state of economicdevelopment, infrastructure choices, and natural resource base.This indicates that international cooperation could significantlyreduce the global cost of reducing emissions. Research suggeststhat, in principle, substantial savings would be possible if emissionsare reduced where it is cheapest to do so. In practice, this requiresinternational mechanisms ensuring appropriate capital flows andtechnology transfers between countries. Conversely, a failureto achieve international cooperation could compromise unilateralattempts by a country or a group of countries to limit greenhousegas emissions. However, estimates of so called leakage effectsvary so widely that they provide little guidance to policymakers. There has been more analysis to date of emission reduction potentialsand costs for developed countries than for other parts of theworld. Moreover, many existing models are not well-suited to studyeconomies in transition or economies of developing countries.Much work is needed to develop and apply models for use outsidedeveloped countries (for example, to represent more explicitlymarket imperfections, institutional barriers, and traditionaland informal economic sectors). In addition, the discussion belowand the bulk of the underlying report deal with costs of responseoptions at the national or regional level in terms of effect onGDP. Further analysis is required concerning effects of responseoptions on employment, inflation, trade competitiveness, and otherpublic issues. A large number of studies using both top-down and bottom-up approaches(see box for definitions) were reviewed. Estimates of the costsof limiting fossil fuel carbon dioxide emissions (expressed ascarbon) vary widely and depend upon choice of methodologies, underlyingassumptions, emission scenarios, policy instruments, reportingyear, and other criteria. For specific results of individual studies,see Chapter 9. OECD Countries. Although it is difficult to generalize,top-down analyses suggest that the costs of substantial reductionsbelow 1990 levels could be as high as several percent of GDP.In the specific case of stabilizing emissions at 1990 levels,most studies estimate that annual costs in the range of -0.5%of GDP (equivalent to a gain of about $60 billion in total forOECD countries at today's GDP levels) to 2% of GDP (equivalentto a loss of about $240 billion) could be reached over the nextseveral decades. However, studies also show that appropriate timingof abatement measures and the availability of low-cost alternativesmay substantially reduce the size of the overall bill. Bottom-up studies are more optimistic about the potential forlow or negative cost emission reductions, and the capacity toimplement that potential. Such studies show that the costs ofreducing emissions by 20% in developed countries within two tothree decades are negligible to negative. Other bottom-up studiessuggest that there exists a potential for absolute reductionsin excess of 50% in the longer term, without increasing and perhapseven reducing total energy system costs. The results of top-down and bottom-up analyses differ becauseof such factors as higher estimates of no-regrets potential andtechnological progress, and earlier saturation in energy servicesper unit GDP. In the most favourable assessments, savings of 10-20%in the total cost of energy services can be achieved. Economies in transition. The potential for cost-effectivereductions in energy use is apt to be considerable, but the realizablepotential will depend upon what economic and technological developmentpath is chosen, as well as the availability of capital to pursuedifferent paths. A critical issue is the future of structuralchanges in these countries that are apt to change dramaticallythe level of baseline emissions and the emission reduction costs. Developing countries. Analyses suggest that there may besubstantial low-cost fossil fuel carbon dioxide emission reductionopportunities for developing countries. Development pathways thatincrease energy efficiency, promote alternative energy technologies,reduce deforestation, and enhance agricultural productivity andbiomass energy production can be economically beneficial. To embarkupon this pathway may require significant international cooperationand financial and technology transfers. However, these are likelyto be insufficient to offset rapidly increasing emissions baselines,associated with increased economic growth and overall welfare.Stabilization of carbon dioxide emissions is likely to be costly. It should be noted that analyses of costs to economies in transitionand developing countries typically neglect the general equilibriumeffects of unilateral actions taken by developed countries. Theseeffects may be either positive or negative and their magnitudeis difficult to quantify. It should also be noted that estimates of costs or benefits ofthe order of a few percent of GDP may represent small differencesin GDP growth rates, but are nevertheless substantial in absoluteterms. Preservation and augmentation of carbon sinks offer a substantialand often cost-effective component of a greenhouse gas mitigationstrategy. Studies suggest that as much as 15-30% of 1990 globalenergy-related emissions could be offset by carbon sequestrationin forests for a period of 50-100 years. The costs of carbon sequestration,which are competitive with source control options, may differamong regions of the world. Control of emissions of other greenhouse gases, especially methaneand nitrous oxide, can provide significant cost-effective opportunitiesin some countries. About 10% of anthropogenic methane emissionscould be reduced at negative or low cost using available mitigationoptions for such methane sources as natural gas systems, wastemanagement, and agriculture. 10. INTEGRATED ASSESSMENT Integrated assessment models combine knowledge from a wide rangeof disciplines to provide insights that would not be observedthrough traditional disciplinary research. They are used to explorepossible states of human and natural systems, analyze key questionsrelated to policy formulation, and help set research priorities.Integration helps coordinate assumptions from different disciplinesand allows feedbacks and interactions absent from individual disciplinesto be analyzed. However, the results of such analyses are no betterthan the information drawn from the underlying economic, atmospheric,and biological sciences. Integrated assessment models are limitedboth by the underlying knowledge base upon which they draw andby the relatively limited experiential base. Most current integrated assessment models do not reflect the specificsocial and economic dynamics of the developing and transitioneconomies well; for example, none of the existing models addressesmost market imperfections, institutional barriers, or the operationof the informal sector in these countries. This can lead to biasesin global assessments when mitigation options and impacts on developingor transition economies are valued as if their economies operatedlike those in the developed countries. While relatively new, integrated assessment models of climatechange have evolved rapidly. Integrated assessment models tendto fall into two categories: policy evaluation and policyoptimization models. Policy evaluation models are rich inphysical detail and have been used to analyze the potential fordeforestation as a consequence of interactions between demographics,agricultural productivity, and economic growth, and the relationshipbetween climate change and the extent of potentially malarialregions. Policy optimization models optimize over key variables(e.g., emission rates, carbon taxes) to achieve formulated policygoals (e.g., cost minimization or welfare optimization). Key uncertainties in current integrated assessments include thesensitivity of the climate system to changes in greenhouse gasconcentrations, the specification and valuation of impacts wherethere are no markets, changes in national and regional demographics,the choice of discount rates, and assumptions regarding the cost,availability, and diffusion of technologies. 11. AN ECONOMIC ASSESSMENT OF POLICY INSTRUMENTS TO COMBATCLIMATE CHANGE Governments may have different sets of criteria for assessinginternational as well as domestic greenhouse policy instruments.Among these criteria are efficiency and cost-effectiveness, effectivenessin achieving stated environmental targets, distributional (includingintergenerational) equity, flexibility in the face of new knowledge,understandability to the general public, and consistency withnational priorities, policies, institutions and traditions. Thechoice of instruments may also partly reflect a desire on thepart of governments to achieve other objectives, such as sustainableeconomic development, meeting social development goals and fiscaltargets, or influencing pollution levels that are indirectly relatedto greenhouse gas emissions. A further concern of governmentsmay lie with the effect of policies on competitiveness. The world economy and indeed some individual national economiessuffer from a number of price distortions which increase greenhousegas emissions, such as some agricultural and fuel subsidies anddistortions in transport pricing. A number of studies of thisissue indicate that global emission reductions of 4-18%, togetherwith increases in real incomes, are possible from phasing outfuel subsidies. For the most part, reducing such distortions couldlower emissions and increase economic efficiency. However, subsidiesare often introduced and price distortions maintained for socialand distributional reasons, and they may be difficult to remove. Policy instruments may be identified at two different levels:those that might be used by a group of countries and those thatmight be used by individual nations unilaterally or to achievecompliance with a multilateral agreement. A group of countries may choose from policy measures and instrumentsincluding encouragement of voluntary actions and further research,tradable quotas, joint implementation (specifically activitiesimplemented jointly under the pilot phase), harmonized domesticcarbon taxes, international carbon taxes, nontradable quotas,and various international standards. If the group did not includeall major greenhouse gas emitters, then there might be a tendencyfor fossil fuel use to increase in countries not participatingin this group. This outcome might reduce the international competitivenessof some industries in participating countries as well as the environmentaleffectiveness of the countries' efforts. At both the international and national levels, the economic literatureindicates that instruments that provide economic incentives, suchas taxes and tradable quotas/permits, are likely to be more cost-effectivethan other approaches. Uniform standards among groups of countriesparticipating in an international agreement are likely to be difficultto achieve. However, for one group of countries there has beenagreement on the application of some uniform standards. At the international level, all of the potentially efficient market-basedinstruments could be examined during the course of future negotiations.A tradable quota system has the disadvantage of making the marginalcost of emissions uncertain, while a carbon tax (and related instruments)has the disadvantage of leaving the effect on the level at whichemissions are controlled uncertain. The weight given to the importanceof reducing these different types of uncertainty would be onecrucial factor in further evaluating these alternative instruments.Because of the lack of appropriate scientific knowledge, therewould remain a high degree of uncertainty about the results oflimiting emissions at specific levels. The adoption of eithera tradable quota scheme or international taxes would have implicationsfor the international distribution of wealth. The distributionalconsequences would be the subject of negotiation. To insure thepracticability of such instruments, there is a need for additionalstudies on the possible design of tradable quotas and harmonizedtaxes and on the institutional framework in which they might operate. Individual countries that seek to implement mitigation policiescan choose from among a large set of potential policies and instruments,including carbon taxes, tradable permits, deposit refund systems(and related instruments), and subsidies, as well as technologystandards, performance standards, product bans, direct governmentinvestment, and voluntary agreements. Public education on thesustainable use of resources could play an important part in modifyingconsumption patterns and other human behaviour. The choice ofmeasures at the domestic level may reflect objectives other thancost-effectiveness, such as meeting fiscal targets. Revenue fromcarbon taxes or auctioned tradable permits could be used to replaceexisting distortionary taxes. The choice of instruments may alsoreflect other environmental objectives, such as reducing non-greenhousepollution emissions, or increasing forest cover, or other concernssuch as specific impacts on particular regions or communities. Box 1: Top-Down and Bottom-Up Models Top-down models are aggregate models of the entire macroeconomythat draw on analysis of historical trends and relationships topredict the large-scale interactions between the sectors of theeconomy, especially the interactions between the energy sectorand the rest of the economy. Top-down models typically incorporaterelatively little detail on energy consumption and technologicalchange compared with bottom-up models. In contrast, bottom-up models incorporate detailed studies ofthe engineering costs of a wide range of available and forecasttechnologies and describe energy consumption in great detail.However, compared with top-down models, they typically incorporaterelatively little detail on nonenergy consumer behaviour and interactionswith other sectors of the economy. This simple characterization of top-down and bottom-up modelsis increasingly misleading as more recent versions of each approachhave tended to provide greater detail in the aspects that wereless developed in the past. As a result of this convergence inmodel structure, model results are tending to converge, and theremaining differences reflect differences in assumptions abouthow rapidly and effectively market institutions adopt cost-effectivenew technologies or can be induced to adopt them by policy interventions. Many existing models are not well suited to study economies intransition or those of developing countries. More work is neededto develop the appropriate methodologies, data, and models andto build the local institutional capacity to undertake analyses.WT03-B20-58IA006-000055-B013-380http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind/sgpcal.html 138.80.61.12 19970221181437 text/html 2155HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:44:57 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1984Last-modified: Tue, 09 Jul 1996 02:26:09 GMT  Wind Plants of California's San Gorgonio Pass Wind Plants of California's San Gorgonio Pass The San Gorgonio Pass near Palm Springs hosts the third largest concentration of wind turbines in California. There are more than 3,500 wind turbines located in the pass, many massed on the floor of the Whitewater Wash (an ephemeral stream) against the dramatic backdrop of Mount San Jacinto. Wind Turbines on Whitewater Wash Many of the wind turbines on the Wash were installed during the height of California's great wind rush in the early 1980s. The wind turbines in these wind plants were installed much closer together than in modern arrays, creating a veritable forest. These older turbines are also less reliable and operate less frequently than contemporary designs. Some of the older turbines have already been removed as wind companies begin to repower the aging wind plants with newer more cost effective wind turbines. Only two hours east of Hollywood, the wind turbines near Palm Springs have appeared in several movies, including Rain Man. Photo copyright © by Paul Gipe. All rights reserved. Paul Gipe, 20.08.95, pgipe@igc.apc.org More information on wind energy. Wind Energy Comesof Age Wind Power for Home & Business Workgroup Windturbines WT03-B20-59IA006-000055-B013-418http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind/tehcal.html 138.80.61.12 19970221181449 text/html 2286HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:45:09 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 2115Last-modified: Tue, 09 Jul 1996 02:26:09 GMT Wind Plants of California's Tehachapi Pass Wind Plants of California's Tehachapi Pass The Tehachapi Pass is the world's largest producer of wind-generated electricity. The more than 5,000 wind turbines in the Tehachapi-Mojave wind resource area of Kern County generate approximately 1.3 Terawatt-hours (1,300,000,000 kWh) per year. This is enough electricity to meet the residential needs of more than 500,000 Southern Californians or nearly one million Europeans. The Tehachapi Pass is two hours north of Los Angeles and is frequently used by Hollywood producers for its dramatic scenery as in the movie Terminal Velocity. Lunch break on the PCT Every May the Kern Wind Energy Association and the Kern-Kaweah Chapter of the Sierra Club lead a hike on the Pacific Crest Trail to view the spring wildflowers and the wind turbines. The hikers shown here have stopped for lunch atop Cameron Ridge, which affords spectacular views of the area's wind power plants, the Tehachapi Mountains, and the Mojave Desert. The rotors on these Micon wind turbines are 19 meters in diameter and drive 108 kW generators. The site is operated by Cannon Energy. Photo copyright © by Paul Gipe. All rights reserved. Answers to Common Questions about Wind Turbines in the Tehachapi-Mojave Wind Resource Area More information on wind energy. Wind Energy Comes of Age Wind Power for Home & Business Workgroup Windturbines Paul Gipe, 20.08.95, pgipe@igc.apc.orgWT03-B20-60IA005-000051-B017-17http://lacebark.ntu.edu.au:80/j_mitroy/sid101/oil/gasoline.html 138.80.61.12 19970221151439 text/html 186228HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:43:12 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 186055Last-modified: Tue, 09 Jul 1996 02:25:44 GMT F-Body Homepage: Automotive Gasoline FAQ This file is made available to you by the F-BODY HOMEPAGE Converted to html by Kyle Hamar Last-modified: 18 Jan 1995Version: 1.00 FAQ: Automotive Gasoline Bruce Hamilton B.Hamilton@irl.cri.nz Subject: 1. Introduction and Intent The intent of this FAQ is to provide some basic information on gasolines andother fuels for spark ignition engines used in automobiles. The toxicity andenvironmental reasons for recent and planned future changes to gasoline arediscussed, along with recent and proposed changes in composition of gasoline.This FAQ intended to help readers choose the most appropriate fuel forvehicles, assist with the diagnosis of fuel-related problems, and tounderstand the significance of most gasoline properties listed in fuelspecifications. I make no apologies for the fairly heavy emphasis onchemistry, it is the only sensible way to describe the oxidation ofhydrocarbon fuels to produce energy, water, and carbon dioxide. Subject: 2. Table of Contents         1. Introduction and Intent        2. Table of Contents        3. What Advantage will I gain from reading this FAQ?        4. What is Gasoline?          4.1  Where does crude oil come from?.          4.2  When will we run out of crude oil?.          4.3  What is the history of gasoline?          4.4  What are the hydrocarbons in gasoline?          4.5  What are oxygenates?          4.6  Why were alkyl lead compounds added?          4.7  Why not use other organometallic compounds?          4.8  What do the refining processes do?          4.9  What energy is released when gasoline is burned?          4.10 What are the gasoline specifications?          4.11 What are the effects of the specified fuel properties?          4.12 Are brands different?          4.13 What is a typical composition?          4.14 Is gasoline toxic or carcinogenic?          4.15 Is unleaded gasoline more toxic than leaded?        5. Why is Gasoline Composition Changing?          5.1  Why pick on cars and gasoline?          5.2  Why are there seasonal changes?          5.3  Why were alkyl lead compounds removed?          5.4  Why are evaporative emissions a problem?          5.5  Why control tailpipe emissions?          5.6  Why do exhaust catalysts influence fuel composition?          5.7  Why are "cold start" emissions so important?          5.8  When will the emissions be "clean enough"?          5.9  Why are only some gasoline compounds restricted?          5.10 What does "renewable" fuel/oxygenate mean?          5.11 Will oxygenated gasoline damage my vehicle?          5.12 What does "reactivity" of emissions mean?          5.13 What are "carbonyl" compounds?          5.14 What are "gross polluters"?        6. What do Fuel Octane ratings really indicate?          6.1  Who invented Octane Ratings?          6.2  Why do we need Octane Ratings?          6.3  What fuel property does the Octane Rating measure?          6.4  Why are two ratings used to obtain the pump rating?          6.5  What does the Motor Octane rating measure?          6.6  What does the Research Octane rating measure?          6.7  Why is the difference called "sensitivity"?          6.8  What sort of engine is used to rate fuels?          6.9  How is the Octane rating determined?          6.10 What is the Octane Distribution of the fuel?          6.11 What is a "delta Research Octane number"?          6.12 How do other fuel properties affect octane?          6.13 Can higher octane fuels give me more power?          6.14 Does low octane fuel increase engine wear?          6.15 Can I mix different octane fuel grades?          6.16 What happens if I use the wrong octane fuel?          6.17 Can I tune the engine to use another octane fuel?          6.18 How can I increase the fuel octane?          6.19 Are aviation gasoline octane numbers comparable?        7. What parameters determine octane requirement?          7.1  What is the effect of Compression ratio?          7.2  What is the effect of changing the air/fuel ratio?          7.3  What is the effect of changing the ignition timing          7.4  What is the effect of engine management systems?          7.5  What is the effect of temperature and Load?          7.6  What is the effect of engine speed?          7.7  What is the effect of engine deposits?          7.8  What is the Road octane requirement of an vehicle?          7.9  What is the effect of air temperature?.          7.10 What is the effect of altitude?.          7.11 What is the effect of humidity?.          7.12 What does water injection achieve?.        8. How can I identify and cure other fuel-related problems?          8.1  What causes an empty fuel tank?          8.2  Is knock the only abnormal combustion problem?          8.3  Can I prevent carburetter icing?          8.4  Should I store fuel to avoid the oxygenate season?          8.5  Can I improve fuel economy by using quality gasolines?          8.6  What is "stale" fuel, and should I use it?          8.7  How can I remove water in the fuel tank?          8.8  Can I use unleaded on older vehicles?        9. Alternative Fuels and Additives          9.1  Do fuel additives work?          9.2  Can a quality fuel help a sick engine?          9.3  What are the advantages of alcohols and ethers?          9.4  Why are CNG and LPG considered "cleaner" fuels.          9.5  Why are hydrogen-powered cars not available?          9.6  What are "fuel cells" ?          9.7  What is a "hybrid" vehicle?          9.8  What about other alternative fuels?          9.9  What about alternative oxidants?       10. Historical Legends         10.1  The myth of Triptane         10.2  From Honda Civic to Formula 1 winner.       11. References         11.1  Books and Research Papers         11.2  Suggested Further Reading Subject: 3. What Advantage will I gain from reading this FAQ? This FAQ is intended to provide a fairly technical description of whatgasoline contains, how it is specified, and how the properties affect theperformance in your vehicle. The regulations governing gasoline havechanged, and are continuing to change. These changes have made much of thetraditional lore about gasoline obsolete. Motorists may wish to understanda little more about gasoline to ensure they obtain the best value, and themost appropriate fuel for their vehicle. There is no point in prematurelydestroying your second most expensive purchase by using unsuitable fuel,just as there is no point in wasting hard-earned money on higher octanefuel that your automobile can not utilize. Note that this FAQ does notdiscuss the relative advantages of specific brands of gasolines, it isonly intended to discuss the generic properties of gasolines. Subject: 4. What is Gasoline? 4.1 Where does crude oil come from?. The generally-accepted origin of crude oil is from plant life up to 3billion years ago, but predominantly from 100 to 600 million years ago [1]."Dead vegetarian dino dinner" is more correct than "dead dinos".The molecular structure of the hydrocarbons and other compounds presentin fossil fuels can be linked to the leaf waxes and other plant molecules ofmarine and terrestrial plants believed to exist during that era. There arevarious biogenic marker chemicals such as isoprenoids from terpenes,porphyrins and aromatics from natural pigments, pristane and phytane fromthe hydrolysis of chlorophyll, and normal alkanes from waxes, whose sizeand shape can not be explained by known geological processes [2]. Thepresence of optical activity and the carbon isotopic ratios also indicate abiological origin [3]. There is another hypothesis that suggests crude oilis derived from methane from the earth's interior. The current mainproponent of this abiotic theory is Thomas Gold, however abiotic andextraterrestrial origins for fossil fuels were also considered at the turnof the century, and were discarded then. 4.2 When will we run out of crude oil? It has been estimated that the planet contains over 1.4 x 10^15 tonnes ofpetroleum, however much of this is too dilute or inaccessible for currenttechnology to recover [4]. The petroleum industry uses a measure calledthe Reserves/Production ratio (R/P) to monitor how production andexploration are linked. This is based on the concept of "proved" reservesof crude oil, which are generally taken to be those quantities whichgeological and engineering information indicate with reasonable certaintycan be recovered in the future from known reservoirs under existing economicand operating conditions. The Reserves/Production ratio is the abovereserves divided by the production in the last year, and the result is thelength of time that those remaining reserves would last if production wereto continue at the current level [5]. It is important to note thosedefinitions, as the price of oil increases, marginal fields become "provedreserves", thus we are unlikely to "run out" of oil, as more fields willbecome economic as the price rises. If the price exceeds $30/bbl thenalternative fuels may become competitive, and at $50-60/bbl coal-derivedliquid fuels are economic, as are many biomass-derived fuels and otherenergy sources [6]. One barrel of oil equals 0.158987 m3. The current pricefor Brent Crude is approx. $18/bbl. The R/P ratio has increased from 27years (1979) to 43.1 years (1993) [5]. Now, some numbers. ( billion = 1 x 10^9. trillion = 1 x 10^12 ).Crude Oil              Proved Reserves                  R/P RatioMiddle East                89.6 billion tonnes           95.1 yearUSA                         4.0                           9.9 yearsTotal World               136.7                          43.1 yearsCoal                   Proved Reserves                  R/P RatioUSA                       240.56 billion tonnes         267 yearsTotal World             1,039.182                       236 yearsNatural Gas            Proved Reserves                  R/P RatioUSA                         4.7 trillion cubic metres     8.8 yearsTotal World               142.0                          64.9 years. 4.3 What is the history of gasoline? In the late 19th Century the most suitable fuels for the automobilewere coal tar distillates and the lighter fractions from the distillationof crude oil. During the early 20th Century the oil companies wereproducing gasoline as a simple distillate from petroleum, but theautomotive engines were rapidly being improved and required a moresuitable fuel. During the 1910s, laws prohibited the storage of gasolineson residential properties, so Charles F. Kettering ( yes - he of ignitionsystem fame ) modified an IC engine to run on kerosine. However thekerosine-fuelled engine would "knock" and crack the cylinder head andpistons. He assigned Thomas Midgley Jr. to confirm that the cause wasfrom the kerosine droplets vaporising on combustion as they presumed .Midgley demonstrated that the knock was caused by a rapid rise inpressure after ignition, not during preignition as believed [7]. Thisthen lead to the long search for anti-knock agents, culminating intetra ethyl lead [8]. Typical mid-1920s gasolines were 40 - 60 Octane [9]. Because sulfur in gasoline inhibited the octane-enhancing effectof the alkyl lead, the sulfur content of the thermally-cracked refinerystreams for gasolines was restricted. By the 1930s, the petroleumindustry had determined that the larger hydrocarbon molecules (kerosine)had major adverse effects on the octane of gasoline, and were developingconsistent specifications for desired properties. By the 1940s catalyticcracking was introduced, and gasoline compositions became fairly consistentbetween brands during the various seasons. The 1950s saw the start of the increase of the compression ratio, requiringhigher octane fuels. Lead levels were increased, and some new refiningprocesses ( such as hydrocracking ), specifically designed to providehydrocarbons components with good lead response and octane, were introduced.Minor improvements were made to gasoline formulations to improve yields andoctane until the 1970s - when unleaded fuels were introduced to protectthe exhaust catalysts that were also being introduced for environmentalreasons. From 1970 until 1990 gasolines were slowly changed as lead wasphased out. In 1990 the Clean Air Act started forcing major compositionalchanges on gasoline, and these changes will continue into the 21st Centurybecause gasoline is a major pollution source. 4.4 What are the hydrocarbons in gasoline? Hydrocarbons ( HCs ) are any molecules that just contain hydrogen andcarbon, both of which are fuel molecules that can be burnt ( oxidised )to form water ( H2O ) or carbon dioxide ( CO2 ). If the combustion isnot complete, carbon monoxide ( CO ) may be formed. As CO can be burntto produce CO2, it is also a fuel. The way the hydrogen and carbons hold hands determines which hydrocarbonfamily they belong to. If they only hold one hand they are called"saturated hydrocarbons" because they can not absorb additional hydrogen.If the carbons hold two hands they are called "unsaturated hydrocarbons"because they can be converted into "saturated hydrocarbons" by theaddition of hydrogen to the double bond. Hydrogens are omitted from thefollowing, but if you remember C = 4 hands, H = 1 hand, and O = 2 hands,you can draw the full structures of most HCs. Gasoline contains over 500 hydrocarbons that may have between 3 to 12carbons, and gasoline used to have a boiling range from 30C to 220C atatmospheric pressure. The boiling range is narrowing as the initial boilingpoint is increasing, and the final boiling point is decreasing, bothchanges are for environmental reasons. Detailed descriptions of structurescan be found in any chemical or petroleum text discussing gasolines [10]. 4.4.1 Saturated hydrocarbons ( aka paraffins, alkanes ) stable, the major component of gasolines tend to burn in air with a clean flame alkanes   normal = continuous chain of carbons ( Cn H2n+2 )    normal heptane	C-C-C-C-C-C-C                    C7H16  iso = branched chain of carbons  ( Cn H2n+2 )    iso octane =                       C   C    ( aka 2,2,4-trimethylpentane )     |   |                                     C-C-C-C-C           C8H18                                       |                                       C  cyclic = circle of carbons  ( Cn H2n )  ( aka Naphthenes )    cyclohexane  =                 C                                  / \                                 C   C                                 |   |                   C6H12                                 C   C                                  \ /                                   C 4.4.2 Unsaturated Hydrocarbons Unstable, are the remaining component of gasoline. Tend to burn in air with a smoky flame. Alkenes ( aka olefins, have carbon=carbon double bonds ) These are unstable, and are usually limited to a few %.                                  C                                 |                       C5H10          2-methyl-2-butene    C-C=C-C Alkynes ( aka acetylenes, have carbon-carbon triple bonds ) These are even more unstable, are only present intrace amounts, and only in some poorly-refined gasolines.                                  _          Acetylene             C=C                      C2H2 Arenes ( aka aromatics ) Used to be up to 40%, gradually being reduced to <20%.                         C                       C                      // \                    // \                     C    C                C-C    C           Benzene   |   ||      Toluene     |   ||                     C    C                  C    C                      \\ /                    \\ /                        C                       C                      C6H6                    C7H8 Polynuclear Aromatics ( aka PNAs or PAHs ) These are high boiling, and are only present in small amountsin gasoline. They contain benzene rings joined together, andthe simplest is Naphthalene. The multi-ringed PNAs are highlytoxic, and are not present in gasoline.                                   C    C                                // \ / \\                               C    C    C           Naphthalene         |    ||   |               C10H8                               C    C    C                                \\ / \ //                                  C    C 4.5 What are oxygenates? Oxygenates are just preused hydrocarbons :-). They contain oxygen, whichcan not provide energy, but their structure provides a reasonableanti-knock value, thus they are good substitutes for aromatics, andthey may also reduce the smog-forming tendencies of the exhaust gases [11].     Ethanol                                  C-C-O-H      C2H5OH                                               C                                               |    Methyl tertiary butyl ether              C-C-O-C      C4H90CH3    (aka tertiary butyl methyl ether )         |                                               C They can be produced from fossil fuels eg methanol (MeOH), methyl tertiarybutyl ether (MTBE), tertiary amyl methyl ether (TAME), or from biomass, egethanol(EtOH), ethyl tertiary butyl ether (ETBE)). Most oxygenates used ingasolines are either alcohols ( Cx-O-H ) or ethers (Cx-O-Cy), and contain1 to 6 carbons. MTBE is produced by reacting methanol ( from natural gas )with isobutylene in the liquid phase over an acidic ion-exchange resincatalyst at 100C. The isobutylene was initially from refinery catalyticcrackers or petrochemical olefin plants, but these days larger plantsproduce it from butanes. Production has increased at the rate of 10 to 20%per year, and the spot market price in June 1993 was around $270/tonne [11].The "ether" starting fluids for vehicles are usually diethyl ether( liquid ) or dimethyl ether ( aerosol ). Note that " petroleum ether " isactually a volatile hydrocarbon fraction, it is not a Cx-O-Cy compound. Oxygenates are added to gasolines to reduce the reactivity of emissions,but they are only effective if the hydrocarbon fractions are carefullymodified to utilise the octane and volatility properties of the oxygenates.If the hydrocarbon fraction is not correctly modified, oxygenates canincrease the undesirable smog-forming and toxic emissions. The majorreduction in the reactivity of exhaust and evaporative emissions will occurwith reformulated gasolines, due to be introduced in January 1995, whichhave oxygenates and major composition changes to the hydrocarbon component.Oxygenates do not necessarily reduce all individual exhaust toxins, norare they intended to. Oxygenates have significantly different physical properties to hydrocarbons,and the levels that can be added to gasolines are controlled by the EPA inthe US, with waivers being granted for some combinations. The change toreformulated gasoline requires oxygenates, but also that the hydrocarboncomposition must be significantly more modified than the existingoxygenated gasolines to reduce unsaturates, volatility, benzene, and thereactivity of emissions. Oxygenates that are added to gasoline function in two ways. Firstly theyhave high blending octane, and so can replace high octane aromaticsin the fuel. These aromatics are responsible for disproportionate amountsof CO and HC exhaust emissions. This is called the "aromatic substitutioneffect". Oxygenates also cause engines without sophisticated enginemanagement systems to move to the lean side of stoichiometry, thus reducingemissions of CO ( 2% oxygen can reduce CO by 16% ) and HC ( 2% oxygencan reduce HC by 10%). However, on vehicles with engine management systems,the fuel volume will be increased to bring the stoichiometry back tothe preferred optimum setting. Oxygen in the fuel can not contributeenergy, consequently the fuel has less energy content. For the sameefficiency and power output, more fuel has to be burnt, and the slightimprovements in efficiency that oxygenates provide on some engines usuallydo not completely compensate for the oxygen [12]. There are huge number of chemical mechanisms involved in the pre-flamereactions of gasoline combustion. Although both alkyl leads and oxygenatesare effective at suppressing knock, the chemical modes through which theyact are entirely different. MTBE works by retarding the progress of the lowtemperature or cool-flame reactions, consuming radical species, particularlyOH radicals and producing isobutene. The isobutene in turn consumesadditional OH radicals and produces unreactive, resonantly stabilisedradicals such as allyl and methyl allyl, as well as stable species such asallene, which resist further oxidation [13,14]. 4.6 Why were alkyl lead compounds added? The efficiency of a spark-ignited gasoline engine can be related to thecompression ratio up to at least compression ratio 17:1 [15]. However any"knock" caused by the fuel will rapidly mechanically destroy an engine, andGeneral Motors was having major problems trying to improve engines withoutinducing knock. The problem was to identify economic additives that couldbe added to gasoline or kerosine to prevent knock, as it was apparent thatengine development was being hindered. The kerosine for home fuels soonbecame a secondary issue, as the magnitude of the automotive knock problemincreased throughout the 1910s, and so more resources were poured into thequest for an effective "anti-knock". A higher octane aviation gasoline wasrequired urgently once the US entered WWI, and almost every possiblechemical ( including melted butter ) was tested for anti-knock ability [16]. Originally, iodine was the best anti-knock available, but was not a practicalgasoline additive, and was used as the benchmark. In 1919 aniline was foundto have superior antiknock ability to iodine, but also was not a practicaladditive, however aniline became the benchmark anti-knock, and variouscompounds were compared to it. The discovery of tetra ethyl lead, and thescavengers required to remove it from the engine were made by teams lead byThomas Midgley Jr. in 1922 [7,8,16]. They tried selenium oxychloride whichwas an excellent antiknock, however it reacted with iron and "dissolved" theengine. Midgley was able to predict that other organometallics would work,and slowly focused on organoleads. They then had to remove the lead, whichwould otherwise accumulate and coat the engine and exhaust system with lead.They discovered and developed the halogenated lead scavengers that are stillused in leaded fuels. The scavengers, ( ethylene dibromide and ethylenedichloride ), function by providing halogen atoms that react with the leadto form volatile lead halide salts that can escape out the exhaust. Thequantity of scavengers added to the alkyl lead concentrate is calculatedaccording to the amount of lead present. If sufficient scavenger is addedto theoretically react with all the lead present, the amount is called one"theory". Typically, 1.0 to 1.5 theories are used, but aviation gasolinesmust only use one theory. This ensures there is no excess bromine that couldreact with the engine. The alkyl leads rapidly became the most cost-effectivemethod of enhancing octane. The development of the alkyl leads ( tetra methyl lead, tetra ethyl lead )and the toxic halogenated scavengers meant that petroleum refiners couldthen configure refineries to produce hydrocarbon streams that wouldincrease octane with small quantities of alkyl lead. If you keep addingalkyl lead compounds, the lead response of the gasoline decreases, and sothere are economic limits to how much lead should be added. Up until the late 1960s, alkyl leads were added to gasolines in increasingconcentrations to obtain octane. The limit was 1.14g Pb/l, which is wellabove the diminishing returns part of the lead response curve for mostrefinery streams, thus it is unlikely that much fuel was ever made at thatlevel. I believe 1.05 was about the maximum, and articles suggest that 1970100 RON premiums were about 0.7-0.8 g Pb/l and 94 RON regulars 0.6-0.7 gPb/l, which matches published lead response data [17] eg. For             Catalytic Reformate           Straight Run Naphtha.Lead g/l                    Research Octane Number   0                   96                           72  0.1                  98                           79  0.2                  99                           83  0.3                 100                           85  0.4                 101                           87  0.5                 101.5                         88  0.6                 102                           89  0.7                 102.5                         89.5  0.8                 102.75                        90 The alkyl lead anti-knocks work in a different stage of the pre-combustionreaction to oxygenates. In contrast to oxygenates, the alkyl lead interfereswith hydrocarbon chain branching in the intermediate temperature rangewhere HO2 is the most important radical species. Lead oxide, either assolid particles, or in the gas phase, reacts with HO2 and removes it fromthe available radical pool, thereby deactivating the major chain branchingreaction sequence that results in undesirable, easily-autoignitablehydrocarbons [13,14]. 4.7 Why not use other organometallic compounds? As the toxicity of the alkyl lead and the halogenated scavengers became ofconcern, alternatives were considered. The most famous of these ismethylcyclopentadienyl manganese tricarbonyl (MMT), which was used in theUSA until banned by the EPA from 27 Oct 1978 [18], but is approved for usein Canada and Australia. It is more expensive than alkyl leads and has beenreported to increase unburned hydrocarbon emissions and block exhaustcatalysts [19]. Other compounds that enhance octane have been suggested,but usually have significant problems such as toxicity, cost, increasedengine wear etc.. Examples include dicyclopentadienyl iron and nickelcarbonyl. 4.8 What do the refining processes do? Crude oil contains a wide range of hydrocarbons, organometallics and othercompounds containing sulfur, nitrogen etc. The HCs contain between 1 and 60carbon atoms. Gasoline requires hydrocarbons with carbon atoms between 3 and12, arranged in specific ways to provide the desirable properties. Obviously,a refinery has to either sell the remainder as marketable products, orconvert the larger molecules into smaller gasoline molecules. A refinery will distill crude oil into various fractions and, depending onthe desired final products, will further process and blend those fractions.Typical final products could be:- gases for chemical synthesis and fuel(CNG), liquified gases (LPG), butane, aviation and automotive gasolines,aviation and lighting kerosines, diesels, distillate and residual fuel oils,lubricating oil base grades, paraffin oils and waxes. Many of the commonprocesses are intended to increase the yield of blending feedstocks forgasolines. Typical modern refinery processes for gasoline components include Catalytic cracking - breaks larger, higher-boiling, hydrocarbons into gasoline range product that contains 30% aromatics and 20-30% olefins. Hydrocracking - cracks and adds hydrogen to molecules, producing a more saturated, stable, gasoline fraction. Isomerisation - raises gasoline fraction octane by converting straight chain hydrocarbons into branched isomers. Reforming - converts saturated, low octane, hydrocarbons into higher octane product containing about 60% aromatics. Alkylation - reacts gaseous olefin streams with isobutane to produce liquid high octane iso-alkanes. The changes that the Clean Air Act and other legislation ensures that therefineries will continue to modify their processes to produce a lessvolatile gasoline with fewer toxins and toxic emissions. Options include:- Reducing the "severity" of reforming to reduce aromatic production. Distilling the C5/C6 fraction from reformer feeds and treating that stream to produce non-aromatic high octane components. Distilling the higher boiling fraction ( which contains 80-100% of aromatics that can be hydrocracked ) from catalytic cracker product [20]. Convert butane to isobutane or isobutylene for alkylation or MTBE feed. 4.9 What energy is released when gasoline is burned? It is important to note that the theoretical energy content of gasolinewhen burned in air is only related to the hydrogen and carbon contents.Octane rating is not fundamentally related to the energy content, and theactual hydrocarbon and oxygenate components used in the gasoline willdetermine both the energy release and the anti-knock rating. Two important reactions are:- C + O2 = CO2 H + O2 = H2O The mass or volume of air required to provide sufficient oxygen to achievethis complete combustion is the "stoichiometric" mass or volume of air.Insufficient air = "rich", and excess air = "lean", and the stoichiometricmass of air is related to the carbon:hydrogen ratio of the fuel. Theprocedures for calculation of stoichiometric air/fuel ratios are fullydocumented in an SAE standard [21]. Atomic masses used are:- Hydrogen = 1.00794, Carbon = 12.011,Oxygen = 15.994, Nitrogen = 14.0067, and Sulfur = 32.066. The composition of sea level air ( 1976 data, hence low CO2 value ) is Gas            Fractional      Molecular Weight         RelativeSpecies          Volume            kg/mole                MassN2              0.78084             28.0134             21.873983O2              0.209476            31.9988              6.702981Ar              0.00934             39.948               0.373114CO2             0.000314            44.0098              0.013919Ne              0.00001818          20.179               0.000365He              0.00000524           4.002602            0.000021Kr              0.00000114          83.80                0.000092Xe              0.000000087        131.29                0.000011CH4             0.000002            16.04276             0.000032H2              0.0000005            2.01588             0.000001                                                        ---------Air                                                     28.964419 For normal heptane C7H16 with a molecular weight = 100.204            C7H16 + 11O2 = 7CO2 + 8H2O thus 1.000 kg of C7H16 required 3.513 kg of O2 = 15.179 kg air. The chemical stoichiometric combustion of hydrocarbons with oxygencan be written as:- CxHy + (x + (y/4))O2  ->  xCO2 + (y/2)H2O Often, for simplicity, the remainder of air is assumed to be nitrogen,which can be added to the equation when exhaust compositions are required.As a general rule, maximum power is achieved at slightly rich, whereasmaximum fuel economy is achieved at slightly lean. The energy content of the gasoline is obtained by burning all the fuelinside a bomb calorimeter and measuring the temperature increase.The energy available depends on what happens to the water produced from thecombustion of the hydrogen. If the water remains as a gas, then it cannotrelease the heat of vaporisation, thus producing the Nett Calorific Value.If the water were condensed back to the original fuel temperature, thenGross Calorific Value of the fuel, which will be larger, is obtained. The calorific values are fairly constant for families of HCs, which is notsurprising, given their fairly consistent carbon/hydrogen ratios. For liquid( l ) or gaseous ( g ) fuel converted to gaseous products - except for the2-methylbutene-2, where only gaseous is reported. * = Blending Octane Number Typical Heats of Combustion are [22]:- Fuel     State  Heat of Combustion      Research        Motor                    MJ/kg                Octane         Octane	n-heptane  l        44.592                  0              0           g        44.955i-octane   l        44.374                100            100           g        44.682toluene    l        40.554                124*           112*           g        40.9672-methylbutene-2    44.720                176*           141*  Because all the data is available, the calorific value of fuels can beestimated quite accurately from hydrocarbon fuel properties such as thedensity, sulfur content, and aniline point ( which indicates the aromaticscontent ). It should be noted that because oxygenates contain oxygen that cannot provide energy, they will have significantly lower energy contents.They are added to provide octane, not energy. For an engine that can beoptimised for oxygenates, more fuel is required to obtain the same power,but they can burn slightly more efficiently, thus the power ratio is notidentical to the energy content ratio. They also require more energy tovaporise.             Energy Content   Heat of Vaporisation   Oxygen Content              Nett MJ/kg          MJ/kg                   wt%Methanol        19.95             1.154                  49.9Ethanol         26.68             0.913                  34.7MTBE            35.18             0.322                  18.2ETBE            36.29             0.310                  15.7TAME            36.28             0.323                  15.7Gasoline       42 - 44            0.297                   0.0 Typical values for commercial fuels in megajoules/kilogram are [23]:-                                 Gross        Nett      Hydrogen                        141.9       120.0Carbon to Carbon monoxide        10.2          -Carbon to Carbon dioxide         32.8          -Sulfur to sulfur dioxide          9.16         -Natural Gas                      53.1         48.0Liquified petroleum gas          49.8         46.1Aviation gasoline                46.0         44.0Automotive gasoline              45.8         43.8Kerosine                         46.3         43.3Diesel                           45.3         42.5 Obviously, for automobiles, the nett calorific value is appropriate. Thecalorific value is the maximum energy that can be obtained from the fuel,but the reality of modern SI engines is that efficiencies of 20-40% may beobtained, this limit being due to engineering and material constraintsthat prevent optimum combustion conditions being used. The CI engine canachieve higher efficiencies, usually over a wider operating range as well. 4.10 What are the gasoline specifications? Gasolines are usually defined by government regulation, where properties andtest methods are clearly defined. In the US, several government and statebodies can specify gasoline properties. The US gasoline specifications andtest methods are listed in several readily available publications, includingthe Society of Automotive Engineers (SAE) [24], and the American Society forTesting Materials (ASTM) [25]. The 1994 ASTM edition has:- D4814-93a Specification for Automotive Spark-Ignition Engine Fuel. This specification lists various properties that all fuels have to complywith, and may be updated throughout the year. Typical properties are:- 4.10.1 Vapour Pressure and Distillation Classes. 6 different classes according to location and/or season. As gasoline is distilled, the temperatures at which various fractions areevaporated are calculated. Specifications define the temperatures at whichvarious percentages of the fuel are evaporated. Distillation limitsinclude maximum temperatures that 10% is evaporated (50-70C), 50% isevaporated (110-121C), 90% is evaporated (185-190C), and the final boilingpoint (225C). A minimum temperature for 50% evaporated (77C), and a maximumamount of Residue (2%) after distillation. Vapour pressure limits foreach class ( 54, 62, 69, 79, 93, 103 kPa ) are also specified. Note that theEPA has issued a waiver that does not require gasoline/ethanol blends tomeet the required specifications. 4.10.2 Vapour Lock Protection Classes 5 classes for vapour lock protection, according to location and/or season.The limit is a maximum Vapour/Liquid ratio of 20 at test temperatures of41, 47, 51, 56, 60C. 4.10.3 Antiknock Index ( aka (RON+MON)/2, "Pump Octane" ) The ( Research Octane Number + Motor Octane Number ) divided by two. Limitsare not specified, but changes in engine requirements according season andlocation are discussed. Fuels with an Antiknock index of 87, 89, 91( Unleaded), and 88 ( Leaded ) are listed as typical for the US. 4.10.4 Lead Content Leaded = 1.1 g Pb / L maximum, and Unleaded = 0.013 g Pb / L maximum. 4.10.5 Copper strip corrosion Ability to tarnish clean copper, indicating the presence of any corrosivesulfur compounds 4.10.6 Maximum Sulfur content Sulfur adversely affects exhaust catalysts and fuel hydrocarbon leadresponse, and also may be emitted as polluting sulfur oxides. Leaded = 0.15 %mass maximum, and Unleaded = 0.10 %mass maximum. Typical US gasoline levels are 0.03 %mass. 4.10.7 Maximum Existent Gum Limits the amount of gums present in fuel at the time of testing to5 mg/100mls. The results do not correlate well with actual engine depositscaused by fuel vaporisation [26]. 4.10.8 Minimum Oxidation Stability This ensures the fuel remains chemically stable, and does not form additionalgums during periods in distribution systems, which can be up to 3-6 months.The sample is heated with oxygen inside a pressure vessel, and the delayuntil significant oxygen uptake is measured. 4.10.9 Water Tolerance Highest temperature that causes phase separation of oxygenated fuels.The limits vary according to location and month. For Alaska - North of 62latitude, it changes from -41C in Dec/Jan to 9C in July, but remains 10C allyear in Hawaii. As well as the above, there are various restrictions introduced by the CleanAir Act and state bodies such as California's Air Resources Board (CARB) thatoften have more stringent limits for the above properties, as well asadditional limits. The Clean Air Act also specifies some regions that exceedair quality standards have to use reformulated gasolines (RFGs) all year,starting January 1995. Other regions are required to use oxygenatedgasolines for four winter months, beginning November 1992. The RFGs alsocontain oxygenates. Metropolitan regions with severe ozone air qualityproblems must use reformulated gasolines in 1995 that;- contain at least2.0 wt% oxygen, reduce 1990 volatile organic carbon compounds by 15%, andreduce specified toxic emissions by 15% (1995) and 25% (2000). Metropolitanregions that exceeded carbon monoxide limits were required to use gasolineswith 2.7 wt% oxygen during winter months, starting in 1992. Because phosphorus adversely affects exhaust catalysts, the EPA limitsphosphorus in all gasolines to 0.0013 gP/L. The 1990 Clean Air Act (CAA) amendments and CARB phase 2 (1996)specifications for reformulated gasoline establish the following limits,compared with typical 1990 gasoline. Because of a lack of data, the EPAwere unable to define the CAA required parameters , so they instituteda two-stage system. The first stage, the "Simple Model" is an interimstage that run from 1/Jan/1995 to 1/May/1997. The second stage, the"Complex Model" would be developed, with the following parameters likelyto be controlled - reid vapour pressure, benzene, oxygen, sulfur, olefinsdistillation ( 90% Evaporated ), and aromatics. Each refiner must havetheir RFG recertified using the Complex model by 1/May/1997 [27].                        1990           Clean Air Act       CARBbenzene                 2 %              1 % maximum      1.0 vol% maximumoxygen                  0.2 %            2 % minimum    1.8-2.0 mass%sulfur                150 ppm            no increase     40 ppmaromatics              32.0 %           25 % maximum     25 vol% maximumolefins                 9.9 %            5 % maximum      6 vol% maximumreid vapour pressure   60 kPa           56 kPa (north)   48 kPa                                        50 kPa (south)90% evaporated        170 C              -              149 C These regulations also specify emissions criteria. eg CAA specifies noincrease in nitric oxides (NOx) emissions, reductions in VOC by 15% duringthe ozone season, and specified toxins by 15% all year. These criteriaindirectly establish vapour pressure and composition limits that refinershave to meet. Note that the EPA also can issue CAA Section 211 waivers thatallow refiners to choose which oxygenates they use. In 1981, the EPA alsodecided that fuels with up to 2% alcohols and ethers (except methanol) were"substantially similar" to 1974 unleaded gasoline, and thus were not "new"gasoline additives. That level was increased to 2.7 wt% in 1991. Some otheroxygenates have also been granted waivers, eg ethanol to 3.5 wt% in1979/1982, and tert-butyl alcohol to 3.5 wt% in 1981. 4.11 What are the effects of the specified fuel properties? Volatility This affects evaporative emissions and driveability, it is the property thatmust change with location and season. Fuel for mid-summer Arizona would bedifficult to use in mid-winter Alaska. The US is divided into zones,according to altitude and seasonal temperatures, and the fuel volatility isadjusted accordingly. Incorrect fuel may result in difficult starting incold weather, carburetter icing, vapour lock in hot weather, and crankcaseoil dilution. Volatility is controlled by distillation and vapour pressurespecifications. The higher boiling fractions of the gasoline have significanteffects on the emission levels of undesirable hydrocarbons and aldehydes,and a reduction of 40C in the final boiling point will reduce the levels ofbenzene, butadiene, formaldehyde and acetaldehyde by 25%, and will reduceHC emissions by 20% [28]. Combustion Characteristics As gasolines contain mainly hydrocarbons, the only significant variablebetween different grades is the octane rating of the fuel, as most otherproperties are similar. Octane is discussed in detail in Section 6. Thereare only slight differences in combustion temperatures ( most are around2000C in isobaric adiabatic combustion [29]). Note that the actualtemperature in the combustion chamber is also determined by other factors,such as load and engine design. The addition of oxygenates changes thepre-flame reaction pathways, and also reduces the energy content of the fuel.The levels of oxygen in the fuel is regulated according to regional airquality standards. Stability Motor gasolines may be stored up to six months, consequently they must notform gums which may precipitate. Gums are usually the result ofcopper-catalysed reactions of the unsaturated HCs, so antioxidants and metaldeactivators are added. Existent Gum is used to measure the gum in the fuelat the time tested, whereas the Oxidation Stability measures the time ittakes for the gasoline to break down at 100C with 100psi of oxygen. A 240minutes test period has been found to be sufficient for most storage anddistribution systems. Corrosiveness Sulfur in the fuel creates corrosion, and when combusted will form corrosivegases that attack the engine, exhaust and environment. Sulfur also adverselyaffects the alkyl lead octane response and may poison exhaust catalysts. Thecopper strip corrosion test and the sulfur specification are used to ensurefuel quality. The copper strip test measures active sulfur, whereas thesulfur content reports the total sulfur present. 4.12 Are brands different? Yes. The above specifications are intended to ensure minimal qualitystandards are maintained, however as well as the fuel hydrocarbons, themanufacturers add their own special ingredients to provide additionalbenefits. A quality gasoline additive package would include:- octane-enhancing additives ( improve octane ratings ) anti-oxidants ( inhibit gum formation, improve stability ) metal deactivators ( inhibit gum formation, improve stability ) deposit modifiers ( reduce deposits, spark-plug fouling and preignition ) surfactants ( prevent icing, improve vaporisation, inhibit deposits, reduce NOx emissions ) freezing point depressants ( prevent icing ) corrosion inhibitors ( prevent gasoline corroding storage tanks ) dyes ( product colour for safety or regulatory purposes ). During the 1980s significant problems with deposits accumulating on intakevalve surfaces occurred as new fuel injections systems were introduced.These intake valve deposits (IVD) were different to the injector deposits,in part because the valve can reach 300C. Engine design changes that preventdeposits usually consist of ensuring the valve is flushed with liquidgasoline, and provision of adequate valve rotation. Gasoline factors thatcause deposits are the presence of alcohols or olefins. Gasolinemanufacturers now routinely use additives that prevent IVD and also maintainthe cleanliness of injectors. These usually include a surfactant and lightoil to maintain the wetting of important surfaces. A more detaileddescription of additives is provided in Section 9.1. Texaco demonstrated that a well-formulated package could improve fueleconomy, reduce NOx emissions, and restore engine performance because, aswell as the traditional liquid-phase deposit removal, some additives canwork in the vapour phase to remove existing engine deposits withoutadversely affecting performance ( as happens when water is poured into arunning engine to remove carbon deposits:-) )[30]. Most suppliers of qualitygasolines will formulate similar additives into their products, and cheaperlines are less like to have such additives added. As different brands usedifferent additives and oxygenates, it is probable that important parameters,such as octane distribution, are different, even though the pump octaneratings are the same. So, if you know your car is well-tuned, and in good condition, but thedriveability is pathetic on the correct octane, try another brand. Rememberthat the composition will change with the season, so if you losedriveability, try yet another brand. As various Clean Air Act changes areintroduced over the next few years, gasoline will continue to change. 4.13 What is a typical composition? There seems to be a perception that all gasolines of one octane grade arechemically similar, and thus general rules can be promulgated about "energycontent ", "flame speed", "combustion temperature" etc. etc.. Nothing isfurther from the truth. The behaviour of manufactured gasolines in octanerating engines can be predicted, using previous octane ratings of specialblends intended to determine how a particular refinery stream responds to anoctane-enhancing additive. Refiners can design and reconfigure refineries toefficiently produce a wide range of gasolines feedstocks, depending onmarket and regulatory requirements. The last 10 years of various compositional changes to gasolines forenvironmental and health reasons have resulted in fuels that do not followhistorical rules, and the regulations mapped out for the next decade alsoensure the composition will remain in a state of flux. The reformulatedgasoline specifications, especially the 1/May/1997 Complex model, willprobably introduce major reductions in the distillation range, as well asthe various limits on composition and emissions. I'm not going to list all 500+ HCs in gasolines, but the following arerepresentative of the various classes typically present in a gasoline. Thenumbers after each chemical are:- Research Blending Octane : Motor BlendingOctane : Boiling Point (C): Density (g/ml @ 15C) : Minimum AutoignitionTemperature (C). It is important to realise that the Blending Octanes arederived from a 20% mix of the HC with a 60:40 iC8:nC7 base, and theextrapolation of this 20% to 100%. This is different from rating the purefuel, which often requires adjustment of the test engine conditions outsidethe acceptable limits of the rating methods. Generally the actual octanes ofthe pure fuel are similar for the alkanes, but are up to 30 octane numberslower than the blending octanes for the aromatics and olefins [31]. A traditional composition I have dreamed up would be like the following,whereas newer oxygenated fuels reduce the aromatics and olefins, narrow theboiling range, and add oxygenates up to about 12-15% to provide the octane. 15% n-paraffins                       RON   MON    BP      d     AIT        n-butane                      113 : 114 :  -0.5:  gas  : 370        n-pentane                      62 :  66 :  35  : 0.626 : 260        n-hexane                       19 :  22 :  69  : 0.659 : 225        n-heptane (0:0 by definition)   0 :   0 :  98  : 0.684 : 225        n-octane                      -18 : -16 : 126  : 0.703 : 220     ( you would not want to have the following alkanes in gasoline,       so you would never blend kerosine with gasoline )        n-decane                      -41 : -38 : 174  : 0.730 : 210        n-dodecane                    -88 : -90 : 216  : 0.750 : 204        n-tetradecane                 -90 : -99 : 253  : 0.763 : 20030%  iso-paraffins        2-methylpropane               122 : 120 : -12  :  gas  : 460        2-methylbutane                100 : 104 :  28  : 0.620 : 420        2-methylpentane                82 :  78 :  62  : 0.653 : 306        3-methylpentane                86 :  80 :  64  : 0.664 :  -        2-methylhexane                 40 :  42 :  90  : 0.679 :        3-methylhexane                 56 :  57 :  91  : 0.687 :        2,2-dimethylpentane            89 :  93 :  79  : 0.674 :        2,2,3-trimethylbutane         112 : 112 :  81  : 0.690 : 420        2,2,4-trimethylpentane        100 : 100 :  98  : 0.692 : 415          ( 100:100 by definition )12% cycloparaffins        cyclopentane                  141 : 141 :  50  : 0.751 : 380        methylcyclopentane            107 :  99 :  72  : 0.749 :        cyclohexane                   110 :  97 :  81  : 0.779 : 245        methylcyclohexane             104 :  84 : 101  : 0.770 : 25035% aromatics        benzene                        98 :  91 :  80  : 0.874 : 560        toluene                       124 : 112 : 111  : 0.867 : 480        ethyl benzene                 124 : 107 : 136  : 0.867 : 430        meta-xylene                   162 : 124 : 138  : 0.868 : 463        para-xylene                   155 : 126 : 138  : 0.866 : 530        ortho-xylene                  126 : 102 : 144  : 0.870 : 530        3-ethyltoluene                162 : 138 : 158  : 0.865 :        1,3,5-trimethylbenzene        170 : 136 : 163  : 0.864 :        1,2,4-trimethylbenzene        148 : 124 : 168  : 0.889 :8% olefins        2-pentene                     154 : 138 :  37  : 0.649 :        2-methylbutene-2              176 : 140 :  36  : 0.662 :        2-methylpentene-2             159 : 148 :  67  : 0.690 :        cyclopentene                  171 : 126 :  44  : 0.774 :    ( the following olefins are not present in significant amounts      in gasoline, but have some of the highest blending octanes )        1-methylcyclopentene          184 : 146 :  75  : 0.780 :        1,3 cyclopentadiene           218 : 149 :  42  : 0.805 :        dicyclopentadiene             229 : 167 : 170  : 1.071 : Oxygenates Published octane values vary a lot because the rating conditions aresignificantly different to standard conditions, for example the API Project45 numbers used above for the hydrocarbons, reported in 1957, gave MTBEblending RON as 148 and MON as 146, however that was based on the leadresponse, whereas today we use MTBE in place of lead.         methanol                      133 : 105 :  65  : 0.796 : 385        ethanol                       129 : 102 :  78  : 0.794 : 365        iso propyl alcohol            118 :  98 :  82  : 0.790 : 399        methyl tertiary butyl ether   116 : 103 :  55  : 0.745 :        ethyl tertiary butyl ether    118 : 102 :  72  : 0.745 :        tertiary amyl methyl ether    111 :  98 :  86  : 0.776 : There are some other properties of oxygenates that have to be consideredwhen they are going to be used as fuels, particularly their ability toform very volatile azeotropes that cause the fuel's vapour pressure toincrease, the chemical nature of the emissions, and their tendency toseparate into a separate water/oxygenate phase when water is present.The reformulated gasolines address these problems more successfully thanthe original oxygenated gasolines. Before you rush out to make a highly aromatic or olefinic gasoline toproduce a high octane fuel, remember they have other adverse properties,eg the aromatics attack elastomers and generate smoke, and the olefins areunstable ( besides smelling foul ) and form gums. The art of correctlyformulating a gasoline that does not cause engines to knock apart, does notcause vapour lock in summer - but is easy to start in winter, does not formgums and deposits, burns cleanly without soot/residues, and does not dissolveor poison the car catalyst or owner, is based on knowledge of the gasolinecomposition. 4.14 Is gasoline toxic or carcinogenic? There are several known toxins in gasoline, some of which are confirmedhuman carcinogens. The most famous of these toxins are lead and benzene, andboth are regulated. The other aromatics and some toxic olefins are alsocontrolled. Lead alkyls also require ethylene dibromide and/or ethylenedichloride scavengers to be added to the gasoline, both of which aresuspected human carcinogens. In 1993 an International Symposium on the HealthEffects of Gasoline was held [32]. Major review papers on the carcinogenic,neurotoxic, reproductive and developmental toxicity of gasoline, additives,and oxygenates were presented. The oxygenates are also being evaluated forcarcinogenicity, and even ethanol and ETBE may be carcinogens. It shouldbe noted that the oxygenated gasolines were not expected to reduce thetoxicity of the emissions, however the reformulated gasolines will producedifferent emissions, and specific toxins must be reduced by 15% all year. There is little doubt that gasoline is full of toxic chemicals, and shouldtherefore be treated with respect. However the biggest danger remains theflammability, and the relative hazards should always be kept in perspective.The major toxic risk from gasolines comes from breathing the tailpipe,evaporative, and refuelling emissions, rather than occasional skin contactfrom spills. Breathing vapours and skin contact should always be minimised. 4.15 Is unleaded gasoline more toxic than leaded? The short answer is no. However that answer is not global, as some countrieshave replaced the lead compound octane-improvers with aromatic or olefinoctane-improvers without introducing exhaust catalysts. Some aromatics aremore toxic that paraffins. Unfortunately, the manufacturers of alkyl leadcompounds have embarked on a worldwide misinformation campaign in countriesconsidering emulating the lead-free US. The use of lead precludes the use ofexhaust catalysts, thus the emissions of aromatics are only slightlydiminished, and other pollutants can not reduced by exhaust catalysts. The use of unleaded on modern vehicles with engine management systems andcatalysts can reduce aromatic emissions to 10% of the level of vehicleswithout catalysts [33]. Alkyl lead additives can only substitute for some ofthe aromatics in gasoline, consequently they do not eliminate aromatics,which will produce benzene emissions [34]. Alkyl lead additives also requiretoxic organohalogen scavengers, which also react in the engine to form andemit other organohalogens, including highly toxic dioxin [35]. Leaded fuelsemit lead, organohalogens, and much higher levels of regulated toxinsbecause they preclude the use of exhaust catalysts. In the USA the gasolinecomposition is being changed to reduce fuel toxins ( olefins, aromatics )as well as emissions of specific toxins. Subject: 5. Why is Gasoline Composition Changing? 5.1 Why pick on cars and gasoline? Cars emit several pollutants as combustion products out the tailpipe,(tailpipe emissions), and as losses due to evaporation (evaporativeemissions, refuelling emissions). The volatile organic carbon (VOC)emissions from these sources, along with nitrogen oxides (NOx) emissionsfrom the tailpipe, will react in the presence of ultraviolet light(wavelengths of less than 430nm) to form ground-level (tropospheric) ozone,which is one of the major components of photochemical smog [36]. Smog hasbeen a major pollution problem ever since coal-fired power stations weredeveloped in urban areas, but their emissions are being cleaned up. Now it'sthe turn of the automobile. Cars currently use gasoline that is derived from fossil fuels, thus whengasoline is burned to completion, it produces additional CO2 that is addedto the atmospheric burden. The effect of the additional CO2 on the globalenvironment is not known, but the quantity of man-made emissions of fossilfuels must cause the system to move to a new equilibrium. Even if currentresearch doubles the efficiency of the IC engine/gasoline combination, andreduces HC, CO, NOx, SOx, VOCs, particulates, and carbonyls, the amount ofcarbon dioxide from the use of fossil fuels may still cause global warming.More and more scientific evidence is accumulating that warming is occurring[37]. The issue is whether it is natural, or induced by human activities.There are international agreements to limit CO2 emissions to 1990 levels,a target that will require more efficient, lighter, or appropriately-sizedvehicles, - if we are to maintain the current usage. One option is to use"renewable" fuels in place of fossil fuels. Consider the amount ofenergy-related CO2 emissions for selected countries in 1990 [38].                               CO2 Emissions                         ( tonnes/year/person )USA                               20.0Canada                            16.4Australia                         15.9Germany                           10.4United Kingdom                     8.6Japan                              7.7New Zealand                        7.6 The number of new vehicles provides an indication of the magnitude of theproblem. Although vehicle engines are becoming more efficient, the distancetravelled is increasing, resulting in a gradual increase of gasolineconsumption. The world production of vehicles (in thousands) over the lastfew years was [39];- CarsRegion                       1990      1991     1992     1993Africa                        222       213      194      201Asia-Pacific               12,064    12,112   11,869   11,467Central & South America       800       888    1,158    1,524Eastern Europe              2,466       984    1,726    1,783Middle East                    35        24      300      377North America               7,762     7,230    7,470    8,172Western Europe             13,688    13,286   13,097   11,124Total World                37,039    34,739   35,815   34,649Trucks ( including heavy trucks and buses )Region                       1990      1991     1992    1993Africa                        133       123      108     109Asia-Pacific                5,101     5,074    5,117   5,054Central & South America       312       327      351     417Eastern Europe                980       776      710     708Middle East                    36        28      100     110North America               4,851     4,554    5,371   6,037Western Europe              1,924     1,818    1,869   1,345Total World                13,336    12,701   13,627  13,779 To fuel all operating vehicles, considerable quantities of gasolineand diesel have to be consumed. Major consumption in 1993 of gasolineand middle distillates ( which may include some heating fuels, butnot fuel oils ) in million tonnes.                              Gasoline    Middle DistillatesUSA                           335.6            233.9Canada                         25.0             24.4Western Europe                166.0            264.0Japan                          56.4             89.6Total World                   802.0            989.0 The USA consumption of gasoline increased from 294.4 (1982) to 335.6 (1989)then dipped to 324.2 (1991), and has continued to rise since then to reach335.6 million tonnes in 1993. In 1993 the total world production of crude oilwas 3164.8 million tonnes, of which the USA consumed 787.5 million tonnes[40]. Transport is a very significant user of crude oil products, thusimproving the efficiency of utilisation, and minimising pollution fromvehicles, can produce immediate reductions in emissions of CO2, HCs, VOCs,CO, NOx, carbonyls, and other chemicals. 5.2 Why are there seasonal changes? Only gaseous hydrocarbons burn, consequently if the air is cold, then thefuel has to be very volatile. But when summer comes, a volatile fuel canboil and cause vapour lock, as well as producing high levels of evaporativeemissions. The solution was to adjust the volatility of the fuel accordingto altitude and ambient temperature. This volatility change has beenautomatically performed for decades by the oil companies without informingthe public of the changes. It is one reason why storage of gasoline throughseasons is not a good idea. Gasoline volatility is being reduced as modernengines, with their fuel injection and management systems, can automaticallycompensate for some of the changes in ambient conditions - such as altitudeand air temperature, resulting in acceptable driveability using less volatilefuel. 5.3 Why were alkyl lead compounds removed? " With the exception of one premium gasoline marketed on the east coastand southern areas of the US, all automotive gasolines from the mid-1920suntil 1970 contained lead antiknock compounds to increase antiknock quality.Because lead antiknock compounds were found to be detrimental to theperformance of catalytic emission control system then under development,U.S. passenger car manufacturers in 1971 began to build engines designed tooperate satisfactorily on gasolines of nominal 91 Research Octane Number.Some of these engines were designed to operate on unleaded fuel while othersrequired leaded fuel or the occasional use of leaded fuel. The 91 RON waschosen in the belief that unleaded gasoline at this level could be madeavailable in quantities required using then current refinery processingequipment. Accordingly, unleaded and low-lead gasolines were introducedduring 1970 to supplement the conventional gasolines already available. Beginning with the 1975 model year, most new car models were equippedwith catalytic exhaust treatment devices as one means of compliance withthe 1975 legal restrictions in the U.S. on automobile emissions. The needfor gasolines that would not adversely affect such catalytic devices hasled to the large scale availability and growing use of unleaded gasolines,with all late-model cars requiring unleaded gasoline."[41]. There was a further reason why alkyl lead compounds were subsequentlyreduced, and that was the growing recognition of the highly toxic nature ofthe emissions from a leaded-gasoline fuelled engine. Not only were toxiclead emissions produced, but the added toxic lead scavengers ( ethylenedibromide and ethylene dichloride ) could react with hydrocarbons to producehighly toxic organohalogen emissions such as dioxin. Even if catalysts wereremoved, or lead-tolerant catalysts discovered, alkyl lead compounds wouldremain banned because of their toxicity and toxic emissions [42]. 5.4 Why are evaporative emissions a problem? As tailpipe emissions are reduced due to improved exhaust emission controlsystems, the hydrocarbons produced by evaporation of the gasoline duringdistribution, vehicle refuelling, and from the vehicle, become more andmore significant. A recent European study found that 40% of man-madevolatile organic compounds came from vehicles [43]. Many of the problemhydrocarbons are the aromatics and olefins that have relatively high octanevalues. Any sensible strategy to reduce smog and toxic emissions will attackevaporative and tailpipe emissions. The health risks to service station workers, who are continuously exposedto refuelling emissions remain a concern [44]. Vehicles will soon berequired to trap the refuelling emissions in larger carbon canisters, aswell as the normal evaporative emissions that they already capture. Thisrecent decision went in favour of the oil companies, who were opposed by theauto companies. The automobile manufacturers felt the service stationshould trap the emissions. The activated carbon canisters adsorb organicvapours, and these are subsequently desorbed from the canister and burnt inthe engine during normal operation, once certain vehicle speeds and coolanttemperatures are reached. A few activated carbons used in older vehiclesdo not function efficiently with oxygenates. 5.5 Why control tailpipe emissions? Tailpipe emissions were responsible for the majority of pollutants in thelate 1960s after the crankcase emissions had been controlled. Ozone levelsin the Los Angeles basin reached 450-500ppb in the early 1970s, well abovethe typical background of 30-50ppb [45]. Tuning a carburetted engine can only have a marginal effect on pollutantlevels, and there still had to be some frequent, but long-term, assessmentof the state of tuning. Exhaust catalysts offered a post-engine solutionthat could ensure pollutants were converted to more benign compounds. Asengine management systems and fuel injection systems have developed, thevolatility properties of the gasoline have been tuned to minimiseevaporative emissions, and yet maintain low exhaust emissions. The design of the engine can have very significant effects on the type andquantity of pollutants, eg unburned hydrocarbons in the exhaust originatemainly from combustion chamber crevices, such as the gap between the pistonand cylinder wall, where the combustion flame can not completely use the HCs.The type and amount of unburned hydrocarbons are related to the fuelcomposition (volatility, olefins, aromatics, final boiling point), as wellas state of tune, engine condition, and age/condition of the enginelubricating oil [46]. Particulate emissions, especially the size fractionsmaller than ten micrometres, are a serious health concern. The currentmajor source is from compression ignition ( CI = diesel ) engines, and themodern SI engine system has no problem meeting regulatory requirements. The ability of reformulated gasolines to actually reduce smog has not yetbeen confirmed. The composition changes will reduce some compounds, andincrease others, making predictions of environmental consequences extremelydifficult. Planned future changes, such as the CAA 1997 Complex modelspecifications, that are based on several major ongoing government/industrygasoline and emission research programmes, are more likely to provideunambiguous environmental improvements. The rules for tailpipe emissionswill continue to become more stringent as countries try to minimise localproblems ( smog, toxins etc.) and global problems ( CO2 ). Reformulationdoes not always lower all emissions, as evidenced by the following aldehydesfrom an engine with an adaptive learning managementsystem [33].                            FTP-weighted emission rates (mg/mi)                                Gasoline      ReformulatedFormaldehyde                      4.87           8.43Acetaldehyde                      3.07           4.71 The type of exhaust catalyst and management system can have significanteffects on the emissions [33].                            FTP-weighted emission rates. (mg/mi)                         Total Aromatics          Total Carbonyls                     Gasoline  Reformulated    Gasoline  ReformulatedNoncatalyst          1292.45     1141.82        174.50     198.73Oxidation Catalyst    168.60      150.79         67.08      76.943-way Catalyst        132.70       93.37         23.93      23.07Adaptive Learning     111.69      105.96         17.31      22.35 If we take the five compounds listed as toxics under the Clean Air Act,then the beneficial effects of catalysts are obvious [33].                         FTP-weighted emission rates. (mg/mi)                      Benzene       Formaldehyde      Acrolein                    Gas   Reform    Gas   Reform     Gas   ReformNoncatalyst       156.18  138.48   73.25   85.24    11.62   13.20Oxidation Cat.     27.57   25.01   28.50   35.83     3.74    3.753-way Catalyst     19.39   15.69    7.27    7.61     1.11    0.74Adaptive Learn.    19.77   20.39    4.87    8.43     0.81    1.16                    Acetaldehyde    1,3 Butadiene                    Gas   Reform    Gas   ReformNoncatalyst        19.74   21.72    2.96    1.81Oxidation Cat.     11.15   11.76    0.02    0.333-way Catalyst      4.43    3.64    0.07    0.05Adaptive Learn.     3.07    4.71    0.00    0.14 The author reports analytical problems with the 1,3 Butadiene, and onlyNoncatalyst values are considered reliable. Emission Standards There are several bodies responsible for establishing standards, and theypromulgate test cycles, analysis procedures, and the % of new vehicles thatmust comply each year. The test cycles and procedures do change ( usuallyindicated by an anomalous increase in the numbers in the table ), and Ihave not listed the percentages of the vehicle fleet that are required tocomply. This table is only intended to convey where we have been, and wherewe are going. It does not cover any regulation in detail - readers areadvised to refer to the relevant regulations. Additional limits for otherpollutants, such as formaldehyde and particulates, are omitted. The 1994tests signal the transition from 50,000 to 75,000 mile compliance testing,and I have not listed the subsequent 50,000 mile limits [47,48]. Year                    Federal                      California                HCs    CO    NOx    Evap       HCs    CO    NOx    Evap               g/mi   g/mi  g/mi   g/test     g/mi   g/mi  g/mi   g/testBefore regs   10.6   84.0   4.1    47        10.6   84.0   4.1    47add crankcase +4.1                           +4.11966                                          6.3   51.0   6.01968           6.3   51.0   6.01970           4.1   34.0                     4.1   34.0           61971           4.1   34.0                     4.1   34.0   4.0     61972           3.0   28.0                     2.9   34.0   3.0     21973           3.0   28.0   3.0               2.9   34.0   3.0     21974           3.0   28.0   3.0               2.9   34.0   2.0     21975           1.5   15.0   3.1     2         0.90   9.0   2.0     21977           1.5   15.0   2.0     2         0.41   9.0   1.5     21980           0.41   7.0   2.0     6         0.41   9.0   1.0     21981           0.41   3.4   1.0     2         0.39   7.0   0.7     21993           0.41   3.4   1.0     2         0.25   3.4   0.4     21994 50,000    0.26   3.4   0.3     ?   TLEV  0.13   3.4   0.41994 75,000    0.31   4.2   0.6     ?1997                                    LEV   0.08   3.4   0.21997                                    ULEV  0.04   1.7   0.21998                                    ZEV   0.0    0.0   0.02004           0.13   1.8   0.16    ? It's also worth noting that exhaust catalysts also emit platinum, and thesoluble platinum salts are some of the most potent sensitizers known.Early research [49] reported the presence of 10% water-soluble platinum inthe emissions, however later work on monolithic catalysts has determined thequantities of water soluble platinum emissions are negligible [50]. Theparticle size of the emissions has also been determined, and the emissionshave been correlated with increasing vehicle speed. Increasing speed alsoincreases the exhaust gas temperature and velocity, indicating the emissionsare probably a consequence of physical attrition.            Estimated Fuel                           Median AerodynamicSpeed       Consumption         Emissions           Particle Diameterkm/h          l/100km            ng/m-3                    um60              7                  3.3                     5.1100             8                 11.9                     4.2140            10                 39.0                     5.6US Cycle-75                        6.4                     8.5 Using the estimated fuel consumption, and about 10m3 of exhaust gas perlitre of gasoline, the emissions are 2-40ng/km. These are 2-3 ordersof magnitude lower than earlier reported work on pelletised catalysts.These emissions may be controlled directly in the future. They are currentlyindirectly controlled by the cost of platinum, and the new requirement forthe catalyst to have an operational life of at least 100,000 miles. 5.6 Why do exhaust catalysts influence fuel composition? Modern adaptive learning engine management systems control the combustionstoichiometry by monitoring various ambient and engine parameters, includingexhaust gas recirculation rates, the air flow sensor, and exhaust oxygensensor outputs, This closed loop system using the oxygen sensor cancompensate for changes in fuel content and air density. The oxygen sensoris also known as the lambda sensor, because the stoichiometric mass Air/Fuelratio is known as lambda. Typical stoichiometric air/fuel ratios are [51]:-  6.4  methanol 9.0  ethanol11.7  MTBE12.1  ETBE, TAME14.6  gasoline without oxygenates The engine management system rapidly switches the stoichiometry betweenslightly rich and slightly lean, except under wide open throttle conditions- when the system runs open loop. The response of the oxygen sensor tocomposition changes is about 3 ms, and closed loop switching is typically1-3 times a second, going between 50mV ( lambda = 1.05 (Lean)) to 900mV(lambda = 0.99 ( Rich)). The catalyst oxidises about 80% of the H2, CO,and HCs, and reduces the NOx [47]. Typical reactions that occur in a modern 3-way catalyst are:-                2H2 + O2  ->  2H2O                2CO + O2  ->  2CO2    CxHy + (x + (y/4))O2  ->  xCO2 + (y/2)H2O               2CO + 2NO  ->  N2 + 2CO2   CxHy + 2(x + (y/4))NO  ->  (x + (y/4))N2 + (y/2)H2O + xCO2               2H2 + 2NO  ->  N2 + 2H2O                CO + H20  ->  CO2 + H2             CxHy + xH2O  ->  xCO + (x + (y/2))H2 The use of exhaust catalysts have resulted in reaction pathways that canaccidentally be responsible for increased pollution. An example is theCARB-mandated reduction of fuel sulfur. A change from 450ppm to 50ppm, whichwill reduce HC & CO emissions by 20%, may increase formaldehyde by 45% [19]. The requirement that the exhaust catalysts must now endure for 10 years or100,000 miles will also encourage automakers to push for lower levels ofknown catalyst "poisons" such as sulfur and phosphorus in both the gasolineand lubricant. Modern catalysts are unable to reduce the relatively highlevels of NOx that are produced during lean operation down to approvedlevels, thus preventing the application of lean-burn engine technology.Recently Mazda has announced they have developed a "lean burn" catalyst,which may enable automakers to move the fuel combustion towards the leanside, and different gasoline properties may be required to optimise thecombustion and reduce pollution. Mazda claim that fuel efficiency isimproved by 5-8% while meeting all emission regulations [52] . Catalysts also inhibit the selection of gasoline octane-improving andcleanliness additives ( such as MMT and phosphorus-containing additives )that may result in refractory compounds known to physically coat thecatalyst and increase pollution. 5.7 Why are "cold start" emissions so important? The catalyst requires heat to reach the temperature ( >300-350C ) where itfunctions most efficiently, and the delay until it reaches operatingtemperature can produce more hydrocarbons than would be produced duringthe remainder of many typical urban short trips. It has been estimated that70-80% of the non-methane HCs that escape conversion by the catalystsare emitted during the first two minutes after a cold start. As exhaustemissions have been reduced, the significance of the evaporative emissionsincreases. Several engineering techniques are being developed, including theFord Exhaust Gas Igniter ( uses a flame to heat the catalyst - lots ofpotential problems ), zeolite hydrocarbon traps, and relocation of thecatalyst closer to the engine [47]. Reduced gasoline volatility and composition changes, along with cleanlinessadditives and engine management systems, can help minimise cold startemissions, but currently the most effective technique appears to be rapid,deliberate heating of the catalyst, and the new generation of low thermalinertia "fast light-up" catalysts reduce the problem, but further researchis necessary [53]. As the evaporative emissions are also starting to be reduced, the emphasishas shifted to the refuelling emissions. These will be mainly controlledon the vehicle, and larger canisters may be used to trap the vapours emittedduring refuelling. 5.8 When will the emissions be "clean enough"? The California ZEV regulations effectively preclude IC vehicles, becausethey stipulate zero emissions. However, the concept of regulatory forcingof alternative vehicle propulsion technology may have to be modified toinclude hybrid or fuel-cell vehicles, as the major failing of EVs remainsthe lack of a cheap, light, safe, and easily-rechargeable electricalstorage device [54,55]. There are several major projects intending tofurther reduce emissions from automobiles, mainly focusing on vehicle massand engine fuel efficiency, but gasoline specifications and alternativefuels are also being investigated. It may be that changes to IC engines andgasolines will enable the IC engine to continue well into the 21st centuryas the prime motive force for personal transportation. 5.9 Why are only some gasoline compounds restricted? The less volatile hydrocarbons in gasoline are not released in significantquantities during normal use, and the more volatile alkanes are considerablyless toxic than many other chemicals encountered daily. The newer gasolineadditives also have potentially undesirable properties before they are evencombusted. Most hydrocarbons are very insoluble in water, with the loweraromatics being the most soluble, however the addition of oxygen tohydrocarbons significantly increases the mutual solubility with water.                       Compound in Water            Water in Compound                      % mass/mass @  C             % mass/mass @  C normal decane            0.0000052  25               0.0072      25iso-octane               0.00024    25               0.0055      20normal hexane            0.00125    25               0.0111      20cyclohexane              0.0055     25               0.010       201-hexene                 0.00697    25               0.0477      30toluene                  0.0515     25               0.0334      25benzene                  0.1791     25               0.0635      25methanol                complete    25              complete     25ethanol                 complete    25              complete     25MTBE                     4.8        20               1.4         20TAME                      -                          0.6         20 The concentrations and ratios of benzene, toluene, ethyl benzene, and xylenes( BTEX ) in water are often used to monitor groundwater contamination fromgasoline storage tanks or pipelines. The oxygenates and other new additivesmay increase the extent of water and soil pollution by acting as co-solventsfor HCs. Various government bodies ( EPA, OSHA, NIOSH ) are charged with ensuringpeople are not exposed to unacceptable chemical hazards, and maintainongoing research into the toxicity of liquid gasoline contact, water and soilpollution, evaporative emissions, and tailpipe emissions [56]. As toxicityis found, the quantities in gasoline of the specific chemical ( benzene ),or family of chemicals ( alkyl leads, aromatics, olefins ) are regulated. The recent dramatic changes caused by the need to reduce alkyl leads,halogens, olefins, aromatics has resulted in whole new families of compounds( ethers, alcohols ) being introduced into fuels without prior detailedtoxicity studies being completed. If adverse results appear, these compoundsare also likely to be regulated to protect people and the environment. Also, as the chemistry of emissions is unravelled, the chemical precursorsto toxic tailpipe emissions ( such as higher aromatics that produce benzeneemissions ) are also controlled, even if they are not toxic. 5.10 What does "renewable" fuel/oxygenate mean? The general definition of "renewable" is that the carbon originates fromrecent biomass, and thus does not contribute to the increased CO2 emissions.A truly "long-term" view could claim that fossil fuels are "renewable" on a100 million year timescale :-). There is currently a major battle betweenthe ethanol/ETBE lobby ( agricultural, corn growing ), and the methanol/MTBElobby ( oil company, petrochemical ) over an EPA mandate demanding that aspecific percentage of the oxygenates in gasoline are produced from"renewable" sources [57]. Unfortunately, "renewable" ethanol is not cost competitive when crude oilis $18/bbl, so a federal subsidy ( $0.54/US Gallon ) and additional statesubsidies ( 11 states - from $0.08(Michigan) to $0.66(Tenn.)/US Gal.) areprovided. A judgement on the use of "renewable" oxygenates is expected inearly 1995. 5.11 Will oxygenated gasoline damage my vehicle? The following comments assume that your vehicle was designed to operate onunleaded, if not, then damage like valve seat recession may also occur.Damage should not occur if the gasoline is correctly formulated, and youselect the appropriate octane, but oxygenated gasoline will hurt your pocket.In the first year of mandated oxygenates, it appears some refiners did notcarefully formulate their oxygenated gasoline, and driveability and emissionsproblems occurred. Most reputable brands are now carefully formulated.Some older activated carbon canisters may not function efficiently withoxygenated gasolines, but this is a function of the type of carbon used.How your vehicle responds to oxygenated gasoline depends on the enginemanagement system and state of tune. A modern system will automaticallycompensate for all of the currently-permitted oxygenate levels, thus yourfuel consumption will increase. Older, poorly-maintained, engines mayrequire a tune up to maintain acceptable driveability. Be prepared to try several different brands of reformulated gasolines toidentify the most suitable brand for your vehicle, and be prepared to changeagain with the seasons. This is because the refiners can choose theoxygenate they use to meet the regulations, and may choose to set some fuelproperties, such as volatility, differently to their competitors. Most stories of corrosion etc, are derived from anhydrous methanol corrosionof light metals (aluminum, magnesium), however the addition of either 0.5%water to pure methanol, or corrosion inhibitors to methanol/gasoline blendswill prevent this. If you observe corrosion, talk to your gasoline supplier.Oxygenated fuels may either swell or shrink some elastomers on older cars,depending on the aromatic and olefin content of the fuels. Cars later than1990 should not experience compatibility problems, and cars later than 1994should not experience driveability problems, but they will experienceincreased fuel consumption, depending on the state of tune and enginemanagement system. 5.12 What does "reactivity" of emissions mean? The traditional method of exhaust regulations was to specify the actual HC,CO, NOx, and particulate contents. With the introduction of oxygenates andreformulated gasolines, the volatile organic carbon (VOC) species in theexhaust also changed. The "reactivity" refers to the ozone-forming potentialof the VOC emissions when they react with NOx, and is being introduced as aregulatory means of ensuring that automobile emissions do actually reducesmog formation. The ozone-forming potential of chemicals is defined as thenumber of molecules of ozone formed per VOC carbon atom, and this is calledthe Incremental Reactivity. Typical values ( big is bad :-) ) are [45]: Maximum Incremental Reactivities as mg Ozone / mg VOC                  carbon monoxide           0.054alkanes           methane                   0.0148                  ethane                    0.25                  propane                   0.48                  n-butane                  1.02olefins           ethylene                  7.29                  propylene                 9.40                  1,3 butadiene            10.89aromatics         benzene                   0.42                  toluene                   2.73                  meta-xylene               8.15                  1,3,5-trimethyl benzene  10.12oxygenates        methanol                  0.56                  ethanol                   1.34                  MTBE                      0.62                  ETBE                      1.98 5.13 What are "carbonyl" compounds? Carbonyls are produced in large amounts under lean operating conditions,especially when oxygenated fuels are used. Most carbonyls are toxic, and thecarboxylic acids can corrode metals. The emission of carbonyls can becontrolled by combustion stoichiometry and exhaust catalysts. Typical carbonyls are:- aldehydes ( containing -CHO ), formaldehyde (HCHO) - which is formed in large amounts during lean combustion of methanol [58]. acetaldehyde (CH2CHO) - which is formed during ethanol combustion. acrolein (CH2=CHCHO) - a very potent irritant. ketones ( containing C=0 ), acetone (CH3COCH3) carboxylic acids ( containing -COOH ), formic acid (HCOOH) - formed during lean methanol combustion. acetic acid (CH3COOH). 5.14 What are "gross polluters"? It has always been known that the EPA emissions tests do not reflect realworld conditions. There have been several attempts to identify vehicles onthe road that do not comply with emissions standards. Recent remote sensingsurveys have demonstrated that the highest 10% of CO emitters produce over50% of the pollution, and the same ratio applies for the HC emitters - whichmay not be the same vehicles [59,60,61]. 20% of the CO emitters areresponsible for 80% of the CO emissions, consequently modifying gasolinecomposition is only one aspect of pollution reduction. The new additives canhelp maintain engine condition, but they can not compensate for out-of-tune,worn, or tampered-with engines. The most famous of these remote sensing systems is the FEAT ( Fuel EfficiencyAutomobile Test ) team from the University of Denver [62]. This team isprobably the world leader in remote sensing of auto emissions to identifygrossly polluting vehicles. The system measures CO/CO2 ratio, and theHC/CO2 ratio in the exhaust of vehicles passing through an infra-red lightbeam crossing the road 25cm above the surface. The system also includes avideo system that records the licence plate, date, time, calculated exhaustCO, CO2, and HC. The system is effective for traffic lanes up to 18 metreswide, however rain, snow, and water spray can cause scattering of the beam.Reference signals monitor such effects and, if possible, compensate. Thesystem has been comprehensively validated, including using vehicles withon-board emissions monitoring instruments. They can monitor up to 1000 vehicles an hour and, as an example,they wereinvited to Provo, Utah to monitor vehicles, and gross polluters would beoffered free repairs [63]. They monitored over 10,000 vehicles and mailed114 letters to owners of vehicles newer than 1965 that had demonstrated highCO levels. They received 52 responses and repairs started in Dec 1991, andcontinued to Mar 1992. They offered to purchase two vehicles at blue bookprice. They were declined, and so attempted to modify those vehicles, eventhough their condition did not justify the expense.  The entire monitored fleet at Provo (Utah) during Winter 1991/1992 Model year               Grams CO/gallon            Number of                    (Median value) (mean value)      Vehicles    92                    40             80              247   91                    55                            1222   90                    75                            1467   89                    80                            1512   88                    85                            1651   87                    90                            1439   86                   100            300             1563   85                   120                            1575   84                   125                            1206   83                   145                             719   82                   170                             639   81                   230                             612   80                   220            500              551   79                   350                             667   78                   420                             584   77                   430                             430   76                   770                             317   75                   760            950              163   Pre 75               920           1060              878 As observed elsewhere, over half the CO was emitted by about 10% of thevehicles. If the 47 worst polluting vehicles were removed, that achievesmore than removing the 2,500 lowest emitting vehicles from the total testedfleet. Surveys of vehicle populations have demonstrated that emissions systems hadbeen tampered with on over 40% of the gross polluters, and an additional 20%had defective emission control equipment [64]. No matter what changes aremade to gasoline, if owners "tune" their engines for power, then the majorityof such "tuned" vehicle will become gross polluters. Professional repairs togross polluters usually improves fuel consumption, resulting in a low cost toowners ( $32/pa/Ton CO year ). The removal of CO in the Provo example abovewas costed at $200/Ton CO, compared to Inspection and Maintenance programs($780/Ton CO ), and oxygenates ( $1034-$1264/Ton CO in Colorado 1991-2 ), andUNOCALs vehicle scrapping programme ( $1025/Ton of all pollutants ). Thus, identifying and repairing or removing gross polluters can be far morecost-effective than playing around with reformulated gasolines andoxygenates. Subject: 6. What do Fuel Octane ratings really indicate? 6.1 Who invented Octane Ratings? Since 1912 the spark ignition internal combustion engine's compression ratiohad been constrained by the unwanted "knock" that could rapidly destroyengines. "Knocking" is a very good description of the sound heard from anengine using fuel of too low octane. The engineers had blamed the "knock"on the battery ignition system that was added to cars along with theelectric self-starter. The engine developers knew that they could improvepower and efficiency if knock could be overcome. Kettering assigned Thomas Midgley, Jr. to the task of finding the exactcause of knock [16]. They used a Dobbie-McInnes manograph to demonstratethat the knock did not arise from preignition, as was commonly supposed, butarose from a violent pressure rise _after_ ignition. The manograph was notsuitable for further research, so Midgley and Boyd developed a high-speedcamera to see what was happening. They also developed a "bouncing pin"indicator that measured the amount of knock [7]. Ricardo had developed analternative concept of HUCF ( Highest Useful Compression Ratio ) using avariable-compression engine. His numbers were not absolute, as there weremany variables, such as ignition timing, cleanliness, spark plug position,engine temperature. etc. In 1926 Graham Edgar suggested using two hydrocarbons that could be producedin sufficient purity and quantity [9]. These were "normal heptane", thatwas already obtainable in sufficient purity from the distillation of Jeffreypine oil, and " an octane, named 2,4,4-trimethyl pentane " that he firstsynthesized. Today we call it " iso-octane " or 2,2,4-trimethyl pentane. Theoctane had a high anti-knock value, and he suggested using the ratio of thetwo as a reference fuel number. He demonstrated that all the commercially-available gasolines could be bracketed between 60:40 and 40:60 parts byvolume heptane:iso-octane. The reason for using normal heptane and iso-octane was because they bothhave similar volatility properties, specifically boiling point, thus thevarying ratios 0:100 to 100:0 should not exhibit large differences involatility that could affect the rating test.                                                            Heat of               Melting Point  Boiling Point  Density    Vaporisation                     C              C          g/ml         MJ/kg normal heptane    -90.7           98.4       0.684          0.365 @ 25Ciso octane       -107.45          99.3       0.6919         0.308 @ 25C Having decided on standard reference fuels, a whole range of engines andtest conditions appeared, but today the most common are the Research OctaneNumber ( RON ), and the Motor Octane Number ( MON ). 6.2 Why do we need Octane Ratings? To obtain the maximum energy from the gasoline, the compressed fuel/airmixture inside the combustion chamber needs to burn evenly, propagating outfrom the spark plug until all the fuel is consumed. This would deliver anoptimum power stroke. In real life, a series of pre-flame reactions willoccur in the unburnt "end gases" in the combustion chamber before the flamefront arrives. If these reactions form molecules or species that canautoignite before the flame front arrives, knock will occur [13,14]. Simply put, the octane rating of the fuel reflects the ability of theunburnt end gases to resist spontaneous autoignition under the engine testconditions used. If autoignition occurs, it results in an extremely rapidpressure rise, as both the desired spark-initiated flame front, and theundesired autoignited end gas flames are expanding. The combined pressurepeak arrives slightly ahead of the normal operating pressure peak, leadingto a loss of power and eventual overheating. The end gas pressure waves aresuperimposed on the main pressure wave, leading to a sawtooth pattern ofpressure oscillations that create the "knocking" sound. The combination of intense pressure waves and overheating can induce pistonfailure in a few minutes. Knock and preignition are both favoured by hightemperatures, so one may lead to the other. Under high-speed conditionsknock can lead to preignition, which then accelerates engine destruction[17]. 6.3 What fuel property does the Octane Rating measure? The fuel property the octane ratings measure is the ability of the unburntend gases to spontaneously ignite under the specified test conditions.Within the chemical structure of the fuel is the ability to withstandpre-flame conditions without decomposing into species that will autoignitebefore the flame-front arrives. Different reaction mechanisms, occurring atvarious stages of the pre-flame compression stroke, are responsible for theundesirable, easily-autoignitable, end gases. During the oxidation of a hydrocarbon fuel, the hydrogen atoms are removedone at a time from the molecule by reactions with small radical species(such as OH and HO2), and O and H atoms. The strength of carbon-hydrogenbonds depends on what the carbon is connected to. Straight chain HCs such asnormal heptane have secondary C-H bonds that are significantly weaker thanthe primary C-H bonds present in branched chain HCs like iso-octane [13,14]. The octane rating of hydrocarbons is determined by the structure of themolecule, with long, straight hydrocarbon chains producing large amounts ofeasily-autoignitable pre-flame decomposition species, while branched andaromatic hydrocarbons are more resistant. This also explains why the octaneratings of paraffins consistently decrease with carbon number. In real life,the unburnt "end gases" ahead of the flame front encounter temperatures upto about 700C due to piston motion and radiant and conductive heating, andcommence a series of pre-flame reactions. These reactions occur at differentthermal stages, with the initial stage ( below 400C ) commencing with theaddition of molecular oxygen to alkyl radicals, followed by the internaltransfer of hydrogen atoms within the new radical to form an unsaturated,oxygen-containing species. These new species are susceptible to chainbranching involving the HO2 radical during the intermediate temperaturestage (400-600C), mainly through the production of OH radicals. Above 600C,the most important reaction that produces chain branching is the reaction ofone hydrogen atom radical with molecular oxygen to form O and OH radicals. The addition of additives such as alkyl lead and oxygenates cansignificantly affect the pre-flame reaction pathways. Anti-knock additiveswork by interfering at different points in the pre-flame reactions, withthe oxygenates retarding undesirable low temperature reactions, and thealkyl lead compounds react in the intermediate temperature region todeactivate the major undesirable chain branching sequence [13,14]. The antiknock ability is related to the "autoignition temperature" of thehydrocarbons. Antiknock ability is _not_ substantially related to:- The energy content of fuel, this should be obvious, as oxygenates have lower energy contents, but high octanes. The flame speed of the conventionally ignited mixture, this should be evident from the similarities of the two reference hydrocarbons. Although flame speed does play a minor part, there are many other factors that are far more important. ( such as compression ratio, stoichiometry, combustion chamber shape, chemical structure of the fuel, presence of antiknock additives, number and position of spark plugs, turbulence etc.) Flame speed does not correlate with octane. 6.4 Why are two ratings used to obtain the pump rating? The correct name for the (RON+MON)/2 formula is the "antiknock index",and it remains the most important quality criteria for motorists [25]. The initial octane method developed in the 1920s was the Motor Octane methodand, over several decades, a large number of octane test methods appeared.These were variations to either the engine design, or the specified operatingconditions [65]. During the 1950-1960s attempts were made to internationallystandardise and reduce the number of Octane Rating test procedures. During the late 1930s - mid 1960s, the Research method became the importantrating because it more closely represented the octane requirements of themotorist using the fuels/vehicles/roads then available. In the late 1960sGerman automakers discovered their engines were destroying themselves onlong Autobahn runs, even though the Research Octane was within specification.They discovered that either the MON or the Sensitivity ( the numericaldifference between the RON and MON numbers ) also had to be specified. Todayit is accepted that no one octane rating covers all use. In fact, during1994, there have been increasing concerns in Europe about the highSensitivity of some commercially-available unleaded fuels. The design of the engine and car significantly affect the fuel octanerequirement for both RON and MON. In the 1930s, most vehicles would run onthe specified Research Octane fuel, almost regardless of the Motor Octane,whereas most 1990s engines have a 'severity" of one, which means the engineis unlikely to knock if a changes of one RON is matched by an equal andopposite change of MON [19]. 6.5 What does the Motor Octane rating measure? The conditions of the Motor method represent severe, sustained high speed,high load driving. For most hydrocarbon fuels, including those with eitherlead or oxygenates, the motor octane number (MON) will be lower than theresearch octane number (RON). Test Engine conditions                Motor OctaneTest Method                         ASTM D2700-92 [66]Engine                       Cooperative Fuels Research ( CFR )Engine RPM                               900 RPMIntake air temperature                    38 CIntake air humidity           3.56 - 7.12 g H2O / kg dry airIntake mixture temperature               149 CCoolant temperature                      100 COil Temperature                           57 CIgnition Advance - variable     Varies with compression ratio                                 ( eg 14 - 26 degrees BTDC )Carburettor Venturi                       14.3 mm 6.6 What does the Research Octane rating measure? The Research method settings represent typical mild driving, withoutconsistent heavy loads on the engine. Test Engine conditions               Research OctaneTest Method                         ASTM D2699-92 [67]Engine                       Cooperative Fuels Research ( CFR )Engine RPM                               600 RPMIntake air temperature       Varies with barometric pressure                           ( eg 88kPa = 19.4C, 101.6kPa = 52.2C )Intake air humidity           3.56 - 7.12 g H2O / kg dry airIntake mixture temperature            Not specifiedCoolant temperature                      100 COil Temperature                           57 CIgnition Advance - fixed            13 degrees BTDCCarburettor Venturi           Set according to engine altitude                           ( eg 0-500m=14.3mm, 500-1000m=15.1mm ) 6.7 Why is the difference called "sensitivity"? RON - MON = Sensitivity. Because the two test methods use different testconditions, especially the intake mixture temperatures and engine speeds,then a fuel that is sensitive to changes in operating conditions will havea larger difference between the two rating methods. Modern fuels typicallyhave sensitivities around 10. The US 87 (RON+MON/2) unleaded gasoline isrequired to have a 82+ MON, thus preventing very high sensitivity fuels [25]. 6.8 What sort of engine is used to rate fuels? Automotive octane ratings are determined in a special single-cylinder enginewith a variable compression ratio ( CR 4:1 to 18:1 ) known as a CooperativeFuels Research ( CFR ) engine. The cylinder bore is 82.5mm, the stroke is114.3mm, giving a displacement of 612 cm3. The piston has four compressionrings, and one oil control ring. The intake valve is shrouded. The head andcylinder are one piece, and can be moved up and down to obtain the desiredcompression ratio. The engines have a special four-bowl carburettor thatcan adjust individual bowl air/fuel ratios. This facilitates rapid switchingbetween reference fuels and samples. A magnetorestrictive detonation sensorin the combustion chamber measures the rapid changes in combustion chamberpressure caused by knock, and the amplified signal is measured on a"knockmeter" with a 0-100 scale [66,67]. A complete Octane Rating enginesystem costs about $200,000 with all the services installed. Only onecompany manufactures these engines, the Waukesha Engine Division of DresserIndustries, Waukesha. WI 53186. 6.9 How is the Octane rating determined? To rate a fuel, the engine is set to an appropriate compression ratio thatwill produce a knock of about 50 on the knockmeter for the sample when theair/fuel ratio is adjusted on the carburettor bowl to obtain maximum knock.Normal heptane and iso-octane are known as primary reference fuels. Twoblends of these are made, one that is one octane number above the expectedrating, and another that is one octane number below the expected rating.These are placed in different bowls, and are also rated with each air/fuelratio being adjusted for maximum knock. The higher octane reference fuelshould produce a reading around 30-40, and the lower reference fuel shouldproduce a reading of 60-70. The sample is again tested, and if it does notfit between the reference fuels, further reference fuels are prepared, andthe engine readjusted to obtain the required knock. The actual fuel ratingis interpolated from the knockmeter readings [66,67]. 6.10 What is the Octane Distribution of the fuel? The combination of vehicle and engine can result in specific requirementsfor octane that depend on the fuel. If the octane is distributed differentlythroughout the boiling range of a fuel, then engines can knock on one brandof 87 (RON+MON/2), but not on another brand. This "octane distribution" isespecially important when sudden changes in load occur, such as high load,full throttle, acceleration. The fuel can segregate in the manifold, withthe very volatile fraction reaching the combustion chamber first and, ifthat fraction is deficient in octane, then knock will occur until the lessvolatile, higher octane fractions arrive [17]. Some fuel specifications include delta RONs, to ensure octane distributionthroughout the fuel boiling range was consistent. Octane distribution wasseldom a problem with the alkyl lead compounds, as the tetra methyl leadand tetra ethyl lead octane volatility profiles were well characterised, butit can be a major problem for the new, reformulated, low aromatic gasolines,as MTBE boils at 55C, whereas ethanol boils at 78C. Drivers have discoveredthat an 87 (RON+MON/2) from one brand has to be substituted with an 89(RON+MON/2) of another, and that is because of the combination of theirdriving style, engine design, vehicle mass, fuel octane distribution, fuelvolatility, and the octane-enhancers used. 6.11 What is a "delta Research Octane number"? To obtain an indication of behaviour of a gasoline during any manifoldsegregation, an octane rating procedure called the Distribution OctaneNumber was used. The rating engine had a special manifold that allowedthe heavier fractions to be separated before they reached the combustionchamber [17]. That method has been replaced by the "delta" RON procedure. The fuel is carefully distilled to obtain a distillate fraction that boilsto the specified temperature, which is usually 100C. Both the parent fueland the distillate fraction are rated on the octane engine using theResearch Octane method [68]. The difference between these is the deltaRON(100C), usually just called the delta RON. 6.12 How do other fuel properties affect octane? Several other properties affect knock. The most significant determinant ofoctane is the chemical structure of the hydrocarbons and their response tothe addition of octane enhancing additives. Other factors include:- Front End Volatility - Paraffins are the major component in gasoline, and the octane number decreases with increasing chain length or ring size, but increases with chain branching. Overall, the effect is a significant reduction in octane if front end volatility is lost, as can happen with improper or long term storage. Fuel economy on short trips can be improved by using a more volatile fuel, at the risk of carburettor icing and increased evaporative emissions. Final Boiling Point.- Decreases in the final boiling point increase fuel octane. Aviation gasolines have much lower final boiling points than automotive gasolines. Note that final boiling points are being reduced because the higher boiling fractions are responsible for disproportionate quantities of pollutants and toxins. Preignition tendency - both knock and preignition can induce each other. 6.13 Can higher octane fuels give me more power? Not if you are already using the proper octane fuel. The engine will bealready operating at optimum settings, and a higher octane should have noeffect on the management system. Your driveability and fuel economy willremain the same. The higher octane fuel costs more, so you are just throwingmoney away. If you are already using a fuel with an octane rating slightlybelow the optimum, then using a higher octane fuel will cause the enginemanagement system to move to the optimum settings, possibly resulting inboth increased power and improved fuel economy. You may be able to changeoctanes between seasons ( reduce octane in winter ) to obtain the mostcost-effective fuel without loss of driveability. Once you have identified the fuel that keeps the engine at optimum settings,there is no advantage in moving to an even higher octane fuel. Themanufacturer's recommendation is conservative, so you may be able tocarefully reduce the fuel octane. The penalty for getting it badly wrong,and not realising that you have, could be expensive engine damage. 6.14 Does low octane fuel increase engine wear? Not if you are meeting the octane requirement of the engine. If you are notmeeting the octane requirement, the engine will rapidly suffer major damagedue to knock. You must not use fuels that produce sustained audible knock,engine damage will occur. If the octane is just sufficient, the enginemanagement system will move settings to a less optimal position, and theonly major penalty will be increased costs due to poor fuel economy.Whenever possible, engines should be operated at the optimum position forlong-term reliability. Engine wear is mainly related to design,manufacturing, maintenance and lubrication factors. Once the octane andrun-on requirements of the engine are satisfied, increased octane will haveno beneficial effect on the engine. The quality of gasoline, and theadditive package used, would be more likely to affect the rate of enginewear, rather than the octane rating. 6.15 Can I mix different octane fuel grades? Yes, however attempts to blend in your fuel tank should be carefullyplanned. You should not allow the tank to become empty, and then add 50% oflower octane, followed by 50% of higher octane. The fuels may not completelymix immediately, especially if there is a density difference. You may get aslug of low octane that causes severe knock. You should refill when yourtank is half full. In general the octane response will be linear for mosthydrocarbon and oxygenated fuels eg 50:50 of 87 and 91 will give 89. Attempts to mix leaded high octane to unleaded high octane to obtain higheroctane are useless. The lead response of the unleaded fuel does not overcomethe dilution effect, thus 50:50 of 96 leaded and 91 unleaded will give 94.Some blends of oxygenated fuels with ordinary gasoline can result inundesirable increases in volatility due to volatile azeotropes, and thatsome oxygenates can have negative lead responses. Also note that the octanerequirement of some engines is determined by the need to avoid run-on, notto avoid knock. 6.16 What happens if I use the wrong octane fuel? If you use a fuel with an octane rating below the requirement of the engine,the management system may move the engine settings into an area of lessefficient combustion, resulting in reduced power and reduced fuel economy.You will be losing both money and driveability. If you use a fuel with anoctane rating higher than what the engine can use, you are just wastingmoney by paying for octane that you can not utilise. Forget the storiesabout higher octanes having superior additive packages - they do not. Ifyour vehicle does not have a knock sensor, then using an octane significantlybelow the requirement means that the little men with hammers will gleefullypummel your engine to pieces. You should initially be guided by the vehicle manufacturer's recommendations,however you can experiment, as the variations in vehicle tolerances canmean that Octane Number Requirement for a given vehicle model can rangeover 6 Octane Numbers. Caution should be used, and remember to compensateif the conditions change, such as carrying more people or driving indifferent ambient conditions. You can often reduce the octane of the fuelyou use in winter because the temperature decrease and possible humiditychanges may significantly reduce the octane requirement of the engine. Use the octane that provides cost-effective driveability and performance,using anything more is waste of money, and anything less could result inan unscheduled, expensive visit to your mechanic. 6.17 Can I tune the engine to use another octane fuel? In general, modern engine management systems will compensate for fuel octane,and once you have satisfied the optimum octane requirement, you are at theoptimum overall performance area of the engine map. Tuning changes to obtainmore power will probably adversely affect both fuel economy and emissions.Unless you have access to good diagnostic equipment that can ensureregulatory limits are complied with, it is likely that adjustments may beregarded as illegal tampering by your local regulation enforcers. If you areskilled, you will be able to legally wring slightly more performance fromyour engine by using a dynamometer in conjunction with engine and exhaust gasanalyzers and a well-designed, retrofitted, performance engine managementchip. 6.18 How can I increase the fuel octane? Not simply, you can purchase additives, however these are not cost-effectiveand a survey in 1989 showed the cost of increasing the octane rating of oneUS gallon by one unit ranged from 10 cents ( methanol ), 50 cents (MMT),$1.00 ( TEL ), to $3.25 ( xylenes ) [69]. It is preferable to purchase ahigher octane fuel such as racing fuel, aviation gasolines, or methanol.Sadly, the price of chemical grade methanol has almost doubled during 1994.If you plan to use alcohol blends, ensure your fuel handling system iscompatible, and that you only use dry gasoline by filling up early in themorning when the storage tanks are cool. Also ensure that the service stationstorage tank has not been refilled recently. Retailers are supposed to waitseveral hours before bringing a refilled tank online, to allow suspendedundissolved water to settle out, but they do not always wait the full period. 6.19 Are aviation gasoline octane numbers comparable? Aviation gasolines were all highly leaded and graded using two numbers, withcommon grades being 80/87, 100/130, and 115/145 [70]. The first number is theAviation rating ( aka Lean Mixture rating ), and the second number is theSupercharge rating ( aka Rich Mixture rating ). In the 1970s a new grade,100LL ( low lead = 0.53mlTEL/L instead of 1.06mlTEL/L) was introduced toreplace the 80/87 and 100/130. Soon after the introduction, there was aspate of plug fouling, and high cylinder head temperatures resulting incracked cylinder heads [71]. The old 80/87 grade was reintroduced on alimited scale. The Aviation rating is determined using the automotive MotorOctane test procedure, and then corrected to an Aviation number using atable in the method - it's usually only 1 - 2 Octane units different to theMotor value up to 100, but varies significant above that eg 110MON = 128AN. The second Avgas number is the Rich Mixture method Performance Number ( PN- they are not commonly called octane numbers when they are above 100 ), andis determined on a supercharged version of the CFR engine which has a fixedcompression ratio. The method determines the dependence of the highestpermissible power ( in terms of indicated mean effective pressure ) onmixture strength and boost for a specific light knocking setting. ThePerformance Number indicates the maximum knock-free power obtainable from afuel compared to iso-octane = 100. Thus, a PN = 150 indicates that an enginedesigned to utilise the fuel can obtain 150% of the knock-limited power ofiso-octane at the same mixture ratio. This is an arbitrary scale based oniso-octane + varying amounts of TEL, derived from a survey of enginesperformed decades ago. Aviation gasoline PNs are rated using variations ofmixture strength to obtain the maximum knock-limited power in a superchargedengine. This can be extended to provide mixture response curves which definethe maximum boost ( rich - about 11:1 stoichiometry ) and minimum boost( weak about 16:1 stoichiometry ) before knock [71]. The 115/145 grade is being phased out, but even the 100LL has more octanethan any automotive gasoline. Subject: 7. What parameters determine octane requirement?7.1 What is the effect of Compression ratio? Most people know that an increase in Compression Ratio will require anincrease in fuel octane for the same engine design. Increasing thecompression ratio increases the theoretical thermodynamic efficiency of anengine according to the standard equation Efficiency = 1 - (1/compression ratio)^gamma-1 where gamma = ratio of specific heats at constant pressure and constantvolume of the working fluid ( for most purposes air is the working fluid,and is treated as an ideal gas ). There are indications that thermalefficiency reaches a maximum at a compression ratio of about 17:1 [15]. The efficiency gains are best when the engine is at incipient knock, that'swhy knock sensors ( actually vibration sensors ) are used. Low compressionratio engines are less efficient because they can not deliver as much of theideal combustion power to the flywheel. For a typical carburetted engine,without engine management [17,24]:-    Compression       Octane Number    Brake Thermal Efficiency     Ratio            Requirement         ( Full Throttle )      5:1                 72                      -      6:1                 81                     25 %      7:1                 87                     28 %      8:1                 92                     30 %      9:1                 96                     32 %     10:1                100                     33 %     11:1                104                     34 %     12:1                108                     35 % Modern engines have improved significantly on this, and the changing fuelspecifications and engine design should see more improvements, butsignificant gains may have to await improved engine materials and fuels. 7.2 What is the effect of changing the air/fuel ratio? Traditionally, the greatest tendency to knock was near 13.5:1 air/fuelratio, but was very engine specific. Modern engines, with engine managementsystems, now have their maximum octane requirement near to 14.5:1. For agiven engine using gasoline, the relationship between thermal efficiency,air/fuel ratio, and power is complex. Stoichiometric combustion ( Air/FuelRatio = 14.7:1 for a typical non-oxygenated gasoline ) is neither maximumpower - which occurs around A/F 12-13:1 (Rich), nor maximum thermalefficiency - which occurs around A/F 16-18:1 (Lean). The air-fuel ratio iscontrolled at part throttle by a closed loop system using the oxygen sensorin the exhaust. Conventionally, enrichment for maximum power air/fuel ratiois used during full throttle operation to reduce knocking while providingbetter driveability [24]. If the mixture is weakened, the flame speed isreduced, consequently less heat is converted to mechanical energy, leavingheat in the cylinder walls and head, potentially inducing knock. It ispossible to weaken the mixture sufficiently that the flame is still presentwhen the inlet valve opens again, resulting in backfiring. 7.3 What is the effect of changing the ignition timing The tendency to knock increases as spark advance is increased, eg 2 degreesBTDC requires 91 octane, whereas 14 degrees BTDC requires 96 octane.If you advance the spark, the flame front starts earlier, and the end gasesstart forming earlier in the cycle, providing more time for the autoignitingspecies to form before the piston reaches the optimum position for powerdelivery, as determined by the normal flame front propagation. It becomes arace between the flame front and decomposition of the increasingly-squashedend gases. High octane fuels produce end gases that take longer toautoignite, so the good flame front reaches and consumes them properly. The ignition advance map is partly determined by the fuel the engine isintended to use. The timing of the spark is advanced sufficiently to ensurethat the fuel/air mixture burns in such a way that maximum pressure of theburning charge is about 15-20 degree after TDC. Knock will occur beforethis point, usually in the late compression/early power stroke period.The engine management system uses ignition timing as one of the majorvariables that is adjusted if knock is detected. If very low octane fuelsare used ( several octane numbers below the vehicle's requirement at optimalsettings ), both performance and fuel economy will decrease. The actual Octane Number Requirement depends on the engine design, but forsome 1978 vehicles using standard fuels, the following (R+M)/2 OctaneRequirements were measured. "Standard" is the recommended ignition timingfor the engine, probably a few degrees before Top Dead Centre [24].                           Basic Ignition TimingVehicle   Retarded 5 degrees    Standard     Advanced 5 degrees  A              88                91               93  B              86                90.5             94.5  C              85.5              88               90  D              84                87.5             91  E              82.5              87               90 The actual ignition timing to achieve the maximum pressure from normalcombustion of gasoline will depend mainly on the speed of the engine and theflame propagation rates in the engine. Knock increases the rate of thepressure rise, thus superimposing additional pressure on the normalcombustion pressure rise. The knock actually rapidly resonates around thechamber, creating a series of abnormal sharp spikes on the pressure diagram.The normal flame speed is fairly consistent for most gasoline HCs, regardlessof octane rating, but the flame speed is affected by stoichiometry. Note thatthe flame speeds in this FAQ are not the actual engine flame speeds. A 12:1CR gasoline engine at 1500 rpm would have a flame speed of about 16.5 m/s,and a similar hydrogen engine yields 48.3 m/s, but such engine flame speedsare also very dependent on stoichiometry. 7.4 What is the effect of engine management systems? Engine management systems are now an important part of the strategy toreduce automotive pollution. The good news for the consumer is their abilityto maintain the efficiency of gasoline combustion, thus improving fueleconomy. The bad news is their tendency to hinder tuning for power. A verybasic modern engine system could monitor and control:- mass air flow, fuelflow, ignition timing, exhaust oxygen ( lambda oxygen sensor ), knock( vibration sensor ), EGR, exhaust gas temperature, coolant temperature, andintake air temperature. The knock sensor can be either a nonresonant typeinstalled in the engine block and capable of measuring a wide range of knockvibrations ( 5-15 kHz ) with minimal change in frequency, or a resonant typethat has excellent signal-to-noise ratio between 1000 and 5000 rpm [72]. A modern engine management system can compensate for altitude, ambient airtemperature, and fuel octane. The management system will also control coldstart settings, and other operational parameters. There is a new requirementthat the engine management system also contain an on-board diagnosticfunction that warns of malfunctions such as engine misfire, exhaust catalystfailure, and evaporative emissions failure. The use of fuels with alcoholssuch as methanol can confuse the engine management system as they generatemore hydrogen which can fool the oxygen sensor [47] . The use of fuel of too low octane can actually result in both a loss of fueleconomy and power, as the management system may have to move the enginesettings to a less efficient part of the performance map. The system retardsthe ignition timing until only trace knock is detected, as engine damagefrom knock is of more consequence than power and fuel economy. 7.5 What is the effect of temperature and load? Increasing the engine temperature, particularly the air/fuel chargetemperature, increases the tendency to knock. The Sensitivity of a fuel canindicate how it is affected by charge temperature variations. Increasingload increases both the engine temperature, and the end-gas pressure, thusthe likelihood of knock increases as load increases. 7.6 What is the effect of engine speed?. Faster engine speed means there is less time for the pre-flame reactionsin the end gases to occur, thus reducing the tendency to knock. On engineswith management systems, the ignition timing may be advanced with enginespeed and load, to obtain optimum efficiency at incipient knock. In suchcases, both high and low engines speeds may be critical. 7.7 What is the effect of engine deposits? A new engine may only require a fuel of 6-9 octane numbers lower than thesame engine after 25,000 km. This Octane Requirement Increase (ORI) is due tothe formation of a mixture of organic and inorganic deposits resulting fromboth the fuel and the lubricant. They reach an equilibrium amount becauseof flaking, however dramatic changes in driving styles can also result indramatic changes of the equilibrium position. When the engine starts to burnmore oil, the octane requirement can increase again. ORIs up to 12 are notuncommon, depending on driving style [17,19]. The deposits produce the ORIby several mechanisms:- - they reduce the combustion chamber volume, effectively increasing the compression ratio. - they also reduce thermal conductivity, thus increasing the combustion chamber temperatures. - they catalyse undesirable pre-flame reactions that produce end gases with low autoignition temperatures. 7.8 What is the Road Octane requirement of an vehicle? The actual octane requirements of a vehicle is called the Octane NumberRequirement ( ONR ), and is determined by using standard octane fuels thatcan be blends of iso-octane and normal heptane, or commercial gasolines.The vehicle is tested under a wide range of conditions and loads, usingdifferent octane fuels until trace knock is detected. The conditions thatrequire maximum octane are not consistent, but often are full-throttleacceleration from low starting speeds using the highest gear available. Theycan even be at constant speed conditions [17]. Engine management systemsthat adjust the octane requirement may also reduce the power output on lowoctane fuel, resulting in increased fuel consumption. The maximum ONR is ofmost interest, as that usually defines the recommended fuel. The octane rating engines do not reflect actual conditions in a vehicle,consequently there are standard procedures for evaluating the performanceof the gasoline in an engine. The most common are:-1. The Modified Uniontown Procedure. Full throttle accelerations are made from low speed using primary reference fuels. The ignition timing is adjusted until trace knock is detected at some stage. Several reference fuels are used, and a Road Octane Number v Basic Ignition timing graph is obtained. The fuel sample is tested, and the ignition timing setting is read from the graph to provide the Road Octane Number. This is a rapid procedure but provides minimal information.2. The Modified Borderline Knock Procedure. The automatic spark advance is disabled, and a manual adjustment facility added. Accelerations are performed as in the Modified Uniontown Procedure, however trace knock is maintained throughout the run. A map of ignition advance v engine speed is made for several reference fuels and the sample fuels. This procedure can show the variation of road octane with engine speed. 7.9 What is the effect of air temperature? An increase in ambient air temperature of 5.6C increases the octanerequirement of an engine by 0.44 - 0.54 MON [17,24]. When the combined effectsof air temperature and humidity are considered, it is often possible to useone octane grade in summer, and use a lower octane rating in winter. TheMotor octane rating has a higher charge temperature, and increasing chargetemperature increases the tendency to knock, so fuels with low Sensitivity( the difference between RON and MON numbers ) are less affected by airtemperature. 7.10 What is the effect of altitude? The effect of increasing altitude may be nonlinear, with one study reportinga decrease of the octane requirement of 1.4 RON/300m from sea level to 1800mand 2.5 RON/300m from 1800m to 3600m [17]. Other studies report the octanenumber requirement decreased by 1.0 - 1.9 RON/300m without specifyingaltitude [24]. Modern engine management systems can accommodate thisadjustment, and in some recent studies, the octane number requirement was0.2 - 0.5 Antiknock Index/300m. The reduction on older engines was due to:- - reduced air density provides lower combustion temperature and pressure. - fuel is metered according to air volume, consequently as density decreases the stoichiometry moves to rich, with a lower octane number requirement. - manifold vacuum controlled spark advance, and reduced manifold vacuum results in less spark advance. 7.11 What is the effect of humidity?. An increase of absolute humidity of 1.0 g water/ kg of dry air lowers theoctane requirement of an engine by 0.25 - 0.32 MON [17,24]. 7.12 What does water injection achieve?. Water injection was used in WWII aviation engine to provide a large increasein available power for very short periods. The injection of water doesdecrease the dew point of the exhaust gases. This has potential corrosionproblems. The very high specific heat and heat of vaporisation of watermeans that the combustion temperature will decrease. It has been shown thata 10% water addition to methanol reduces the power and efficiency by about3%, and doubles the unburnt fuel emissions, but does reduce NOx by 25% [73].A decrease in combustion temperature will reduce the theoretical maximumpossible efficiency of an otto cycle engine that is operating correctly,but may improve efficiency in engines that are experiencing abnormalcombustion on existing fuels. Some aviation SI engines still use boost fluids. The water/methanol mixturesare used to provide increased power for short periods, up to 40% more -assuming adequate mechanical strength of the engine. The 40/60 or 45/55water/methanol mixtures are used as boost fluids for aviation engines becausewater would freeze. Methanol is just "preburnt" methane, consequently it onlyhas about half the energy content of gasoline, but it does have a higher heatof vaporisation, which has a significant cooling effect on the charge.Water/methanol blends are more cost-effective than gasoline for combustioncooling. The high Sensitivity of alcohol fuels has to be considered in theengine design and settings. Boost fluids are used because they are far more economical than using thefuel. When a supercharged engine has to be operated at high boost, themixture has to be enriched to keep the engine operating without knock. Theextra fuel cools the cylinder walls and the charge, thus delaying the onsetof knock which would otherwise occur at the associated higher temperatures. The overall effect of boost fluid injection is to permit a considerableincrease in knock-free engine power for the same combustion chambertemperature. The power increase is obtained from the higher allowable boost.In practice, the fuel mixture is usually weakened when using boost fluidinjection, and the ratio of the two fuel fluids is approximately 100 partsof avgas to 25 parts of boost fluid. With that ratio, the resultingperformance corresponds to an effective uprating of the fuel of about 25%,irrespective of its original value. Trying to increase power boosting above40% is difficult, as the engine can drown because of excessive liquid [71]. Note that for water injection to provide useful power gains, the enginemanagement and fuel systems must be able to monitor the knock and adjustboth stoichiometry and ignition to obtain significant benefits. Aviationengines are designed to accommodate water injection, most automobile enginesare not. Returns on investment are usually harder to achieve on engines thatdo not normal extend their performance envelope into those regions. Waterinjection has been used by some engine manufacturers - usually as anexpedient way to maintain acceptable power after regulatory emissionsbaggage was added to the engine, but usually the manufacturer quicklyproduces a modified engine that does not require water injection. Subject: 8. How can I identify and cure other fuel-related problems?8.1 What causes an empty fuel tank? You forgot to refill it. Your friendly neighbourhood thief "borrowed" the gasoline - the unfriendly one took the vehicle. The fuel tank leaked. Your darling child/wife/husband/partner/mother/father used the car. The most likely reason is that your local garage switched to an oxygenated gasoline, and the engine management system compensated for the oxygen content, causing the fuel consumption to increase significantly. 8.2 Is knock the only abnormal combustion problem? No. Many of the abnormal combustion problems are induced by the sameconditions, and so one can lead to another. Preignition occurs when the air/fuel mixture is ignited prematurely byglowing deposits or hot surfaces - such as exhaust valves and spark plugs.If it continues, it can increase in severity and become Run-away SurfaceIgnition (RSI) which prevents the combustion heat being converted intomechanical energy, thus rapidly melting pistons. The Ricardo method uses anelectrically-heated wire in the engine to measure preignition tendency. Thescale uses iso-octane as 100 and cyclohexane as 0. Some common fuel components:-              paraffins       50-100             benzene           26             toluene           93             xylene          >100             cyclopentane      70             di-isobutylene    64             hexene-2         -26 There is no direct correlation between anti-knock ability and preignitiontendency, however high combustion chamber temperatures favour both, and soone may lead to the other. An engine knocking during high-speed operationwill increase in temperature and that can induce preignition, and converselyany preignition will result in higher temperatures than may induce knock. Misfire is commonly caused by either a failure in the ignition system, orfouling of the spark plug by deposits. The most common cause of depositswas the alkyl lead additives in gasoline, and the yellow glaze of variouslead salts was used by mechanics to assess engine tune. From the upperrecess to the tip, the composition changed, but typical compounds ( goingfrom cold to hot ) were PbClBr; 2PbO.PbClBr; PbO.PbSO4; 3Pb3(PO4)2.PbClBr. Run-on is the tendency of an engine to continue running after the ignitionhas been switched off. It is usually caused by the spontaneous ignition ofthe fuel/air mixture, rather than by surface ignition from hotspots ordeposits, as commonly believed. The narrow range of conditions forspontaneous ignition of the fuel/air mixture ( engine speed, chargetemperature, cylinder pressure ) may be created when the engine is switchedoff. The engine may refire, thus taking the conditions out of the criticalrange for a couple of cycles, and then refire again, until overall coolingof the engine drops it out of the critical region. The octane rating of thefuel is the appropriate parameter, and it is not rare for an engine torequire a higher Octane fuel to prevent run-on than to avoid knock [17]. 8.3 Can I prevent carburetter icing? Yes, carburettor icing is caused by the combination of highly volatile fuel,high humidity and low ambient temperature. The extent of cooling, caused bythe latent heat of the vaporised gasoline in the carburettor, can be as muchas 20C, perhaps dropping below the dew point of the charge. If this happens,water will condense on the cooler carburettor surfaces, and will freeze ifthe temperature is low enough. The fuel volatility can not always be reducedto eliminate icing, so anti-icing additives are used. Two types of additive are added to gasoline to inhibit icing:- - surfactants that form a monomolecular layer over the metal parts that inhibits ice crystal formation. These are usually added at concentrations of 30-150 ppm.- cryoscopic additives that depress the freezing point of the condensed water so that it does not turn to ice. Alcohols ( methanol, ethanol, iso-propanol, etc. ) and glycols ( hexylene glycol, dipropylene glycol ) are used at concentrations of 0.03% - 1%.If you have icing problems, the addition of 100-200mls of methanol to a fulltank of dry gasoline will prevent icing under most conditions. If you believethere is a small amount of water in the fuel tank, add 500mls of isopropanolas the first treatment. Oxygenated gasolines using alcohols can also be used. 8.4 Should I store fuel to avoid the oxygenate season? No. The fuel will be from a different season, and will have significantlydifferent volatility properties that may induce driveability problems. Youcan tune your engine to perform on oxygenated gasoline as well as it did ontraditional gasoline, however you will have increased fuel consumption dueto the useless oxygen in the oxygenates. Some engines may not initiallyperform well on some oxygenated fuels, usually because of the slightlydifferent volatility and combustion characteristics. A good mechanic shouldbe able to recover any lost performance or driveability, providing the engineis in reasonable condition. 8.5 Can I improve fuel economy by using quality gasolines? Yes, several manufacturers have demonstrated that their new gasoline additivepackages are more effective than traditional gasoline formulations. Texacoclaim their new vapour phase fuel additive can reduce existing deposits byup to 30%, improve fuel economy, and reduce NOx tailpipe emissions by 15%,when compared to other advanced liquid phase additives. These claims appearto have been verified in independent tests [30]. Other reputable gasolinebrands will have similar additive packages in their quality products.Quality gasolines, of whatever octane ratings, will include a full range ofgasoline additives designed to provide consistent fuel quality. Note that oxygenated gasolines must decrease fuel economy for the same power.If your engine is initially well-tuned on hydrocarbon gasolines, thestoichiometry will move to lean, and maximum power is slightly rich, soeither the management system ( if you have one ) or your mechanic has toincrease the fuel flow. The minor improvements in combustion efficiency thatoxygenates may provide, can not compensate for 2+% of oxygen in the fuelthat will not provide energy. 8.6 What is "stale" fuel, and should I use it? "Stale" fuel is caused by improper storage, and usually smells sour. Thegasoline has been allowed to get warm, thus catalysing olefin decompositionreactions, and perhaps also losing volatile material in unsealed containers.Such fuel will tend to rapidly form gums, and will usually have a significantreduction in octane rating. The fuel can be used by blending with twice thevolume of new gasoline. Some stale fuels can drop several octane numbers, sobe generous with the dilution. 8.7 How can I remove water in the fuel tank? If you only have a small quantity of water, then the addition of 500mls ofdry isopropanol (IPA) to a near-full 30-40 litre tank will absorb the water,and will not significantly affect combustion. Once you have mopped up thewater with IPA. Small, regular doses of any anhydrous alcohol will helpkeep the tank dry. This technique will not work if you have very largeamounts of water, and the addition of greater amounts of IPA may result inpoor driveability. Water in fuel tanks can be minimised by keeping the fuel tank near full, andfilling in the morning from a service station that allows storage tanks tostand for several hours after refilling before using the fuel. Note thatoxygenated gasolines have greater water solubility, and should cope withsmall quantities of water. 8.8 Can I used unleaded on older vehicles? Yes, providing the octane is appropriate. There are some older engines thatcut the valve seats directly into the cylinder head ( eg BMC minis ). Theabsence of lead, which lubricated the valve seat, causes the very hardoxidation products of the valve to wear down the seat. This valve seatrecession is usually corrected by installing seat inserts. Most otherproblems arise because the fuels have different volatility, or the reductionof combustion chamber deposits. These can usually be cured by reference tothe vehicle manufacturer, who will probably have a publication with thechanges. Some vehicles will perform as well on unleaded with a slightlylower octane than recommended leaded fuel, due to the significant reductionin deposits from modern unleaded gasolines. Section: 9. Alternative Fuels and Additives9.1 Do fuel additives work? Most aftermarket fuel additives are not cost-effective. These include theoctane-enhancer solutions discussed in section 6.18. There are various otherpills, tablets, magnets, filters, etc. that all claim to improve either fueleconomy or performance. Some of these have perfectly sound scientificmechanisms, unfortunately they are not cost-effective. Some do not even havesound scientific mechanisms. Because the same model production vehicles canvary significantly, it's expensive to unambiguously demonstrate theseadditives are not cost-effective. If you wish to try them, remember thebiggest gain is likely to be caused by the lower mass of your wallet/purse. There is one aftermarket additive that may be cost-effective, the lubricityadditive used with unleaded gasolines to combat valve seat recession onengines that do not have seat inserts. The long-term solution is to installinserts at the next top overhaul. Some other fuel additives work, especially those that are carefullyformulated into the gasoline by the manufacturer at the refinery. A typical gasoline may contain [17,19,24]:- * Oil-soluble Dye, initially added to leaded gasoline at about 10 ppm to prevent its misuse as an industrial solvent* Antioxidants, typically phenylene diamines or hindered phenols, are added to prevent oxidation of unsaturated hydrocarbons.* Metal Deactivators, typically about 10ppm of chelating agent such as N,N'-disalicylidene-1,2-propanediamine is added to inhibit copper, which can rapidly catalyze oxidation of unsaturated hydrocarbons.* Corrosion Inhibitors, about 5ppm of oil-soluble surfactants are added to prevent corrosion caused either by water condensing from cooling, water-saturated gasoline, or from condensation from air onto the walls of almost-empty gasoline tanks that drop below the dew point. If your gasoline travels along a pipeline, it's possible the pipeline owner will add additional corrosion inhibitor to the fuel.* Anti-icing Additives, used mainly with carburetted cars, and usually either a surfactant, alcohol or glycol.* Anti-wear Additives, these are used to control wear in the upper cylinder and piston ring area that the gasoline contacts, and are usually very light hydrocarbon oils. Phosphorus additives can also be used on engines without exhaust catalyst systems.* Deposit-modifying Additives, usually surfactants. 1. Carburettor Deposits, additives to prevent these were required when crankcase blow-by (PCV) and exhaust gas recirculation (EGR) controls were introduced. Some fuel components reacted with these gas streams to form deposits on the throat and throttle plate of carburettors. 2. Fuel Injector tips operate about 100C, and deposits form in the annulus during hot soak, mainly from the oxidation and polymerisation of the larger unsaturated hydrocarbons. The additives that prevent and unclog these tips are usually polybutene succinimides or polyether amines. 3. Intake Valve Deposits caused major problems in the mid-1980s when some engines had reduced driveability when fully warmed, even though the amount of deposit was below previously acceptable limits. It is believed that the new fuels and engine designs were producing a more absorbent deposit that grabbed some passing fuel vapour, causing lean hesitation. Intake valves operate about 300C, and if the valve is is kept wet deposits tend not to form, thus intermittent injectors tend to promote deposits. Oil leaking through the valve guides can be either harmful or beneficial, depending on the type and quantity. Gasoline factors implicated in these deposits include unsaturates and alcohols. Additives to prevent these deposits contain a detergent and/or dispersant in a higher molecular weight solvent or light oil whose low volatility keeps the valve surface wetted. 4. Combustion Chamber Deposits have been targeted in the 1990s, as they are responsible for significant increases in emissions. Recent detergent-dispersant additives have the ability to function in both the liquid and vapour phases to remove existing carbon and prevent deposit formation.* Octane Enhancers, these are usually formulated blends of alkyl lead or MMT compounds in a solvent such as toluene, and added at the 100-1000 ppm levels. They have been replaced by hydrocarbons with higher octanes such as aromatics and olefins. These hydrocarbons are now being replaced by a mixture of saturated hydrocarbons and and oxygenates.If you wish to play with different fuels and additives, be aware thatsome parts of your engine management systems, such as the oxygen sensor,can be confused by different exhaust gas compositions. An example isincreased quantities of hydrogen from methanol combustion. 9.2 Can a quality fuel help a sick engine? It depends on the ailment. Nothing can compensate for poor tuning and wear.If the problem is caused by deposits or combustion quality, then modernpremium quality gasolines have been shown to improve engine performancesignificantly. The new generation of additive packages for gasolines includecomponents that will dissolve existing carbon deposits, and have been shownto improve fuel economy, NOx emissions, and driveability. 9.3 What are the advantages of alcohols and ethers? This section discusses only the use of high ( >80% ) alcohol or ether fuels.Alcohol fuels can be made from sources other than imported crude oil, and thenations that have researched/used alcohol fuels have mainly based theirchoice on import substitution. Alcohol fuels can burn more efficiently, andcan reduce photochemically-active emissions. Most vehicle manufacturersfavoured the use of liquid fuels over compressed or liquified gases. Thealcohol fuels have high research octane ratings, but also high sensitivityand high latent heats [6,17,51,74].                                 Methanol       Ethanol     Unleaded GasolineRON                               106            107           92 - 98MON                                92             89           80 - 90Heat of Vaporisation    (MJ/kg)     1.154          0.913        0.3044Nett Heating Value      (MJ/kg)    19.95          26.68        42 - 44Vapour Pressure @ 38C    (kPa)     31.9           16.0         48 - 108Flame Temperature        ( C )   1870           1920          2030Stoich. Flame Speed.    ( m/s )     0.43           -             0.34Minimum Ignition Energy ( mJ )      0.14           -             0.29Lower Flammable Limit   ( vol% )    6.7            3.3           1.3Upper Flammable Limit   ( vol% )   36.0           19.0           7.1Autoignition Temperature ( C )    460            360          260 - 460Flash Point              ( C )     11             13          -43 - -39 The major advantages are gained when pure fuels ( M100, and E100 ) are used,as the addition of hydrocarbons to overcome the cold start problems alsosignificantly reduces, if not totally eliminates, any emission benefits.Methanol will produce significant amounts of formaldehyde, a suspectedhuman carcinogen, until the exhaust catalyst reaches operating temperature.Ethanol produces acetaldehyde. The cold-start problems have been addressed,and alcohol fuels are technically viable, however with crude oil at<$30/bbl they are not economically viable, especially as the demand for thenas precursors for gasoline oxygenates has elevated the world prices.Methanol almost doubled in price during 1994. There have also been trialsof pure MTBE as a fuel, however there are no unique or significant advantagesthat would outweigh the poor economic viability [11]. 9.4 Why are CNG and LPG considered "cleaner" fuels. CNG ( Compressed Natural Gas ) is usually around 70-90% methane with 10-20%ethane, 2-8% propanes, and decreasing quantities of the higher HCs up tobutane. The fuel has a high octane and usually only trace quantities ofunsaturates. The emissions from CNG have lower concentrations of thehydrocarbons responsible for photochemical smog, reduced CO, SOx, and NOx,and the lean misfire limit is extended [75]. There are no technicaldisadvantages, providing the installation is performed correctly. The majordisadvantage of compressed gas is the reduced range. Vehicles may havebetween one to three cylinders ( 25 MPa, 90-120 litre capacity), and theyusually represent about 50% of the gasoline range. As natural gas pipelinesdo not go everywhere, most conversions are dual-fuel with gasoline. Theignition timing and stoichiometry are significantly different, but goodconversions will provide about 85% of the gasoline power over the fulloperating range, with easy switching between the two fuels [76]. CNG has been extensively used in Italy and New Zealand ( NZ had 130,000dual-fuelled vehicles with 380 refuelling stations in 1987 ). The conversioncosts are usually around US$1000, so the economics are very dependent on thenatural gas price. The typical 15% power loss means that driveability ofretrofitted CNG-fuelled vehicles is easily impaired, consequently it is notrecommended for vehicles of less than 1.5l engine capacity, or retrofittedonto engine/vehicle combinations that have marginal driveability on gasoline.The low price of crude oil, along with installation and ongoing CNGtank-testing costs, have reduced the number of CNG vehicles in NZ. The USCNG fleet continues to increase in size ( 60,000 in 1994 ). LPG ( Liquified Petroleum Gas ) is predominantly propane with iso-butaneand n-butane. It has one major advantage over CNG, the tanks do not haveto be high pressure, and the fuel is stored as a liquid. The fuel offersmost of the environmental benefits of CNG, including high octane.Approximately 20-25% more fuel is required, unless the engine is optimised( CR 12:1 ) for LPG, in which case there is no decrease in power or increasein fuel consumption [17,76].                                   methane        propane        iso-octaneRON                                 120            112           100MON                                 120             97           100Heat of Vaporisation    (MJ/kg)       0.5094         0.4253        0.2712Net Heating Value       (MJ/kg)      50.0           46.2          44.2Vapour Pressure @ 38C   ( kPa )       -               -           11.8Flame Temperature        ( C )     1950           1925          1980Stoich. Flame Speed.    ( m/s  )      0.45           0.45          0.31Minimum Ignition Energy  ( mJ )       0.30           0.26           -Lower Flammable Limit   ( vol% )      5.0            2.1           0.95Upper Flammable Limit   ( vol% )     15.0            9.5           6.0Autoignition Temperature  ( C )    540 - 630       450           415 9.5 Why are hydrogen-powered cars not available? The Hindenburg.The technology to operate IC engines on hydrogen has been investigated indepth since before the turn of the century. One attraction was touse the hydrogen in airships to fuel the engines instead of venting it.Hydrogen has a very high flame speed ( 3.24 - 4.40 m/s ), wide flammabilitylimits ( 4.0 - 75 vol% ), low ignition energy ( 0.017 mJ ), high autoignitiontemperature ( 520C ), and flame temperature of 2050 C. Hydrogen has a veryhigh specific energy ( 120.0 MJ/kg ), making it very desirable as atransportation fuel. The problem has been to develop a storage system thatwill pass all safety concerns, and yet still be light enough for automotiveuse. Although hydrogen can be mixed with oxygen and combusted moreefficiently, most proposals use air [73,77]. Unfortunately the flame temperature is sufficiently high to dissociateatmospheric nitrogen and form undesirable NOx emissions. The high flamespeeds mean that ignition timing is at TDC, except when running lean, whenthe ignition timing is advanced 10 degrees. The high flame speed, coupledwith a very small quenching distance mean that the flame can sneak pastinlet narrow inlet valve openings and cause backfiring. The advantage of awide range of mixture strengths and high thermal efficiencies are matchedby the disadvantages of pre-ignition and knock unless weak mixtures, cleanengines, and cool operation are used. Interested readers are referred to the group sci.energy.hydrogen for detailsabout this fuel. 9.6 What are "fuel cells" ? Fuel cells are electrochemical cells that directly oxidise the fuel atelectrodes producing electrical and thermal energy. The oxidant is usuallyoxygen from the air and the fuel is usually gaseous, with hydrogenpreferred. There has, so far, been little success using low temperature fuelcells ( < 200C ) to perform the direct oxidation of hydrocarbon-based liquidsor gases. Methanol can be used as a source for the hydrogen by adding anon-board reformer. The main advantage of fuel cells is their high fuel-to-electricity efficiency of about 40-60% of the nett calorific value of thefuel. As fuel cells also produce heat that can be used for vehicle climatecontrol. Fuel Cells are the most likely candidate to replace the IC engineas a primary energy source. Fuel cells are quiet and produce virtually notoxic emissions, but they do require a clean fuel ( no halogens, CO, S, orammonia ) to avoid poisoning. They currently are expensive to produce, andhave a short operational lifetime, when compared to an IC engine [78,79]. 9.7 What is a "hybrid" vehicle? A hybrid vehicle has three major systems [80]. A primary power source, either an IC engine driven generator where the IC engine only operates in the most efficient part of it's performance map, or alternatives such as fuel cells and turbines. A power storage unit, which can be a flywheel, battery, or ultracapacitor. A drive unit, almost always now an electric motor that can used as a generator during braking. Regenerative braking may increase the operational range about 8-13%. Battery technology has not yet advanced sufficiently to economicallysubstitute for an IC engine, while retaining the carrying capacity, range,performance, and driveability of the vehicle. Hybrid vehicles may enablethis problem to be at least partially overcome, but they remain expensive,and the current ZEV proposals exclude fuel cells and hybrids systems, butthis is being re-evaluated. 9.8 What about other alternative fuels? 9.8.1 Ammonia Anhydrous ammonia has been researched because it does not contain any carbon,and so would not release any CO2. The high heat of vaporisation requiresa pre-vaporisation step, preferably also with high jacket temperatures( 180C ) to assist decomposition. Power outputs of about 70% of that ofgasoline under the same conditions have been achieved [73]. 9.8.2 Water Mr Gunnerman has been promoting his patents that claim mixing one part ofgasoline with 2 parts water can provide as much power from an IC engine asthe same flow rate of gasoline. He claims the increased efficiency is fromcatalysed dissociation of water to H2 and 02, as the combustion chamber ofthe test engine contained a catalyst. It takes the same amount of energy todissociate water, as you reclaim when you burn the H2 with 02. So he has touse heat energy that is normally lost. He appears to have modified hisclaims a little with his new A55 fuel. A recent article claims a 29%increase in fuel economy for a test bus in Reno, but also claims that hisfuel combusts so efficiently that it can pass an emissions test withoutrequiring a catalytic converter [81]. Caterpillar are working withGunnerman to evaluate his claims and develop the product. 9.9 What about alternative oxidants? 9.9.1 Nitrous Oxide Nitrous oxide ( N2O ) contains 33 vol% of oxygen, consequently the combustionchamber is filled with less useless nitrogen. It is also metered in as aliquid, with can cool the incoming charge further, thus effectivelyincreasing the charge density. With all that oxygen, a lot more fuel canbe squashed into the combustion chamber. The advantage of nitrous oxide isthat it has a flame speed, when burned with hydrocarbon and alcohol fuels,that can be handled by current IC engines, consequently the power isdelivered in an orderly fashion, but rapidly. The same is not true forpure oxygen combustion with hydrocarbons, so leave that oxygen cylinder onthe gas axe alone :-). The following are for common premixed flames [82].                                Temperature     Flame Speed  Fuel         Oxidant            ( C )           ( m/s )Acetylene        Air               2400         1.60 - 2.70   "         Nitrous Oxide         2800             2.60   "            Oxygen             3140         8.00 - 24.80Hydrogen         Air               2050         3.24 - 4.40   "         Nitrous Oxide         2690             3.90   "            Oxygen             2660         9.00 - 36.80Propane          Air               1925             0.45Natural Gas      Air               1950             0.39 Nitrous oxide is not yet routinely used on standard vehicles, but thetechnology is well understood. 9.9.2 Membrane Enrichment of Air Over the last two decades, extensive research has been performed on theuse of membranes to enrich the oxygen content of air. Increasing the oxygencontent can make combustion more efficient due to the higher flametemperature and less nitrogen. The optimum oxygen concentration for existingautomotive engine materials is around 30 - 40%. There are several commercialmembranes that can provide that level of enrichment. The problem is that thesurface area required to produce the necessary amount of enriched air for anSI engine is very large. The membranes have to be laid close together, orwound in a spiral, and significant amounts of power are required to forcethe air along the membrane surface for sufficient enriched air to run aslightly modified engine. Most research to date has centred on CI engines,with their higher efficiencies. Several systems have been tried on researchengines and vehicles, however the higher NOx emissions remain a problem[83,84]. Subject: 10. Historical Legends10.1 The myth of Triptane This post is an edited version of some posted after JdA posted some claims from a hot-rod enthusiast reporting that triptane + 4cc TEL had a rich power octane rating of 270. This was followed by another that claimed the unleaded octane was 150. In WWII there was a major effort to increase the power of the aviationengines continuously, rather than just for short periods using boost fluids.Increasing the octane of the fuel had dramatic effects on engines that couldbe adjusted to utilise the fuel ( by changing boost pressure ). There was a12% increase in cruising speed, 40% increase in rate of climb, 20% increasein ceiling, and 40% increase in payload for a DC-3, if the fuel went from 87to 100 Octane, and further increases if the engine could handle 100+ PN fuel[85]. A 12 cylinder allison aircraft engine was operated on a 60% blend oftriptane ( 2,2,3-trimethylbutane ) in 100 octane leaded gasoline to produce2500hp when the rated take-off horsepower with 100 octane leaded was 1500hp[10]. Triptane was first shown to have high octane in 1926 as part of the GeneralMotors Research Laboratories investigations [86]. As further interestdeveloped, gallon quantities were made in 1938, and a full size productionplant was completed in late 1943. The fuel was tested, and the high leadsensitivity resulted in power outputs up to 4 times that of iso-octane, andas much as 25% improvement in fuel economy over iso-octane [10]. All of this sounds incredibly good, but then, as now, the cost of octaneenhancement has to be considered, and the plant producing triptane was notreally viable. the fuel was fully evaluated in the aviation test engines,and it was under the aviation test conditions - where mixture strength isvaried, that the high power levels were observed over a narrow range ofengine adjustment. If turbine engines had not appeared, then maybe triptanewould have been used as an octane agent in leaded aviation gasolines.Significant design changes would have been required for engines to utilisethe high anti-knock rating. As an unleaded additive, it was not that much different to other isoalkanes,consequently the modern manufacturing processes for aviation gasolines arealkylation of unsaturated C4 HCs with isobutane, to produce a highlyiso-paraffinic product, and/or aromatization of naphthenic fractions toproduce aromatic hydrocarbons possessing excellent rich-mixture antiknockproperties. So, the myth that triptane was the wonder anti-knock agent that would provideheaps of power arose. In reality, it was one of the best of the iso-alkanes( remember we are comparing it to iso-octane which just happened to be worsethan most other iso-alkanes), but it was not _that_ different from othermembers. It was targeted, and produced, for supercharged aviation enginesthat could adjust their mixture strength, used highly leaded fuel, and wantedshort period of high power for takeoff, regardless of economy. The blending octane number, which is what we are discussing, of triptaneis designated by the American Petroleum Institute Research Project 45 surveyas 112 Motor and 112 Research [31]. Triptane does not have a significantlydifferent blending number for MON or RON, when compared to iso-octane.When TEL is added, the lead response of a large number of paraffins is wellabove that of iso-octane ( about +45 for 3ml TEL/US Gal ), and this can leadto Performance Numbers that can not be used in conventional automotiveengines [10]. 10.2 From Honda Civic to Formula 1 winner. The following is edited from a post in a debate over the advantages ofwater injection. I tried to demonstrate what modifications would be requiredto convert my own 1500cc Honda Civic into something worthwhile :-). There are many variables that will determine the power output of an engine.High on the list will be the ability of the fuel to burn evenly withoutknock. No matter how clever the engine, the engine power output limit isdetermined by the fuel it is designed to use, not the amount of oxygenstuffed into the cylinder and compressed. Modern engines designs andgasolines are intended to reduce the emission of undesirable exhaustpollutants, consequently engine performance is mainly constrained by thefuel available. My Honda Civic uses 91 RON fuel, but the Honda Formula 1 turbocharged 1.5litre engine was only permitted to operate on 102 Research Octane fuel, andhad limits placed on the amount of fuel it could use during a race, themaximum boost of the turbochargers was specified, as was an additional40kg penalty weight. Standard 102 RON gasoline would be about 96 R+M/2 ifsold as a pump gasoline. The normally-aspirated 3.0 litre engines could useunlimited amounts of 102RON fuel. The F1 race duration is 305 km or 2 hours,and it's perhaps worth remembering that Indy cars run at 7.3 psi boost. Engine                 Standard                Formula OneYear                     1986              1987            1989Size                   1.5 litre         1.5 litre       1.5 litreCylinders                 4                 12              12Aspiration              normal            turbo           turboMaximum Boost             -               58 psi           36.3 psiMaximum Fuel              -              200 litres       150 litresFuel                    91 RON           102 RON          102 RONHorsepower @ rpm      92 @ 6000         994 @ 12000      610 @ 12500Torque (lb-ft @ rpm)  89 @ 4500         490 @  9750      280 @ 10000 Lets consider the transition from Standard to Formula 1, without consideringmaterials etc. Replace the exhaust system. HP and torque climb to 100. Double the rpm while improving breathing, you now have 200hp but still only about 100 torque. Boost it to 58psi which equals 4 such engines, so 1000hp and 500 torque. Simple?, not with 102RON fuel, the engine would detonate to pieces. so.. Lower the compression ratio to 7.4:1, and the higher rpm is a big advantage - there is much less time for the end gases to ignite and cause detonation. Optimise engine design. 80 degree bank angles V for aerodynamic reasons and go to six cylinders = V-6 Cool the air. The compression of 70F air at 14.7psi to 72.7psi raises its temperature to 377F. The turbos churn the air and although they are about 75% efficient the air is now at 479F. The huge intercoolers could reduce the air to 97F, but that was too low to properly vaporise the fuel. Bypass the intercoolers to maintain 104F. Change the Air:Fuel ratio to 23% richer than stoichiometric to reduce combustion temperature. Change to 84:16 toluene/heptane fuel, harder to vaporise, but complies with the 102 RON requirement Add sophisticated electronic timing and engine manangement controls to ensure reliable combustion with no detonation. You now have a six-cylinder, 1.5 litre, 1000hp Honda Civic. For subsequent years the restrictions were even more severe, 150 litresand 36.3 maximum boost, in a still vain attempt to give the 3 litre,normally-aspirated engines a chance. Obviously Honda took advantageof the reduced boost by increasing CR to 9.4:1, and only going to 15%rich air/fuel ratio. They then developed an economy mode that involvedheating the liquid fuel to 180F to improve vaporisation, and increasedthe air temp to 158F, and leaned out the air-fuel ratio to just 2% rich.The engine output dropped to 610hp @ 12,500 ( from 685hp @ 12,500 andabout 312 lbs-ft of torque @ 10,000 rpm ), but 32% of the energy inthe fuel was converted to mechanical work. The engine still had crispthrottle response, and still beat the normally aspirated engines thatdid not have the fuel limitation. So turbos were banned. No otherF1 racing engine has ever come close to converting 32% of the fuelenergy into work [87]. Subject: 11. References 11.1 Books and Research Papers    1.  Modern Petroleum Technology - 5th edition.       Editor, G.D.Hobson.       Wiley. ISBN 0 471 262498 (1984).       - Chapter 1. G.D.Hobson.   2.  Hydrocarbons from Fossil Fuels and their Relationship with Living       Organisms.       I.R.Hills, G.W.Smith, and E.V.Whitehead.       J.Inst.Petrol., v.56 p.127-137 (May 1970).   3.  Reference 1.       - Chapter 9. R.E.Banks and P.J.King.   4.  Ullmann's Encyclopedia of Industrial Chemistry - 5th edition.       Editor, B.Elvers.       VCH. ISBN 3-527-20123-8 (1993).       - Volume A23. Resources of Oil and Gas.   5.  BP Statistical Review of World Energy - June 1994.       - Proved Reserves at end 1993. p.2.   6.  Kirk-Othmer Encyclopedia of Chemical Technology - 4th edition.       Editor M.Howe-Grant.       Wiley. ISBN 0-471-52681-9 (1993)       - Volume 1. Alcohol Fuels.   7.  Midgley: Saint or Serpent?.       G.B.Kauffman.       Chemtech, December 1989. p.717-725.   8.  ?       T.Midgley Jr., T.A.Boyd.       Ind. Eng. Chem., v.14 p.589,849,894 (1922).   9.  Measurement of the Knock Characteristics of Gasoline in terms of a       Standard Fuel.       G. Edgar.       Ind. Eng. Chem., v.19 p.145-146 (1927).  10.  The Effect of the Molecular Structure of Fuels on the Power and       Efficiency of Internal Combustion Engines.       C.F.Kettering.       Ind. Eng. Chem., v.36 p.1079-1085 (1944).  11.  Experiments with MTBE-100 as an Automobile Fuel.       K.Springer, L.Smith.       Tenth International Symposium on Alcohol Fuels.       - Proceedings, v.1 p.53 (1993).  12.  Oxygenates for Reformulated Gasolines.       W.J.Piel, R.X.Thomas.       Hydrocarbon Processing, July 1990. p.68-73.  13.  The Chemical Kinetics of Engine Knock.       C.K.Westbrook, W.J. Pitz.       Energy and Technology Review, Feb/Mar 1991. p.1-13.  14.  The Chemistry Behind Engine Knock.       C.K.Westbrook.       Chemistry & Industry (UK), 3 August 1992. p.562-566.  15.  A New Look at High Compression Engines.       D.F.Caris and E.E.Nelson.       SAE Paper 812A. (1958)  16.  Problem + Research + Capital = Progress       T.Midgley,Jr.       Ind. Eng. Chem., v.31 p.504-506 (1939).  17.  Reference 1.       - Chapter 20. K.Owen.  18.  Automotive Gasolines - Recommended Practice       SAE J312 Jan93.       - Section 3.       SAE Handbook, volume 1. ISBN 1-56091-461-0 (1994).  19.  Reference 6.       - Volume 12. Gasoline and Other Motor Fuels  20.  Refiners have options to deal with reformulated gasoline.       G.Yepsin and T.Witoshkin.       Oil & Gas Journal, 8 April 1991. p.68-71.  21.  Stoichiometric Air/Fuel Ratios of Automotive Fuels - Recommended       Practice.       SAE J1829 May92.       SAE Handbook, volume 1. ISBN 1-56091-461-0 (1994).  22.  Chemical Engineers' Handbook - 5th edition       R.H.Perry and C.H.Chilton.       McGraw-Hill. ISBN 07-049478-9 (1973)       - Chapter 3.  23.  Alternative Fuels       E.M.Goodger.       MacMillan. ISBN 0-333-25813-4 (1980)       - Appendix 4.  24.  Automotive Gasolines - Recommended Practice.       SAE J312 Jan93.       SAE Handbook, volume 1. ISBN 1-56091-461-0 (1994).  25.  Standard Specification for Automotive Spark-Ignition Engine Fuel.       ASTM D 4814-93a.       Annual Book of ASTM Standards v.05.03 (1994).  26.  Criteria for Quality of Petroleum Products.       Editor, J.P. Allinson.       Applied Science. ISBN 0 85334 469 8       - Chapter 5. K.A.Boldt and S.T.Griffiths.  27.  Meeting the challenge of reformulated gasoline.       R.J. Schmidt, P.L.Bogdan, and N.L.Gilsdorf.       Chemtech, February 1993. p.41-42.  28.  The Relationship between Gasoline Composition and Vehicle Hydrocarbon       Emissions: A Review of Current Studies and Future Research Needs.       D. Schuetzle, W.O.Siegl, T.E.Jensen, M.A.Dearth, E.W.Kaiser, R.Gorse,       W.Kreucher, and E.Kulik.       Environmental Health Perspectives Supplements v.102 s.4 p.3-12. (1994)  29.  Reference 23.       - Chapter 5.  30.  Texaco to introduce clean burning gasoline.       Oil & Gas Journal, 28 February 1994. p.22-23.  31.  Knocking Characteristics of Pure Hydrocarbons.       ASTM STP 225. (1958)  32.  Health Effects of Gasoline.       Environmental Health Perspectives Supplements v.101. s.6 (1993)  33.  Speciated Measurements and Calculated Reactivities of Vehicle Exhaust       Emissions from Conventional and Reformulated Gasolines.       S.K.Hoekman.       Environ. Sci. Technol., v.26 p.1206-1216 (1992).  34.  Effect of Fuel Structure on Emissions from a Spark-Ignited Engine.       2. Naphthene and Aromatic Fuels.       E.W.Kaiser, W.O.Siegl, D.F.Cotton, R.W.Anderson.       Environ. Sci. Technol., v.26 p.1581-1586 (1992).  35.  Determination of PCDDs and PCDFs in Car Exhaust.       A.G.Bingham, C.J.Edmunds, B.W.L.Graham, and M.T.Jones.       Chemosphere, v.19 p.669-673 (1989).  36.  Volatile Organic Compounds: Ozone Formation, Alternative Fuels and       Toxics.       B.J.Finlayson-Pitts and J.N.Pitts Jr..       Chemistry and Industry (UK), 18 October 1993. p.796-800.  37.  The rise and rise of global warming.       R.Matthews.       New Scientist, 26 November 1994. p.6.  38.  Energy-related Carbon Dixode Emissions per Capita for OECD Countries       during 1990.       International Energy Agency. (1993)  39.  Market Data Book - 1991, 1992, 1993 and 1994 editions.       Automobile News       - various tables  40.  BP Statistical Review of World Energy - June 1994.       - Crude oil consumption p.7.  41.  Automotive Gasolines - Recommended Practice       SAE J312 Jan93.       - Section 4       SAE Handbook, volume 1. ISBN 1-56091-461-0 (1994).  42.  The Rise and Fall of Lead in Petrol.       IDG Berwick       Phys. Technol., v.18 p.158-164 (1987)  43.  E.C. seeks gasoline emission control.       Hydrocarbon Processing, September 1990. p.43.  44.  Health Effects of Gasoline Exposure. I. Exposure assessment for U.S.       Distribution Workers.       T.J.Smith, S.K.Hammond, and O.Wong.       Environmental Health Perspectives Supplements. v.101 s.6 p.13 (1993)  45.  Atmospheric Chemistry of Tropospheric Ozone Formation: Scientific and       Regulatory Implications.       B.J.Finlayson-Pitts and J.N.Pitts, Jr.       Air & Waste, v.43 p.1091-1100 (1993).  46.  Trends in Auto Emissions and Gasoline Composition.       R.F.Sawyer       Environmental Health Perspectives Supplements. v.101 s.6 p.5 (1993)  47.  Reference 6.       - Volume 9. Exhaust Control, Automotive.  48.  Achieving Acceptable Air Quality: Some Reflections on Controlling       Vehicle Emissions.       J.G.Calvert, J.B.Heywood, R.F.Sawyer, J.H.Seinfeld       Science v261 p37-45 (1993).  49.  Radiometric Determination of Platinum and Palladium attrition from       Automotive Catalysts.       R.F.Hill and W.J.Mayer.       IEEE Trans. Nucl. Sci., NS-24, p.2549-2554 (1977).  50.  Determination of Platinum Emissions from a three-way       catalyst-equipped Gasoline Engine.       H.P.Konig, R.F.Hertel, W.Koch and G.Rosner.       Atmospheric Environment, v.26A p.741-745 (1992).  51.  Alternative Automotive Fuels - SAE Information Report.       SAE J1297 Mar93.       SAE Handbook, volume 1. ISBN 1-56091-461-0 (1994).  52.  Lean-burn Catalyst offers market boom.       New Scientist, 17 July 1993. p.20.  53.  Catalysts in cars.       K.T.Taylor.       Chemtech, September 1990. p.551-555.  54.  Advanced Batteries for electric vehicles.       G.L.Henriksen, W.H.DeLuca, D.R.Vissers.       Chemtech, November 1994. p.32-38.  55.  The great battery barrier.       IEEE Spectrum, November 1992. p.97-101.  56.  Exposure of the general Population to Gasoline.       G.G.Akland       Environmental Health Perspectives Supplements. v.101 s.6 p.27-32 (1993)  57.  Court Ruling Spurs Continued Debate Over Gasoline Oxygenates.       G.Peaff.       Chemical & Engineering News, 26 September 1994. p.8-13.  58.  The Application of Formaldehyde Emission Measurement to the       Calibration of Engines using Methanol as a Fuel.       P.Waring, D.C.Kappatos, M.Galvin, B.Hamilton, and A.Joe.       Sixth International Symposium on Alcohol Fuels.       - Proceedings, v.2 p.53-60 (1984).  59.  Emissions from 200,000 vehicles: a remote sensing study.       P.L.Guenther, G.A.Bishop, J.E.Peterson, D.H.Stedman.       Sci. Total Environ., v.146/147 p.297-302 (1994)  60.  Remote Sensing of Vehicle Exhaust Emissions.       S.H.Cadle and R.D.Stephens.       Environ. Sci. Technol., v.28 p.258A-264A. (1994)  61.  Real-World Vehicle Emissions: A Summary of the Third Annual CRC-APRAC       On-Road Vehicle Emissions Workshop.       S.H.Cadle, R.A.Gorse, D.R.Lawson.       Air & Waste, v.43 p.1084-1090 (1993)  62.  IR Long-Path Photometry: A Remote Sensing Tool for Automobile       Emissions.       G.A.Bishop, J.R.Starkey, A.Ihlenfeldt, W.J.Williams, and D.H.Stedman.       Analytical Chemistry, v.61 p.671A-677A (1989)  63.  A Cost-Effectiveness Study of Carbon Monoxide Emissions Reduction       Utilising Remote Sensing.       G.A.Bishop, D.H.Stedman, J.E.Peterson, T.J.Hosick, and P.L.Guenther       Air & Waste, v.42 p.978-985 (1993)  64.  A presentation to the California I/M Review Committee of results of       a 1991 pilot programme.       D.R.Lawson, J.A.Gunderson       29 January 1992.  65.  Methods of Knock Rating. 15. Measurement of the Knocking       Characteristics of Automotive Fuels.       J.M.Campbell, T.A.Boyd.       The Science of Petroleum. Oxford Uni. Press. v.4 p.3057-3065 (1938).  66.  Standard Test Method for Knock Characteristics of Motor and Aviation       Fuels by the Motor Method.       ASTM D 2700 - 92. IP236/83       Annual Book of ASTM Standards v.05.04 (1994).  67.  Standard Test Method for Knock Characteristics of Motor Fuels by the       Research Method.       ASTM D 2699 - 92. IP237/69       Annual Book of ASTM Standards v.05.04 (1994).  68.  Preparation of distillates for front end octane number ( RON 100C )       of motor gasoline       IP 325/82       Standard Methods for Analysis and Testing of Petroleum and Related       Products. Wiley. ISBN 0 471 94879 9 (1994).  69.  Octane Enhancers.       D.Simanaitis and D.Kott.       Road & Track, April 1989. p.82,83,86-88.  70.  Specification for Aviation Gasolines       ASTM D 910 - 93       Annual Book of ASTM Standards v.05.01 (1994).  71.  Reference 1.       - Chapter 19. R.A.Vere  72.  Automotive Sensors Improve Driving Performance.       L.M.Sheppard.       Ceramic Bulletin, v.71 p.905-913 (1992).  73.  Reference 23.       - Chapter 7.  74.  Investigation of Fire and Explosion Accidents in the Chemical, Mining       and Fuel-Related Industries - A Manual.       Joseph M. Kuchta.       US Dept. of the Interior. Bureau of Mines Bulletin 680 (1985).  75.  Natural Gas as an Automobile Fuel, An Experimental study.       R.D.Fleming and J.R.Allsup.       US Dept. of the Interior. Bureau of Mines Report 7806 (1973).  76.  Comparative Studies of Methane and Propane as Fuels for Spark Ignition       and Compression Ignition Engines.       G.A.Karim and I.Wierzba.       SAE Paper 831196. (198?).  77.  The Outlook for Hydrogen.       N.S.Mayersohn.       Popular Science, October 1993. p.66-71,111.  78.  Reference 6.       - Volume 11. Fuel Cells.  79.  The Clean Machine.       R.H.Williams.       Technology Review, April 1994. p.21-30.  80.  Hybrid car promises high performance and low emissions.       M. Valenti.       Mechanical Engineering, July 1994. p.46-49.  81.  ?       Automotive Industries Magazine, December 1994.  82.  Instrumental Methods of Analysis - 6th edition.       H.H.Willard, L.L.Merritt, J.A.Dean, F.A.Settle.       D.Van Nostrand. ISBN 0-442-24502-5 (1981).  83.  Research into Asymmetric Membrane Hollow Filter Device for Oxygen-       Enriched Air Production.       A.Z.Gollan. M.H.Kleper.       Dept.of Energy Report DOE/ID/12429-1 (1985).  84.  New Look at Oxygen Enrichment. I. The diesel engine.       H.C.Watson, E.E.Milkins, G.R.Rigby.       SAE Technical Paper 900344 (1990)  85.  Thorpe's Dictionary of Applied Chemistry - 4th edition.       Longmans. 1949.       - Petroleum  86.  Detonation Characteristics of Some Paraffin Hydrocarbons.       W.G.Lovell, J.M.Campbell, and T.A.Boyd.       Ind. Eng. Chem., v.23 p.26-29. (1931)  87.  Secrets of Honda's horsepower heroics.       C. Csere.       Road & Track/Car & Driver?, May 1991. p.29. 11.2 Suggested Further Reading    1.  Modern Petroleum Technology - any edition.       Editor, G.D.Hobson.       Wiley. ISBN 0 471 262498 (5th=1984).   2.  Hydrocarbon Fuels.       E.M.Goodger.       MacMillan. (1975)   3.  Alternative Fuels       E.M.Goodger.       MacMillan. ISBN 0-333-25813-4 (1980)   4.  Kirk-Othmer Encyclopedia of Chemical Technology - 4th edition.       Editor, M.Howe-Grant.       Wiley. ISBN 0-471-52681-9 (1993)       - especially Alcohol Fuels, Gasoline and Other Motor Fuels, and         Fuel Cells chapters.   5.  The Automotive Handbook. - any edition.       Bosch.   6.  SAE Handbook, volume 1. - issued annually.       SAE. ISBN 1-56091-461-0 (1994).       - especially J312, and J1297.   7.  Proceedings of the xxth International Symposium on Alcohol Fuels.       - Held every two years and most of the 10 conferences have lots of         good technical information, especially the earlier ones.       - various publishers.   8.  Alternative Transportation Fuels - An Environmental and Energy       solution.       Editor, D.Sperling.       Quorum Books. ISBN 0-89930-407-9 (1989).   9.  The Gasohol Handbook.       V. Daniel Hunt.       Industrial Press. ISBN 0-8311-1137-2 (1981).WT03-B20-61IA005-000051-B017-56http://lacebark.ntu.edu.au:80/j_mitroy/sid101/acid/acidFAQ.html 138.80.61.12 19970221151455 text/html 10554HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:45:12 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 10382Last-modified: Tue, 09 Jul 1996 02:26:04 GMT  Acid Rain FAQ Acid Rain The following information came from Environment Canada's A Primer on Environmental Citizenship . Despite progress in recent years, acid rain remains a significant environmental and economic concern for many regions of Canada. This section explains what acid rain is, where it comes from, and what it's effects are. Q3.49 What causes acid deposition? Acid deposition - commonly called acid rain - is caused by emissions of sulphur dioxide and nitrogen oxides. Although natural sources of sulphur oxides and nitrogen oxides do exist, more than 90% of the sulphur and 95% of the nitrogen emissions occurring in eastern North America are of human origin. These primary air pollutants arise from the use of coal in the production of electricity, from base-metal smelting, and from fuel combustion in vehicles. Once released into the atmosphere, they can be converted chemically into such secondary pollutants as nitric acid and sulfuric acid, both of which dissolve easily in water. The resulting acidic water droplets can be carried long distances by prevailing winds, returning to Earth as acid rain, snow, or fog. Q 3.50 Is acid deposition always wet? No. The acids can be transformed chemically into sulphur dioxide gas or into sulphur and nitrogen salts. In this form they are deposited 'dry', causing the same damage as when they land dissolved in rain or snow. In this form they can also do internal damage to plants as they are taken up from the soil. About 40% of nitrogen oxides come from transportation (cars, trucks, buses, trains), about 25% from thermoelectric generating stations, and the balance from other industrial, commercial, and residential combustion processes. Q 3.51 Is natural precipitation acidic? Yes. Water solutions vary in their degree of acidity. If pure water is defined as neutral, baking soda solutions are basic (alkaline) and household ammonia is very basic (very alkaline). On the other side of this scale there are ascending degrees of acidity; milk is slightly acidic, tomato juice is slightly more acidic, vinegar is mediumly acidic, lemon juice is still more acidic, and battery acid is extremely acidic.If there were no pollution at all, normal rainwater would fall on the acid side of this scale, not the alkaline side. Normal rainwater is less acidic than tomato juice, but more acidic than milk. What pollution does is cause the acidity of rain to increase. In some areas of Canada, rain can be as acidic as vinegar or lemon juice. Q 3.52If rain is naturally acidic, why does it matter if pollution makes it more acidic? The problem is one of balance; nature depends upon balance. Normal precipitation reacts with alkaline chemicals - derived from the region's bedrock and found in the air, soils, lakes, and streams - and is thereby neutralized.However, if precipitation is more highly acidic, then acid-buffering chemicals can eventually become depleted. In this case, the buffering effect will no longer occur, and nature's ability to maintain balance will have been destroyed. Q 3.53Do all regions have the same acid-neutralizing capacity? No. Different types of bedrock contain variable amounts of contain variable amounts of alkaline chemicals. Regions with bedrock containing less alkali have a lower capacity for reducing acidity, and thus are more sensitive to acid deposition. Q 3.54What happens when this buffering effect is disrupted When the environment cannot neutralize acid rain, damage occurs to forests, crops, lakes,and fish. Toxic metals such as copper and lead can also be leached from water pipes into drinking water. Q 3.55 How does acid deposition affect aquatic ecosystems? The interactions between living organisms and the chemistry of their aquatic habitats are extremely complex. If the number of one species or group of species changes in response to acidification, then the ecosystem of the entire water body is likely to be affected through the predator-prey relationships of the food web. At first, the effects of acid deposition may be almost imperceptible, but as acidity increases, more and more species of plants and animals decline or disappear. As the water pH approaches 6.0, crustaceans, insects, and some plankton species begin to disappear. As pH approaches 5.0, major changes in the makeup of the plankton community occur, less desirable species of mosses and plankton may begin to invade, and the progressive loss of some fish populations is likely, with the more highly valued species being generally the least tolerant of acidity. Below pH of 5.0, the water is largely devoid of fish, the bottom is covered with undecayed material, and the nearshore areas may be dominated by mosses. Terrestrial animals dependent on aquatic ecosystems are also affected. Waterfowl, for example, depend on aquatic organisms for nourishment and nutrients. As these food sources are reduced or eliminated, the quality of habitat declines and the reproductivesuccess of the birds is affected. Q 3.56How does acid deposition affect terrestrial plant life? Both natural vegetation and crops can be affected. It can alter the protective waxy surface of leaves, lowering disease resistance. It may inhibit plant germination and reproduction. It accelerates soil weathering and removal of nutrients. It makes some toxic elements, such as aluminum, more soluble. High aluminum concentrations in soil can prevent the uptake and use of nutrients by plants. Q 3.57How does acid deposition affect animal life? The effects on terrestrial wildlife are hard to assess. As a result of pollution-induced alteration of habitat or food resources, acid deposition may cause population decline through stress (because of decreases in available resources) and lower reproductivesuccess. Q 3.58What are the socioeconomic consequences of acidification? Lower productivity in fisheries, forestry, and agriculture translates to lower profits and fewer jobs for some of Canada's important industries. Acid deposition causes accelerated corrosion, fracturing, and discoloration of buildings,structures, and monuments. Q 3.59How does acid deposition affect human health? We eat food, drink water, and breathe air that has come in contact with acid deposition. Canadian and U.S. studies indicate that there is a link between this pollution andrespirator problems in sensitive populations such as children and asthmatics. Acid deposition can increase the levels of toxic metals such as aluminum, copper, and mercury in untreated drinking water supplies. It has been estimated that acid rain causes $1 billion worth of damage in Canada every year. Thousands of lakes have been damaged; a large part of the salmon habitat in the Maritimes has been lost; a significant proportion of eastern Canada's forests has beenaffected; and considerable damage to buildings and monuments has been documented. The Canadian Council of Resource and Environment Ministers in established 20 kg/hectare per year as the target for Canadian sulphur dioxide loading. In eastern Canada, 96% of the land with high capability for forestry is subject to acidic depositionin excess of 20 kg/ha per year. In recent years, important instances of dieback anddeclines in growth rate have been noted in sugar maple groves in parts of Canada that receive high levels of these and other air pollutants, such as ozone. Significant growth declines in northern Ontario forests, most notable over the past 30 years, coincide with a period of rapidly increasing industrialization and urbanization across much of the province. More than 80% of all Canadians live in areas with high acid rain-related pollution levels. Q 3.60Is acid deposition occurring to the same extent across Canada? No. Sulphur emissions tend to be concentrated in relatively few locations, while the sources of nitrogen emissions are widely distributed; however, where they are deposited depends on more than just where they are produced. Airborne acidic pollutants are often transported by large scale weather systems thousands of kilometres from their point oforigin before being deposited. In eastern North America, weather systems generally travelfrom southwest to northeast. Thus, pollutants emitted from sources in the industrialheartland of the midwestern states and central Canada regularly fall on the more ruraland comparatively pristine areas of the northeastern U.S. and southeastern Canada. The challenge is to reduce sulphur and nitrogen emissions. The two principal waysindividuals can help are by reducing the amount of energy used in the home (energy efficiency) by reducing the stress your driving habits put on the environment. It has been estimated that about 50% of the sulphate deposited in Canada is derived from sources in the U.S. WT03-B20-62IA005-000051-B019-148http://lacebark.ntu.edu.au:80/j_mitroy/sid101/solar1/Smith.html 138.80.61.12 19970221152356 text/html 40839HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:54:03 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 40667Last-modified: Tue, 09 Jul 1996 02:25:44 GMT Technology Review: July 95: Solar Power Revisiting Solar Power's Past By Charles Smith Inventors unlocked the secrets of turning the sun's rays into mechnical power more than a century ago, only to see their dream machines collapse from lack of public support. Modern solar engineers must not be doomed to relive their fate. Charles Smith is an adjunct faculty member in the Department of Technology at Appalachian State University, and a doctoral candidate in the Department of Science and Technology Studies at Virginia Polytechnic Institute. His primary area of research is the history of energy. Many of us assume that the nation's first serious push to develop renewable fuels was spawned while angry Americans waited in gas lines during the "energy crisis" of the 1970s. Held hostage by the OPEC oil embargo, the country suddenly seemed receptive to warnings from scientists, environmentalists, and even a few politicians to end its overreliance on finite coal and oil reserves or face severe economic distress and political upheaval. But efforts to design and construct devices for supplying renewable energy actually began some 100 years before that turbulent time--ironically, at the very height of the Industrial Revolution, which was largely founded on the promise of seemingly inexhaustible supplies of fossil fuels. Contrary to the prevailing opinion of the day, a number of engineers questioned the practice of an industrial economy based on nonrenewable energy and worried about what the world's nations would do after exhausting the fuel supply. More important, many of these visionaries did not just provide futuristic rhetoric but actively explored almost all the renewable energy options familiar today. In the end, most decided to focus on solar power, reasoning that the potential rewards outweighed the technical barriers. In less than 50 years, these pioneers developed an impres-sive array of innovative techniques for capturing solar radiation and using it to produce the steam that powered the machines of that era. In fact, just before World War I, they had outlined all of the solar thermal conversion methods now being considered. Unfortunately, despite their technical successes and innovative designs, their work was largely forgotten for the next 50 years in the rush to develop fossil fuels for an energy-hungry world. Now, a century later, history is repeating itself. After following the same path as the early inventors--in some cases reinventing the same techniques--contemporary solar engineers have arrived at the same conclusion: solar power is not only possible but eminently practical, not to mention more environmentally friendly. Alas, once again, just as the technology has proven itself from a practical standpoint, public support for further development and implementation is eroding, and solar power could yet again be eclipsed by conventional energy technologies. The First Solar Motor The earliest known record of the direct conversion of solar radiation into mechanical power belongs to Auguste Mouchout, a mathematics instructorat the Lyc�e de Tours. Mouchout began his solar work in 1860 after expressingrave concerns about his country's dependence on coal. "It would be prudent and wise not to fall asleep regarding this quasi-security," he wrote. "Eventually industry will no longer find in Europe the resources to satisfyits prodigious expansion. Coal will undoubtedly be used up. What will industry do then?" By the following year he was granted the first patentfor a motor running on solar power and continued to improve his design until about 1880. During this period the inventor laid the foundation forour modern understanding of converting solar radiation into mechanical steam power. Mouchout's initial experiments involved a glass-enclosed iron cauldron: incoming solar radiation passed through the glass cover, and the trapped rays transmitted heat to the water. While this simple arrangement boiled water, it was of little practical value because the quantities and pressures of steam it produced were minimal. However, Mouchout soon discovered that by adding a reflector to concentrate additional radiationonto the cauldron, he could generate more steam. In late 1865, he succeeded in using his apparatus to operate a small, conventional steam engine. By the following summer, Mouchout displayed his solar motor to Emperor Napoleon III in Paris. The monarch, favorably impressed, offered financialassistance for developing an industrial solar motor for France. With the newly acquired funds, Mouchout enlarged his invention's capacity, refinedthe reflector, redesigning it as a truncated cone, like a dish with slanted sides, to more accurately focus the sun's rays on the boiler. Mouchout also constructed a tracking mechanism that enabled the entire machine to follow the sun's altitude and azimuth, providing uninterrupted solar reception. After six years of work, Mouchout exhibited his new machine in the library courtyard of his Tours home in 1872, amazing spectators. One reporter described the reflector as an inverted "mammoth lamp shade...coated on the inside with very thin silver leaf" and the boiler sitting in the middle as an "enormous thimble" made of blackened copper and "covered with a glass bell". Anxious to put his invention to work, he connected the apparatus to a steam engine that powered a water pump. On what was deemed "an exceptionally hot day," the solar motor produced one-half horsepower. Mouchout reported the results and findings to the French Academy of Science. The government, eager to exploit the new invention to its fullest potential, decided that the most suitable venue for the new machine would be the tropical climes of the French protectorate of Algeria,a region blessed with almost constant sunshine and entirely dependent on coal, a prohibitively expensive commodity in the African region. Mouchout was quickly deployed to Algeria with ample funding to construct a large solar steam engine. He first decided to enlarge his invention's capacity yetagain to 100 liters (70 for water and 30 for steam) and employ a multi-tubed boiler instead of the single cauldron. The boiler tubes had a better surface-area-to-water ratio, yielding more pressure and improved engine performance. In 1878, Mouchout exhibited the redesigned invention at the Paris Exposition. Perhaps to impress the audience or, more likely, his government backers, he coupled the steam engine to a refrigeration device. The steam from the solar motor, after being routed through a condenser, rapidly cooled the inside of a separate insulated compartment. He explained the result: "In spite of the seeming paradox of the statement, [it was] possible to utilize the rays of the sun to make ice." Mouchout was awarded a medal for his accomplishments. By 1881 the French Ministry of Public Works, intrigued by Mouchout's machine, appointed two commissioners to assess its cost efficiency. But after some 900 observations at Montpelier, a city in southern France, and Constantine, Algeria, the government deemed the device a technical success but a practical failure. One reason was that France had recently improved its system for transporting coal and developed a better relationship with England, on which it was dependent for that commodity.The price of coal had thus dropped, rendering the need for alternatives less attractive. Unable to procure further financial assistance, Mouchout returned to his academic pursuits. The Tower of Power During the height of Mouchout's experimentation, William Adams, the deputy registrar for the English Crown in Bombay, India, wrote an award-winning book entitled Solar Heat: A Substitute for Fuel in Tropical Countries. Adams noted that he was intrigued with Mouchout's solar steam engine after reading an account of the Tours demonstration, but that the invention was impractical, since "it would be impossible to construct [a dish-shaped reflector] of much greater dimensions" to generate more than Mouchout's one-half horsepower. The problem, he felt, was that the polished metal reflector would tarnish too easily, and would be too costly to build and too unwieldy to efficiently track the sun. Fortunately for the infant solar discipline, the English registrar did not spend all his time finding faults in the French inventor's efforts, but offered somecreative solutions. For example, Adams was convinced that a reflector of flat silvered mirrors arranged in a semicircle would be cheaper to construct and easier to maintain. His plan was to build a large rack of many small mirrors and adjust each one to reflect sunlight in a specific direction. To track the sun's movement, the entire rack could be rolled around a semicircular track, projecting the concentrated radiation onto a stationary boiler. The rack could be attended by a laborer and would have to be moved only "three or four times during the day," Adams noted, or more frequently to improve performance. Confident of his innovative arrangement, Adams began construction in late1878. By gradually adding 17-by-10-inch flat mirrors and measuring the rising temperatures, he calculated that to generate the 1,200� F necessary to produce steam pressures high enough to operate conventional engines, the reflector would require 72 mirrors. To demonstrate the power of the concentrated radiation, Adams placed a piece of wood in the focus of the mirrored panes where, he noted, "it ignited immediately." He then arrangedthe collectors around a boiler, retaining Mouchout's enclosed cauldron configuration, and connected it to a 2.5-horsepower steam engine that operated during daylight hours "for a fortnight in the compound of [his] bungalow." Eager to display his invention, Adams notified newspapers and invited his important friends--including the Army's commander in chief, a colonel from the Royal Engineers, the secretary of public works, various justices, and principal mill owners--to a demonstration. Adams wrote that all were impressed, even the local engineers who, while doubtful that solar power could compete directly with coal and wood, thought it couldbe a practical supplemental energy source. Adams's experimentation ended soon after the demonstration, though, perhaps because he hadachieved his goal of proving the feasibility of his basic design, but more likely because, as some say, he lacked sufficient entrepreneurial drive. Even so, his legacy of producing apowerful and versatile way to harness and convert solar heat survives. Engineers today know this design as the Power Tower concept, which is one of the best configurations for large scale, centralized solar plants. In fact, most of the modern tower-type solar plants follow Adams's basic configuration: flat or slightly curved mirrors that remain stationary or travel on a semicircular track and either reflect light upward to a boiler in a receiver tower or downward to a boiler atground level, thereby generating steam to drive an accompanying heat engine. Collection without Reflection Even with Mouchout's abandonment and the apparent disenchantment of England's sole participant, Europe continued to advance the practical application of solar heat, as the torch returned to France and engineer Charles Tellier. Considered by many the father of refrigeration, Tellier actually began his work in refrigeration as a result of his solar experimentation, which led to the design of the first nonconcentrating, or non-reflecting, solar motor. In 1885, Tellier installed a solar collector on his roof similar to the flat-plate collectors placed atop many homes today for heating domestic water. The collector was composed of ten plates, each consisting of two iron sheets riveted together to form a watertight seal, and connected by tubes to form a single unit. Instead of filling the plates with water to produce steam, Tellier chose ammonia as a working fluid because of its significantly lower boiling point. After solar exposure, the containers emitted enough pressurized ammonia gas to power a water pump he had placed in his well at the rate of some 300 gallons per hour during daylight. Tellier considered his solar water pump practical for anyone with a south-facing roof. He also thought that simply adding plates, thereby increasing the size of the system, would makeindustrial applications possible. By 1889 Tellier had increased the efficiency of the collectors by enclosing the top with glass and insulating the bottom. He published the results in The Elevation of Water with the Solar Atmosphere, which included details on his intentions to use the sun to manufacture ice. Like his countryman Mouchout, Tellier envisioned that the large expanses of the African plains could become industrially and agriculturally productive through the implementation of solar power. In The Peaceful Conquest of West Africa, Tellier argued that a consistent and readily available supply of energy would be required to power the machinery of industry before the French holdings in Africa could be properly developed. He also pointed out that even though the price of coal had fallen since Mouchout's experiments, fuel continued to be a significant expense in French operations in Africa. He therefore concluded that the construction costs of his low-temperature, nonconcentrating solar motor were low enough to justify its implementation. He also noted that his machine was far less costly than Mouchout's device, with its dish-shaped reflector and complicated tracking mechanism. Yet despite this potential, Tellier evidently decided to pursue his refrigeration interests instead, and do so without the aid of solar heat. Most likely the profits from conventionally operated refrigerators proved irresistible. Also, much of the demand for the new cooling technology now stemmed from the desire to transport beef to Europe from North and South America. The rolling motion of the ships combined with space limitations precluded the use of solar power altogether. And as Tellier redirected his focus, France saw the last major development of solar mechanical power on her soil until well into the twentieth century. Most experimentation in the fledgling discipline crossed the Atlantic to that new bastion of mechanical ingenuity, the United States. The Parabolic Trough Though Swedish by birth, John Ericsson was one of the most influential and controversial U.S. engineers of the nineteenth century. While he spent his most productive years designing machines of war--his most celebrated accomplishment was the Civil War battleship the Monitor--he dedicated the last 20 years of his life largely to more peaceful pursuits such as solar power. This work was inspired by a fear shared by virtually all of his fellow solar inventors that coal supplies would someday end. In 1868 he wrote, "A couple of thousand years dropped in the ocean of time will completely exhaust the coal fields of Europe, unless, in the meantime, the heat of the sun be employed." Thus by 1870 Ericsson had developed what he claimed to be the first solar-powered steam engine, dismissing Mouchout's machine as "a mere toy." In truth, Ericsson's first designs greatly resembled Mouchout's devices, employing a conical, dish-shaped reflector that concentrated solar radiation onto a boiler and a tracking mechanism that kept the reflector directed toward the sun. Though unjustified in claiming his design original, Ericsson soon did invent a novel method for collecting solar rays--the parabolic trough. Unlike a true parabola, which focuses solar radiation onto a single, relatively small area, or focal point, like a satellite television dish, a parabolic trough is more akin to an oil drum cut in half lengthwise that focuses solar rays in a line across the open side of the reflector. This type of reflector offered many advantages over its circular (dish-shaped) counterparts: it was comparatively simple, less expensive to construct, and, unlike a circular reflector, had only to track the sun in a single direction (up and down, if lying horizontal, or east to west if standing on end), thus eliminating the need for complex tracking machinery. The downside was that the device's temperatures and efficiencies were not as high as with a dish-shaped reflector, since the configuration spread radiation over a wider area--a line rather than a point. Still, when Ericsson constructed a single linear boiler (essentially a pipe), placed it in the focus of the trough, positioned the new arrangement toward the sun, and connected it to a conventional steam engine, he claimed the machine ran successfully, though he declined to provide power ratings. The new collection system became popular with later experimenters and eventually became a standard for modern plants.In fact, the largest solar systems in the last decade have opted for Ericsson's parabolic trough reflector because it strikes a good engineering compromise between efficiency and ease of operation. For the next decade, Ericsson continued to refine his invention, trying lighter materialsfor the reflector and simplifying its construction. By 1888, he was so confidentof his design's practical performance that he planned to mass-produce and supplythe apparatus to the "owners of the sunburnt lands on the Pacific coast" for agricultural irrigation. Unfortunately for the struggling discipline, Ericsson died the following year. And because he was a suspicious and, some said, paranoid man who kept his designs to himself until he filed patent applications, the detailed plans for his improved sun motor died with him. Nevertheless, the search for a practical solar motor was not abandoned. In fact, the experimentation and development of large-scale solar technology was just beginning. The First Commercial Venture Boston resident Aubrey Eneas began his solar motor experimentation in 1892, formed the first solar power company (The Solar Motor Co.) in 1900, and continued his work until 1905. One of his first efforts resulted in a reflector much like Ericsson's early parabolic trough. But Eneas found that it could not attain sufficiently high temperatures, and, unable to unlock his predecessor's secrets, decided to scrap the concept altogether and return to Mouchout's truncated-cone reflector. Unfortunately, while Mouchout's approach resulted in higher temperatures, Eneas was still dissatisfied with the machine's performance. His solution was to make the bottom of the reflector's truncated cone-shaped dish larger by designing its sides to be more upright to focus radiation onto a boiler that was 50 percent larger. Finally satisfied with the results, he decided to advertise his design by exhibiting it in sunny Pasadena, Calif., at Edwin Cawston's ostrich farm, a popular tourist attraction. The monstrous machine did not fail to attract attention. Its reflector, which spanned 33 feet in diameter, contained 1,788 individual mirrors. And its boiler, which was about 13 feet in length and a foot wide, held 100 gallons of water. After exposure to the sun, Eneas's device boiled the water and transferred steam through a flexible pipe to an engine that pumped 1,400 gallons of water per minute from a well onto the arid California landscape. Not everyone grasped the concept. In fact, one man thought the solar machine had something to do with the incubation of ostrich eggs. But Eneas's marketing savvy eventually paid off. Despite the occasional misconceptions, thousands who visited the farm left convinced that the sun machine would soon be a fixture in the sunny Southwest. Moreover, many regional newspapers and popular-science journals sent reporters to the farm to cover the spectacle. To Frank Millard, a reporter for the brand new magazine World's Work, the potential of solar motors placed in quantity across the land inspired futuristic visions of a region "where oranges may be growing, lemons yellowing, and grapes purpling, under the glare of the sun which, while it ripens the fruits it will also water and nourish them." He also predicted that the potential for this novel machine was not limited to irrigation: "If the sun motor will pump water, it will also grind grain and saw lumber and run electric cars." The future, like the machine itself, looked bright and shiny. In 1903 Eneas, ready to market his solar motor, moved his Boston-based company to Los Angeles, closer to potential customers. By early the following year he had sold his first complete system for $2,160 to Dr. A. J. Chandler of Mesa, Ariz. Unfortunately,after less than a week, the rigging supporting the heavy boiler weakened during a windstorm and collapsed, sending it tumbling into the reflector and damaging the machine beyond repair. But Eneas, accustomed to setbacks, decided to push onward and constructed another solar pump near Tempe, Ariz. Seven long months later, in the fall of 1904, John May, a rancher in Wilcox, Ariz., bought another machine for $2,500. Unfortunately, shortly afterward, it was destroyed by a hailstorm. This second weather-related incident all but proved that the massive parabolic reflector was too susceptible to the turbulent climactic conditions of the desert southwest. And unable to survive on such measly sales, the company soon folded. Though the machine did not become a fixture as Eneas had hoped, the inventor contributed a great deal of scientific and technical data about solar heat conversion and initiated more than his share of public exposure. Despite his business failure, the lure of limitless fuel was strong, and while Eneas and the Solar Motor Company were suspending their operations, another solar pioneer was just beginning his. Moonlight Operation Henry E. Willsie began his solar motor construction a year before Eneas's company folded. In his opinion, the lessons of Mouchout, Adams, Ericsson, and Eneas proved the cost inefficiency of high-temperature, concentrating machines. He was convinced that a nonreflective, lower-temperature collection system similar to Tellier's invention was the best method for directly utilizing solar heat. The inventor also felt that a solar motor would never be practical unless it could operate around the clock. Thus thermal storage, a practice that lent itself to low-temperature operation, was the focus of his experimentation. To store the sun's energy, Willsie built large flat-plate collectors that heated hundreds of gallons of water, which he kept warm all night in a huge insulated basin. He then submerged a series of tubes, or vaporizing pipes, inside the basin to serve as boilers. When the acting medium--Willsie preferred sulfur dioxide to Tellier's ammonia--passed through the pipes, it transformed into a high-pressure vapor, which passed to the engine, operated it, and exhausted into a condensing tube, where it cooled, returned to a liquid state, and was reused. In 1904, confident that his design would produce continuous power, he built two plants, a 6-horsepower facility in St. Louis, Mo., and a 15-horsepower operation in Needles, Calif. And after several power trials, Willsie decided to test the storage capacity of the larger system. After darkness had fallen, he opened a valve that "allowed the solar-heated water to flow over the exchanger pipes and thus start up the engine." Willsie had created the first solar device that could operate at night using the heat gathered during the day. He also announced that the 15-horsepower machine was the most powerful arrangement constructed up to that time. Beside offering a way to provide continuous solar power production, Willsie also furnished detailed cost comparisons to justify his efforts: the solar plant exacted a two-year payback period, he claimed, an exceptional value even when compared with today's standards for alternative energy technology. Originally, like Ericsson and Eneas before him, Willsie planned to market his device for desert irrigation. But in his later patents Willsie wrote that the invention was "designed for furnishing power for electric light and power, refrigerating and ice making, for milling and pumping at mines, and for other purposes where large amounts of power are required." Willsie determined all that was left to do was to offer his futurist invention for sale. Unfortunately, no buyers emerged. Despite the favorable long-term cost analysis, potential customers were suspicious of the machine's durability, deterred by the high ratio of machine size to power output, and fearful of the initial investment cost of Willsie's ingenious solar power plant. His company, like others before it, disintegrated. A Certain Technical Maturity Despite solar power's dismal commercial failures, some proponents continued to believe that if they could only find the right combination of solar technologies, the vision of a free and unlimited power source would come true. Frank Shuman was one who shared that dream. But unlike most dreamers, Shuman did not have his head in the clouds. In fact, his hardheaded approach to business and his persistent search for practical solar power led him and his colleagues to construct the largest and most cost-effective machine prior to the space age. Shuman's first effort in 1906 was similar to Willsie's flat-plate collector design except that it employed ether as a working fluid instead of sulfur dioxide. The machine performed poorly, however, because even at respectable pressures, the steam--or more accurately, the vapor--exerted comparatively little force to drive a motor because of its low specific gravity. Shuman knew he needed more heat to produce steam, but felt that using complicated reflectors and tracking devices would be too costly and prone to mechanical failure. He decided that rather than trying to generate more heat, the answer was to better conserve the heat already being absorbed. In 1910, to improve the collector's insulation properties, Shuman enclosed the absorption plates not with a single sheet of glass but with dual panes separated by a one-inch air space. He also replaced the boiler pipes with a thin, flat metal container similar to Tellier's original greenhouse design. The apparatus could now consistently boil water rather than ether. Unfortunately, however, the pressure was still insufficient to drive industrial-size steam engines, which were designed to operate under pressures produced by hotter-burning coal or wood. After determining that the cost of building a larger absorber would be prohibitive, Shuman reluctantly conceded that the additional heat would have to be provided through some form of concentration. He thus devised a low-cost reflector stringing together two rows of ordinary mirrors to double the amount of radiation intercepted. And in 1911, after forming the Sun Power Co., he constructed the largest solar conversion system ever built. In fact, the new plant, located near his home in Talcony, Penn., intercepted more than 10,000 square feet of solar radiation. The new arrangement increased the amount of steam produced, but still did not provide the pressure he expected. Not easily defeated, Shuman figured that if he couldn't raise the pressure of the steam to run a conventional steam engine, he would have to redesign the engine to operate at lower pressures. So he teamed up with E.P. Haines, an engineer who suggested that more precise milling, closer tolerances in the moving components, and lighter-weight materials would do the trick. Haines was right. When the reworked engine was connected to the solar collectors, it developed 33 horsepower and drove a water pump that gushed 3,000 gallons per minute onto the Talcony soil. Shuman calculated that the Talcony plant cost $200 per horsepower compared with the $80 of a conventionally operated coal system--a respectable figure, he pointed out, considering that the additional investment would be recouped in a few years because the fuel was free. Moreover, the fact that this figure was not initially competitive with coal or oil-fired engines in the industrial Northeast did not concern him because, like the French entrepreneurs before him, he was planning to ship the machine to the vast sunburnt regions in North Africa. To buy property and move the machine there, new investors were solicited from England and the Sun Power Co. Ltd. was created. But with the additional financial support came stipulations. Shuman was required to let British physicist C. V. Boys review the workings of the machine and suggest possible improvements. In fact, the physicist recommended a radical change. Instead of flat mirrors reflecting the sun onto a flat-plate configuration, Boys thought that a parabolic trough focusing on a glass-encased tube would perform much better. Shuman's technical consultant A.S.E. Ackermann agreed, but added that to be effective, the trough would need to track the sun continuously. Shuman felt that his conception of a simple system was rapidly disintegrating. Fortunately, when the machine was completed just outside of Cairo, Egypt, in 1912, Shuman's fears that the increased complexity would render the device impractical proved unfounded. The Cairo plant outperformed the Talcony model by a large margin--the machine produced 33 percent more steam and generated more than 55 horsepower--which more than offset the higher costs. Sun Power Co.'s solar pumping station offered an excellent value of $150 per horsepower, significantly reducing the payback period for solar-driven irrigation in the region. It looked as if solar mechanical power had finally developed the technical sophistication it needed to compete with coal and oil. Unfortunately, the beginning was also the end. Two months after the final Cairo trials, Archduke Ferdinand was assassinated in the Balkans, igniting the Great War. The fighting quickly spread to Europe's colonial holdings, and the upper regions of Africa were soon engulfed. Shuman's solar irrigation plant was destroyed, the engineers associated with the project returned to their respective countries to perform war-related tasks, and Frank Shuman died before the armistice was signed. Whether or not Shuman's device would have initiated the commercial success that solar power desperately needed, we will never know. However, the Sun Power Co. can boast a certain technical maturity by effectively synthesizing the ideas of its predecessors from the previous 50 years. The company used an absorber (though in linear form) of Tellier and Willsie, a reflector similar to Ericsson's, simple tracking mechanisms first used by Mouchout and later employed by Eneas, and combined them to operate an engine specially designed to run with solar-generated steam. In effect, Shuman and his colleagues set the standard for many of the most popular modern solar systems 50 to 60 years before the fact. The Most Rational Source The aforementioned solar pioneers were only the most notable inventors involved in the development of solar thermal power from 1860 to 1914. Many others contributed to the more than 50 patents and the scores of books and articles on the subject. With all this sophistication, why couldn't solar mechanical technology blossom into a viable industry? Why did the discipline take a 50-year dive before again gaining a measure of popular interest and technical attention? First, despite the rapid advances in solar mechanical technology, the industry's future was rendered problematic by a revolution in the use and transport of fossil fuels. Oil and coal companies had established a massive infrastructure, stable markets, and ample supplies. Also, besides trying to perfect the technology, solar pioneers had the difficult task of convincing skeptics to see solar energy as something more than a curiosity. Visionary rhetoric without readily tangible results was not well received by a population accustomed to immediate gratification. Improving and adapting existing power technology, deemed less risky and more controlled, seemed to make far more sense. Finally, the ability to implement radically new hardware requires either massive commitment or the failure of existing technology to get the job done. Solar mechanical power production in the late nineteenth and early twentieth centuries did not meet either criterion. Despite warnings from noted scientists and engineers, alternatives to what seemed like an inexhaustible fuel supply did not fit into the U.S. agenda. Unfortunately, in many ways, these antiquated sentiments remain with us today. During the 1970s, while the OPEC nations exercised their economic power and as the environmental and "no-nuke" movements gained momentum, Americans plotted an industrial coup whose slogans were energy efficiency and renewable resources. Consequently, mechanical solar power--along with its space-age, electricity-producing sibling photovoltaics, as well as other renewable sources such as wind power--underwent a revival. And during the next two decades, solar engineers tried myriad techniques to satisfy society's need for power. They discovered that dish-shaped reflectors akin to Mouchout's and Eneas's designs were the most efficient but also the most expensive and difficult to maintain. Low-temperature, nonconcentrating systems like Willsie's and Tellier's, though simple and less sensitive to climatic conditions, were among the least powerful and therefore suited only to small, specific tasks. Stationary reflectors like those used in Adams's device, now called Power Tower systems, offered a better solution but were still pricey and damage prone. By the mid-1980s, contemporary solar engineers, like their industrial-revolution counterparts Ericsson and Shuman, determined that for sunny areas, tracking parabolic troughs were the best compromise because they exhibited superior cost-to-power ratios in most locations. Such efforts led engineers at the Los Angeles-based Luz Co. to construct an 80-megawatt electric power plant using parabolic trough collectors to drive steam-powered turbines. The company had already used similar designs to build nine other solar electric generation facilities, providing a total of 275 megawatts of power. In the process, Luz engineers steadily lowered the initial costs by optimizing construction techniques and taking advantage of economiesof buying material in bulk to build ever-larger plants until the price dropped from 24 to 12 cents per kilowatt hour. The next, even larger plant--a 300-megawatt facility--scheduled for completion last year, promised to provide 6 to 7 cents per kilowatt hour, near the price of electricity produced by coal, oil, or nuclear technology. Once again, as with Shuman and his team, the gap was closing. But once again these facilities would not be built. Luz, producer of more than 95 percent of the world's solar-based electricity, filed for bankruptcy in 1991. According to Newton Becker, Luz's chairman of the board, and other investors, the demise of the already meager tax credits, declining fossil fuel prices, and the bleak prospects for future assistance from both federal and state governments drove investors to withdraw from the project. As Becker concluded, "The failure of the world's largest solar electric company was not due to technological or business judgment failures but rather to failures of government regulatory bodies to recognize the economic and environmental benefits of solar thermal generating plants." Other solar projects met with similar financial failure. For example, two plants that employed the tower power concept, Edison's 10-megawatt plant in Daggett, Calif., and a 30-megawatt facility built in Jordan performed well despite operating on a much smaller scale and without Luz's advantages of heavy initial capital investment and a lengthy trial-and-error process to improve efficiency. Still they were assessed as too costly to compete in the intense conventional fuel market. Although some of our brightest engineers have produced some exemplary solar power designs during the past 25 years, their work reflects a disjointed solar energy policy. Had the findings of the early solar pioneers and the evolution of their machinery been more closely scrutinized, perhaps by Department of Energy officials or some other oversight committee, contemporary efforts might have focused on building a new infrastructure when social and political attitudes were more receptive to solar technology. Rather than rediscovering the technical merits of the various systems, we might have been better served by reviewing history, selecting a relatively small number of promising systems, and combining them with contemporary materials and construction techniques. Reinventing the wheel when only the direction of the cart seems suspect is certainly not the best way to reach one's destination. While the best period to make our energy transition may have passed and though our energy future appears stable, the problems that initiated the energy crisis of the 1970s have not disappeared. Indeed, the instability of OPEC and the recent success in the Gulf War merely created an artificial sense of security about petroleum supplies. While we should continue to develop clean, efficient petroleum and coal technology while our present supplies are plentiful, this approach should not dominate our efforts. Alternative, renewable energy technologies must eventually be implemented in tandem with their fossil-fuel counterparts. Not doing so would simply provide an excuse for maintaining the status quo and beg for economic disruption when reserves run low or political instability again erupts in oil-rich regions. Toward that end, we must change the prevailing attitude that solar power is an infant field born out of the oil shocks and the environmental movement of the past 25 years. Such misconceptions lead many to assert that before solar power can become a viable alternative, the industry must first pay its dues with a fair share of technological evolution. Solar technology already boasts a century of R&D, requires no toxic fuel and relativelylittle maintenance, is inexhaustible, and, with adequate financial support, is capable of becoming directly competitive with conventional technologies in many locations. These attributes make solar energy one of the most promising sources for many current and future energy needs. As Frank Shuman declared more than 80 years ago, it is "the most rational source of power." Solar Energy-related Webpages: International Association for Solar Energy Education El Paso Solar Energy Association National Renewable Energy Laboratory (NREL) Freiburg Institute for Solar Energy U.S. Department of Energy International Solar Energy Society MIT Technology Review Home page TECHNOLOGY REVIEW ON-LINE COPYRIGHT NOTICE WT03-B20-63IA005-000051-B018-98http://lacebark.ntu.edu.au:80/j_mitroy/sid101/ipcc/syntrep.html 138.80.61.12 19970221151858 text/html 124157HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:49:04 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 123984Last-modified: Tue, 09 Jul 1996 02:26:01 GMT  IPCC Second Assessment Synthesis Report (August 1995 draft) Note to US Government Reviewers OFFICE OF THE U. S. GLOBAL CHANGE RESEARCH PROGRAM300 D Street, S.W., Suite 840Washington, D.C. 20024Phone: (202) 651-8250/651-8240Fax: (202) 554-6715/554-6858 Dear Prospective Reviewer: The U. S. Government has received a draft for review of the IPCC Second Assessment Synthesis Report. Drawing from the three IPCC reports, this synthesis report is intended to provide an integrated and comprehensive perspective of the knowledge relevant to the interpretation of Article 2 of the UN Framework Convention on Climate Change. Although the individual working group reports have not yet been completed, to meet the IPCC schedule, it is necessary that the initial review of the synthesis report occur at this time. This office is collating comments to be considered for the U.S. Government review and is not the mechanism to be used for individual reviews. THIS IS A DRAFT DOCUMENT AND SHOULD NOT BE CITED OR DISTRIBUTED. The report includes 14 figures and several tables which could not be downloaded onto the Web. While it may be somewhat difficult to review the synthesis report without approved versions of the Summaries for Policymakers from the three working groups, they are currently being revised and are not available. However, the Synthesis report will be updated as these documents change. Despite these difficulties, we believe it is important to participate at this stage in this review process and to comment on both the broad structure of the report and the detailed presentation of information. Comments must be received by the USGCRP office by noon on September 6. Because this date provides for less than a week for us to assemble and integrate the comments and send forth a set of U. S. Government comments, this deadline is very firm. It would be most helpful if comments could be sent via email to office@usgcrp.gov, or please deliver an electronic disc to the address above. Comments should be divided into general comments which should come first, and then specific comments. List page and line numbers for each suggestion or explanation for a specific working change. If you do not have a paper copy available for referral to line numbers, list page and paragraph numbers so we can understand where they should be inserted. Please provide your name, affiliation, address, telephone and fax numbers, and email address with your submission. If you need a paper copy contact Ms. Sandra Vaughn-Cooke at 202-651-8250 (fax: 202-554-6715; email office@usgcrp.gov). Mike MacCracken, Director, Office of the USGCRP DRAFTFOR INTERNAL USE ONLYDO NOT CITE/DISTRIBUTETHE IPCC ASSESSMENT OF KNOWLEDGERELEVANT TO THE INTERPRETATION OFARTICLE 2 OF THE UN FRAMEWORK CONVENTION ON CLIMATE CHANGE: A SYNTHESIS REPORT1995Draft Summary for Policymakers1. Introduction1.1 The UN Framework Convention on Climate Change recognizes that human activities could alter the Earth's climate. Its ultimate objective as expressed in its Article 2 is:"... stabilization of greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic interference with the climate system. Such a level should be achieved within a time-frame sufficient to allow ecosystems to adapt naturally to climate change and to ensure that food production is not threatened and to enable economic development to proceed in a sustainable manner". 1.2 A determination of what concentrations of greenhouse gases are "dangerous" is a policy judgement. This summary provides an assessment of scientific and technical information for policymakers in their consideration of Article 2. It is based on the IPCC reports of 1994 and 1995.2. Is There Evidence that Human Activities are Changing the Climate? 2.1 Human activities are causing increases in atmospheric concentrations of greenhouse gases, particularly carbon dioxide and methane, and of aerosols (tiny airborne particles) . Greenhouse gases warm the atmosphere while aerosols tend to cool it. The cooling effect of aerosols is regional because of the regional nature of their distributions.2.2 Comparing the observed change in the global mean temperature with model simulations suggests that the observed increase over the last century (0.3 C - 0.6 C) is unlikely to be entirely due to natural causes and that a pattern of climatic response to human activities is identifiable in the climatological record. It is more difficult to determine whether global precipitation has changed; increases are evident in some places but not in others. On a global scale, there is little evidence of sustained trends in variability or extremes of weather events such as hurricanes and floods. On regional scales, there is clear evidence of changes in them.3. How is Climate Projected to Change in the Absence of Mitigation Policies? 3.1 Based on inputs such as population, economic growth, land-use, technological change and energy availability, and assuming no climate change policies, the IPCC developed a range of emissions scenarios - the so-called IPCC IS 92 scenarios. For these scenarios, carbon dioxide emissions in the year 2100 are projected to be in the range of about 6 GtC per year (for low population projections), similar to current emissions, to as much as 36 GtC per year. Cumulative emissions of carbon dioxide for the same scenarios for the period 1990-2100 would range from 700 to 2080 GtC (compared to 240 GtC for 1860-1994). The wide range in the projections reflects different prevailing views on population and economic growths and other factors. 3.2 Climate models, using the IS 92 scenarios, project an increase in the global mean temperature of 0.8 C to 3.5 C by 2100, the range in the projections arising from uncertainties in climate processes and future greenhouse gas emissions. (The range would be 0.8 C-4.6 C if aerosols were excluded in the models.) However, only 50-70% of the eventual equilibrium temperature change would have been realized by that time. These temperature changes are expected to be accompanied by changes in the regional and temporal patterns and intensity of rainfall (increased tendency for both floods and droughts) and, by the year 2100, an increase in the global mean sea-level of 0.1 to 0.8 m (0.1 to 1.2 m if aerosols were excluded in the models). These projections indicate changes more rapid than any experienced in the last 10,000 years, the current interglacial when modern society has evolved. Even if atmospheric concentrations of the greenhouse gases were immediately stabilized, society would be committed to a further increase in temperature of 0.5 C-2 C.3.3 Ability to project climate changes at the regional level remains low. 4. How Vulnerable are Ecological and Socio-Economic Systems and Human Health to Climate Change?4.1 Quantitative projections of the impacts of the magnitude and rate of climate change on any particular ecological or socio-economic system are difficult because (i) regional scale climate change predictions are more uncertain than global climate change, (ii) our understanding of many critical processes is limited and (iii) the systems areinfluenced by multiple environmental and non-environmental stresses. Various aspects of climate affect ecosystems and socio-economic systems differently: some systems are sensitive to a change in mean climate, some to changes in the frequency and severity of extreme weather events, some to the rate of change and some to changes in climate variability.4.2 Most impact studies have only dealt with how systems would respond to a climate change resulting from a (arbitrary) doubling of atmospheric carbon dioxide concentration over its pre-industrial level. Few have considered the evolving responses of systems to steadily increasing greenhouse gas concentrations over the corresponding period, and fewer still have examined the consequences of greenhouse gas increases beyond this arbitrary doubling.4.3 While there would be some beneficial effects of climate change, there would be many adverse effects, with some being potentially irreversible, e.g., loss of biological diversity and land. While population increases and human decisions about land use and harvesting intensities potentially represent the greatest pressures on most terrestrial and marine ecosystems in the foreseeable future, climate change represents an important additional stress factor which could result in the following impacts. a. Natural ecosystems: The boundaries of most ecosystems would shift with possible loss of biological diversity and affecting the goods and services ecosystem types (e.g., forests, grasslands, savannah) provide society. For example: even a rate of temperature change as low as 0.1 C per decade (the lower end of the range of IPCC projections), if sustained for a century, would result in significant loss of forest tree species as they cannot migrate quickly enough to keep pace; changes of a few degrees in temperature can result in the loss of species in the upper elevations of montane systems because no migration would be feasible for them as climate warms. b. Food security: There may be significant adverse consequences for food security in some regions of the world even though the current assessment has confirmed the 1990 IPCC conclusion that the aggregate effects of climate change on global agricultural production may be small to moderate. Studies tend to show more negative impacts for areas in the tropics where many of the world's poorest people live and where individuals are at greatest risk of hunger.c. Sustainable economic development: Sustainable economic development in some countries will be threatened by loss of habitat, increases in human diseases and loss of life. Some human habitats are likely to be destroyed by sea level rise and possible change in extreme weather events, causing migration of populations and placing additional stresses on already stressed social and political systems. For human health, a critical issue is the projected increase in the incidence of vector-borne diseases such as malaria, especially in tropical and subtropical countries, and increased periods of severe heat stress in temperate zones.4.4 Because both the rate and magnitude of climate change are important, the rates of change of atmospheric concentrations of greenhouse gases may be as important as stabilization levels. Non-disruptive adaptation in the structure and functioning of natural ecosystems and agriculture and other socio- economic systems is more likely, but not assured, with a slow rate of climate change. Rapid changes, on the other hand, may preclude non-disruptive adaptation.4.5 Adaptation strategies for managed systems such as agriculture and water supply are generally improving because of technological advances. These advances, however, are not available in many parts of the world. In contrast, measures to assist adaptation in unmanaged ecosystems, such as provision of migration corridors, are not well developed.4.6 The vulnerability of human health and socio-economic systems, and to a lesser extent of ecological systems, depends upon economic circumstances and institutional infrastructure. This implies that systems are typically more vulnerable in developing countries where technologies are fewer and less advanced and financial resources more scarce.4.7 Incorporation of environmental concerns into resource-use and development decisions will enhance society's resilience to climate change. 5. How do Emissions Relate to Levels of Stabilization of Greenhouse Gases? 5.1 The Convention in Article 3 provides that the type of strategy used in reaching its objective should be comprehensive, covering all relevant sources, sinks and reservoirs of greenhouse gases. The individual radiative forcing (radiative forcing drives climate change) due to increases in the concentrations of different greenhouse gases - CO2, CH4, N2O and halocarbons and thereby implicitly stratospheric ozone - can be combined and the total expressed in terms of an "equivalent" CO2 concentration. This concept is helpful in analysing the comprehensive approach. 5.2 Carbon cycle models can be used to estimate emission profiles and cumulative emissions of carbon dioxide for various stabilization levels. Such estimates have been made for an illustrative set of CO2 stabilization levels ranging from 450 ppmv to 750 ppmv. Table 1 shows the cumulative emissions. The cumulative emissions could be distributed in different ways over the time until stabilization: higher emissions earlier with lower emissions later or vice versa, for example. 5.3 The cumulative emissions in table 1 are illustrated for two cases: with and without projected increases in methane and nitrous oxide until 2050 in accordance with IS 92a scenario. Accepting, for example, continued growth in the emissions of these gases would significantly reduce the room for cumulative emissions of carbon dioxide for a given stabilization level of "equivalent" carbon dioxide. 5.4 Table 2 shows the cumulative emissions for all IS 92 scenarios. These figures together with table 1 suggest that to stabilize "equivalent" carbon dioxide at 750 ppmv would require emissions to be much lower than the central IS 92a (and IS92b) scenarios. Stabilization of "equivalent" carbon dioxide at 550 ppmv (about twice the pre-industrial level) or lower would require emissions to be lower than any of the IS 92 scenarios except IS 92c (where a low increase of world population to 6.3 billion in 2100 is assumed). 5.5 Stabilization of greenhouse gas concentrations at any of the arbitrary levels explored above would be possible only if anthropogenic emissions were eventually reduced well below 1990 levels. It is possible to choose other stabilization levels and time scales and pathways for achieving them.5.6 The global average annual per capita emissions of carbon dioxide due to the combustion of fossil fuels is at present about 1.1 tonnes (as carbon). In addition, about 0.2 (net) tonnes are emitted per capita from deforestation and land-use changes. The average annual fossil fuel per capita emissions in developed countries is about 2.8 tonnes andTable 1Stabilization levelEquivalent CO2, ppmvCumulative CO2Emissions, GtC(1990-2100)Global Mean TemperatureChange at Equilibrium, CWithout CH4and N2Oincrease to2050With CH4and N2Oincrease to2050450560-760450-6201.1-3.3550800-980660-8101.5-4.5650920-1160760-9601.8-5.67501140-1340930-10902.1-6.3varies from 1.5 to 5.5 tonnes. This compares to the figure of 0.5 tonnes for the same parameter in developing countries, ranging from 0.1 to 2.0 (in some cases more) tonnes. 5.7 Given cumulative emissions and population figures, global annual average per capita carbon emissions can be derived. As an illustration, for the median UN population projections, the future average annual per capita use of fossil fuel cannot much exceed the current global average if the atmospheric concentration of carbon dioxide is to remain below 550 ppmv. Annual average per capita emissions would be higher for stabilization levels above 750 ppmv, but still less than 2 tonnes. Table 2Emission ScenarioCumulative CO2 Emission, GtC(1990-2100)IS 92e2190IS 92f1830IS 92a1500IS 92b1430IS 92d980IS 92c7706. Technology and Policy Options Available to Approach the Objective of the Convention6.1 An extensive array of technologies and policy measures capable of reducing greenhouse gas emissions is available. However, there are social, institutional, financial, market, and legislative barriers to their application and implementation. Hence their technical potential may not be fully realized. a. Energy Supply: Emissions from the supply sector can be substantially reduced by:- more efficient conversion of fossil fuels; - increased use of low carbon fossil fuels, by switching from coal to oil to natural gas;- decarbonizing flue gases and fuels coupled with carbon dioxide storage; - increased use of renewable sources of energy, in particular biomass, solar, wind, hydropower, and geothermal;- switching to nuclear energy.b. Energy Demand: The technical potential for energy efficiency improvements in the industry, transport and human settlements sectors is large. It is estimated to be 10-30% globally at little or no cost, and, if relevant technologies and financing become available, 50-60% in developing countries. Options include: - Industry: improved electric motor drives and heating systems, efficient use of materials and integrating local heat and power systems; - Transport: changing vehicle design to use more efficient drivetrains, body shapes and materials, switching energy sources for propulsion, altering land-use and traffic patterns, transport systems and life styles to reduce the level of passenger and freight transport activity and shifting to less energy intensive transportation modes;- Human Settlements: more efficient space-conditioning systems, reduced heat losses through walls, ceilings and windows and more efficient lighting and appliances including refrigerators, water heaters, cook stoves, etc. c. Enhancement of Sinks: Management of agricultural lands, rangelands and forests can play an important role in reducing emissions and/or enhancing the sinks of carbon dioxide, methane, and nitrous oxide. Agricultural lands and forests can also be used to provide biomass fuels to replace fossil fuels. Estimates of the total amount of carbon that could be sequestered in the forestry and agriculture sectors over the next 50 years range from 90 to 155 GtC, which is equivalent to 8 to 40% of projected emissions in IS 92a scenario over the same period. Emissions can be reduced and atmospheric carbon can be sequestered at an estimated cost of $2-$8 per tonne (excluding land costs) through measures such as:- altered management of agricultural soils and rangelands; - restoration of degraded agricultural lands and rangelands; - slowing deforestation;- natural forest generation;- establishment of tree plantations for biomass fuels and fibre; - promoting agroforestry.6.6 Population and economic growth will drive future food and energy demands. The potential of the agricultural and forestry mitigation options needs to be further assessed in the context of competition for land in the face of such demands. 6.9 Several policy instruments are available to facilitate penetration of low carbon intensive technologies and the modification of consumption patterns. The optimum mix of instruments will vary from country to country and could include: - labelling;- market pull and demonstration programmes that stimulate the development and application of advanced technologies; - incentives such as provision for accelerated depreciation and reduced cost to consumer;- utility demand-side management;- negotiated agreements with industry;- energy pricing (e.g., carbon or energy taxes) and reduced fossil fuel subsidies;- tradeable emissions permits;- regulatory programmes including minimum energy efficiency standards and fuel economy standards;- reduction of agricultural subsidies.7. Factors in Decision-Making7.1 Article 2 provides the framework for cooperative decision-making in the international arena.7.2 The uncertainties in the knowledge base make it difficult to assess the risks posed by anthropogenic climate change quantitatively. The inherent time lags between greenhouse gas emissions and climatic response and between climate change and ecosystems' adaptability (with potential irreversible impacts), and the lead times for infrastructure and capital turnover and for national and international political processes to come to fruition, need to be taken into account in making decisions. 7.3 Action to reduce greenhouse gas emissions could have other benefits such as reduced traffic congestion, air pollution and soil erosion. The market and no-market value of such secondary benefits depends upon local circumstances, but can be considerable. Action to reduce greenhouse gas emissions are likely to be more acceptable if designed to address simultaneously other concerns that impede sustainable development, and be more effective if tailored to local situations and developed through consultations with stakeholders.7.3 Impacts and the costs of mitigation and adaptation will vary within and among countries raising issues of intranational, international and intergenerational equity. Perceived equity is an important element for legitimizing decisions and promoting national and international cooperation.7.4 A sequential decision-making approach offers a prudent strategy that can be adjusted in the light of new information and could take into account factors such as future flexibility and current and future costs. Near term decisions along an optimal path (i.e., modest cost mitigation measures) will be the same for a wide range of ultimate stabilization concentrations.7.5 A broad portfolio of actions aimed at mitigation, adaptation and reducing uncertainties through further research provides a balanced approach to managing the risks of anthropogenic climate change. The appropriate portfolio will differ from country to country. A well-chosen portfolio of climate change investments will yield greater benefit for a given cost than any one option undertaken by itself. 7.6 Delaying action might reduce the overall costs of mitigation, but would increase both the adaptation and damage costs because of the rate and the eventual magnitude of climate change.7.7 Stabilizing CO2 concentrations, for example, near or below even 750 ppmv would require immediate initiation of near term actions that include: (i) research and development on energy efficiency improvements, alternative sources of energy and strategies to accelerate diffusion of new technologies into the market place;(ii) policies to encourage replacement of long-lived energy, transportation, and industrial infrastructure in normal investment cycles with plant and equipment that provides the highest amount of service and the lowest greenhouse gas emissions per unit of input energy and materials;(iii) research and monitoring to promote a better understanding of the climate system and the impacts of climate variability; (iv) action on other measures with multiple benefits and no or low cost. 7.11 "No regrets" measures are those whose benefits, such as reduced energy costs and lower emissions of local and regional pollutants, equal or exceed their cost to society, excluding the benefits of mitigation of climate change. Such "no regrets" mitigation and adaptation measures would appear justified on technical grounds unrelated to the risks of rapid climate change due to greenhouse gases. The expectation of net damages from climate change and the precautionary principle provide a rationale for going beyond "no regrets".Content-Type: TEXT/PLAIN; name="synrept.asc" Content-ID:  Content-Description:DRAFTFOR INTERNAL USE ONLYDO NOT CITE/DISTRIBUTETHE IPCC ASSESSMENT OF KNOWLEDGERELEVANT TO THE INTERPRETATION OFARTICLE 2 OF THE UN FRAMEWORK CONVENTION ON CLIMATE CHANGE: A SYNTHESIS REPORT19951.1 Since the last Ice Age, generations of human beings have adapted their settlements, agriculture, water use, responses to health hazards and commercial activities to current climate (temperature and rainfall) and soils. Nonetheless there are losses incurred yearly due to variability in climate and large losses incurred frequently as a result of extreme weather events such as floods, storms and droughts. Human-induced climate change would be a stress, in some cases a major stress, that would add to existing stresses such as population and the non-sustainable exploitation of natural resources. A large and/or rapid change in climate is likely to affect the distribution and availability of these resources.1.2 Assessments of climate change in the last decade have consistently concluded that human activities are significantly changing the composition of the atmosphere so that a climate change will occur if greenhouse gases (GHGs) continue to be emitted at current or greater rates of emission. However, scientists are as yet not confident about how climate will change regionally nor about its impacts. 1.3 In recognition of these concerns, governments have negotiated, signed and ratified the UN Framework Convention on Climate Change (hereinafter called the Convention). The Convention has entered into force. Its objective is expressed in Article 2 (see box 1).Box 1The ultimate objective of this Convention and any related legal instruments that the Conference of the Parties may adopt is to achieve, in accordance with the relevant provisions of the Convention, stabilization of greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic interference with the climate system. Such a level should be achieved within a time-frame sufficient to allow ecosystems to adapt naturally to climate change, to ensure that food production is not threatened and to enable economic development to proceed in a sustainable manner.1.4 A determination of what concentrations of GHGs are "dangerous" inherently requires policy judgements. It is not the role of the IPCC to make such a determination. But the IPCC assessments can provide useful information on questions such as: the projected response of the climate system to different scenarios of future GHG emissions; the vulnerability of natural and socio-economic systems to changes in climate; potential outcomes under different timings and levels of stabilization of GHG concentrations; and technological and economic policy options available to reduce future GHG emissions and their effectiveness and associated costs and their distributions. The report is based on the contributions of the three IPCC Working Groups to the IPCC Second Assessment Report.1.5 The elements and their interactions within and between the global climate system and the global socio-economic system are shown in figure 1. The upper part details the interactions in the climate system and the lower part focuses on the human system. FIGURE 1Increasing Greenhouse Gas Concentrations, Radiative Forcing and Changes in Climate and Sea Level2.1 There is agreement that the atmospheric concentrations of GHGs have increased globally due to human activities (see table 1). As a result, the radiative balance between the Earth and the atmosphere has changed. This change can be expressed in terms of a radiative forcing of global climate (see figure 2). 2.2 Small particles in the atmosphere (aerosols) arising from sulphur emissions from industrial sources and from biomass burning also have an important effect on radiative forcing. In contrast to most GHGs whose atmospheric lifetimes can vary from years to centuries, the aerosols have very short lifetimes (a few days). The concentrations of aerosols are therefore determined by current emissions. 2.3 Substantial uncertainty surrounds not only the future concentrations of aerosols (because the severe environmental problems of high sulphur emissions are causes of concern) but also the physical mechanisms governing their radiative forcing. Further, aerosols are very inhomogeneous in their geographical distribution. It is important therefore to distinguish between the effects of long-living greenhouse gases and those of aerosols.TABLE 1Summary of key greenhouse gases affected by human activities GasPre-industrialconcentrationConcentrationin 1992Recent rate ofconcentrationchange peryear (over1980s)Atmosphericlifetime(years)RemarksCO2280 ppmv355 ppmv15ppmv/yr(0.4%/yr)50-200*Almostentirelydue tohumanactivitiesCH4700 ppbv1,714 ppbv13ppbv/yr(0.8%/yr)12-17**Natural andanthropo-genicN20275 ppbv311 ppbv0,75ppbv/yr(0.25%/yr)120Natural andanthropo-genicCFC-12zero503 pptv18-20pptv/yr(4%/yr)102EntirelyhumanoriginHCFC-22(a CFCsub-stitute)zero105 pptv7-8pptv/yr(7%/yr)13.3Anthropo-genic, lowconcentra-tions nowbut risingCF4 (aperfluo-rocarbon)zero70 pptv1.1-1.3pptv/yr(2%/yr)50,000Anthropo-genic, verylong life-time, effecti-vely a per-manent atmosphericresidentppmv=parts per million by volume	ppbv=parts per billion by volumepptv=parts per trillion by volume* No single lifetime for CO2 can be defined because of different rates of uptake by different sink processes. ** This has been defined as an adjustment time which takes into account the indirect effect of CH4 on its own lifetime. FIGURE 22.3 Changes in global mean surface temperature and sea level are simple indicators of changes in mean climate. Global mean surface temperature (land and ocean) has increased by between 0.3 C and 0.6 C since the late 19th century and by 0.2 C to 0.3 c over the last 40 years, the period with the most reliable data (figure 3). Urbanization and land degradation account for only about 0.05 C of this increase. The warming has not been globally uniform: summer temperatures In the Northern Hemisphere in recent decades have been the warmest since about 1200 A.D; in some small regions (e.g., Northwestern North Atlantic), cooling has been observed. Over much of the land mass, the warming is associated with warmer nights rather than warmer days, although warmer days are also occurring. Figure 3 shows observed variability of surface temperature on interannual and decadal time scales which is similar to model estimates even when radiative forcing is absent. This variability arises from interactions between different components of the climate system and serves to mask trends due to systematic changes in radiative forcing.(FIGURE 3)2.4 It is more difficult to determine whether global mean precipitation has changed. Increases are evident in some places but not in others. 2.5 Increasing surface temperature melts ice on land and warms the oceans. Thermal expansion of ocean waters and the water flowing from melting ice on land raises the level of the sea. (Melting sea ice does not raise the sea level.) Global mean sea level has risen by between 10 to 25 cm over the past 100 years. About half of this rise could be due to melting glaciers and sea water expansion. The remainder could come from melting of large ice sheets but this is not conclusive. 2.6 There have been both increases and decreases in extremes and variability of weather events during the last several decades. These changes have often been associated with the El Ni�o - Southern Oscillation (ENSO) phenomenon. However, data are inadequate at present to determine whether there have been world-wide changes in variability and extremes.2.7 An important question is whether the rise in the global mean temperature over the past century can be explained as natural variability or whether there is an identifiable contribution from human activities. Compared to the warming projected by models (see paragraph 3.1 also) which take account of the increase of greenhouse gases alone, the observed warming is smaller. But if the effects of aerosols are included in the models, projections agree better with observations. Comparisons of the patterns of the observed warming with model projections have also been performed. These comparisons find much unexplained variation, but agreement is generally better with model patterns that include both aerosols and greenhouse gases than those which include greenhouse gases alone.2.8 Overall, the best evidence to-date suggests that global mean temperature changes over the last century are unlikely to be entirely due to natural causes, and that a pattern of climatic response to human activities is identifiable in the climatological record.3.1 Scenarios of future anthropogenic emissions of greenhouse gases (and their precursors) and the precursors of aerosols are used as inputs for models that project future climates. Starting from emissions, it is possible to calculate future atmospheric concentrations on the basis of atmospheric chemistry and global carbon cycle models. Radiative forcing, which is the fundamental variable used by climate models, is linked to greenhouse gas concentrations through well-established physical relationships. Radiative forcing due to aerosols can also be calculated, but with greater uncertainty than for greenhouse gases.Projected Future Emissions of Greenhouse Gases and Aerosols (Emission scenarios)3.2 In the absence of policies specifically designed to reduce anthropogenic GHG emissions, their atmospheric concentrations will increase. The key factors that determine future GHG emissions include population increase, energy demand, difficulties in moving away from fossil fuels giving rise to continuing heavy dependence on them, pressure on forests , increased use of private vehicles, etc. These various factors are assumed inputs in deriving plausible scenarios of future GHG emissions and contain uncertainties due to differing views on their future values and changes. Emission scenarios, which are not predictions, are divided into those which set out their projections in the absence of mitigation measures and those which take into account mitigation and adaptation measures.3.3 The results of emission scenarios can vary considerably from actual outcomes even over short time horizons because the confidence in the applicability of their underlying assumptions diminishes as the time horizon increases. Therefore, an informed assessment of the possible consequences of uncertain future emissions paths necessitates the use of a range of scenarios. 3.4 A set of six emissions scenarios, the so-called IS92 scenarios a through f, are presented in the IPCC 1992 Supplement to illustrate a range of plausible future emissions. The scenarios assume no new policies to reduce greenhouse gas emissions (and are thus non-intervention scenarios as far as climate change is concerned). They extend to the year 2100 and include emissions of CO2, CH4, N2O, the halocarbons (CFCs and their substitutes HCFCs and HFCs) and sulphur emissions from industry and biomass burning.3.5 The scenario of highest emissions (IS 92e) assumes high economic growth, moderate population growth, high fossil fuel availability and a phase-out of nuclear power. The IS 92a scenario provides a central case (not necessarily the most likely case) projection of global emissions. Use of coal is assumed to increase with time and renewable energy sources are assumed to replace fossil fuels only to a modest degree. Its world population projection is close to the medium projections of three international organizations and global economic growth assumptions are close to the average of other published scenarios. The IS 92c and IS 92d scenarios assume constraints on economic growth and fossil fuel use and low population projection of 6.4 billion in 2100 (for reference, current world population is 5.8 billion) and hence have low emissions. A summary of the assumptions in the scenarios is given in table 2. TABLE 23.6 Most published emission scenarios (including the IS 92 scenarios) show an increase in global annual greenhouse gas emissions, particularly CO2, over the next century. Virtually the only exceptions are scenarios which either incorporate very low population projections or policy measures. 3.7 To understand the potential impact of climate change policies on emissions and resulting concentrations of greenhouse gases, it is illustrative to consider emission scenarios that explicitly include different policy options. These options may contain measures specifically directed at climate change mitigation or measures which have much the same effect although differently motivated (e.g. intended to mitigate local and regional air pollution). The World Energy Council (WEC) developed emissions cases based on a medium population projection (11.3 billion by 2100), economic growth assumptions with faster (5.6% p.a) and slower (4.6% p.a.) possibilities for developing countries, adjustments of the official GDP for purchasing power parity rather than market exchange rates, alternative assumptions about the pace of future energy efficiency improvements and alternative fuel mix possibilities in the light of known reserves. 3.8 Two of the WEC scenarios imply atmospheric CO2 concentrations below 600 ppmv by 2100 (one around 450 ppmv), and even their high growth case implies concentration below 700 ppmv. These and other studies illustrate the scope for policy measures - not necessarily solely based on climate change mitigation - directed at raising energy efficiency, accelerating the diffusion of non-fossil forms of energy, and reducing local and regional expected damages, so far as feasible, arising from conversion and use of fossil fuels.3.9 In figures 5 (a) and 5 (b) are shown the concentrations of CO2 and CH4 in the IS 92 emission scenarios. It is evident that these non-intervention scenarios lead to rising concentrations of the greenhouse gases. FIGURE 5 (a)FIGURE 5 (b)3.7 Atmospheric concentrations of key long-lived greenhouse gases (such as CO2) at a given time are determined, to a first approximation, by the cumulative emissions up to that time. Cumulative emissions have a smaller range than annual emissions. Cumulative CO2 emissions for the years 1990 to 2100 in the IS92 scenarios range between 700 and 2080 GtC and between 230 and 330 GtC for 1990 - 2025. For comparison, emissions due to fossil fuel use between 1860 and 1994 amounted to 240 GtC (215 GtC for 1860 - 1990) and due to deforestation and changing land use to 80 - 120 GtC. On comparable (medium) population projections, the WEC cases indicate a range of 2.5 to 6 times past CO2 emissions (see figure 6). FIGURE 63.11 The emissions of sulphur dioxide, which are transformed into sulphate aerosols, depend primarily upon the sulphur content of oil and coal burned. The IS92 emissions scenarios assume that the anthropogenic sulphur emissions in 1990 amounted to about 80 TgS or million tonnes (for comparison, about 25 TgS were emitted from natural sources). This was projected to increase to 85, 115 and 130 TgS/yr respectively for the IS 92c, IS 92a and IS 92e scenarios in 2050 and to 60, 140 and 200 TgS/yr in 2100. These estimates are quite uncertain, but are the only ones available for use in projecting the future radiative forcing of anthropogenic aerosols. Projected Changes in Global Mean Temperature and Mean Sea Level 3.12 Future changes of climate due to human activities are estimated by first deriving patterns of radiative forcing from scenarios of atmospheric concentrations of greenhouse gases and aerosols. The detailed response to these patterns of radiative forcing is obtained from models of the climate which take into account the physical and dynamical processes in all components of the climate system (atmosphere, ocean, land, ice and biosphere) and the interactions (most of which are non-linear) between them. Key to these model descriptions are feedbacks which act either to amplify or to reduce the basic radiative forcing signal. Because of our limited knowledge of these feedbacks, especially those involving clouds, considerable uncertainty remains in estimates of future climate change. Further uncertainty is associated with the role of the ocean which, because of its large heat capacity, acts to slow down the climate response to radiative forcing; the ocean also, through its ability to redistribute heat within the climate system, has an important role in determining the spatial patterns of climate change. 3.13 Many model estimates of climate change have been made for the particular case of an equilibrium atmospheric state in which the radiative forcing is increased to that which would occur if the CO2 concentration were doubled over its pre-industrial value (such an estimate is known as the climate sensitivity). The IPCC assessments in 1990, 1992 and 1995 reported that the global average temperature rise for doubled CO2 at equilibrium is most likely to be in the range of 1.5 C - 4.5 C with a best estimate of 2.5 C.3.14 Estimates of global mean temperature change for all of the IS92 scenarios are shown in figure 7. The extreme and middle estimates reflecting the range (and hence the uncertainty) in the climate sensitivity are shown in figure 8. FIGURE 7FIGURE 83.15 For the central scenario (IS92a), global mean surface temperature increasesat a rate between 0.l6 C and 0.36 C per decade with greenhouse gases alone. If the effects of aerosols (less certain) are taken into account, the projected rates of warming over the next century are expected to be in the range 0.14 C - 0.28 C per decade. FIGURE 93.16 For the same scenario, global mean sea level would rise by between 0.3 and 1.0 m by the year 2100, if the greenhouse gases alone are considered. Inclusion of aerosols leads to a calculated rise over the same period of 0.2 to 0.8 m. (These values are for the IS92a scenario.) Sea level is expected to continue to rise for several centuries after stabilization of GHG levels.Projected Regional Changes of Climate and Sea Level 3.17 Models with and without aerosols indicate that a change of climate will not be distributed uniformly around the globe (figure 10). The confidence in their projections of regional climate change, however, remains low. This is partly due to approximations in current models, and to the fact that the inherent predictability of climate diminishes as geographical scale is reduced.FIGURE 103.18 Changes in future sea level will not occur uniformly around the globe. Recent model experiments suggest that the regional responses could differ by a factor of 2 to 3 from the mean due to regional differences in heating and changes in ocean circulation. In addition, geological and geophysical processes cause vertical land movements and these affect relative sea level changes on local and regional scales. 3.19 The following general conclusions, nevertheless, can be drawn relating to climate change due to increasing GHGs only: Warming is projected to be in general greater over land than over the oceans;  Over land, maximum warming is expected in high northern latitudes in winter;  Several model experiments indicate that south Asian monsoon will strengthen. 3.20 Since impacts are particularly related to climate variability and the occurrence of extreme weather events, possible changes in these deserve special attention. Model results lead to the following findings: A general warming tends to lead to an increase in high temperature events (heat waves) and a decrease in winter days below freezing;  Models indicate that global average precipitation would increase in a warmer climate. The probability of heavy precipitation events leading to floods is expected to increase;  All model experiments show that warming leads to increased evaporation. In regions that are currently prone to drought, droughts may become longer lasting and more severe; Models indicate changes in the precipitation patterns. In some places, increases in the probability of dry days and the length of dry spells (consecutive days without precipitation) are indicated. In those regions where mean precipitation decreases, droughts may increase markedly; Model experiments do not agree on systematic changes in overall storminess globally in a warmer world, but the possibility of systematic changes on the regional scale cannot be excluded; Changes of tropical cyclone frequency may be small in comparison with observed natural variability. There may, however, be some potential for changes of cyclone intensity in regions where sea surface temperature lies between 26 C and 29 C. 3.22 Questions are often asked regarding the long-term stability of climate and whether large, almost discontinuous, changes could occur as a result of increasing greenhouse gases. One such change is concerned with the circulation of the ocean on the largest spatial scale. For instance, it is this circulation which maintains the region of the North Atlantic ocean several degrees warmer than it would otherwise be. Several experiments with complex climate models indicate that this large scale circulation might weaken with global warming. Observations also show that some change in this direction might have occurred in the last few decades. Further, both the study of palaeo-climatic records and climate model experiments show that transitions to quite different circulation patterns might occur relatively rapidly. How likely is the occurrence of such events in response to global warming, how strong the radiative forcing will have to be for the change to be significant and the details of the accompanying changes are currently subjects of much scientific debate.3.23 There is a very remote possibility that the West Antarctic Ice Sheet might gradually disintegrate because of a warmer climate, causing a significant rise in sea level. The specific circumstances under which such an event may occur are not known. 4.1 Human-induced climate change would represent an important stress in a world where many systems are already threatened by increasing resource demands and non-sustainable management practices. Both the magnitude and the rate of climate change are important in determining the inherent sensitivity and adaptability of ecological and socio-economic system (Box 2). This section assesses the sensitivity, and the potential for adaptation, of human health and selected ecological and socio-economic sectors to changes in climate.4.2	Quantitative projections of the impacts of the magnitudes and the rates of climatechange on any particular ecological or socio-economic system are difficult because (i) regional scale climate change predictions are uncertain, (ii) our understanding of many critical processes is limited, and (iii) the systems are influenced by multiple environmental and non-environmental factors. Moreover, most impact studies have only assessed how systems would respond to a climate change resulting from a (arbitrary) doubling of atmospheric carbon dioxide concentrations over its pre-industrial level. Very few have considered the evolving responses of systems to steadily increasing greenhouse gas concentrations over the corresponding period, and fewer still have examined the consequences of increases in greenhouse gas concentrations beyond doubling.Human Health4.3 Projected changes in climate are likely to result in a wide range of human health impacts, most of them adverse, and many of which would reduce life expectancy. In many cases there is likely a threshold of minimum temperature associated with the occurrence of an adverse health impact. However, quantifying the impacts is difficult and depends on co-existent and interacting factors other than climate change, which include environmental circumstances and socio-economic conditions such as nutritional and immune status, water purity, population density, social infrastructure, working conditions and access to health care.Box 2Sensitivity is the response of a system to a change in climatic conditions. Adaptation refers to adjustments in practices, processes, and structures of systems to projected or actual changes in attributes of climate. Adaptations can be autonomous or planned (planning could be reactive or anticipatory). The ability to adapt determines whether a system can turn new climate conditions into an opportunity to become more successful, or whether it incurs losses that weaken its efficiency or even its ability to survive. Most societies and resource-use sectors already contend with contemporary climatic variability and the wide range of associated natural hazards and unexpected opportunities. The ability of some systems to adapt to changes may be reduced by simultaneous exposure to other stresses (habitat fragmentation, acid deposition, influx of toxic pollutants, etc.).Some societies have more human, technological, and financial resources at their disposal with which to modify their environmental or social/economic systems. Vulnerability is the extent to which a system may be damaged or harmed by climate change and depends not only on the sensitivity of systems to variations in attributes of climate, but also on the ability of these systems to adapt by adjusting to new climatic conditions.Natural ecosystems are more vulnerable to climate change than managed ecosystems because adaptation options for the former are more limited and human intervention can control to some extent needed inputs of water and nutrients for the latter.Developing countries are more vulnerable to climate change than developed countries which have greater resources for adaptation. 4.4 Climate change can directly affect human health by increased exposure to very hot weather events and more frequent weather hazards (e.g., droughts, floods, and severe storms) increasing injuries, death, and destruction of public health infrastructure. An increase in the frequency and/or severity of heat waves would affect even populations that are acclimatized to high background temperatures resulting in several thousand additional deaths per year world-wide. Temperature increases in colder regions should result in fewer cold-related deaths. The ability to withstand severe heat waves depends upon the susceptibility of a given population, which is a function of socio-economic differences, physiological acclimatization, and cultural-technical adaptation. Temperature and humidity thresholds and physiological tolerances vary and generally increase towards the equator.4.5 Climate change can indirectly affect human health through altered disease transmission and nutritional status and exacerbation of existing health disorders. Increases in transmission of vector-borne infectious diseases may result from increases in the geographical distribution of the vector organisms of those diseases (e.g., mosquitoes that spread malaria, dengue and yellow fever; water snails that spread schistosomiasis; and black flies that spread river blindness), and from climate-change driven variations in the life-cycle dynamics of both the vector and the infectious parasite. With malaria for example, the parasite cannot develop inside its mosquito host at temperatures below 16 C. There is already evidence that malaria, yellow fever and dengue fever are persisting at higher altitudes and latitudes. It is estimated that the potential transmission of malaria and river blindness could increase by approximately 20% (i.e., an extra 50-80 million prevalent cases of malaria and 3.5 million cases of river blindness). Any such increase would primarily affect tropical, subtropical, and some less well-protected temperate-zone populations currently at the margins of endemically infected areas. Increasing transmission of certain contagious diseases (cholera, for example, through an increase in algal blooms in warm coastal waters and wetlands) may also be caused by changes in climate. Climate-change related decreases in food production may impair nutritional status, especially in some developing countries whose populations currently have insecure access to food supplies. Conversely, climate change may also help improve food production, and therefore nutritional status, in some regions. Finally, climate change may exacerbate respiratory disorders and allergies by intensifying the effects of air pollution (ozone and particulates), particularly in densely-populated urban areas.4.6 Options for adapting to the potential health effects of climate change are available. However, much of the world's population already suffers from poor environmental health conditions and has little access to health care, and these populations would be the least likely to have access to adaptive measures. At the population level, public health surveillance-and-control measures (especially for infectious diseases), the introduction of protective technologies (e.g. insulated buildings, air-conditioning), improved primary health care (such as use of malarial prophylactics and vaccines such as that for yellow fever), and improved large-scale health monitoring could play a significant role in reducing the range of health impacts. At the individual level, people can be encouraged to limit dangerous exposures (e.g. by use of protective clothing, mosquito nets, repellents, sun screens, etc.). Water Systems4.7 Fresh water, an essential component of national welfare and productivity, is an increasingly scarce commodity. The world's agriculture, hydroelectric and thermal power generation, municipal and industrial water needs, water pollution control, and inland navigation depend on the natural endowment of surface and groundwater resources. Reductions in natural freshwater resources could result in chronic shortages in regions that are already under stress and for which there is considerable competition among users.4.8 Climate change could represent an additional stress on the hydrological cycle that is already under a variety of stresses, such as depletion of aquifers, urbanization, changes in vegetative cover, and chemical contamination. The complexity of the hydrological system and the many influences on it, combined with the uncertainty of the regional distribution of changes in temperature and precipitation in climate model results, make it difficult to project precisely where stresses on water resources would be greatest.4.9 Current evidence suggests that a warmer climate would result in changes in the timing, regional patterns, and intensity of precipitation events. This would lead to changes in the seasonal availability of water for agriculture and supplies of drinking water (timing of precipitation); river flow (regional patterns of precipitation); and flooding (intensity of precipitation). While climate models project increases in global mean precipitation, higher temperatures would increase evapotranspiration. Higher evaporation rates may lead to reduced run-off and streamflow in some areas despite increased precipitation.4.10 Global climate change would have the largest effects on countries which extract a high percentage of their available water resources. In particular, the current arid and semi-arid regions of the world are likely to be significantly affected and could experience the largest decreases in run-off, the latter posing the greatest challenges for water resources management.4.11 Management of water resources should be a continuously adaptive enterprise, responding to changing demands, hydrological conditions, technologies, and economic shifts. Water management practices seek to control demand for water, and to increase supplies, for example, by increasing reservoir capacity. However, experts disagree over whether water supply systems will evolve substantially enough in the future to compensate for the anticipated negative impacts of climate change on water resources and for potential increases in demand. To the extent that existing water supply and quality problems are not addressed, especially in developing countries, optimistic projections about adequate water supplies of sufficient quality will not hold. Natural Ecological Systems4.12 Ecosystems are regulated by numerous aspects of the physical environment such as atmospheric composition, soil properties, topography, and climate variables such as temperature, precipitation, and cloudiness. Changes in climate, and associated changes in the frequency of fires and prevalence of pests could alter the structure (biological diversity) and function (productivity, carbon storage and nutrient recycling) of terrestrial ecosystems, thus affecting the critical goods and services they provide to society. These goods and services include: (i) provision of food, fibre, medicines, and energy; (ii) regulation of water run-off to control floods and soil erosion; (iii) assimilation of wastes and purification of water; and (iv) maintenance of an attractive environment for recreation and tourism. While population increases and human decisions about land use and harvesting intensities will probably represent the greatest pressures on most terrestrial and marine ecosystems for the foreseeable future, climate change would represent a significant additional stress factor. Forests4.13 The consequences of climate change for forest ecosystems depend on the magnitude and the rate of change and their potential to adapt. The rate at which climate zones shift is important because different species migrate at different rates, depending on their growth and reproductive cycles. A global mean warming of 1 C-4 C over the next 100 years would be equivalent to a poleward shift of temperature bands of approximately 160-640 km. The historical migration rates for tree species, based on the palaeo-environmental record, are generally believed to be on the order of 4 - 200 km per century. Thus some forest species may be unable to migrate sufficiently rapidly to keep pace with shifts in the climate zones. 4.14 An increase in mean annual air temperature of 1 C is sufficient to cause changes in the growth and regeneration capacity of many forest species, and hence in the composition of forest ecosystems. Under a doubled-CO2 equilibrium climate scenario, global models project major changes in vegetation types, (i.e., transformation from one equilibrium vegetation class to another) over a substantial fraction (14- 65% with a global mean of 34%) of the existing forested area of the world. Boreal forests will be most affected, with tropical forests being least affected. Such a transition will have implications for animal and microbial biodiversity because of habitat loss, as well as for commodity extraction and management practices within forests. 4.15 While it can be inferred that higher rates and larger magnitudes of change will have stronger impact, a key uncertainty is the fate of forested systems during the transition from one equilibrium climate to another (assuming a new equilibrium will be established). Changes in species range, disturbance regimes (fires, pests, and diseases), and increased temperatures (which increases respiration) may decrease standing biomass during the transition, even though the net primary productivity of forests may increase due to the so-called CO2 fertilization effect (see paragraph 4.24 also).4.16 Measures to assist adaptation in unmanaged ecosystems are not well developed, especially for complex forested ecosystems. Human activities fragment most landscapes limiting the ability of populations to follow changing habitat distribution. One adaptation strategy would be to establish a distributed system of reserves based on model projections of future habitats. Migration to such reserves (or other appropriate habitats) could be assisted by transplantation or by means of migration corridors of relatively undisturbed habitat and by reducing the competition for resources or other stresses at critical stages in life-cycles (e.g. seedling stage for plants, juvenile period for animals). 4.17 Article 2 provides that GHG concentrations be stabilized in a time-frame sufficient to allow ecosystems to adapt naturally to climate change. Ecosystems do not adapt to climate change but individual species do. Hence, the rate of climate change affects the way in which ecosystems disassemble-assemble and reassemble into new ecological systems. At the present rate of GHG emissions, global mean temperatures are projected to increase by about 0.2 C per decade, a rate which is at least twice the historically observed maximum rate. Therefore, forested ecosystems will be unable to completely adjust to such rates and some species may become extinct. Coastal Ecosystems4.18 Coastal ecosystems such as salt-water marshes, mangrove ecosystems, coastal wetlands, coral reefs, coral atolls and reef islands provide a wide range of goods and services to society such as providing important habitats and nurseries for wildlife and fish and centres for tourism and fishing, and protecting islands and coastal areas from wave actions and severe storms. These ecosystems are under various stresses including overexploitation of resources, pollution, sediment starvation and urbanization. They are also exceptionally vulnerable to increases in temperature and sea level. 4.19 Coral reefs, coral atolls and reef islands and deltas are particularly vulnerable to even modest increases in temperature, resulting in a significant increase in the frequency of coral bleaching, submergence, and coastal erosion. If seawater temperature increases of 3 C-4 C are sustained for a period of months, considerable coral mortality can ensue. Severe bleached corals can take decades to centuries to recover.4.20 Sea level is projected to increase at a rate of 2-8 cm per decade (20-80 cms by 2100) for the IS92 emission scenarios, a rate several times faster than experienced during the last 100 years. Reef accretion rates are typically 1- 10 cm per decade and hence corals should not be particularly vulnerable to increases in sea level unless accretion rates significantly decrease due to warmer sea temperatures. Vulnerability of Socio-Economic SectorsAgriculture4.21 Based on several model projections of future global food production, one view holds that global food availability would increase relative to population. An alternative view is that the historical trend of generally improving food supply will not be continued because of resource degradation and the inability of technological advances to keep up with population growth. Climate change would represent yet another stress on some of the world's agroecosystems.4.22 Agricultural productivity is sensitive to two broad classes of climate-induced effects on the quality and quantity of harvestable yield: direct effects from changes intemperature, water balance, atmospheric composition and extreme weather events; and indirect effects through changes in soils and the distribution and frequency of infestation by insects, diseases, weeds, and other predators. The vulnerability of agricultural production to climate change depends not only on the physiological response of the affected plants, but also on the ability of the affected socio-economic systems of production to cope with fluctuations in yield. 4.23 Recent studies have confirmed the conclusion of the IPCC 1990 Assessment that the aggregate effects of climate change on global agricultural production are likely to be small to moderate. However, global food supply is only one aspect of the issue and large differences are projected at local and regional scales. Studies tend to show more negative impacts for areas in the tropics where many of the world's poorest people live and where individuals are currently at greatest risk of hunger. Large yield losses in one locality and for one type of crop are balanced by large gains in other localities, both within and among countries (e.g. frequently more than +/- 20 %). Low- latitude, low-income populations depending on isolated, dryland agricultural systems in semi-arid and arid regions are particularly vulnerable. Many of these populations at risk are in Sub-Saharan Africa, South, East and South East Asia, as well as some islands in the Pacific. The risks may be underestimated because the studies do not include indirect effects of climate change on agriculture via insects, weeds and diseases. 4.24	Several major food crops (e.g., wheat, rice, and soybeans), should benefit fromimproved water use efficiency and increased productivity because of the interaction of increased atmospheric concentrations of carbon dioxide and their particular photosynthetic process (the so-called CO2 fertilization effect). On average, such crops will see a 30 percent increase in yield (for a doubled carbon dioxide environment), although the variation in response is wide (-10 to +80 percent). Temperature increases at high latitudes are likely to increase crop growth, through lengthening and intensifying of the growing season, assuming adequate availability of water and suitable soils. At low and low-to-mid latitudes, where temperatures are already high, temperature increases may cause increases in the frequency of heat stress on crops and decreases in available water through higher rates of evaporation and transpiration. In contrast, some other major crops (e.g., sugar cane, maize, millet, and sorghum) and some common weeds, which have different photosynthetic processes, will benefit less from increased carbon dioxide concentrations.4.25 Adaptation will be important to limit losses or take advantage of changing climatic conditions. Major classes of agricultural adaptation options considered in the literature include: (i) breeding new crops and cultivars for changed climate conditions; (ii) improved management strategies such as altered irrigation practices or planting times; (iii) adding nutrients to take full advantage of the projected CO2 fertilization effect; (iv) employing soil conservation and protection strategies; and (v) research on pest control. The incremental costs of adaptation could be a serious burden for developing countries. Coastal Zones and Small Islands Infrastructure 4.26 Sea-level rise can have a number of negative impacts on tourism, ports and harbours, human settlements, natural freshwater systems in coastal areas, agriculture, insurance industry, and cultural systems and values. Climate change projected for the IS 92 emission scenarios would increase the vulnerability of coastal populations to flooding. Currently an annual average of about 50 million people experience flooding due to storm surges. A 50 cm sea-level rise would increase this number to about 92 million and a 1 meter sea-level to about 118 million. The estimates will be substantially higher if one incorporates population growth projections. Developing country populations, such as those of Bangladesh or China, will be more vulnerable because their existing sea and coastal protection systems are less well established and also because their population growth rates are higher, putting more people at risk. For such countries, sea-level rise could force internal or international migration of populations. 4.27 A number of studies have evaluated the sensitivity to a 1 meter sea level rise (the upper end of the range projected for the IS 92 scenarios). These studies show that small islands and deltaic areas are particularly vulnerable with land losses, for example, of 0.05% in Uruguay, 1.0% in Egypt, 6% in the Netherlands, 17.5% in Bangladash and about 80% in the Marshall Islands. In some cases, such land losses can be reduced through coastal protection measures. However, there are major technical, environmental and economic constraints in implementing them. While annual adaptation/protection costs for these nations are relatively modest (about 0.1% GDP), average annual costs to many small island states and deltaic coasts are much higher amounting to several percent of GDP assuming that adaptation is at all possible. Many nations face loss of capital value in excess of 10% of GDP. Large numbers of people would be affected, e.g., about 70 million in both China and Bangladesh. 4.28 For coastal states with access to financial and technical resources, a wide array of management options are available to prevent, ameliorate or compensate for coastal-zone damages encountered under current climate variability and which could be encountered under future climate change. The emphasis in the past has been on engineering responses to coastal erosion and protection against flooding, with action often being triggered in response to an extreme event. Heavily populated industrial and urban areas are primary candidates for structural protection measures such as dikes, seawalls and breakwaters. These are high-cost options. Economic evaluation principles, especially risk assessments and cost-benefit analyses, should be of use in deciding whether to protect or retreat. In addition, the range of options includes non- structural adaptation such as zoning, land-use regulation and flood-damage insurance, with emphasis on a precautionary approach in locating investments. Conclusions on Vulnerability to Climate Change 4.29 There are a number of important conclusions that are not specific to any one ecological or socio-economic system: The sensitivity of ecological and socio-economic systems is not distributed evenly across the globe. Various aspects of climate affect systems differently. Some systems are sensitive to changes in mean climate; some to changes in the frequency and magnitude of extreme events; some to the rate of climate change; and some to changes in climate variability; While there would be some beneficial effects of climate change, there would be many adverse effects, with some being potentially irreversible, e.g., loss of biological diversity and loss of land; The ability to adapt to a changing climate is improving because of technological advances. However, the majority of human beings do not have access to such technological advances; The vulnerability of human health and socio-economic systems, and to a lesser extent ecological systems, depends upon economic circumstances and institutional infrastructure. This implies that systems are typically more vulnerable in developing countries where economic and institutional circumstances are less favourable;  Both the rate and magnitude of climate change are important. Hence, the rate of change of atmospheric concentrations of greenhouse gases may be as important as the stabilization level. A slow rate of climate change permits, but does not assure, non-disruptive adaptations in the structure and function of natural ecosystems, agriculture, and other socio- economic systems. Rapid change, on the other hand, may preclude non-disruptive adaptations; Uncertainties associated with projections of changes in local and regional climate present a challenge in designing adaptation strategies;  Incorporation of environmental and sustainability concerns into resource-use and development decisions and plans for infrastructure investments will enhance society's resilience to climate change.4.30 Table 3 summarizes the major threats and adaptation potential for natural, agricultural, and socio-economic systems. For the present rate of greenhouse gas emissions, global mean temperatures are projected to increase by between 0.1 C and 0.4oC per decade, with associated changes in other climatic factors, including precipitation. For natural ecosystems ,this would result in: (i) loss of species from forests as they fail to migrate rapidly enough to keep pace with such rates; (ii) loss of montane because migration would not be feasible for them as climate warms; and (iii) the destruction of coral reefs as the surrounding waters become too warm for the coral reef organism to reproduce and grow. Adaptation options for natural ecosystems are limited at best and would be very costly for broad scale implementation.  For food production, the consequences are most important at the local and regional levels. Agricultural systems in the developing countries of the tropics may be subject to increasing drought stress as the result of increased temperatures and reductions in precipitation. While technologies exist that could permit these systems to adapt to climate change, these technologies are often costly and therefore, may not be available to the countries that need them most. Human health and habitats are also at risk. For human health, a critical issue is possible increase in the incidence of vector-borne diseases such as malaria, especially in developing countries, and heat spells. Some parts of human habitats are likely to be destroyed by sea level rise and possible change in extreme weather events, causing migration of populations and placing additional stresses on already stressed social and political systems.TABLE 3SystemConsequenceAdaptation PotentialNatural Ecosystems:ForestsMontaneCoastal marshesCoral reefsLoss of species andgoods and services- ditto -- ditto -- ditto -Limited but costlyNon-existentVery limited but costlyNon-existentCryosphereLoss of glaciers, reducedsnowpackNon-existentAgricultureRedistribution, reducedyields in developingcountriesHigh but potentially costlyWater resourcesReduction in quantity andqualityHigh but costlyHuman settlementsCoastal and river floodingthreatening humanhabitat, causingdisplacement of peopleHigh but costlyHuman healthLoss of life, increaseddiseasesHigh but costly4.31 The economics literature contains a few aggregate estimates of the damages associated with a doubled CO2 concentration scenario. The estimates suggest that the associated warming would impose net damages equivalent to between 1.5% and 2.0% of world GDP on the present world economy. The regional variation in climate change damage would be substantial. However, there is little agreement across the studies about the magnitude of each category of damages or the relative ranking of the damage categories. Climate changes of this magnitude are not expected to occur for some decades and damages in the interim would be smaller. Some impacts, such as potential loss of low-lying areas, may be so significant physically and culturally that no monetary compensation would be sufficient. [NOTE: This paragraph will be made consistent with the 1995 Summary for policymakers of Working Group III.] Box 3The Framework Convention on Climate Change defines its objective (in Article 2) as stabilization of greenhouse gas concentrations in the atmosphere. Stabilization of emissions is often confused with stabilization of concentrations. Stabilization of emissions refers to a situation in which the GHG emissions in one year are the same as in the previous year.A stable concentration of a greenhouse gas refers to a state of dynamic equilibrium between the atmosphere, the ocean, and the terrestrial ecosystems. In this state of equilibrium, aggregate annual emissions of the gas from the surface of the planet are just balanced by removals of the same gas from the atmosphere. As a consequence, the concentration of the gas in the atmosphere remains unchanged. 5.1 As noted in the previous sections, some damage and adaptation costs of a human-induced climate change may be unavoidable. Even though there are major uncertainties about the magnitude and pace of a climate change due GHG emissions and about how serious it will be, the uncertainties do not diminish the risk itself but merely make its quantification difficult. Risks would vary markedly from country to country.5.2 An analytical approach to the issue of stabilization of concentrations is presented in the following pages. It is purely illustrative and designed to expose the dimensions of the problem only. It has no implication of policy recommendation whatsoever. 5.3 The Convention in Article 3 (see box 4) provides that the type of strategy to be used in reaching its objective should be comprehensive, covering all relevant sources, sinks, and reservoirs of greenhouse gases. It is important to keep this provision in mind when synthesizing scientific-technical information relevant to Article 2. Because of the multiplicity of sources and the variety of gases involved, there is no single strategy for stabilization.Box 4In their actions to achieve the objective of the Convention and to implement its provisions, the Parties shall be guided, inter alia, by the following: 1. The Parties should protect the climate system for the benefit of present and future generations of humankind, on the basis of equity and in accordance with their common but differentiated responsibilities and respective capabilities. Accordingly, the developed country Parties should take the lead in combating climate change and the adverse effects thereof. 2. The specific needs and special circumstances of developing country Parties, especially those that are particularly vulnerable to the adverse effects of climate change, and of those Parties, especially developing country Parties, that would have to bear a disproportionate or abnormal burden under the Convention, should be given full consideration. 3. The Parties should take precautionary measures to anticipate, prevent or minimize the causes of climate change and mitigate its adverse effects. Where there are threats of serious or irreversible damage, lack of full scientific certainty should not be used as a reason for postponing such measures, taking into account that policies and measures to deal with climate change should be cost-effective so as to ensure global benefits at the lowest possible cost. To achieve this, such policies and measures should take into account different socio- economic contexts, be comprehensive, cover all relevant sources, sinks and reservoirs of greenhouse gases and adaptation, and comprise all economic sectors. Efforts to address climate change may be carried out cooperatively by interested Parties. 4. The Parties have a right to, and should, promote sustainable development. Policies and measures to protect the climate system against human-induced change should be appropriate for the specific conditions of each Party and should be integrated with national development programmes, taking into account that economic development is essential for adopting measures to address climate change. 5. The Parties should cooperate to promote a supportive and open international economic system that would lead to sustainable economic growth and development in all Parties, particularly developing country Parties, thus enabling them better to address the problems of climate change. Measures taken to combat climate change, including unilateral ones, should not constitute a means of arbitrary or unjustifiable discrimination or a disguised restriction on international trade.5.4 The direct radiative forcing due to enhanced concentrations of all greenhouse gases in the atmosphere was about 2.4 W/m2 in 1994 of which 1.55 W/m2 was due to increased carbon dioxide (see figure 2). The forcing due to all greenhouse gases in 1994 was equivalent to that due to a concentration of CO2 alone of (about) 420 ppmv. Thus the figure of 420 ppmv CO2 is the equivalent CO2 concentration for the radiative forcing of 1994. (For comparison, the actual CO2 concentration in 1994 was 356 ppmv.) The concept of equivalent CO2 concentration is helpful in approaching the simultaneous stabilization of all the GHG concentrations in the atmosphere - the so-called comprehensive approach.5.5 The individual reductions in emissions for stabilizing the concentrations of carbon dioxide, methane, nitrous oxide and halocarbons, and thereby implicitly stratospheric ozone, can be combined in different ways for stabilizing the total global radiative forcing in the comprehensive approach. To these, the forcing due to changes in tropospheric ozone and aerosols should be added; this cannot be easily done - and has not been done in the illustrations that follow - because of limited information on their future concentrations and geographical distributions. 5.6 Different greenhouse gases have different residence times in the atmosphere.  Tropospheric ozone and tropospheric aerosols have residence times of the order of a day and of a week respectively. Their concentrations are governed directly by the emissions of their precursors. Methane has a residence time of 12-17 years and will stabilize in about 50 years if its emissions were stabilized. Other greenhouse gases have longer residence times spanning many decades to centuries. The concentration of each of them is governed by some (determinable) fraction of its emissions accumulating in the atmosphere over decades to centuries. The concept of cumulative emissions of the long-lived greenhouse gases helps in understanding the complexities associated with stabilization. Carbon Emissions and Stabilization of CO2 Concentrations 5.7 Because carbon dioxide presently contributes more than half of the direct annual increase in the radiative forcing (with probably a greater fraction in the future), stabilizing CO2 concentrations in the atmosphere would appear to be necessary for stabilizing future total global radiative forcing.5.8 Profiles of atmospheric CO2 concentrations can be constructed assuming smooth variations in the rates of increase in its concentrations from the current value to different stabilization levels. Such profiles for concentrations of 350, 450, 550, 650 and 750 ppmv, published previously by the IPCC, are shown in figure 11. It is possible to choose other stabilization levels and time scales and pathways for achieving them. FIGURE 115.9 Carbon cycle models can be used to derive profiles of total anthropogenic carbon emissions corresponding to the concentration profiles shown in figure 11. Such emission profiles are shown in figure 12. More rapid increases in the concentrations in the beginning would lead to higher initial emissions than those given in figure 12, but greater reductions would be necessary later if stabilization were to be achieved at any given concentration. Emissions to achieve the stabilization levels illustrated here are, after some decades, lower than those of the IS 92 scenarios. Also, all emissions eventually have to be below the 1990 emissions and be maintained there. Emissions would be greater for stabilization levels higher than those shown in figure 11. FIGURE 125.10 Figure 13 provides, as an illustration, estimates of cumulative global carbon emissions until the year 2100 for various stabilization levels of CO2. The figure also illustrates cumulative emissions for the IS92a emissions scenario. (These values may be compared to the estimated emissions of 240 GtC from fossil fuel use between 1860 and 1994, about 60% of which came from burning solid fuels.) FIGURE 135.11 Because of the inertia of the carbon cycle and of the socio-economic system, it would take a decade or two for significant deviations from non-intervention emissions profiles to become noticeable. The selection of the illustrative concentration profiles (figure 11) assumes this inertia and consequently the emissions in figure 12 do not deviate much from each other in the first decade or two. A delay beyond a few years hence to initiate appropriate measures to reduce emissions would result in steeper rates of reduction later, if, for example, stabilization of CO2 is aimed for at 550 ppmv or below.5.12 Given cumulative emissions and population figures, global annual average per capita carbon emissions can be derived. Figure 14 shows values of annual average per capita emissions derived for stabilization of CO2 concentrations at 450, 550, 650 and 750 ppmv, taking the corresponding cumulative emissions from figure 13 and assumingthe UN median estimates of population growth to about 9.5 billion in 2050 and about 11 billion persons in 2100. If other population estimates were used, the values would change accordingly.FIGURE 145.13 The average annual per capita carbon emission for the globe as a whole due to fossil fuel burning is at present about 1.1 tonnes. (In addition, a net of 0.2 tonne is emitted from deforestation and land-use change.) It is on average about 2.8 tonnes from fossil fuels in developed countries (including countries with economies in transition). National annual per capita emissions vary between 1.5 and 5.5 tonnes. In the developing world, the average annual per capita emission is about 0.5 tonne (with a range of 0.1 to about 2.0 tonnes). The future global average annual per capita emissions from fossil fuel use cannot exceed the current global average by very much if the atmospheric concentration of carbon dioxide is to remain at 550 ppmv (about double the pre-industrial level) or below. Larger annual per capita emissions would be possible if stabilization at higher concentrations were to be aimed for. Stabilization of Other Greenhouse Gases5.14 Methane: The atmospheric residence time for methane, at 12 to 17 years, is comparatively short. If current emissions and removal rates were held constant, atmospheric concentration would stabilize at about 1.9 ppmv (compared to the 1994 concentration of 1.73 ppmv and the estimated pre-industrial concentration of about 0.65 ppmv). On the other hand, stabilization at present concentration would require a reduction in the emissions by about 10%. 5.15 Nitrous oxide: The lifetime of nitrous oxide in the atmosphere is about 120 years. A reduction of anthropogenic emissions by about 50% is required in order to stabilize the concentration at present level. If emissions were stabilized at the present level, the concentration would increase to about 400 ppbv during the next century (compared to 310 ppbv today and an estimated pre-industrial concentration of approximately 280 ppbv).5.16 Halogen-containing compounds and stratospheric ozone: The concentrations of the CFC-gases in the atmosphere are expected to stabilize gradually in the next few years. A very slow decrease in their concentrations is then expected and a return towards natural stratospheric ozone concentrations during the 22nd century. 5.17 Other than CFCs, most other halocarbons have lifetimes in the atmosphere of years to a few decades. Their contributions to human-induced radiative forcing is slight at present. The lifetimes of HCFC-22 and HFC-134 are 13 and 18 years respectively. The projected increase in their emissions in the IS92a scenario implies that a decade or two into the next century, these gases would be contributing more than 0.1 W/m2 to the radiative forcing.5.18 Some very long-lived greenhouse gases are also being emitted into the atmosphere, although still in very small amounts (e.g., sulphur hexafluoride or SF6 and perfluoromethane or CF4 with lifetimes of 3,200 and 50,000 years respectively). The long lifetimes of these gases make them very difficult to stabilize implying rising concentrations for centuries after steps to eliminate their emissions are put in place. 5.19 Concentrations of tropospheric ozone and of tropospheric aerosols respond quickly to changes in the emissions of the precursors, as has been mentioned earlier. The Concept of Equivalent CO2 Concentrations and the Comprehensive Approach5.20 As an illustration, let us consider the stabilization of radiative forcing due to all greenhouse gases, i.e., the radiative forcing due to the equivalent carbon dioxide concentrations of, say, 450, 550, 650 and 750 ppmv. Other choices of concentrations can equally well be made. For each case, we ask what level of concentration is required for carbon dioxide alone for stabilization, if simultaneously demanding that methane and nitrous oxide are either (i) stabilized at present levels (hereinafter referred to as "N2O and CH4 stabilization now") or (ii) permitted to increase according to IS 92a scenario until 2050 and stabilize thereafter (hereinafter referred to as "N2O and CH4 stabilization in 2050"). It is possible, then, to deduce the approximate levels of stabilization for carbon dioxide required for stabilizing equivalent CO2 concentrations. These are illustrated in table 4.5.22 It may be seen from the table that, for example, stabilization of equivalent carbon dioxide concentration at 450 ppmv would require a stabilization in CO2 concentration alone at about 385 and 365 ppmv, respectively, for N2O and CH4 stabilization now and in 2050. Stabilization of equivalent CO2 concentration at 550 ppmv (equal to almost a doubling of the pre-industrial CO2 concentration) would require a stabilization of CO2 alone at about 480 and 455 ppmv, respectively, for N2O and CH4 stabilization now and in 2050.TABLE 4Stabilization level ofequivalent CO2, ppmv450550650750Enhanced radiativeforcing, W/m23.24.45.46.2Increase of global meantemp. at stabilization, C1.1-3.31.5-4.51.8-5.62.1-6.3Maximum CO2concentration for "N2O andCH4 stabilization now"385480565650Maximum CO2concentration for "N2Oand CH4 stabilization in2050"365455535610Uncertainties in the CO2 concentrations in the table are +/- 30 ppmv. 6.1 An extensive array of technologies and policy measures exist to mitigate anthropogenic greenhouse gas emissions in the energy, industrial, and agriculture sectors, and to enhance natural sinks through forestry and land management. The rate and degree of diffusion of the technologies will be influenced by fiscal and regulatory measures, additional research, and dissemination of information. The policy measures will be more effective if they are developed through consultations with various stakeholders and are carefully tailored to local situations. Energy Sector6.2 The options for reducing energy-related emissions fall into two general categories: (i) those that utilize alternative energy supply technologies and (ii) those that reduce the demand for energy in sectors such as industry, residential/ commercial buildings and transportation.Energy Supply Options6.3 Technologies exist with potential to reduce GHG emissions substantially over 50-100 years. They can be developed to provide energy services at projected costs comparable to the projected costs of conventional sources of energy. Much of the world's commercial energy system will be replaced at least twice by the year 2100. Such replacement times provide opportunities to change the energy systems in the course of normal investment cycles.6.4 In the energy supply sector, GHG emissions reductions are possible through the following technology options, listed in no particular order of priority: * More efficient conversion of fossil fuels New technology offers considerably increased conversion efficiencies. For example, the technical efficiency for coal-fired power generation can be increased from the present world average of 30% to the 43% available with today's most efficient technology and to over 60% in the longer term. Using combined-cycle gas turbines for electricity generation, conversion efficiency can be improved to 54% using currently available technology. Higher efficiencies are possible with combined heat and power schemes and using such schemes in district heating arrangements. * Increasing the use of low carbon fossil fuels and suppressing emissions Emissions can be reduced by switching from coal to oil to natural gas because natural gas contains 1.8 times as much energy as coal per unit carbon in the fuel. Large resources of natural gas exist in many areas. Low-capital cost, high-efficiency advanced combined cycle technology can reduce electricity costs in areas where natural gas is becoming the preferred fuel. Use of natural gas as a substitute for oil as a fuel in the transportation sector can be increased. Methane emissions from natural gas pipelines and from oil and gas wells and coal mines can be substantially reduced. * Increasing the use of renewable sources of energy Renewable energy sources are sufficiently abundant that eventually they could technically provide all of the world energy needs over the next century. In 1990, renewable sources contributed about 17% of the world's primary energy; of this, 2% was derived from solar, modern biomass, wind, geothermal and micro-hydro sources. Technological advances offer declining costs . In addition, renewable sources are often beneficial for local and regional environmental problems such as urban air pollution and acid rain. In the case of large-scale biomass plantations, established on currently non-forested lands, there is not only an increase in the amount of carbon sequestered, but the biomass used as fuel can replace fossil fuel, increasing the effective rate of carbon sequestration. Moreover, there are the prospects of rural income and employment generation, but these need to be balanced against such concerns as land use constraints, loss of biodiversity and natural habitats and other environmental issues. In the case of hydropower, most large-scale plants require dams and reservoirs, which can give rise to significant social and environmental concerns such as dislocation of populations, water diversion, slope alteration, disruptions of ecosystems, etc. * Decarbonization of flue gases and fuels, and CO2 Storage The removal and storage of CO2 from power-station stack gases, which is feasible, will significantly increase the production cost of electricity. Fossil fuels can be used to make hydrogen-rich fuels, applying conversion technologies such as fuel cells, but the by-product stream of CO2 will also require storage. Depleted oil and natural gas fields could be used for storing such CO2 at relatively low cost. * Increasing the use of nuclear energyNuclear energy could replace baseload fossil fuel electricity generation, if public concerns about reactor safety, radioactive waste disposal, and proliferation can be resolved. The long construction lead times and high capital costs make nuclear power a relatively inflexible option.Energy Demand Options6.5 Development of alternative sources of energy can be coupled to efforts to reduce existing inefficiencies in energy end-use. The technical potential for efficiency improvements on the demand side is large. Numerous studies have indicated that 10-30 % efficiency gains are feasible with current plant and equipment in many parts of the world at little or no cost. Using plant and equipment which presently yields the highest output of energy service for a given input of energy, efficiency gains of 50-60 % would be feasible in developing countries if requisite technology and financing became available.6.6 More than a third of global CO2 emissions come from the industrial sector through energy use and production processes. Other greenhouse gases are also emitted through production processes. Basic processes including production of iron and steel, chemicals, building materials and food account for more than half of all the energy used in this sector. Industry has made substantial reductions in energy intensity in the past two decades. Improvements in energy efficiency have, in some countries, permitted major increases in production with little or no increase in energy use. The potential in the relative short-term for efficiency improvements in the manufacturing sector in the leading industrial nations is around 25 percent. 6.7 Emissions reductions are feasible through other efficiency improvements (e.g., materials savings, co-generation and steam recovery, use of more efficient motors and electrical devices), recycling materials and switching to those with lower CO2 content, developing fundamentally new processes ("industrial metabolism" that uses less energy), lowering of materials intensity in manufacturing ("dematerialization"), and feasible reductions in the emissions of gases such as halocarbons, CH4 and N20 in industrial proceses such as production of iron, steel, aluminum, ammonia, etc. The emissions reductions and costs associated with specific technologies or approaches in each of these categories will vary, according to current patterns of industrial energy and materials use. Some countries, for example, use twice as much energy to produce a unit of steel as do other countries, thus affording opportunities to achieve significant emissions reductions.6.8 About 25% of global primary energy use in 1990 and 22% of CO2 emissions from fossil fuel use came from the transportation sector. This is the most rapidly growing source of GHG emissions. Whereas transportation world-wide consumed 68 EJ (1 EJ = 1018 J) in 1990, mostly in oil products, this sector could account for 65-170 EJ by 2025 and 30-520 EJ by 2100 according to the IS 92 and WEC scenarios. About 75 percent of energy use and greenhouse gas emissions in this sector are now in industrialized countries; greater growth is expected in developing countries in the future. Vehicle ownership and use ranges from 1.7 people per car or light truck in the USA to about 600 people per car in China. The nature of production and demand for vehicles in OECD countries influences the pattern of demand for transportation services throughout the world.6.9 Two key areas in this sector provide opportunities to reduce emissions significantly: Changing the design of all vehicle types to use more efficient drive trains, body shapes and materials, as well as switching fuels for propulsion. The potential for reduction in energy intensity through these measures in 2025, without losses in vehicle performance and size, is 35-60% in cars, 20-40% in heavy trucks, and 30-50% in aircraft. If performance and size were changed, greater energy intensity reductions could be achieved. More than 95% reductions in GHG emissions relative to 1990 levels are feasible per unit of transport service, if a switch to alternative fuel and electric powered vehicles using renewable energy sources is coupled with the foregoing reductions in energy intensity.  Altering land-use patterns, transport systems, mobility patterns and life styles to reduce the level of passenger miles travelled and freight transport activity, and shifting to less energy intensive transportation modes. 6.10 Actions to reduce greenhouse gas emissions from transport can simultaneously address other problems, including traffic congestion, high accident rates, noise and local air pollution including emissions of particulates, NOx and VOC that are precursors to tropospheric ozone. Much experience has already been gained in the effective use of policies such as fuel and vehicle taxes and fuel economy standards to encourage energy efficiency improvements. Research will continue to be needed to develop new propulsion systems and affordable, lightweight materials. 6.11 Human settlements currently account for about one-third of GHG emissions. The largest portion is in the form of CO2 emissions from energy use in buildings; most of the remainder is in the form of methane from solid waste and industrial/domestic waste water. While the range in current and projected energy use and emissions in this sector is large, the "best guess" estimate is 2% growth in energy use per year (assuming continuing economic growth and improvements in energy efficiency), leading to a doubling of energy use by 2030.6.12 Some examples of technologies that could cut projected growth in emissions by one-half over the next 35 years (and more in the longer run) without diminishing energy services include more efficient space-conditioning systems, reduced heat losses through walls, ceilings and windows, more efficient lighting and more efficient appliances (refrigerators, water heaters, cook stoves, etc). Other technologies that can capture, reduce, or prevent methane emissions (e.g., in landfills) are also increasingly available. 6.13 Activities that provide the best opportunities for reductions in emissions growth in buildings are: (i) support for energy efficiency policies (energy pricing strategies, regulatory programs including minimum energy efficiency standards for buildings and appliances, utility demand-side management programs, and market pull and demonstration programs that stimulate the development and application of advanced technologies); (ii) enhanced research and development in energy efficiency; (iii) enhanced training and added support for financing of efficiency programs in developing and transitional economy countries.Land Management6.14 There are options for increasing carbon storage and/or for reducing emissions of carbon dioxide, methane and nitrous oxide in managed sectors such as agriculture and forestry. In general, mitigation options in these sectors focus on more sustainable uses of existing resources and have other positive effects such as reducing air and water pollution, slowing the rate of land degradation and conserving biodiversity. The mitigation potential in these options, however, appears insufficient to bring about reductions in emissions to stabilise greenhouse gas concentrations. 6.15 Estimates of the total amount of carbon that could be sequestered in the forestry and agriculture sectors over the next 50 years range from 90 to 150 GtC, which is equivalent to 8 to 40% of the projected cumulative fossil fuel emissions over the same period in the IS92 emission scenarios. Significant uncertainties are associated with estimating the amount of carbon that can thus be conserved and/or sequestered. They include the magnitude of the CO2 fertilisation effect, interactions between carbon-storage potential and changes in temperature, precipitation and the availability of other nutrients, changes in the frequency of disturbances such as fires and pest outbreaks, land availability for forestation and regeneration programmes, lack of information about current land-use practices and rates at which deforestation can be reduced. Carbon sequestration through improved management of forests and agricultural lands6.16 The categories of promising forestry activities include: (i) management and conservation of existing carbon in forests as for example by slowing deforestation; (ii) expanding carbon storage by increasing the area and/or carbon density of native forests; and (iii) increasing the use of forest biomass in products such as bio-fuels and long-lived wood products which can substitute for fossil-fuel intensive products. Under baseline conditions (i.e., today's climate and no change in the estimated availability of land over the period), 60 to 87 GtC could be conserved and sequestered over the period 1995 to 2050 by slowing deforestation and promoting natural forest regeneration. The tropics have the potential to conserve and sequester the largest quantity of carbon (80 percent of the global total in the forestry sector), more than half of which would be due to promoting forest regeneration and slowing deforestation. The temperate and boreal zones could sequester about 20 percent of the global total, mainly in North America, temperate-zone Asia, the former Soviet Union, China, and New Zealand. Projections of increased demand for agricultural land in the tropics could reduce these estimates significantly.6.17 The cumulative cost, excluding land costs, to conserve and sequester the above amounts of carbon range from US$250 billion to $300 billion (thousand million) , at a unit cost ranging from $2 to $8/tonne of carbon. Realised costs will depend on national circumstances, including costs of land, establishment of infrastructure, tree nurseries, training programs and protection. Costs per unit of carbon sequestered or conserved generally increase from slowing deforestation to establishing plantations. 6.18 In agriculture, a variety of practices could increase storage of carbon. Recent studies indicate that 20-30 GtC could be sequestered through improved management of agricultural soils, including return of crop residues and reduced tillage. An additional9-37 Gt C could be sequestered due to restoration of degraded agricultural lands and improved management of rangelands.Reductions in methane and nitrous oxide emissions through improved management of agricultural practices6.19 Emissions from the agriculture sector contribute now about one-fifth to overall anthropogenic emissions. This sector accounts for about 50% of anthropogenic methane, and about 70% of anthropogenic nitrous oxide, emissions. 6.20 Significant decreases in methane emissions from agriculture can be achieved through improved nutrition of ruminant animals and better management of paddy rice fields (e.g., irrigation, nutrients, use of new cultivars). Further methane reductions are possible with altered treatment and management of animal wastes and by reducing agricultural biomass burning. These practices could be combined to reduce methane emissions from agriculture by 25 to 100 Tg/yr (estimated annual anthropogenic methane emissions in the 1980s were 300-450 Tg/yr). The projected increase in world population during the next century would imply larger, however, methane emissions. 6.21 Sources of nitrous oxide in agriculture are mineral fertilisers, legume cropping, animal waste and biomass burning. The N2O emissions could be reduced by 0.3 to 0.9 Tg N/yr by improving fertiliser and manure-use efficiency with presently available techniques (estimated emissions in the 1980s were 3-8 Tg/yr). Other benefits of mitigation options6.22 Measures to reduce greenhouse gas emissions often yield additional economic benefits (such as reduced traffic congestion) and/or environmental benefits (such as reduced emissions of urban smog precursors). What matters from a policy perspective is the net cost (total cost adjusted for other benefits and costs) of a mitigation or an adaptation option apart from its climate change benefits. The magnitude of these secondary benefits depends on local circumstances. Studies for European countries and the United States indicate that secondary benefits could offset 30% to 100% of abatement costs. For many countries and peoples there are problems which are perceived as being more pressing than potential climate change. These include provision of basic needs, and regional and local pollution. By raising efficiency and energy use, local and regional pollution can be curbed, and costs reduced, releasing financial and other resources for other needs. These are sometimes described as secondary benefits, although given the different local and regional priorities, they may in some countries well be considered primary benefits and climate change mitigation a valuable secondary benefit.Barriers to Implementing Mitigation Options 6.23 Historically, gaps have existed between the most cost-effective technologies available or rapidly becoming available and those actually in use. There are also gaps between what existing industrial plants and equipment should be able to achieve in terms of efficiency and what is actually being achieved. 6.24 There are time lags in exploiting new technologies, because of the risk of financial losses, if the new technology is introduced before earlier investments are fully amortized. Several other factors may be even greater discouragements : uncertainty about the costs of the new technology (operating and maintenance costs, reliability and training in of new skills) and the level of service provided;  uncertainty about the long-term prices, lack of information, poor decision processes, imperfect market structures, institutional deficiencies including restrictive government regulations and property tenure;  the status of economic and cultural development;  the motivation for innovation, which is ultimately determined by prevailing incentive systems that are often based on societal values other than the risk of climate change.Addressing Barriers6.25 Many policy instruments are available to facilitate the penetration of lower carbon intensity technologies and modified consumption patterns. They include energy pricing strategies, e.g., carbon or energy taxes and reduced energy subsidies, incentives such as provisions for accelerated depreciation and reduced cost for the consumers, tradable emission permits, reduction of market imperfections, negotiated agreements with industry, new standards and product labelling and increased support for research and development. (See chapter 11 of the contribution of Working Group III to the IPCC Second Assessment Report for more detailed analyses of the advantages and disadvantages of such instruments.)6.26 The optimal mix of policy instruments will vary from country to country depending upon political structure and societal receptiveness. It will also vary between economic sectors and on whether or not there is a tradition of negotiated agreements between government and industry.6.27 The best combination of policies will also vary over time. Studies indicate that high priority should initially be given to the removal of existing barriers, to the implementation of best available technologies, elimination of barriers due to existing investments in raw materials, processes and products, vested interests, institutional inertia and lack of information and awareness. The longer-run costs of abatement can be reduced by improved price signals, research and development and emphasis on effective information programmes.6.28 No significant shift in technical consumption patterns towards low-emitting products and services is likely if relative prices do not encourage it. The impact of price signals, whether achieved through taxes or incentives or tradable permits, will ultimately depend on: continuity of the policy, including confidence in its long-run stability, prevention of "free riders", and progressiveness in implementation;  the way in which revenues arising from the price signals are recycled in the economy.6.29 The use of taxes to alter market prices requires careful consideration in order to avoid market distortions and decreasing economic efficiency, which could negate the overall impact of climate change policies. On the other hand, if the proceeds of a carbon tax are used to reduce other taxes which have a distortionary impact on labour and capital, an effective use of the tax might be achieved.. Such a move might simultaneously help to reduce long-term concerns about climate change with concern about unemployment and public expenditure levels and their allocation. Combining Mitigation Options6.30 Each of the technologies and measures discussed above has the potential to contribute to the reduction of GHG emissions and the enhancement of GHG sinks. Evaluating the effectiveness of these individual options is relatively straightforward. However, understanding the overall effectiveness of combinations of these options to reduce energy demand and increase use of energy resources with lower emissions requires development and evaluation of an integrated mitigation strategy. There is no unique path to realization of deep emissions reductions. 6.31 Several modelling experiments suggest that achieving deep reductions in emissions is possible at low cost over the long term. However, it is not possible to identify a least-cost future energy system for the longer term, as the relative costs of options depend on resource constraints, technological opportunities that are imperfectly known and on actions to be taken by governments and the private sector. Published estimates of the cost of mitigating greenhouse gas emissions and increasing carbon sequestration vary substantially. Despite significant progress in reconciling methodological and technical differences concerning model structures and data, major differences concerning underlying assumptions remain. These differences reflect alternative perspectives about such factors as the efficiency of energy markets, distortions in fiscal systems, availability of new technologies and the costs o implementation.7.1 Article 2 provides the framework for international cooperation in decision-making to address climate change. Article 3 sets out a number of principles for the implementation of the provisions of the Convention including Article 2. 7.2 Climate change presents the decision-maker with a formidable set of challenges:  very long planning horizon; large uncertainties; wide regional variations in causes, potential impacts - positive and negative - and costs; many greenhouse gases involving many sectors of societies. Yet another challenge is that the atmosphere is an international public good in that all countries are affected by each country's greenhouse gas emissions. Decisions in the short term should be taken with the long-term perspective in mind and vice versa. 7.3 These challenges have several characteristics that affect the decision making process: Long time scales of relevance suggest the need for early decision-making: (a) from emissions of greenhouse gases to the response of the climate system, (b) for adaptation of the ecosystems and natural resource systems to climate change, (c) for the climate system to come to equilibrium once greenhouse gas concentrations are stabilized, (d) for turnover of infrastructure and capital in the energy, industrial, transport and commercial/residential buildings sectors; and (e) the nature of international and national political processes.  While knowledge about climate change, its potential effects, and the advantages and disadvantages of various responses is increasing, there are still critical uncertainties regarding basic scientific and socio-economic issues. These uncertainties make it difficult to assess the risks posed by anthropogenic climate change, and increase the costs of insurance and new infrastructure.  Some potential impacts of climate change, such as loss of biological diversity and land, are irreversible. Some other impacts may be disproportionate to the changes in climate because of likely thresholds.  Impacts and the costs of mitigation and adaptation will vary both within and among countries raising issues of intranational and international equity. Perceived equity is an important element for legitimizing decisions and promoting cooperation. Sustainable development is often defined as "meeting the needs of the present without compromising the ability of future generations to meet their own needs". Because future generations are not able to influence directly the actions taken that will affect their well-being, climate change raises issues of inter-generational equity. These issues are generally addressed through the choice of a discount rate. Making the choice is a question of values, a profound ethical question, since the choice inherently compares the costs of present measures against possible damages suffered by future generations if no action is taken.  Economic efficiency requires that emission reductions occur where their cost is lowest, irrespective of who bears the financial burden. For purposes of analysis, it is useful to separate efficiency (what to do and how it is done) from equity (who bears the burden) considerations. This analytical separation can be implemented in practice only if effective institutions exist or can be created for appropriate redistribution of climate change costs. Measures to reduce greenhouse gas emissions and increase sinks can yield multiple additional benefits (e.g., rural employment generation, reduced traffic congestion, reduced emissions of urban smog precursors). The magnitude of these other benefits depends on local circumstances. When other priorities such as provision of basic needs or reducing regional or local pollution are perceived to be more important, actions taken to address them are generally likely to advance the goal of greenhouse gas mitigation simultaneously.  Radiative forcing of climate warming is caused by anthropogenic emissions of other greenhouse gases in addition to CO2. Cooling due to anthropogenic aerosols should also be taken into account. To achieve stabilization of all greenhouse gas concentrations, the concept of equivalent CO2 can be used. It is possible to increase the efficiency of strategies to stabilize concentrations by reducing initially the emissions of those gases which have the lowest marginal costs of control.7.4 A stabilization objective could be based upon a benefit-cost analysis, a cost-effectiveness approach or an absolute standard. Benefit-cost analysis or trade-off analysis attempts to identify the most efficient climate change strategy by balancing the costs of mitigation and adaptation measures against the damages avoided by these measures, including non-market costs such as damage to ecosystems or loss of species.7.5 A cost-effectiveness approach would begin with a maximum atmospheric concentration of greenhouse gases, based on an assessment of the risks associated with different concentrations and the costs of achieving those concentrations. The objective would be reach the specified target at the lowest total cost, including non-market costs. 7.6 An absolute standard approach would define a maximum atmospheric concentration of greenhouse gases that is considered to constitute "dangerous anthropogenic interference with the climate system" on the basis of predicted biophysical impacts of climate change, independent of the economic and social impacts of achieving this concentration. Economists generally reject the absolute standard approach, believing that trade-offs are always relevant. At the same time, it is recognized that many governments set policies on the basis of underlying legislation that is itself based on an absolute standard. Even under the absolute standard approach, mitigation and adaptation measures should be chosen to achieve the standard at least cost. 7.7 Given the interrelated nature of the global economic system, attempts to mitigate climate change through actions in one region or sector may have offsetting effects which increase emissions from other regions or sectors. These emission reduction leakages can be reduced through coordinated actions of groups of countries. Considerations for moving towards the ultimate objective 7.8 Article 2 of the Convention introduces several specific considerations to be taken into account in moving towards stabilization of GHG concentrations. The ability of ecosystems to adapt naturally depends critically on the rate of climate change. Analyses suggest that in some regions significant reductions in food security may occur but that global food production may not be seriously threatened. Thus, with free, fair and unrestricted trade, food security is unlikely to be threatened by the risks of climate change due to a greenhouse warming. Economic development may not be able to proceed in a sustainable manner in those areas where climate change could worsen existing water and food shortages, public health concerns and exposure to natural disasters.7.9 A number of levels for stabilization of greenhouse gas concentrations have been explored in the scientific literature. While some effects of different levels can be identified or inferred from this literature (for example, a rate of global mean temperature increase of 0.1 C per decade may result in significant loss of species from forest and montane ecosystems), there is not enough information to specify completely the consequences of various concentrations or emissions pathways for ecosystems, food production, or sustainable development.7.10 Nonetheless, a number of observations of relevance to policymaking can be made about stabilization and potential options to achieve it:  The impacts of stabilizing at a level of 750 ppmv (of equivalent CO2, implying a value for CO2 alone close to thrice its pre-industrial concentration) or higher are outside documented past fluctuations in greenhouse gas concentrations and climate and associated impacts; In order to achieve stabilization near or below 750 ppmv, emissions will have to fall substantially below those of the IPCC IS92a emissions scenario within the next few decades. Higher initial emissions would require more drastic reductions later to achieve stabilization; Stabilizing concentrations at any of the levels illustrated in this discussion requires initiation of near term measures including: (i) research and development on energy efficiency improvements, alternative sources of energy, and strategies to accelerate diffusion of new technologies into the market place; (ii) policies to encourage replacement of long-lived energy, transportation and industrial infrastructure in normal investment cycles with plant and equipment that provides the highest amount of service and the lowest level of greenhouse gas emissions per unit of input energy and materials; (iii) research and monitoring to promote a better understanding of the climate system and the impacts of climate variability; and (iv) action of other measures with multiple benefits and negative or low cost. Such measures would begin the process of moving towards eventual stabilization while further information is developed;  Delaying action might conceivably reduce the overall costs of mitigation, but would increase both the rate and the eventual magnitude of climate change, and hence the adaptation and damage costs.7.11 "No regrets" measures are those whose benefits, such as reduced energy costs and lower emissions of local and regional pollutants, equal or exceed their cost to society, excluding the benefits of mitigation of climate change. Such "no regrets" mitigation and adaptation measures would appear justified on technical grounds unrelated to the risks of rapid climate change due to greenhouse gases. The expectation of net damages from climate change and the precautionary principle provide a rationale for going beyond "no regrets".7.12 A sequential decision-making approach offers a prudent strategy that can be adjusted in the light of new information and could take into account factors such as future flexibility and current and future costs. Near term decisions along an optimal path (i.e., modest cost mitigation measures) will be the same for a wide range of ultimate stabilization concentrations (see chapter 5 also). 7.13 A broad portfolio of actions aimed at mitigation, adaptation and reducing uncertainties through further research provides a balanced approach to managing the risks of anthropogenic climate change. The appropriate portfolio will differ from country to country. A well-chosen portfolio of climate change investments will yield greater benefit for a given cost than any one option undertaken by itself. WT03-B20-64IA006-000055-B013-306http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind/location.html 138.80.61.12 19970221181347 text/html 2065HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:44:09 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1894Last-modified: Tue, 09 Jul 1996 02:26:08 GMT Wind Farm Location Maps Wind Farm Location Maps The following maps show the approximate location of the wind power plants on the tour. California Altamont Pass Tehachpi Pass San Gorgonio Pass Asia Okha Continental Europe Velling Maersk Groothusen Eemshaven Wind Plants of the United Kingdom Haverigg Royd Moor Elliott's Hill More information on wind energy. Wind Energy Comes of Age Wind Power for Home & Business Workgroup Windturbines  Paul Gipe, 20.08.95, pgipe@igc.apc.orgWT03-B20-65IA005-000051-B020-63http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind/overview.html 138.80.61.12 19970221152803 text/html 22398HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:58:12 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 22226Last-modified: Tue, 09 Jul 1996 02:26:08 GMT Overview of Worldwide Wind Generation by Paul Gipe Tehachapi, California Copyright © 1994 by Paul Gipe. All rights reserved. This document may be reprinted for noncommercial purposes only. No portion may be used for commercial purposes without the express written permission of the author. Portions of this paper have appeared in past issues of Independent Energy Magazine. Data herein has been compiled from numerous sources including Finn Godtfredsen, Risøe National Laboratory; Birger Madsen, BTM Consult; and Andrew Garrad, Garrad Hassan. ABSTRACT An overview of activities in the United States and Europe including a look at installed capacity, generation, specific yield, price, and environmental impact. The 1992-1993 period saw the wind industry reach several significant milestones. Worldwide wind generation exceeded 4 TWh during 1992 for the first time and will surpass 5 TWh in 1993. See Table 1. By 1995, total generation will approach 7 TWh. During 1993, Europe's total installed wind generating capacity will exeeded 1,000 MW for the first time: the Netherlands and the United Kingdom will each top 100 MW and Germany will exceed 300 MW. Table 1. North American and European Wind Development        North America Europe        World         MW     TWh    MW     TWh    MW     TWh   1992  1,700    2.8    900    1.5  2,650    4.4   1993  1,700    2.9  1,270    2.3  3,050    5.4   1994  1,700    3.1  1,550    2.8  3,350    6.1   1995  1,750    3.1  1,950    3.6  3,800    6.8 Europe's rapid growth will push wind energy beyond another milestonein 1993 when sales of wind turbines and wind-generated electricity exceed $1 billion (thousand million), an increase of nearly $200 million over 1992 sales. 1993 turnover includes $600 million in project development, and sales of wind-generated electricity worth an estimated $450 million. The industry has not seen turnover in excess of $1 billion (thousand million) since 1985 during the height of California's wind rush. At that time, revenues were due almost entirely to sales of new wind turbines. Europe's feverish development of wind energy shows little sign of abating. Altogether, Europe will install 350 MW of capacity in 1993, up from nearly 250 MW last year. European development is far outpacing the 15 MW installed in North America during 1992, and the 25 MW expected this year. By the end of 1993, Europeans will be operating 1,200 MW of wind generation, and if the present pace continues, will outstrip the United States in total installed capacity by the end of 1995. Denmark currently accounts for half the capacity installed in Europe and continues to add 40-50 MW per year in one of Europe's most stable domestic markets. Germany accounts for another fifth, with new capacity additions in Germany now rivaling that of any other country in the world, including Denmark. See Figure 1. Figure 1. European Wind Capacity. EUROPE TO SURPASS NORTH AMERICA IN 1995 North America's share of world wind generation fell to 66 percent in 1992, the lowest level in a decade. As recently as 1990, California alone accounted for 78 percent of world-wide wind generation. The slide in America's leading role in the world's wind industry began with a boom in European wind development while growth stagnated in the United States. A lack of new installations and a poor wind year in 1992 contributed to the first fall in California production since wind development began in the early 1980s. If present trends continue, European generation will exceed that of North America by the end of 1995. See Figure 2. Figure 2. North American and European Wind Generation. TEHACHAPI LEADS IN GENERATION California electric utilities report that the Tehachapi Pass led the world in wind-electric production during 1992. See Figure 3. Figure 3. Ranking of Major Production Centers. For the first time, production in the Tehachapi Pass exceeded that of the Altamont Pass east of San Francis-co. Southern California Edison Co. estimates that wind turbines near Tehachapi, Calif. generated 1.158 TWh during 1992, while Pacific Gas & Electric Co. estimates that total Altamont production fell from a high in 1991 of 1.079 TWh to approximately 1.010 TWh in 1992. Despite poor winds during 1992, Tehachapi's generation increased over that of the 1.064 billion kWh produced during 1991. Altamont contains about one-sixth more wind turbines than Tehachapi, but Tehachapi's more powerful wind resource, and greater number of state-of-the-art turbines has led to continuing improve-ments in production. The gap between the two sites will widen further this year. Production by mid-1993 is down 20-30% in the Altamont Pass, while above average winds in Te-hachapi have led SCE to project record-breaking production approaching 1.4 TWh. By the end of 1993, Denmark should surpass 1 TWh placing it alongside the Altamont Pass in annual generation. CURRENT ACTIVITY IN NORTH AMERICA During 1993 and early 1994, wind developers will erect 100 MW of capacity in North America: 25 MW in California, 18 MW in Alberta, 26 MW in Iowa, and 25 MW in Minnesota. Between 1995 and 1997, wind companies will install 300-350 MW in North America outside of California: 10 MW in Saskatchewan, 5 MW in Quebec, 100 MW in Washington, 60 MW in Wyoming, 50 MW or more in Texas, 75 MW in Minnesota, 10 MW in Wisconsin, and 65 MW in Maine. Another 200 MW or more may be added in California during this period. However, this may be insuffi-cient to offset retirements of existing capacity in the state. Several firms are also active in Mexico, where the first small project should be completed in 1993. PROJECT PRICE DECLINES The market price paid for wind power plants has decreased dramatically since the first projects were built in Califor-nia. However, the price of installed wind plants bears little resemblance to the much-touted cost of $1,000/kW of installed capacity with the exception of projects installed by Danish utilities.(1) See Figure 4. California projects reached a low of $1,250/kW in 1987. Subsequent projects were more costly because of the construction of a 45 mile high voltage transmission line.(2) These projects also included the developer's up front profit. Similarly, non-utility projects in England and Wales are costing more than $2,000/kW, among the most costly in Europe. Only Danish utility projects are showing a steady decline in installed costs, resulting from competitive procurement and low-interest financing. Danish utilities also take their profit out over the life of the project. Figure 4. Market Price of Capacity. SPECIFIC YIELD STILL INCREASING According to data from the CEC's Performance Reporting System, BTM Consult and Denmark's Risoe National Laboratory, the specific yield of individual wind turbines and wind power plants in California and Denmark have increased stead-ily since the early 1980s. See Figure 5. The exception is average yield in California wind plants and the yield of post-1985 wind plants in California. The specific yield in California reached a peak in 1990 and has declined since. There are several possible explanations. Wind speeds have suffered during the late 1980s and early 1990s due to "El Nino" and from a volcanic eruption in the Philippines. The turbine stock may also be showing signs of age. Figure 5. Average Annual Specific Yield. Of note is the steady improvement in specific yield with succeeding design iterations, both in California and Den-mark. Medium-sized wind turbines have steadily increased in size since introduction of the 55 kW Danish wind turbine (15-16 meters in diameter) in the early 1980s. Machine designs have increased in modest increments from 55 kW to 100 kW, to 200 kW, to 400 kW, and to today's state-of-the-art machines in the 500 kW range. See Figure 6. Figure 6. Increasing Rotor Diameter in Successive Design Iterations. Though the specific yield (the kWh generated per year per square meter of rotor area) increases for each Danish wind turbine model in succeeding years, the most dramatic im-provement is from one wind turbine model to the next.(3) The average specific yield of the 450 kW model of 850-900 kWh/m2/yr is twice that of the early 55 kW model. Even the later 55 kW model shows a marked performance improvement over the previous version. The same results can be seen in California, when the specific yield of all wind turbines is contrasted with those installed since 1985. The improved performance is attributable to greater reliability, improved airfoils, and taller towers. Next to greater reliability, the most important contributor to higher specific yields is taller towers. Tower heights have nearly doubled from 18 meters to 35 meters since the early 1980s. Doubling the tower height alone will increase the power available more than 30%. For sites yielding 600 kWh/m2/yr the taller towers will add nearly 200 kWh/m2/yr, bringing total yield to 800 kWh/m2/yr at a good site. At exceptional sites, such as on the west coast of the Jutland peninsula (7 m/s), or atop Whitewater Hill near Palm Springs, contemporary turbines should yield 1,000-1,250 kWh/m2/yr. See Figure 7. San Gorgonio Farms, which operates the world's most productive wind plant, consistently produc-es within this range. There are numerous sites in Northern Europe where specific yields exceed 1,000 kWh/m2/yr, includ-ing coastal Germany, and the Netherlands. Figure 7. Typical Specific Yields. REPOWERING CALIFORNIA As mentioned, California's turbine stock is comprised large-ly of early, less cost-effective designs. Some of the tur-bines are ten years old. The aging stock may have partially contributed to the decline of average specific yield during 1991 and 1992. Nearly 3,100 turbines comprising 230 MW of capacity are of first generation design. See Table 2. These turbines are costly to maintain, often sited poorly, in-stalled on short towers, and unreliable. Many of them are unsalvageable. The bulk of the capacity in the state is provided by second generation designs, including 100-150 kW Danish machines and U.S. Windpower's 56-100. There are 1,300 state-of-the-art turbines representing 300 MW of capacity. Table 2. Repowering California Wind Plants  Current Fleet                           Units     MW     TWh/yr --------------------------------------------- Junk                  3078      233  Second Generation    12509     1211  State-of-the Art      1286      317                    ---------------------------                      16873     1761      2.8  After Repowering --------- Junk                     0        0  Second Generation        0        0  State-of-the Art      7012     1761                    ---------------------------                                          3.6  The American Wind Energy Association's west coast office, in a study for the California Energy Commission, estimated that repowering California's wind plants by replacing first and second generation designs with contemporary machines could lower operations and maintenance costs, reduce the density of turbines on the landscape by more than 50%, and increase annual generation by 30% to 3.6 TWh. The AWEA study concluded that repowering with modern turbines would make the California industry more competitive with other resources, preserve jobs, and reduce the industry's aesthetic impact, thus improving its public acceptance. The AWEA study took the first-ever truly comprehensive employment survey of California wind plant operators and their service providers. See Table 3. AWEA found that there are 1,250 people working directly with wind energy in California for 460 jobs/TWh/yr. This compares well with estimates by BTM Consult of the number of people employed in Denmark. BTM Consult estimates there are nearly 600 manufac-turing jobs in Denmark for 100 jobs/MW of manufacturing and another 400 people are employed in the service sector for 440 jobs/TWh.(4) Table 3. Wind Industry Jobs in California and Denmark                   California        Denmark                    Jobs   Jobs/TWh   Jobs   Jobs/TWh  ----------------------------------------------------  Manufacturing          0        0      600 (100/MW)  O&M and support     1250      460      400      440   Indirect            4350     1500     ?        ?  ---------------- ENVIRONMENTAL IMPACT During 1992 and 1993, the wind industry has made consider-able strides towards quieting wind turbine noise. Bonus, Vestas, and WEG have demonstrated that noise emissions can be reduced significantly by focusing attention on tip design, trailing edge thickness, drive-train compliance, noise insulation, and nacelle isolation. Keeping tip speeds to 60 m/s or less is an important means for reducing aerody-namic noise. These wind turbines are 5-7 dB(A) quieter in their noise emissions than competitive machines designed during the mid-1980s operating at higher tip speeds. Despite the industry's best efforts, wind turbines will introduce noise into many rural environments previously noted for their solitude. This intrusion creates concern, fear, and objections until the public has had ample time to become familiar with the technology. See Figure 8. It has taken the community of Tehachapi a decade to become comfortable with its wind industry. It was Tehachapi's experience that prior to development there was broad support of wind energy in the abstract. Yet when wind projects were first developed, there was a loud outcry and consternation that the wind turbines would devour the small town of 5,000. Several years after installation qualitative acceptance has resumed to near pre-project levels. Energy Connection, a Dutch wind developer, has observed the same effect in the Netherlands.(5) Figure 8. Acceptance after Project Completion. Wind energy's most significant impact remains its use of the visual amenity. Wind energy will only reach its potential when the industry addresses aesthetic impact. However, public opinion surveys and architectural studies in the United States and Europe can provide guidelines for minimiz-ing wind's aesthetic impact. The single most important measure is aesthetic uniformity. All wind turbines and towers within a wind plant must look similar. They need not be identical, but they must appear similar. This is less of a problem in Europe than in the United States. In California, for example, it is common for a developer to install a wind plant containing hundreds of different wind turbines in a seemingly incoherent mix. To maintain visual uniformity within a wind plant that comprises one visual unit, all wind turbines must spin in the same direction, have the same number of blades, use the same tower, and use the same color scheme. If another wind turbine and tower combination will be installed nearby, there must be sufficient visual separation to make the projects distinct from one another. Turbines should all be of the same height unless they are part of a coherent "wind wall", with alternating groups of turbines on towers of different heights. If turbines are of different heights, all towers should appear similar if not identical. This provision alone would eliminate much of the jumble and visual clutter typical of many California wind plants. Further, developers must minimize roads or eliminate them altogether. This prevents unsightly cut and fill slopes in steep terrain, and minimizes the amount of land used by the wind plant. If two-bladed turbines are used, all turbines must park their rotors in the same position. This "synchronized stop" provides visual balance for what many consider an ungainly design choice. Three-bladed rotors, which appear more visu-ally symmetrical to observers than two-bladed rotors, can be parked in any position. No wind turbine should ever go out undressed: none should operate without a nacelle cover. Nose cones and nacelle covers serve a valuable purpose. They smooth the angular lines of the drive train and aid in making the nacelle a part of a visual whole with the tower. Nacelle covers should be replaced immediately if they blow off. Designers should strive toward visual unity between rotor, nacelle, and tower. A wind turbine need not be a "box on a stick." Lattice towers need not appear cluttered with cross braces and angular lines. Lattice towers can be designed with graceful curves and a sparing use of cross braces. The industry must address the question of aesthetics or the publics' general support will be lost. Consider the reversal of public attitudes towards nuclear power during the 1960s. Wind is currently a preferred technology even when account-ing for wind's aesthetic impact.(6) See Figure 9. This support is tenuous and can be squandered by inappropriate development. Figure 9. Public Preference of Generating Technologies. COMMUNITY ASSIMILATION Community acceptance of wind energy can be aided by address-ing community concerns, by providing information about the operation of nearby wind turbines and the companies in-volved, and by low-key, long-term participation of the wind industry in community events. In Tehachapi the Kern Wind Energy Association is the focal point for inquiries about wind energy and the local wind industry. KWEA and member companies offer speakers for local service clubs, partici-pate in local festivals, and provide simple, inexpensive brochures and postcards to local merchants for distribution to tourists and residents alike. KWEA also operates a low-power radio transmitter for broadcasting information about the wind industry to motorists on Highway 58, a major artery crossing the Tehachapi Mountains. More recently KWEA has sponsored a new local event: the Tehachapi Wind Fair. The success of this weekend event, which drew 14,000 the first year and 12,000 the next, confirms that the Tehachapi wind industry has become an accepted part of the community. CONCLUSION Wind generation now meets 1% of California's electrical supply and 3% of Denmark's electrical consumption. Worldwide wind generation will exceed 6 TWh in 1995, when Europe will surpass North America in total generation and installed capacity. At good sites, medium-sized wind turbines today should produce specific yields in excess of 800 kWh/m2/yr. Wind energy can become an accepted part of the community if designers and developers keep community interests in mind, especially those of wind's aesthetic impact on the land-scape. REFERENCES 1. Jens Vesterdal, ELSAM, "Experience with Wind Farms in Denmark," European Wind Energy Association special topic conference on "The Potential of Wind Farms," Herning, Den-mark, September 1992. 2. California Energy Commission, "Wind Project Performance Reporting System," Sacramento, Calif., 1985-1991. 3. Finn Godtfredsen, Risøe National Laboratory, "Wind Energy in Denmark: Development in Wind Turbine Technology and Economics Since 1980," paper presented at Windpower 93, American Wind Energy Association annual conference, San Francisco, Calif., July 1993.. 4. BTM Consult, personal communication, April 1993.. 5. C. Westra and L. Arkesteijn, "Physical Planning, Incentives, and Constraints in Denmark, Germany, and the Netherlands," European Wind Energy Association special topic conference on "The Potential of Wind Farms," Herning, Den-mark, September 1992.. 6. Robert Thayer and Heather Hansen, "Consumer Attitude and Choice in Local Energy Development," Center for Design Research, Department of Environmental Design, University of California, Davis, Calif., May 1989. Written in late 1993. Guide Arbeitsgruppe Windkraftanlagen Workgroup Windturbines More Information on Wind Energy Wind Energy Comes of Age Wind Power for Home & Business Paul Gipe, located in California, India, the United Kingdom and Continental Europe. Wind Farms, or more correctly wind power plants, are arrays of multiple wind turbines. The number of wind turbines in the array varies from no more than two or three in a small cluster to the thousands of machines in California's windy passes. Though it may seem a feature of the modern landscape, the use of multiple wind turbines to perform a task is nothing new. Only by tapping the power of multiple windmills could Jan Leeghwater (literally the "empty water" Jan), and the engineers that followed him, drain the polders and make the Netherlands what it is today. The Dutch called these early wind farms gangs of windmills and a group can still be seen southeast of Rotterdam at Kinderdijk. This water pumping wind plant was in use until the 1950s. Windmills may also have been the driving force of the industrial revolution. During the eighteenth century the Zaan region northwest of Amsterdam became Holland's powerhouse. There Dutch millers constructed what must have been an amazing assembly of more than 700 industrial windmills along the Zaan River. These windmills drove Dutch industry at a time when Britain and Germany were still trying to figure out what to do with a black rock they found underground. Monuments to the region's halcyon days can still be seen at Zaanse Schans. Modern wind plants generate electricity--lots of it. This electricity can be used to pump water in the Netherlands, power a hair dryer for a Hollywood starlet, or cool Bob Hope's mansion near Palm Springs. The 28,000 wind turbines operating worldwide generate more than 7.8 Terawatt-hours (7,800,000,000 kWh) of electricity annually. Wind plants now scan the globe from a fjord in Denmark to the deserts of Southern California, from the polders of the Netherlands to the shores of the Arabian Sea, from the rugged Tehachapi Mountains to the moors of Britain's Pennines. California Wind Plants of California's Altamont Pass Wind Plants of California's San Gorgonio Pass Wind Plants of California's Tehachapi Pass Asia Wind Plants of India Continental Europe Wind Plants of Denmark Wind Plants ofGermany Wind Plants of the Netherlands Wind Plants of the United Kingdom Wind Plants ofCumbria, England Wind Plants ofNorthern Ireland Wind Plants of Yorkshire, England Wind Energy Comes of Age Wind Power for Home & Business Workgroup Windturbines Paul Gipe, 20.08.95, pgipe@igc.apc.org WT03-B20-67IA005-000051-B017-135http://lacebark.ntu.edu.au:80/j_mitroy/sid101/acid/overview.html 138.80.61.12 19970221151556 text/html 21369HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:46:15 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 21197Last-modified: Tue, 09 Jul 1996 02:26:06 GMT Overview Factsheet The overall goal of the Acid Rain Program is to achieve significant environmental and public health benefits throughreductions in emissions of sulfur dioxide (SO2) and nitrogen oxides (NOx), the primary causes of acidrain. To achieve this goal at the lowest cost to society, the program employs both traditional andinnovative, market-based approaches for controlling air pollution. In addition, the programencourages energy efficiency and pollution prevention. Title IV of the Clean Air Act sets as its primary goal the reduction of annual SO2 emissions by 10million tons below 1980 levels. To achieve these reductions, the law requires a two-phase tighteningof the restrictions placed on fossil fuel-fired power plants. Phase I began in 1995 and affects 110 mostly coal-burning electric utility plants located in 21 easternand midwestern states. Phase II, which begins in the year 2000, tightens the annual emissions limitsimposed on these large, higher emitting plants and also sets restrictions on smaller, cleaner plants firedby coal, oil, and gas. The program affects existing utility units serving generators with an output capacity of greater than25 megawatts and all new utility units. The Act also calls for a 2 million ton reduction in NOx emissions by the year 2000. A significantportion of this reduction will be achieved by coal-fired utility boilers that will be required to installlow NOx burner technologies and to meet new emissions standards. Key Features of the Acid Rain Program Operating Principles: Feasible, Flexible, Accountable Environmental Benefits Allowance Trading The Allowance Tracking System Auctions and Direct Sales Voluntary Entry: The Opt-in Program Pollution Prevention The NOx Program Emissions Monitoring and Reporting Excess Emissions Designated Representatives Permitting Compliance Options: Freedom to Choose A Model Program Operating Principles: Feasible, Flexible, Accountable The Acid Rain Program is implemented through an integrated set of rules and guidance designed toaccomplish three primary objectives: Achieve environmental benefits through reductions in S02 and NOx emissions. Facilitate active trading of allowances and use of other compliance options to minimize compliancecosts, maximize economic efficiency, and permit strong economic growth. Promote pollution prevention and energy efficient strategies and technologies. Each individual component fulfills a vital function in the larger program: the allowance trading system creates low-cost rules of exchange that minimize government intrusion and make allowance trading a viable compliance strategy for reducing SO2 the opt-in program allows nonaffected industrial and small utility units to participate in allowance trading the NOx emissions reduction rule sets new NOx emissions standards for existing coal-fired utility boilers and allows emissions averaging to reduce costs the permitting process affords sources maximum flexibility in selectingthe most cost-effective approach to reducing emissions the continuous emission monitoring (CEM) requirements provide credible accounting of emissions to ensure the integrity of the market-basedallowance system and the achievement of the reduction goals the excess emissions provision provides incentives to ensure self-enforcement, greatly reducing the need for government intervention the appeals procedures allow the regulated community to appeal decisions with which it may disagree Together these measures ensure the achievement of environmental benefits at the least cost to society. Environmental Benefits Acid rain causes acidification of lakes and streams and contributes to damage of trees at high elevations (forexample, red spruce trees above 2,000 feet in elevation). In addition, acid rain accelerates the decay of building materials and paints, includingirreplaceable buildings, statues,and sculptures that are part of our nation's cultural heritage.Prior to falling to the earth, SO2 and NOx gases and their particulate matter derivatives, sulfates and nitrates, contribute to visibility degradation and impact public health. Implementation of the Acid Rain Program under the 1990 Clean Air Act Amendments will confer significant benefits on the nation. By reducing SO2 and NOx, many acidified lakes and streams will significantly improve so that they can onceagain support fish life. Visibility will improve, allowing for increased enjoyment of scenic vistas across our country, particularly in National Parks. Stress to our forests that populate the ridges of mountains from Maine to Georgiawill be reduced. Deterioration of our historic buildings and monuments will be slowed. Finally,reductions in SO2 and NOx will reduce sulfates, nitrates, and ground level ozone (smog), leadingto improvements in public health. For more information, see Environmental Benefits Allowance Trading The Acid Rain Program represents a dramatic departure from traditional command and controlregulatory methods that establish specific inflexible emissions limitations with which all affectedsources must comply. Instead, the program introduces an allowance trading system that harnessesthe incentives of the free market to reduce pollution. Under this system, affected utility units were allocated allowances based on their historic fuel consumption and a specific emissionsrate. Each allowance permits a unit to emit 1 ton of SO2 during or after a specified year. For eachton of SO2 discharged in a given year, one allowance is retired, that is, it can no longer be used. Allowances may be bought, sold, or banked. Any person may acquire allowances and participatein the trading system. However, regardless of the number of allowances a source holds, it may notemit at levels that would violate federal or state limits set under Title I of the Clean Air Act to protect publichealth. At the end of the year, utilities are granted a 30-day true-up or grace period, during which SO2allowances may be purchased, if necessary, to cover each unit's emissions for the year. At the endof the grace period, the allowances a unit holds in its compliance account must equal or exceedthe annual SO2 emissions recorded by the unit's monitoring system. Any remaining allowances may be sold orbanked for use in future years. During Phase II of the program, the Act sets a permanent ceiling (or cap) of 8.95 millionallowances for total annual allowance allocations to utilities. This cap firmly restricts emissions and ensures that environmental benefits will be achieved and maintained. For more information, see Allowance System The Allowance Tracking System EPA has instituted an electronic recordkeeping and notification system called the AllowanceTracking System (ATS) to track allowance transactions and the status of allowanceaccounts. ATS is the official tally of allowances by which EPA determines compliancewith the emissions limitations. Any party interested in participating in the trading system mayopen an ATS account by submitting an application to EPA. Accounts contain information onunit account balances, account representatives (which must be appointed by eachtrading party), and serial numbers for each allowance. ATS is computerized to expedite theflow of data and to assist in the development of a viable market for allowances. For more information, see Allowance Tracking System Auctions and Direct Sales EPA holds allowance auctions and a direct sale annually. The auctions help to send the market anallowance price signal, as well as furnish utilities with an additional avenue for purchasing neededallowances. The direct sale offers allowances at a fixed price of $1,500 (adjusted for inflation). Anyone can buy allowances inthe direct sale, but independent power producers (IPPs) can obtain written guarantees from EPAstating that they have first priority. These guarantees, which are awarded on a first-come, first-served basis, secure the option for qualified IPPs to purchase a yearly amount of allowances overa 30 year span. This provision enables IPPs to assure lenders that they will have access to theallowances they need to build and operate new units. For more information, see Allowance Auctions and Direct Sales Voluntary Entry: The Opt-in Program The Opt-in Program expands EPA's Acid RainProgram to include additional sulfur dioxide (SO2)emitting sources. Recognizing that there areadditional emission reduction opportunities in theindustrial sector, Congress established the Opt-inProgram under section 410 of the Clean Air ActAmendments of 1990. The Opt-in Program allowssources not required to participate in the Acid RainProgram the opportunity to enter the program on avoluntary basis, reduce their SO2 emissions, andreceive their own acid rain allowances. The participation of these additional sourceswill reduce the cost of achieving the 10 million tonreduction in SO2 emissions mandated under theClean Air Act. As participating sources reducetheir SO2 emissions at a relatively low cost, theirreductions -- in the form of allowances -- can betransferred to electric utilities where emissionreductions are more expensive. The Opt-in Program offers a combustion sourcea financial incentive to voluntarily reduce its SO2emissions. By reducing emissions below itsallowance allocation, an opt-in source will createunused allowances, which it can sell in the SO2allowance market. Opting in will be profitable ifthe revenue from allowances exceeds the combinedcost of the emissions reduction and the cost ofparticipating in the Opt-in Program. For more information, see The Opt-in Program Pollution Prevention The allowance trading system contains an inherent incentive for utilities to prevent pollution, sincefor each ton of SO2 that a utility avoids emitting, one fewer allowance must be retired. Utilities that reduce emissions through energy efficiency and renewable energy are able to sell, use, or bank their surplus allowances. As also provided in theAct, EPA has set aside a reserve of 300,000 allowances to stimulate energy efficiency and renewable energy generation. Thoseutilities that either implement demand-side energy conservation programs to curtail emissions orinstall renewable energy generation facilities may be eligible to receive extra allowances from thisreserve. For more information, see Conservation and Renewable Energy Reserve The NOx Program The Clean Air Act Amendments of 1990 set a goal of reducing NOx by 2 million tons from 1980 levels. The Acid Rain program focuses on one set of sources that emitNOx, coal-fired electric utility boilers. As with the SO2 emission reduction requirements, theNOx program is implemented in two phases, beginnning in 1996 and 2000. The NOx program embodies many of the same principles of the SO2 trading program inits design: a results-orientation, flexibility in the method to achieve emission reductions, and programintegrity through measurement of the emissions. However, it does not "cap" NOx emissions as the SO2 program does, nor does it utilize an allowance trading system. Emission limitations for the NOx boilers provide flexibility for utilities by focusingon the emission rate to be achieved (expressed in pounds of NOx per million Btu of heat input). Two options for compliance with the emission limitations are provided: compliance with an individual emission rate for a boiler averaging of emission rates over two or more units to meet an overall emission rate limitation (Note: these units must have the same owner or operator) These options give utilities flexibility to meet the emission limitations in the mostcost-effective way and allow for the further development of technologies to reduce the cost ofcompliance. If a utility properly installs and maintains the appropriate control equipment designed tomeet the emission limitation established in the regulations, but is still unable to meet the limitation, the NOx program allows the utility to apply for an alternative emission limitation (AEL) that corresponds to the levelthat the utility demonstrates is achievable. Phase I of the program, which was delayed a year due to litigation, is now scheduled to begin onJanuary 1, 1996, and will affect two types of boilers (which are among those already targeted for Phase I SO2 reductions): dry-bottom wall-fired boilers and tangentially fired boilers. Dry-bottom wall-fired boilers must meet a limitation of 0.50 lbs of NOx per mmBtu averaged over the year, and tangentially fired boilers must achieve a limitation of 0.45 lbs of NOx per mmBtu per year. Approximately 170 boilers must comply with these NOx performance standards during Phase I. The regulations governing the Phase II portion of the program are under development. By January 1, 1997, EPA may issue regulations revising the limitations for dry-bottom wall-fired and tangentially fired boilers first subject to NOx limitations in Phase II and regulations covering the remaining types of coal-fired boilers. For more information, see NOx Reduction Program Emissions Monitoring and Reporting Under the Acid Rain Program, each unit must continuously measure and record its emissions ofS02, NOx, and CO2, as well as volumetric flow and opacity. In most cases, a continuous emission monitoring (CEM) system must be used. There are provisions for initial equipment certification procedures, periodicquality assurance and quality control procedures, recordkeeping and reporting, and procedures forfilling in missing data periods. Units report hourly emissions data to EPA on a quarterly basis. This data is then recorded in the Emissions Tracking System, which serves as a repository of emissions data for the utility industry. The emissions monitoring and reporting systems arecritical to the program. They instill confidence in allowance transactions by certifying the existenceand quantity of the commodity being traded and assure that NOx averaging plans are working. Monitoring also ensures, through accurateaccounting, that the SO2 and NOx emissions reduction goals are met. For more information, see Continuous Emissions Monitoring Excess Emissions If annual emissions exceed the number of allowances held, the owners or operators of delinquent units must pay a penalty of$2,000 (adjusted for inflation) per excess ton of SO2 or NOx emissions. In addition, violating utilities must offset the excess SO2emissions with allowances in an amount equivalent to the excess. A utility mayeither have allowances deducted immediately or submit an excess emissions offset plan to EPAthat outlines how these cutbacks will be achieved. Designated Representatives Each source appoints one individual, the Designated Representative, to represent the owners andoperators of the source in all matters relating to the holding and disposal of allowances for itsunits that are affected by the Clean Air Act. The Designated Representative is also responsible forall submissions pertaining to permits, compliance plans, emission monitoring reports, offset plans,compliance certification, and other necessary information. A source may appoint an Alternate Designated Representative to act on behalf of the Designated Representative. Permitting The Designated Representative for each source is required to file a permit application for thesource and a compliance plan for each affected unit at the source. The Acid Rain permits and complianceplans are simple, allow sources to fashion a compliance strategytailored to their individual needs, and foster trading. For example, theyallow sources to make real-time allowance trading decisions through the use of automatic permitamendments. The permits stipulate the initial allowance allocation for each affected unit at a source. Permit applications must certify that each unit account will hold a sufficient number of allowances to coverthe unit's S02 emissions for the year, will comply with the applicable NOx limit, and will monitor and report emissions. Permits are subject to public comment before approval. For more information, see Permits Compliance Options: Freedom to Choose The Acid Rain Program allows sources to select their own compliance strategy. For example, toreduce SO2 an affected source may repower its units, use cleaner burning fuel, or reassign someof its energy production capacity from dirtier units to cleaner ones. Sources also may decide to reduce electricity generation by adopting conservation or efficiency measures.Some of the options afford the unit special treatment, such as a compliance extension or extraallowances. Most options, like fuel switching, require no special prior approval, allowing the source to respond quickly to market conditions without needing government approval. For NOx, the source may meet the performance standard on a utility-unit basis, enter into an emissions averaging plan, or apply for an alternative emissions limitation. In either case, the program allows affected utilities to combine these and other options in waysthey see fit in order to tailor their compliance plans to the unique needs of each unit or system. A Model Program EPA gained broad input into thedevelopment of the Acid Rain Program by consulting with representatives from various stakeholder groups, including utilities, coal and gascompanies, emissions control equipment vendors, labor, academia, Public Utility Commissions,state pollution control agencies, and environmental groups. EPA is maintaining this open door policy as it implements the program, and it continues to solicitideas from the numerous and diverse individuals and groups interested in acid rain control. Inaddition, EPA is collaborating with groups who wish to evaluate the benefits and effects of the program through economic andenvironmental studies. The Acid Rain Program is already being viewed around the world as a prototype for tacklingemerging environmental issues. The allowance trading system capitalizes on the power of themarketplace to reduce SO2 emissions in the most cost-effective manner possible. The permittingprogram allows sources the flexibility to tailor and update their compliance strategy based on theirindividual circumstances. The continuous emissions monitoring and reporting systems provide the accurate accounting of emissionsnecessary to make the program work, and the excess emissions penalties provide strong incentivesfor self-enforcement. Each of these separate components contributes to the effective working ofan integrated program that lets market incentives do the work to achieve cost-effective emissionsreductions. The General Accounting Office recently confirmed the beneftis of this approach, projecting that the allowance trading system could save as much as $3 billion per year -- over 50% -- compared with a command and control approach typical of previous environmental protection programs. Last Modified in July 1995 Return to Acid Rain Program's Home PageWT03-B20-68IA005-000051-B018-389http://lacebark.ntu.edu.au:80/j_mitroy/sid101/nuke1/mururoa.html 138.80.61.12 19970221152117 text/html 22484HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:51:35 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 22312Last-modified: Tue, 09 Jul 1996 02:25:43 GMTThe URL of this document is http://www.abc.net.au/quantum/info/mururoa.htm Mururoa How safe are the French tests? Since 1945, the nuclear powers have exploded more than 2,000 nuclear devices. The French have exploded 175 in the Pacific . 50 years into the nuclear age, it seems we're finally coming to our senses. Next year the major nuclear powers look set to sign a treaty banning just about all nuclear testing. But before it signs, France insists on slipping in 8 more underground tests. The outrage at the French government's decision is universal: it's been decried as arrogant colonialism, jeopardising progress to a nuclear weapons-free world. But there's also the strong belief that French nuclear tests have contaminated the Pacific and its people. Given the litany of lies we've been told about nuclear tests - from the Marshall Islands to Maralinga - people are understandably sceptical of France's assurance that all is safe. Claims that France's testing has poisoned the environment and caused cancers and birth defects are of great concern, but must be viewed in the light of the available facts. Tonight, we weigh up the scientific evidence. Is radioactivity the real danger? Radioactivity is something we all have to live with, all the time. Cosmic rays from space, traces of radioactive elements in soil and our food; they make up what's called natural or background radioactivity. For Australia and South Pacific nations that's measured as 2 MilliSieverts a year. To put that in perspective: every time you have a medical procedure like a CAT scan or barium meal, you're exposed to about 4 times that radioactivity, up to 8mS. Most experts agree the 2 mS background radiation does us little or no harm. But when it comes to additional radioactivity: the less you're exposed to, the better. FALLOUT FROM ATMOSPHERIC TESTS Fallout is the radioactive byproduct of nuclear explosions. The greatest danger to humans are the radionuclides caesium 137, iodine 131, strontium 90 and plutonium 239. In March 1954 the United States exploded a 15 megaton bomb on Bikini Atoll. People on some nearby Marshall Islands received a tragically high dose of radioactivity, with tragically clear results: thyroid disease and cancers, for which the United States belatedly paid compensation. We know from the survivors of the first nuclear weapons at Hiroshima and Nagasaki that doses of radioactivity above around 500 mS do cause extra cancers and birth defects in a population. The Marshall Islanders were exposed to four times that level, nearly 2,000mS. The case is not so clear cut in French Polynesia. Between 1966 and 1974, France exploded 41 atmospheric tests in French Polynesia. The total yield was 15 megatons equal to one US test on Bikini atoll. Dr Murray Matthews National Radiation Lab New Zealand. "Our estimate is that on average in the Pacific Islands from the entire history of atmospheric weapons tests individuals in the islands would have received around about one miliSievert over their entire life times from that testing." One mS spread over 50 years. How can we be sure the exposure was so low? The figures come from the Australian Radiation Lab and the National Radiation Lab in NZ. Both exist to monitor radiation hazards and protect the populations. The labs have no vested interest in the nuclear industry - NZ doesn't even have a nuclear reactor. During the whole period of French atmospheric testing New Zealand monitored the levels of fallout at their network of South Pacific stations. The fallout was low, but uneven. There were rainouts, times when winds blew a cloud of radioactivity over island populations. If it rained, fallout rained down too. Dr Murray Matthews National Radiation Lab New Zealand. "The most significant event occurred in 1966 when there was what we call a blow back from Mururoa towards Samoa in particular where the from the test went westward instead of eastward and it was caught in a heavy rain event at Samoa and this resulted in quite a lot of local contamination even then though the dose in that year from that event would have only been around naught point two miliSieverts. Q. So that's still a tenth of the background radiation? A. 1/10 of the annual background." There was another rainout in Tahiti in 1974, but again fallout was well below background. We know of one other rainout, in the Gambier Islands, just to the southeast of Mururoa. Fallout was higher, 4 mS, twice background radiation, but many hundreds of times lower than Marshall islands. Now, some of these figures do come from France's monitoring stations, but they closely correspond to the levels and patterns of fallout monitored by New Zealand. Dr Andrew McEwan National Radiation Lab, New Zealand "The radiation doses were so low that no effects from radiation would be expected. If there is no radiation there can be no radiation effects." So what are we to make of the worrying claims that birth defects and cancer rates have increased in French Polynesia since the tests? As harsh as it may seem, reports of an increase of birth defects are all anecdotal - there simply isn't a register of birth defects in French Polynesia. And while evidence is building that cancers are increasing, there are other explanations. Dr Andrew McEwan National Radiation Lab, New Zealand "Increasing cancers will rise if the people live longer, if the life expectancy goes up then the cancer rates go up because cancer rates increase with age. Another cause of increased cancer is changes and life style factors such as increases in smoking and if the population is smoking heavily then there will be a very considerable rise in lung cancers and other cancers." LEAKAGE FROM UNDERGROUND TESTS The French still contend their atmospheric tests were safe, but they did respond to international pressure on health concerns. In 1975, more than a decade after Britain, the US and Russia moved their nuclear tests underground, the French finally followed suit. But while other nuclear powers moved out of the Pacific, the French stayed put. And on this question the scientific consensus is the French were mistaken. An atoll is no place to store nuclear waste. Mururoa is a seamount- formed more than 7 million years ago when a volcano erupted beneath the sea. When lava hits cold water it forms intertwining tubes of rock, which build up a mountain. The mountain erodes leaving a basalt base and a middle layer of soil. The top layer of limestones and corals leaks like a sieve. Prof Michael Michael O'Sullivan University of Auckland "There's a very permeable zone from the level where the arrow at about 400 hundred meters below sea level up to the surface and that consists of limestones which are naturally very permeable and very leaky and the heavy ocean water here drives the water through the atoll up into the lagoon." Any nuclear waste would get through these middle and top layers very quickly. But the shafts for the underground tests are up to 1,000 metres deep in the basalt base, supposedly well clear of the leaky layers. Megan James "What happens when you add a nuclear explosion or two according to the French?" Prof Michael Michael O'Sullivan University of Auckland "Well we detonate a bomb//down in these deep basalts and then what the French claim is this kind of scenario where we have a chamber here which consists of glassified rock which is broken up in little lumps and surrounding that they say the rock is not very badly effected. So the natural flow of water is virtually unaffected by the bomb going off and radioactivity is safe down in the volcanic rock." But there's a problem. The French claim the chamber is sealed, yet cools quickly. The only way it could cool quickly is if the chamber is really so cracked it allows cooling water to get in and out. Prof Michael Michael O'Sullivan, University of Auckland "Now we have a large fractured chamber// Then the water can get down into the bomb site and up again." Professor O'Sullivan concludes radioactivity must, in time, leak out. Is it leaking now? The latest evidence we can now reveal strongly supports France's claim underground testing has not poisoned the marine environment. These are samples of foodstuffs collected on Mururoa by the International Atomic Energy Agency. Fish from the lagoon, spiny lobster, molluscs and coconut milk. The Australian Radiation Laboratory was one of 8 around the world given samples of the same organisms collected at the same time. The analysis shows there is radioactivity in the samples, but the levels are very low. The results are credible because all 8 labs concur. Among them was New Zealand's National Radiation Laboratory. Dr Andrew McEwan National Radiation Lab, New Zealand "The levels in those fish did show traces of Caesium 137 and strontium 90 which one would expect and the levels were fairly consistent with what one would expect from global fallout but there is certainly no evidence of significant leakage of any type." Mururoa may not be leaking now, yet even the French admit the radioactive waste stored under their feet will eventually escape. But they say it won't be for thousands, perhaps 10,000 years. The French claim the basalt that forms the base of Mururoa is not very porous; so any water in the blast chambers will take thousands of years to move through the rock. But there's good evidence it will happen much more quickly than that. Megan James "So these are actually samples from Mururoa itself?" Prof Peter Davies University of Sydney "That's correct." Professor Peter Davies visited Mururoa in the early 80s. He studied just how porous the atoll's base is. Some parts are far leakier than others. Prof Peter Davies "If you look at this sample for example there are nicks and cracks through the sample indicating that there are fisures which run through the sample and that is very important in terms of the conactivity of the pores in other words how water will transport through the rock." Megan James "And from the variety of porosities that you're looking at here how did you redo the sums on the how long it would take for leakage to occur from these basalt chambers?" Prof Peter Davies "From that I calculated best case scenarios of greater than 500 years for leakage fluids from the middle of the atoll." Megan James "And a worst case scenario?" Prof Peter Davies "Well the worst case scenario is related to something happening associated with the test and that's almost instantaneous." And accidents have happened. In July 1979, a 120 kiloton bomb got stuck halfway down the shaft, at 400 metres. They exploded it anyway, and because tests were then on the rim of the atoll, part of the southern side collapsed in an underwater landslide. Prof Peter Davies University of Sydney "The French have admitted to some million cubic meters of rock having come away from the side of the atoll. Well a million cubic meters is substantial however think of what it means: it's a hundred meters by a hundred meters by a hundred meters// that is actually a small portion of the atoll but nevertheless// they've also moved their tests back into the lagoon. And I don't think that they have reported or anybody has reported land slides since." Because the tests are now in the centre of the atoll, and the bombs are now smaller, the risk of a major collapes is very low. Long term leakage remains by far the most realistic scenario. Prof Peter Davies University of Sydney "In 500 years or whatever it is and I don't know what the exact time is but at whatever time, there will be the potential for Mururoa to leak radionuclides into the biosphere." But such leakage may not be as dangerous as we've been lead to believe. THE DANGER FROM LEAKAGE Dr Murray Matthews National Radiation Lab New Zealand. "Well a key factor which seems to be overlooked in most people's arguments is just what the source term is, how much radioactivity is locked up in Mururoa after all of these tests, it seems that in many circles some people think a very large amount of radioactivity is there and it should be called into perspective how much is there. " The total fallout from all atmospheric tests ever conducted is 300 megatons. The total of France's underground tests to date is just under 3 megatons. We know that from New Zealand's seismic monitoring stations. Most of that 3 megatons is locked in the glassy lining of the cavity created by the explosion. Only around 5% is loose in the blast chamber.Let's imagine for a moment that somehow it all leaked out tomorrow. Incredible as it may seem, the sums done by NZ's radiation scientists suggest there'd be no great danger to environment or health. Dr Murray Matthews National Radiation Lab New Zealand "Well most of our reasoning in this area is based on recommendations of the International Commission on radiological protection and that body produces recommendations for limits of intake - they call annual limits of intake and if all of the material presently in Mururoa were to dissolve in a lagoon that size if it were fresh water, one could drink around about 300 litres of that before one would reach the annual limit of intake". Megan James "What about in a couple of hundred years, which is the best estimate, what would be the danger then?" Dr Andrew McEwan National Radiation Lab, New Zealand "Well if one goes to hundreds of years to the future, then the fission products of more particular concern like caesium 137, strontium, they have halflives of 30yrs, so going 90 years is going through three half lives that the total amount is down to 1/8th - go another 90 yrs it is down to a 64th so it is actually decaying away and if you go hundreds of years into the future then you probably haven't got a lot of radioactivity to worry about." None of these scientists is saying that Mururoa is contamination-free. It's known that in 1981 a typhoon washed between 10-20 kgms of plutonium, the legacy of earlier weapons safety tests, into the lagoon. Plutonium, when it's in the air and can be inhaled, is one of the deadlies substances we know. But in a marine environment like the lagoon, plutonium gets very strongly bound up in sediments, very little gets into the food chain. (as confirmed by the latest IAEA study.) Megan James: A lot of people might interpret this information as scientists saying that the testing is ok. That it can go ahead? Dr Murray Matthews National Radiation Lab New Zealand "Well there are two distinct sets of issues related to Mururoa as the public see it. There are the what I would call political philosophical issues of whether we want weapons to be developed, whether we want nuclear proliferation, whether we want more people with nuclear weapons on the planet, there are those political and philosophical issues and then there are the environmental ones which I have been talking about. All I'm saying is that the environmental issues are not as great as people will appear to think they are". Prof Peter Davies University of Sydney "The French are their own worst enemy. I think they have a huge data base which if shared properly with the scientific community would help to dispell many many of the problems that people currently relate to what is happening at Mururoa because on the basis of easily verifiable experiments it would be possible to show that much of the French data is correct. But they label everything confidential and therefore it never sees the light of day- it does them no good at all I said this to them in 1984." THE REAL DANGER OF TESTS The weight of scientific evidence is that the test pose no great danger to human health or the environment of the region. The real danger is that France's and China's resumption of testing may derail progress to a world free of nuclear weapons. Dr Karin von Strokirch Australian National University "Well President Chirac gave the most detailed statement about the purpose of the nuclear tests one month after his announcement of test resumption. He explained to the French senate in Paris that of the eight tests four would be used to perfect computer simulation of nuclear tests two would be used to test the reliability and effectiveness of ageing detonators and fuses and the remainder, that is to say two, would be used to test what he called a new war head." That new warhead may be TN75, the TN100 or a new generation variable yield warhead: potential first strike weapons. Many of the new generation nuclear warheads are small enough for their testing to be hidden. Dr Peter Wills Greenpeace Spokesperson "The thing that really makes me suspicious is that the military the French military wanted to to conduct 20 tests they said before France signed the comprehensive test ban now that makes me wonder if the eight tests which have announced involve something of the order of 20 devices rather than just eight as you would have thought." Dr Karin von Strokirch Australian National University "Well I believe that in the past the United States has conducted two nuclear weapons test explosion simultaneously, there's no reason to believe that France can't do that." If the French do test 2 devices simultaneously, we won't know. The Seimological Centre in Canberra will be the first place in Australia to detect any explosion. But from this distance, they can't identify an explosion under 1 kt, if masked by a larger one. Dr Peter Wills Greenpeace Spokesperson "In the broader picture in the long run the reason to have a comprehensive test ban and to stop testing is to inhibit the development of nuclear weapons and the great offence which France is causing at the moment is that they say they will sign the comprehensive test ban when they have developed the means for circumventing it." Those means are computers. France needs the field data from the Mururoa tests to perfect its computer simulation programs. But even if we stopped the French tests, other nuclear nations could continue the electronic version of the arms race. Because even under the proposed treaty banning all field tests, nuclear weapons are allwed to be developed and refined, via computers. We've directed all our protest efforts at trying to stop this series of French tests, as though stopping them would somehow stop the arms race. Perhaps our protests would be better directed at ensuring next year's treaty is comprehensive in its truest sense. Dr Karin von Strokirch Australian National University "No nuclear tests full stop no simulation full stop don't allow it nothing nothing is going to be allowed that will help a nation to develop nuclear weapons. " "The opportunity we have now for achieving a comprehensive test ban is greater than it's been at any time since nuclear weapons have been invented// And the danger is if we don't achieve a ban within the coming year the political situation could change in any one of the main players' countries. For example the United States is having presidential elections Russia is looking towards presidential elections the Chinese paramount leader may die in the not too distant future and if the political context changes in one of the nuclear weapons states it may change the whole of the nature of the negotiations for a comprehensive test ban". Megan James "We would have lost that opportunity?" Dr Karin von Strokirch Australian National University "Mmm this is a window of opportunity now and we need to take it while its there." END This program was produced by Quantum and 1st screened on August 23 1995 It was written and reported by Megan James. Copyright Australian Broadcasting Corporation 1995 WT03-B20-69IA005-000051-B017-194http://lacebark.ntu.edu.au:80/j_mitroy/sid101/uncc/fs-index.html 138.80.61.12 19970221151614 text/html 9876HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:46:36 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 9705Last-modified: Tue, 09 Jul 1996 02:25:55 GMT Climate Change Fact Sheets-INDEX Index to Climate Change Fact Sheets The 90-plus fact sheets in this section are organised into three series: Causes, Impacts, and International Response. For more information see the Introduction and the forewords by UNEP Executive Director Elizabeth Dowdeswell and WMO Secretary-General G.O.P Obasi. I The Causes of Climate Change The climate system The role of human activities II: The Impacts of Climate Change III The International Response to Climate Change International law and the UN Energy and economics New techniques and strategies Series One: The causes of climate change The climate system An introduction to the science of man-made climate change 1 The role of greenhouse gases 2 An introduction to the climate system 3 Radiation, climate, and climate change 4 Is the Earth warming up yet? 5 How records from past climates support the case for global warming 6 Measuring the "global warming potential" of greenhouse gases 7 Why three hot summers don't mean global warming 8 Why "climate change" and "global warming" are not the same thing 9 The "missing carbon" problem 11 How much will the climate change? 13 How climate models work 14 Are climate models reliable? 15 What happens when we double CO2 in a climate model? 16 How natural climate variability differs from climate change 17 How researchers develop regional scenarios of climate change 18 Oceans and the carbon cycle 21 Ocean circulation patterns 22 The role of human activities Energy and greenhouse gas emissions 25 Fuel and the carbon cycle 26 How extracting and transporting fossil fuel releases greenhouse gases 27 Global energy use during the Industrial Age 28 Why cement-making produces carbon dioxide 30 Chlorofluorocarbons (CFCs) and climate change 31 Emissions of methane from livestock 32 Methane emissions from the disposal of livestock wastes 33 Methane emissions from rice cultivation 34 Series Two: The impacts of climate change The impact of climate change on agriculture 101 Climate change and sea-level 102 Climate change and desertification 103 The impact of climate change on water resources 104 Will climate change lead to more extremes and disasters? 105 How climate change might impact the European Alps 106 A survey of possible social impacts 107 Are we overlooking the social and political implications of climate change? 108 Climate change and North-South relations 109 The issue of winners and losers 110 Why the poor are most vulnerable 111 Will there be growing numbers of environmental migrants? 112 Will the North-South gap widen? 113 More conflict between nations? 114 Societies under stress 115 The possible health effects 116 The possible cultural and psychological impacts 118 Egypt and climate change 119 Climate variability in Brazil 120 How climate change could impact Southeast Asia 121 How researchers assess the implications of climate change for agriculture 122 Vietnam and climate change 123 How researchers assess the impacts of natural climate variability 125 Poland and climate change 126 Climate and food security 127 Zimbabwe's vulnerability to climate change 128 Series Three: The international response to climate change International law and the UN How policy-makers are responding to global climate change 201 International law and climate change 202 The Convention on Climate Change: What does it say? 250 Key legal aspects of the Climate Change Convention 251 The special concerns of coastal areas and small island states 203 The special concerns of developing countries 204 How the UN General Assembly is responding 205 How UNEP and WMO are responding 206 The UNCED "Earth Summit" 207 The Intergovernmental Panel on Climate Change (IPCC) 208 The Intergovernmental Negotiating Committee for a Framework Convention on Climate Change (INC/FCCC) 209 Why IPCC assessment reports are objective 210 The climate policy of the European Community 211 The climate policy of the Group of Seven (G-7) 212 The First World Climate Conference 213 The 1985 Villach Conference and its follow-up workshops on climate change 214 The Toronto and Ottawa conferences and the "Law of the Atmosphere" 215 The Tata Conference on Global Warming 216 The Hague Declaration 217 The Noordwijk Ministerial Declaration 218 The Cairo Compact on climate change 219 The Bergen Conference 220 The Second World Climate Conference 221 Conferences addressing the special concerns of developing countries 222 Regional climate conferences in Africa, Asia, and Latin America 223 Phasing out CFCs: The Vienna Convention and its Montreal Protocol 224 The Geneva Convention on Long-Range Transboundary Air Pollution 225 Energy and economics The economics of climate change 226 The economic argument for adopting a balanced response to climate change 227 How much would it cost to reduce net GHG emissions? 228 The economic benefits of cutting greenhouse gas emissions 229 The economics of carbon taxes 230 Cutting back greenhouse gases with tradable emissions permits 231 The economics of negotiating an international treaty on climate change 232 The case for reducing emissions despite scientific uncertainty 233 The economics of risk avoidance under uncertainty 234 The links between climate change and acid rain policies 235 A 'responsibility index' for climate change 237 Insurance against climate change? 238 Reducing greenhouse gas emissions from the energy sector 240 New techniques and strategies Reducing methane emissions from livestock farming 271 What can Thailand do about climate change? 272 WT03-B20-70IA006-000055-B013-143http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind2/farmlist.html 138.80.61.12 19970221181245 text/html 8306HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:43:01 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 8135Last-modified: Tue, 09 Jul 1996 02:25:46 GMT NFFO-Contracted Projects - listing Map  Generator                     Project Site                        DNC      Developer                 #    MW   Planning Status               Region 1   Heybush Ent. Development      Huntingdon, Cambridgeshire                    2   National Power                Camarthan Bay, West Glamorgan                                                    NOW CLOSED 3   Powergen Plc                  Richborough Power Station, Kent 4   UK Wind Farms Ltd             Mynydd Cemmaes, Powys                        National Wind Power      24    7.2  Granted                       MANWEB 5   UK Wind Farms Ltd             Kirkby Moor, Ulverston, Cumbria              National Wind Power      25    7.5  Granted                       NORWEB 6   Renewable Energy Systems Ltd  St. Mary's, Isles of Scilly                   7   Yorkshire Water Services Ltd  Chelker Water, Addingham, Yorks              Yorkshire Windpower       4    1.2  Granted                       YEB 8   Yorkshire Water Services Ltd  Ovenden Moor, Denholme, Yorks                Yorkshire Windpower      31    9.3  Granted                       YEB 9   Wind Electric Ltd             Delabole, Cornwall                           Wind Electric            10    4.0  Vestas Windane 34             SWEB41   Abbey Produce (Ramsey) Ltd    St. Marys Road, Ramsey, Cambs       42   Ada Projects Ltd              Penrhyddlan Windfarm               5.330     Ecogen                   43   12.9  Granted                       MANWEB43   Ada Projects Ltd              Llidiartywaun Windfarm             7.430     Ecogen                   60   18.0  Granted                       MANWEB44   Ada Projects Ltd              Rhyd-y-Groes 1 & 2 Rhosgoch        3.591     Ecogen                   28    8.4  Granted                       MANWEB45   Ada Projects Ltd              Aber Leri Windfarm, Borth, Dyfed   1.502     Ecogen                   12    3.6  Withdrawn                     MANWEB46   Anglesey Mining Plc           Parys Mountain, Arnlweh, Gwynedd   1.342     Anglesey Mining           8    3.2  Refused                       MANWEB47   Blyth Harbour Comsn /REGEN    Blyth Harbour, Northumberland      1.130     Blyth Harbour Wind Farms  9    2.7  Granted                       NEEB48   Cambridge Health Authority    Fulbourn Hospital                  0.065   49   Carters Wind Turbines         Great Orton Airport, Wigton        1.290     Carter Wind Turbines     10    3.0  Granted                       NORWEB50   Century Steels Ltd            Century Works, Penistone           0.084   51   Century Steels Ltd            Century Windfarm, Century Steels   1.677     Century Steel            13    5.2  Submitted                     YEB52   Cornwall Light & Power Co Ltd Goonhilly Downs, Cury, Helston     2.348     Cornwall Light and Power 14    5.6  Granted                       SWEB53   D Gillson & Son Ltd           Naylor Hill Quarry, Howarth        0.095  54   Ecogen Ltd                    Buttriss Downs, Rame, Cornwall     1.502     Ecogen                   12    3.6  Submitted                     SWEB55   Ecogen Ltd                    Four Burrows, Blackwater, Cornwall 2.002     Ecogen                   16    4.8  Submitted                     SWEB56   Ecogen Ltd                    Ventongimps, Goodhavern            1.001     Ecogen                    8    2.4  Submitted without ES          SWEB57   Ecogen Ltd                    Meddon Moor, Hartland, Devon       0.876     Ecogen                    7    2.1  Refused                       SWEB58   Ecogen Ltd                    Crimp Moor, Kilkhampton, Bodmin    0.751     Ecogen                    6    1.8  Refused                       SWEB59   Ecogen Ltd                    Landcarrow Wind Farm               0.375     Ecogen                    3    0.9  Submitted                     SWEB60   Ecogen Ltd                    St. Breock, Wadebridge, Cornwall   2.127     Ecogen                   17    5.5  Submitted without ES          SWEB61   Ecogen Ltd                    Trebullett, near Launceston        2.002     Ecogen                   16    4.8  Submitted                     SWEB62   Ecogen Ltd                    Liftondown Wind Farm               2.878     Ecogen                   23    6.9  Submitted without ES          SWEB63   Windstar Turbines Ltd         Werfa Mynydd Llangeinwyr           0.210     Windstar Turbines        10    0.5  Submitted                     SWaEB64   Windstar Turbines Ltd         Caer-Bran Farm, Cornwall           0.420     Windstar Turbines        20    1    Unknown                       SWEB65   Windstar Turbines Ltd         Hailglower Farm Area, Cornwall     0.420     Windstar Turbines        20    1    Unknown                       SWEB66   Windstar Turbines Ltd         Tregerest Farm, Cornwall           0.420     Windstar Turbines        20    1    Unknown                       SWEB67   James Paget Hospital          James Paget Hospital, Norfolk      0.095  68   National Wind Power           Cold Northcott, Launceston         2.826     Ecogen                   21    6.6  Granted                       SWEB69   Oil Tools Offshore Services   Blood Hill, Winterton-on-Sea       0.946     Oiltool Offshore Serv    10    2.25 Granted                       EEB70   Perma Energy Ltd              Taff-Ely, Bridgend                 3.842     Perma Energy             20    9.0  Granted                       SWaEB71   Renewable Energy Systems Ltd  Carland Cross, Cornwall            2.535     Renewable Energy Systems 15    6.0  Granted                       SWEB72   Renewable Energy Systems Ltd  Coal Clough Farm, West Yorks       4.055     Renewable Energy Systems 24    9.6  Awaiting response from SE     NORWEB73   Renewable Energy Systems Ltd  Penrhys, Ystrad, Rhondda           2.028     Renewable Energy Systems 12    4.8  Refused-Resubmitting          SWaEB74   Resource Conservation Plc     High Mowthorpe EHF, Malton         0.058  75   UK Wind Farms Ltd             Llangwyryfon Windfarm, Dyfed       2.580     National Wind Power      20    6.0  Granted                       MANWEB76   UK Wind Farms Ltd             Lluest Dolgwiail, near Llangurig   3.354     National Wind Power      24    7.2  In-progress                   MANWEB77   UK Wind Farms Ltd             Marchwini Wind Farm, Dolhelfa      4.257     National Wind Power      33   10.0  In-progress                   SWaEB78   Vestas DWT                    J Stobbart & Sons, Heskett - 1     0.086   79   Vestas DWT                    J Stobbart & Sons, Heskett - 2     0.168  80   Vestas DWT                    Caton Moor, Lancashire             1.677     Whitendale Generation    10    4.0  Submitted                     NORWEB81   Vestas DWT                    Dry Hill Farm, Huddersfield        0.168 82   Waywind Farm                  Waywind Farm Way, Barton, Devon    0.561     Aylescott Driers          6    1.35 Refused-Appealing             SWEB83   West Beacon Farm Power        West Beacon Farm, Leics            0.022   84   West Coast Wind Farms         Fullabrook/Crackaway, North Devon  4.988     West Coast Wind Farms    29   11.6  Awaiting outcome of PE        SWEB85   Windcluster Ltd               Haverigg Airfield, Cumbria         0.478     Windcluster               5    1.25 Granted                       NORWEB86   Wind Energy Group Ltd         Slade Hill, Slade, Illfracombe     0.086  87   Wind Power Systems Ltd        Dyffryn-Brodyn, Blaenwaun, Dyfed   2.348     Wind Power Systems       14    5.6  Granted                       SWaEB88   Yorkshire Wind Turbines Ltd   Kexgill, Blubberhouses             3.827     Yorkshire Wind Turbines  20    9.0  In-progress                   YEB89   Yorkshire Water Enterprises   Royd Moor Farm, Sheffield          2.492     Yorkshire Water          13    6.0  In-progress                   YEB back to U.K. Wind Farms with N.F.F.O. Contracts WT03-B20-71IA006-000055-B005-4http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl2/c02.html 138.80.61.12 19970221174335 text/html 15696HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:13:44 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 15524Last-modified: Tue, 09 Jul 1996 02:25:47 GMT Chapter II THE RELEASE, DISPERSION AND DEPOSITION OF RADIONUCLIDES The source term The "source term" is a technical expression used todescribe the accidental release of radioactive material from anuclear facility to the environment. Not only are the levels ofradioactivity released important, but also their distributionin time as well as their chemical and physical forms. The initialestimation of the Source Term was based on air sampling and theintegration of the assessed ground deposition within the thenSoviet Union. This was clear at the IAEA Post-Accident ReviewMeeting in August 1986 (IA86), when the Soviet scientistsmade their presentation, but during the discussions it was suggestedthat the total release estimate would be significantly higherif the deposition outside the Soviet Union territory were included.Subsequent assessments support this view, certainly for the caesiumradionuclides (Wa87, Ca87, Gu89). The initial estimateswere presented as a fraction of the core inventory for the importantradionuclides and also as total activity released. Atmospheric releases In the initial assessment of releases made by the Soviet scientistsand presented at the IAEA Post-Accident Assessment Meeting inVienna (IA86), it was estimated that 100 per cent of thecore inventory of the noble gases (xenon and krypton) was released,and between 10 and 20 per cent of the more volatile elements ofiodine, tellurium and caesium. The early estimate for fuel materialreleased to the environment was 3 ± 1.5 per cent (IA86).This estimate was later revised to 3.5 ± 0.5 per cent (Be91).This corresponds to the emission of 6 t of fragmented fuel. The IAEA International Nuclear Safety Advisory Group (INSAG) issuedin 1986 its summary report (IA86a) based on the informationpresented by the Soviet scientists to the Post-Accident ReviewMeeting. At that time, it was estimated that 1 to 2 exabecquerels(EBq) were released. This did not include the noble gases, andhad an estimated error of ±50 per cent. These estimates ofthe source term were based solely on the estimated depositionof radionuclides on the territory of the Soviet Union, and couldnot take into account deposition in Europe and elsewhere, as thedata were not then available. However, more deposition data (Be90) were available when,in their 1988 Report (UN88), the United Nations ScientificCommittee on the Effects of Atomic Radiation (UNSCEAR) gave releasefigures based not only on the Soviet data, but also on worldwidedeposition. The total caesium-137 release was estimated to be70 petabecquerels (PBq) of which 31 PBq were deposited in theSoviet Union. Later analyses carried out on the core debris and the depositedmaterial within the reactor building have provided an independentassessment of the environmental release. These studies estimatethat the release fraction of caesium-137 was 20 to 40 per cent(85 ± 26 PBq) based on an average release fraction from fuelof 47 per cent with subsequent retention of the remainder withinthe reactor building (Be91). After an extensive reviewof the many reports (IA86, Bu93), this was confirmed.For iodine-131, the most accurate estimate was felt to be 50 to60 per cent of the core inventory of 3,200 PBq. The current estimateof the source term (De95) is summarised in Table 1. The release pattern over time is well illustrated in Figure 3(Bu93). The initial large release was principally due tothe mechanical fragmentation of the fuel during the explosion.It contained mainly the more volatile radionuclides such as noblegases, iodines and some caesium. The second large release betweenday 7 and day 10 was associated with the high temperatures reachedin the core melt. The sharp drop in releases after ten days mayhave been due to a rapid cooling of the fuel as the core debrismelted through the lower shield and interacted with other materialin the reactor. Although further releases probably occurred after6 May, these are not thought to have been large. Figure 3. Daily release rate of radioactive substances into the atmosphere (modif. from IA86a) Chemical and physical forms The release of radioactive material to the atmosphere consistedof gases, aerosols and finely fragmented fuel. Gaseous elements,such as krypton and xenon escaped more or less completely fromthe fuel material. In addition to its gaseous and particulateform, organically bound iodine was also detected. The ratios betweenthe various iodine compounds varied with time. As mentioned Table 1. Current estimate of radionuclide releases during the Chernobyl accident (modif. from De95)        Core inventory                        Total release during      on 26 April 1986                           the accident Nuclide    Half-life    Activity         Percent of     Activity                              (PBq)           inventory       (PBq)         33Xe       5.3 d     6 500               100        6500            131I       8.0 d     3 200             50 - 60     ~1760          134Cs       2.0 y       180             20 - 40       ~54            137Cs      30.0 y       280             20 - 40       ~85            132Te      78.0 h     2 700             25 - 60     ~1150          89Sr      52.0 d     2 300              4 - 6       ~115            90Sr      28.0 y       200              4 - 6        ~10            140Ba      12.8 d     4 800              4 - 6       ~240          95Zr       1.4 h     5 600               3.5         196             99Mo      67.0 h     4 800              >3.5        >168          103Ru      39.6 d     4 800              >3.5        >168         106Ru       1.0 y     2 100              >3.5         >73          141Ce      33.0 d     5 600               3.5         196            144Ce     285.0 d     3 300               3.5        ~116         239Np       2.4 d    27 000               3.5         ~95          238Pu      86.0 y         1               3.5           0.035       239Pu  24 400.0 y         0.85            3.5           0.03           240Pu   6 580.0 y         1.2             3.5           0.042          241Pu      13.2 y       170               3.5          ~6          242Cm     163.0 d        26               3.5          ~0.9    above, 50 to 60 per cent of the core inventory of iodine was thoughtto have been released in one form or another. Other volatile elementsand compounds, such as those of caesium and tellurium, attachedto aerosols, were transported in the air separate from fuel particles.Air sampling revealed particle sizes for these elements to be0.5 to 1 mm. Unexpected features of the source term, due largely to the graphitefire, were the extensive releases of fuel material and the longduration of the release. Elements of low volatility, such as cerium,zirconium, the actinides and to a large extent barium, lanthaniumand strontium also, were embedded in fuel particles. Larger fuel particles were deposited close to the accidentsite, whereas smaller particles were more widely dispersed. Othercondensates from the vaporised fuel, such as radioactive ruthenium,formed metallic particles. These, as well as the small fuel particles,were often referred to as "hot particles", and werefound at large distances from the accident site (De95). Dispersion and deposition Within the former Soviet Union During the first 10 days of the accident when important releasesof radioactivity occurred, meteorological conditions changed frequently,causing significant variations in release direction and dispersionparameters. Deposition patterns of radioactive particles dependedhighly on the dispersion parameters, the particle sizes, and theoccurrence of rainfall. The largest particles, which were primarilyfuel particles, were deposited essentially by sedimentation within100 km of the reactor. Small particles were carried by the windto large distances and were deposited primarily with rainfall. The radionuclide composition of the release and of the subsequentdeposition on the ground also varied considerably during the accidentdue to variations in temperature and other parameters during therelease. Caesium-137 was selected to characterise the magnitudeof the ground deposition because (1) it is easily measurable,and (2) it was the main contributor to the radiation doses receivedby the population once the short-lived iodine-131 had decayed. The three main spots of contamination resulting from the Chernobylaccident have been called the Central, Bryansk-Belarus, and Kaluga-Tula-Orelspots (Figure 4). The Central spot was formed during the initial,active stage of the release predominantly to the West and North-west (Figure 5). Ground depositionsof caesium-137 of over 40 kilobecquerels per square metre [kBq/m2]covered large areas of the Northern part of Ukraine and of theSouthern part of Belarus. The most highly contaminated area wasthe 30-km zone surrounding the reactor, where caesium-137 grounddepositions generally exceeded 1,500 kBq/m2 (Ba93). The Bryansk-Belarus spot, centered 200 km to the North-northeastof the reactor, was formed on 28-29 April as a result of rainfallon the interface of the Bryansk region of Russia and the Gomeland Mogilev regions of Belarus. The ground depositions of caesium-137in the most highly contaminated areas in this spot were comparableto the levels in the Central spot and reached 5,000 kBq/m2 insome villages (Ba93). Figure 4. Main spots of caesium-137 contamination Figure 5. Central spot of caesium-137 contamination The Kaluga-Tula-Orel spot in Russia, centered approximately 500km North-east of the reactor, was formed from the same radioactivecloud that produced the Bryansk-Belarus spot, as a result of rainfallon 28-29 April. However, the levels of deposition of caesium-137were lower, usually less than 600 kBq/m2 (Ba93). In addition, outside the three main hot spots in the greater partof the European territory of the former Soviet Union, there weremany areas of radioactive contamination with caesium-137 levelsin the range 40 to 200 kBq/m2. Overall, the territory of the formerSoviet Union initially contained approximately 3,100 km2 contaminatedby caesium-137 with deposition levels exceeding 1,500 kBq/m2;7,200 km2 with levels of 600 to 1,500 kBq/m2; and 103,000 km2with levels of 40 to 200 kBq/m2 (US91). Outside the former Soviet Union Radioactivity was first detected outside the Soviet Union at aNuclear Power station in Sweden, where monitored workers werenoted to be contaminated. It was at first believed that the contaminationwas from a Swedish reactor. When it became apparent that the Chernobylreactor was the source, monitoring stations all over the worldbegan intensive sampling programmes. The radioactive plume was tracked as it moved over the Europeanpart of the Soviet Union and Europe (Figure 6). Initially thewind was blowing in a Northwesterly direction and was responsiblefor much of the deposition in Scandinavia, the Netherlands andBelgium and Great Britain. Later the plume shifted to the South and much of Central Europe, as well as the NorthernMediterranean and the Balkans, received some deposition, the actualseverity of which depended on the height of the plume, wind speedand direction, terrain features and the amount of rainfall thatoccurred during the passage of the plume. The radioactive cloud initially contained a large number of differentfission products and actinides, but only trace quantities of actinideswere detected in most European countries, and a very small numberwere found in quantities that were considered radiologically significant.This was largely due to the fact that these radionuclides werecontained in the larger and heavier particulates, which tendedto be deposited closer to the accident site rather than furtheraway. The most radiologically important radionuclides detectedoutside the Soviet Union were iodine-131, tellurium/iodine-132,caesium-137 and caesium-134. Figure 6. Areas covered by the main body of the radioactivecloud on various days during the release Most countries in Europe experienced some deposition of radionuclides,mainly caesium-137 and caesium-134, as the plume passed over thecountry. In Austria, Eastern and Southern Switzerland, parts of Southern Germanyand Scandinavia, where the passage of the plume coincided withrainfall, the total deposition from the Chernobyl release wasgreater than that experienced by most other countries, whereasSpain, France and Portugal experienced the least deposition. Forexample, the estimated average depositions of caesium-137 in theprovinces of Upper Austria, Salzburg and Carinthia in Austriawere 59, 46 and 33 kBq/m2 respectively, whereas the average caesium-137deposition in Portugal was 0.02 kBq/m2 (Un88). It was reportedthat considerable secondary contamination occurred due to resuspensionof material from contaminated forest. This was not confirmed bylater studies. While the plume was detectable in the Northern hemisphere as faraway as Japan and North America, countries outside Europe receivedvery little deposition of radionuclides from the accident. Nodeposition was detected in the Southern hemisphere (Un88). In summary it can be stated that there is now a fairly accurateestimate of the total release. The duration of the release wasunexpectedly long, lasting more than a week with two periods ofintense release. Another peculiar feature was the significantemission (about 4 per cent) of fuel material which also containedembedded radionuclides of low volatility such as cerium, zirconiumand the actinides. The composition and characteristics of theradioactive material in the plume changed during its passage dueto wet and dry deposition, decay, chemical transformations andalterations in particle size. The area affected was particularlylarge due to the high altitude and long duration of the releaseas well as the change of wind direction. However, the patternof deposition was very irregular, and significant deposition ofradionuclides occurred where the passage of the plume coincidedwith rainfall. Although all the Northern hemisphere was affected,only territories of the former Soviet Union and part of Europeexperienced contamination to a significant degree. WT03-B20-72IA006-000055-B005-109http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl2/c04.html 138.80.61.12 19970221174459 text/html 23042HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:14:48 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 22870Last-modified: Tue, 09 Jul 1996 02:25:47 GMT Chapter IV DOSE ESTIMATES The exposure of the population as a result of the accident resultedin two main pathways of exposure. The first is the radiation doseto the thyroid as a result of the concentration of radioiodineand similar radionuclides in the gland. The second is the whole-bodydose caused largely by external irradiation mainly from radiocesium. The absorbed dose to the whole body is thought to be about 20times more deleterious, in terms of late health effects incidence,than the same dose to the thyroid (IC90). The population exposed to radiation following the Chernobyl accidentcan be divided into four categories: (1) the staff of the nuclearpower plant and workers who participated in clean-up operations(referred to as "liquidators"); (2) the nearby residentswho were evacuated from the 30-km zone during the first two weeksafter the accident; (3) the population of the former Soviet Union,including especially the residents of contaminated areas; and(4) the population in countries outside the former Soviet Union. A number of liquidators estimated to amount up to 800,000 tookpart in mitigation activities at the reactor and within the 30-kmzone surrounding the reactor. The most exposed workers were thefiremen and the power plant personnel during the first days ofthe accident. Most of the dose received by the workers resultedfrom external irradiation from the fuel fragments and radioactiveparticles deposited on various surfaces. About 135,000 people were evacuated during the first days followingthe accident, mainly from the 30-km zone surrounding the reactor.Prior to evacuation, those individuals were exposed to externalirradiation from radioactive materials transported by the cloudand deposited on the ground, as well as to internal irradiationessentially due to the inhalation of radioactive materials inthe cloud. The relative contributions to the external whole-body dose fromthe main radionuclides of concern for that pathway of exposureand during the first few months after the accident are shown inFigure 7. It is clear that tellurium-132 played a major role inthe first week after the accident, and that, after one month,the radiocaesiums (caesium-134 and caesium-137) became predominant.Subsequently, however, caesium-134 decayed to levels much lowerthan those of caesium-137, which became after a few years theonly radionuclide of importance for practical purposes. It isusual to refer to caesium-137 only, even when the mix of caesium-134and caesium-137 is meant, because the values for the constituentscan be easily derived from those for caesium-137. Figure 7. Relative contribution of gamma radiation from individual radionuclides to the absorbed dose rate in air during the first several months after the Chernobyl accident (Go93) With regard to internal doses from inhalation and ingestion ofradionuclides, the situation is similar: radioiodine was importantduring the first few weeks after the accident and gave rise tothyroid doses via inhalation of contaminated air, and, more importantly,via consumption of contaminated foodstuffs, mainly cow's milk.After about one month, the radiocaesiums (caesium-134 and caesium-137)again became predominant, and, after a few years, caesium-137became the only radionuclide of importance for practical purposes,even though strontium-90 may in the future play a significantrole at short distances from the reactor. Among the population of the former Soviet Union, it is usual tosingle out the residents of the contaminated areas, defined asthose with caesium-137 deposition levels greater than 37 kBq/m2.About 4 million people live in those areas. Of special interestare the inhabitants of the spots with caesium-137 deposition levelsgreater than 555 kBq/m2. In those areas, called "strict controlzones", protection measures are applied, especially as faras control of consumption of contaminated food is concerned. Early after the accident, the All-Union Dose Registry (AUDR) wasset up by the Soviet Government in 1986 to record medical anddosimetric data on the population groups expected to be the most exposed: (1) the liquidators,(2) the evacuees from the 30-km zone, (3) the inhabitants of thecontaminated areas, and (4) the children of those people. In 1991,the AUDR contained data on 659,292 persons. Starting from 1992,national registries of Belarus, Russian Federation, and Ukrainereplaced the AUDR. Outside the former Soviet Union, the radionuclides of importanceare, again, the radioiodines and radiocaesiums, which, once depositedon the ground, give rise to doses from ingestion through the consumption of foodstuffs.Deposited radiocaesium is also a source of long-term exposurefrom external irradiation from the contaminated ground and othersurfaces. Most of the population of the Northern hemisphere wasexposed, in varying degrees, to radiation from the Chernobyl accident.The caesium-137 deposition outside the former Soviet Union rangedfrom negligible levels to about 50 kBq/m2. The liquidators Most of the liquidators can be divided into two groups: (1) thepeople who were working at the Chernobyl power station at thetime of the accident viz. the staff of the station and the firemenand people who went to the aid of the victims. They number a fewhundred persons; and (2) the workers, estimated to amount up to800,000, who were active in 1986-1990 at the power station orin the zone surrounding it for the decontamination, sarcophagusconstruction and other recovery operations. On the night of 26 April 1986, about 400 workers were on the siteof the Chernobyl power plant. As a consequence of the accident,they were subjected to the combined effect of radiation from severalsources: (1) external gamma/beta radiation from the radioactivecloud, the fragments of the damaged reactor core scattered overthe site and the radioactive particles deposited on the skin,and (2) inhalation of radioactive particles (UN88). All of the dosimeters worn by the workers were over-exposed anddid not allow an estimate of the doses received. However, informationis available on the doses received by the 237 persons who wereplaced in hospitals and diagnosed as suffering from acute radiationsyndrome. Using biological dosimetry, it was estimated that 140of these patients received whole-body doses from external irradiationin the range 1-2 Gy, that 55 received doses between 2 and 4 Gy, that 21 received between 4 and 6 Gy, and that the remaining21 received doses between 6 and 16 Gy. In addition, it was estimatedfrom thyroid measurements that the thyroid dose from inhalationwould range up to about 20 Sv, with 173 individuals in the 0-1.2Sv range and five workers with thyroid doses greater than 11 Sv(UN88). The second category of liquidators consists of the large numberof adults who were recruited to assist in the clean-up operations.They worked at the site, in towns, forests and agricultural areasto make them fit to work and live in. Several hundreds of thousandsof individuals participated in this work. Initially, 50 per centof those workers came from the Soviet armed forces, the otherhalf including personnel of civil organisations, the securityservice, the Ministry of Internal Affairs, and other organisations.The total number of liquidators has yet to be determined accurately,since only some of the data from some of those organisations havebeen collected so far in the national registries of Belarus, Russia,Ukraine and other republics of the former Soviet Union (So95).Also, it has been suggested that, because of the social and economicadvantages associated with being designated a liquidator, manypersons have contrived latterly to have their names added to thelist. There are only fragmented data on the doses received by the liquidators.Attempts to establish a dosimetric service were inadequate untilthe middle of June of 1986; until then, doses were estimated fromarea radiation measurements. The liquidators were initially subjectedto a radiation dose limit for one year of 250 mSv. In 1987 thislimit was reduced to 100 mSv and in 1988 to 50 mSv (Ba93).The registry data show that the average recorded doses decreasedfrom year to year, being about 170 mSv in 1986, 130 mSv in 1987,30 mSv in 1988 and 15 mSv in 1989 (Se95a). It is, however,difficult to assess the validity of the results as they have beenreported. It is interesting to note that a small special group of 15 scientistswho have worked periodically inside the sarcophagus for a numberof years have estimated accumulated whole-body doses in the range0.5 to 13 Gy (Se95a). While no deterministic effects havebeen noted to date, this group may well show radiation healtheffects in the future. The evacuees from the 30-km zone Immediately after the accident monitoring of the environment wasstarted by gamma dose rate measurements. About 20 hours afterthe accident the wind turned in the direction of Pripyat, gammadose rates increased significantly in the town, and it was decidedto evacuate the inhabitants. About 20 hours later the 49,000 inhabitantsof Pripyat had left the town in nearly 1,200 buses. About a further80,000 people were evacuated in the following days and weeks fromthe contaminated areas. Information relevant for the assessment of the doses receivedby these people have been obtained by responses of the evacueesto questionnaires about the location where they stayed, the typesof houses in which they lived, the consumption of stable iodine,and other activities (Li94). Doses to the thyroid gland The iodine activity in thyroid glands of evacuees was measured.More than 2,000 measurements of former inhabitants of Pripyathad sufficient quality to be useful for dose reconstruction (Go95a).A comparative analysis with the questionnaire responses showedthat thyroid doses were mainly due to inhalation of iodine-131.Average individual doses and collective doses to the thyroid areshown in Table 3 for three age groups. Individual doses in theage classes were distributed over two orders of magnitude. Themain factor influencing the individual doses was found to be thedistance of the residence from the reactor. Table 3. Average doses to the thyroid glandand collective thyroid doses to the evacuees from Pripyat(Go95a). Year of birth Number of people Average individual dose (Sv) Collective dose (person-Sv) 1983 - 1986 2,400 1.4 3,300 1971 - 1982 8,100 0.3 2,400 < = 1970 38,900 0.07 2,600 Assessments of the doses to the thyroid gland of the evacueesfrom the 30-km zone (Li93a) showed similar doses for young childrenas those for the Pripyat evacuees. Exposures to adults were higher.These high doses were due to a greater consumption of food contaminatedwith iodine-131 among those evacuated later from the 30-km zone. Whole-body doses The whole-body doses to the evacuees were mainly due to externalexposure from deposited tellurium-132/iodine-132, caesium-134and caesium137 and short lived radionuclides in the air. Measurementsof the gamma dose rate in air were performed every hour at aboutthirty sites in Pripyat and daily at about eighty sites in the30-km zone. Based on these measurements and using the responsesto the questionnaires, whole-body doses were reconstructed forthe 90,000 persons evacuated from the Ukrainian part of the 30-kmzone (Li94). There was a wide range of estimated doseswith an average value of 15 mSv. The collective dose was assessedto be 1,300 person-Sv. The 24,000 persons evacuated in Belarusmight have received slightly higher doses, since the prevailingwind was initially towards the north. People living in the contaminated areas Doses to the thyroid gland The main information source for the dose reconstruction is thevast amount of iodine activity measurements in thyroid glands.In Ukraine 150,000 measurements, in Belarus several hundreds ofthousands of measurements and in the Russian Federation more than60,000 measurements were performed in May/June 1986. Some of themeasurements were performed with inadequate instrumentation andmeasurement conditions and are not useful for dose assessmentpurposes. The large variability of individual doses makes estimates of dosedistributions difficult and current dose estimates are still subjectto considerable uncertainties, especially in areas where onlya few activity measurements in the thyroid were performed. Childrenin the Gomel oblast (region) in Belarus received the highest doses.An estimate (Ba94) of the dose distribution among thesechildren is shown in Table 4. For the whole Belarus the collectivethyroid dose to children (0 to 14 years) at the time of the accidentwas assessed to be about 170,000 person-Sv (Ri94). In theeight most contaminated districts of Ukraine where thyroid measurementswere performed, the collective dose to this age group was about60,000 person-Sv and for the whole population about 200,000 person-Sv(Li93). In the Russian Federation the collective dose tothe whole population was about 100,000 person-Sv (Zv93). Table 4. Distribution of thyroid doses tochildren (0-7 years) in the Gomel oblast of Belarus (Ba94). Thyroid dose (Sv) Number of children Collective dose (person-Sv) 0 - 0.3 15,100 2,300 0.3 - 2 13,900 11,500 2 - 10 3,100 13,700 10 - 40 300 4,700 Evaluations of questionnaires on food consumption rates in theperiod May/June 1986 and measurements of food contamination showediodine-131 in milk as the major source for the thyroid exposureof the population living in the contaminated areas. However, inindividual cases the consumption of fresh vegetables contributedsignificantly to the exposure. Whole-body doses Two major pathways contributed to the whole-body doses of thepopulation in contaminated areas, the exposure to external irradiationfrom deposited radionuclides (Iv95) and the incorporationinto the body of radio-caesium in food. The external exposure is directly related to the radionuclideactivity per unit area and it is influenced by the gamma doserates in air at the locations of occupancy. Forestry workers andother workers living in woodframe houses received the highestdoses. Most of the higher contaminated areas are rural and a large partof the diet is locally produced. Therefore, the uptake of caesiumby the plants from the soil is a deciding factor in the internalexposure. These are regions with extraordinarily high transferfactors, as the Rovno region in Ukraine, where even moderate soilcontaminations led to high doses. In order of decreasing magnitudeof transfer factors these regions are followed by regions withpeaty soil, sandy podzol (acidic infertile forest soil), loamypodzol, and chernozem which is rich black soil. In the first years after the accident the caesium uptake was dominatedpractically everywhere by the consumption of locally producedmilk (Ho94). However, later mushrooms began to contributesignificantly in many settlements to the caesium incorporationfor two reasons. First, the milk contamination decreased withtime, whereas the mushroom contamination remained relatively constant.Second, due to changes in the economic conditions in the threerepublics, people are again collecting more mushrooms than theywere in the first years after the accident . Table 5 summarises a recent estimate of whole-body doses to peopleliving in the higher contaminated areas. On average, externalirradiation was by far the highest contributor to the total populationexposure (Er94). However, the highest doses to individualswere produced by the consumption of food from areas with hightransfers of radionuclides. Table 5. Distribution of externaland total whole-body doses during 1986-89, to inhabitants ofcontaminated areas (caesium137 activity per unit area > 555 kBg/m²)(Ba94) Whole-body dose (mSv) External exposure Total exposure No. of persons Collective dose (person-Sv) No. of persons Collective dose (person-Sv) 5 - 20 20 - 50 50 - 100 100 - 150 150 - 200 > 200 132,000 111,000 24,000 2,800 530 120 1,700 3 ,500 1,600 330 88 26 88,000 132,000 44,000 6,900 1,500 670 1,200 4,200 3,000 820 250 160 Total 270,000 7,300 273,000 9,700 Populations outside the former Soviet Union Even though the releases of radioactive materials during the Chernobylaccident mainly affected the populations of Belarus, Russia andUkraine, the released materials became further dispersed throughoutthe atmosphere and the volatile radionuclides of primary importance(iodine-131 and caesium-137) weredetected in most countries of the Northern hemisphere. However,the doses to the population were in most places much lower thanin the contaminated areas of the former Soviet Union; they reflectedthe deposition levels of caesium-137 and were higher in areaswhere the passage of the radioactive cloud coincided with rainfall.Generally speaking, however, and with a few notable exceptions,the doses decreased as a function of distance from the reactor(Ne87). During the first few weeks, iodine-131 was the main contributorto the dose, via ingestion of milk (Ma91). Infant thyroiddoses generally ranged from 1 to 20 mSv in Europe, from 0.1 to5 mSv in Asia, and were about 0.1 mSv in North America. Adultthyroid doses were lower by a factor of about 5 (UN88). Later on, caesium-134 and caesium-137 were responsible for mostof the dose, through external and internal irradiation (Ma89).The whole-body doses received during the first year followingthe accident generally ranged from 0.05 to 0.5 mSv in Europe,from 0.005 to 0.1 mSv in Asia, and of the order of 0.001 mSv inNorth America. The total whole-body doses expected to be accumulatedduring the lifetimes of the individuals are estimated to be afactor of 3 greater than the doses received during the first year(UN88). In summary, a large number of people received substantial dosesas a result of the Chernobyl accident: Liquidators - Hundreds of thousands of workers,estimated to amount up to 800,000, were involved in clean-up operations.The most exposed, with doses of several grays, were the workersinvolved immediately after the beginning of the accident and thescientists who have performed special tasks in the sarcophagus.The average doses to liquidators are reported to have ranged between170 mSv in 1986 and 15 mSv in 1989. Evacuees - More than 100,000 persons wereevacuated during the first few weeks following the accident. Theevacuees were exposed to internal irradiation arising from inhalationof radioiodines, especially iodine-131, and to external irradiationfrom radioactivity present in the cloud and deposited on the ground.Thyroid doses are estimated to have been, on average, about 1Sv for small children under 3 years of age and about 70 mSv foradults. Whole-body doses received from external irradiation priorto evacuation from the Ukrainian part of the 30-km zone showeda large range of variation with an average value of 15 mSv. People living in contaminated areas of the former SovietUnion - About 270,000 people live in contaminated areaswith caesium-137 deposition levels in excess of 555 kBq/m2. Thyroiddoses, due mainly to the consumption of cow's milk contaminatedwith iodine-131, were delivered during the first few weeks afterthe accident; children in the Gomel region of Belarus appear tohave received the highest thyroid doses with a range from negligiblelevels up to 40 Sv and an average close to 1 Sv for children aged0 to 7. Because of the control of foodstuffs in those areas, mostof the radiation exposure since the summer of 1986 is due to externalirradiation from the caesium-137 activity deposited on the ground;the whole-body doses for the 1986-1989 time period are estimatedto range from 5 to 250 mSv with an average of 40 mSv. In areaswithout food control, there are places, such as the Rovno regionof Ukraine, where the transfer of caesium137 from soil to plantis very high, resulting in doses from internal exposure beinggreater than those from external exposure. Populations outside the former Soviet Union -The radioactive materials of a volatile nature (such as iodineand caesium) that were released during the accident spread throughoutthe entire northern hemisphere. The doses received by populationsoutside the former Soviet Union were relatively low, and showedlarge differences from one country to another depending mainlyupon whether rainfall occurred during the passage of the radioactivecloud. WT03-B20-73IA006-000055-B005-300http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl2/c05.html 138.80.61.12 19970221174715 text/html 46262HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:15:56 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 46090Last-modified: Tue, 09 Jul 1996 02:25:47 GMT Chapter V HEALTH IMPACT As ionising radiation passes through the body, it interacts withthe tissues transfering energy to cellular and other constituentsby ionisation of their atoms. This phenomenon has been extensivelystudied in the critical genetic material, DNA, which controlsthe functions of the cells. If the damage to DNA is slight andthe rate of damage production is not rapid, i.e. at low dose rate,the cell may be able to repair most of the damage. If the damageis irreparable and severe enough to interfere with cellular function,the cell may die either immediately or after several divisions. At low doses, cell death can be accommodated by the normal mechanismsthat regulate cellular regeneration. However, at high doses anddose rates, repair and regeneration may be inadequate, so thata large number of cells may be destroyed leading to impaired organfunction. This rapid, uncompensatable cell death at high dosesleads to early deleterious radiation effects which become evidentwithin days or weeks of exposure, and are known as "deterministiceffects". These deterministic effects can be life-threateningin the short term if the dose is high enough, and were responsiblefor most of the early deaths in the Chernobyl accident. Lower doses and dose rates do not produce these acute early effects,because the available cellular repair mechanisms are able to compensatefor the damage. However, this repair may be incomplete or defective,in which case the cell may be altered so that it may develop intoa cancerous cell, perhaps many years into the future, or its transformationmay lead to hereditable defects in the long term. These late effects,cancer induction and hereditary defects, are known as "stochasticeffects" and are those effects whose frequency, not severity,is dose dependent. Moreover, they are not radiation-specific and,therefore, cannot be directly attributed to a given radiationexposure. For this reason, low dose health effects in humans cannot be measuredand, therefore, risk projections of the future health impact oflow-dose ionising radiation exposure have to be extrapolated frommeasured high-dose effects. The assumption was made that no doseof ionising radiation was without potential harm and that thefrequency of stochastic effects at low doses would be proportionalto that occurring at high doses. This prudent assumption was adoptedto assist in the planning of radiation protection provisions whenconsidering the introduction of practices involving ionising radiations.The ICRP has estimated the risk of fatal cancer to the generalpopulation from whole-body exposure to be 5 per cent per sievert(IC90). The health impact of the Chernobyl accident can be classifiedin terms of acute health effects ("deterministic effects")and of late health effects ("stochastic effects"); moreoverthere are also psychological effects which can influence health. Acute health effects All the acute deterministic health effects occurred among thepersonnel of the plant, or in those persons brought in for firefighting and immediate clean-up operations. Two deaths were immediately associated with the accident: oneperson killed by the explosion and another who suffered a coronarythrombosis. A third person died early the morning of the accidentfrom thermal burns. Twenty-eight other persons died later in thetreatment centres, bringing the total to 31 deaths in the firstweeks after the accident (UN88). All symptomatic exposed persons from the site were placed in hospitals.Of the total of 499 persons admitted for observation, 237 of thesewere initially diagnosed as suffering from acute radiation syndromeand most of these were hospitalised in the first 24 hours. Theseverity and rapidity of onset of their symptoms depended on theirdose. The initial early signs and symptoms of radiation sicknessfrom high doses included diarrhoea, vomiting, fever and erythema.Over 200 patients were placed in regional hospitals and specialisedcentres in the first 24 hours. Patients were allocated to fourcategories of radiation sickness severity according to their symptoms,signs and dose estimates. The differential white blood cell countshowed reduced circulating lymphocytes (lymphocytopenia) whichwas the initial indicator of the severity of the exposure andbecame evident in the first 24-36 hours for those most severelyirradiated. No members of the general public received such high whole-bodydoses as to induce Acute Radiation Syndrome (IA86). Thiswas confirmed in Belarus, where, between May and June 1986, 11,600people were investigated without the discovery of any cases ofacute radiation sickness. In the highest exposure group (6-16 Gy), the first reaction wasusually vomiting, occurring within 15-30 minutes of exposure.These patients were desperately ill; fever and intoxication aswell as diarrhoea and vomiting, were prominent features. Mucousmembranes were severely affected, becoming swollen, dry and ulcerated,making breathing and swallowing extremely painful and difficult.Extensive burns both thermal and due to beta radiation often complicatedthe illness. Within the first two weeks white blood cells andplatelets fell dramatically, indicating a very high dose whichhad compromised the production of blood cells in the bone marrow,making it virtually impossible for the patients to fight infectionor to retain the natural clotting activity of the blood. Almostall the patients with such high doses died (20 of 21), in spiteof the intensive specialised medical treatment provided. At lower exposures, the symptoms, signs and laboratory findingsimproved. Vomiting began later, platelet and white cell countsdid not drop so precipitously and the fever and toxaemia wereless pronounced. Beta radiation burns to the skin were a majorcomplicating factor and mucous membrane damage was difficult totreat, but survival improved markedly at lower doses, so thatno early deaths were noted in the less than 1-2 Gy exposure group(Table 6). Table 6. Outcome of radiation exposureamong persons hospitalised for acute radiation syndrome. Number of patients Estimated Dose (Gy) Deaths 21 21 55 140 6 - 16 4 - 6 2 - 4 less than 2 20 7 1 0 Total 237 28 There is a large range of medical treatments that can be attemptedto mitigate the acute radiation syndrome. All these procedureswere applied to the persons hospitalised with varying degreesof success. The hospital treatment following the accident includedreplacement therapy with blood constituents, fluids and electrolytes;antibiotics; antifungal agents; barrier nursing and bone marrowtransplantation. The treatment of the depression of bone-marrow function encounteredafter the accident was largely supportive. Special hygienic measureswere taken; patients' clothes were changed at least twice a dayand aseptic techniques used. Those patients who received dosesabove 2 Gy were given anti-fungal agents after the second week.Antibiotics and gamma globulin were also administered. Bone-marrow transplantation was undertaken in 13 patients whowere judged to have irreversible bone marrow damage after dosesgreater than 4 Gy. All but two of these patients died, some beforethe transfused marrow had had a chance to "take", butothers had short-term takes. It was concluded that, even aftervery high radiation doses, the bone marrow may well not be completelydestroyed and may recover at least some function at a later stage.It is this recovery which may lead to later rejection of the transplantedmarrow through a "Host versus Graft" reaction. The physiciansresponsible for treating the victims of the accident concludedthat bone marrow transplantation should play a very limited rolein treatment. Burns, both thermal and from beta radiation, were treated withsurgical excision of tissue that was not viable, and any fluidand electrolyte loss was compensated for by the parenteral feedingset up to treat the gastro-intestinal syndrome which is a prominentfeature of acute radiation sickness. The oro-pharyngeal syndromeof mucosal destruction, oedema and the absence of lubricationcaused by radiation damage to the mucosa of the mouth and pharynxwas extremely difficult to treat, and severely impaired swallowingand breathing. The organisational aspects of treating large numbers of very illpatients also presented significant problems. Intensive nursingcare and monitoring had to be provided 24 hours a day in smallunits. Personnel had to be taught new techniques of care and patienthandling, and a large number of diagnostic samples had to be examined.The logistic requirements of medical handling needed to be well-establishedbefore any therapeutic programme could be run efficiently. Late health effects There have been many reports of an increase in the incidence ofsome diseases as a result of the Chernobyl accident. In fact,the accident has, according to present knowledge, given rise toan increase in the incidence of thyroid cancers. Also, it hashad negative psychological consequences. As far as other diseasesare concerned, the scientific community has not been able to relatethose to the effects of ionising radiation. However, large researchprojects have been conducted and are under way to further studythe matter. For example, the WHO (WH95) established theInternational Programme on the Health Effects of the ChernobylAccident (IPHECA). This programme initially concentrated on pilotprojects involving leukaemia, thyroid diseases, oral health inBelarus, mental health in children irradiated before birth andthe development of epidemiological registries. The pilot phasecame to an end in 1994 and, as a result of the findings, effortsare underway to develop long-term permanent programmes involvingthyroid diseases, the accident recovery workers, dose reconstructionand guidance to the public in the event of an accident. It isexpected that these new projects will provide further insightinto any future health effects. An estimate (An88) of the total lifetime cancers whichcould be expected in Europe as a result of the accident suggestedan increase of about 0.01 per cent above their natural incidence.Another assessment placed the increase in cancer incidence at0.004 per cent in the Northern hemisphere, a lower percentageincrease due probably to including the large population of thewhole hemisphere (Pa89). These predictions are remarkablysimilar and support the view that the average doses to the generalpopulation of the Northern hemisphere were so low that only fractionsof a percent increases in cancer incidence could be expected inthis population (Pe88, Re87). Large parts of theNorthern hemisphere, such as North America (Hu88, Br88),Asia and Siberia, were not significantly contaminated and doseswere inconsequential. Therefore, the following sections focuson the late health effects in the population of the contaminatedregions of the former Soviet Union. In the International Chernobyl Project organised by the IAEA(IA91), field studies were undertaken in the latter halfof 1990 on the permanent residents of the rural settlements witha surface caesium contamination of greater than 555 kBq/m2, andon control settlements of 2,000 to 50,000 persons, using an agematched study design. Seven contaminated and six control settlementswere chosen by the medical team of the Chernobyl Project. Sinceall persons could not be examined, representative samples weretaken from various age groups. In all, 1,356 people were examined,and the aim was to examine about 250 from each of the larger settlements.Three medical teams each spent two weeks conducting medical examinationsto provide the data for these assessments. The medical examinations were quite comprehensive, and the generalconclusions reached were that there were no health abnormalitieswhich could be attributed to radiation exposure, but that therewere significant non-radiation related health disorders whichwere similar in both contaminated and control settlements. Theaccident had had substantial negative psychological consequenceswhich were compounded by the socio-economic and political changesoccurring in the former Soviet Union. The official data providedto the medical teams was incomplete and difficult to evaluate,and were not detailed enough to exclude or confirm the possibilityof an increase in the incidence of some tumour types. On thissubject, it was suggested in 1991 that the incidence of cancerin Ukraine showed no large increase even in the most contaminatedareas (Pr91). The International Chernobyl Project Report (IA91) suggestedthat the reported high thyroid doses in some children were suchthat there could be a statistically detectable increase in theincidence of future thyroid tumours. The Chernobyl Project teamfinally concluded that, on the basis of the doses estimated bythe team and the currently accepted radiation risk estimates,future increases over the natural incidence of cancer or hereditarydefects would be difficult if not impossible to discern, evenwith very large and well-designed long-term epidemiological studies.However, it should be remembered that this health survey tookplace four years after the accident, before any increase in cancerincidence might be expected and reflects the status of the peopleexamined in a few months of 1990. The sample size was also criticisedas being too small. Nevertheless, the dose estimates generally accepted indicate that,with the exception of thyroid disease, it is unlikely that theexposure would lead to discernible radiation effects in the generalpopulation. Many predictions of the future impact of the accidenton the health of populations have been made, all of which, apartfrom thyroid disease, indicate that the overall effect will besmall when compared with the natural incidence and therefore notexpected to be discernible (An88, Be87, Hu87, Mo87,De87, Be87). Thyroid cancer Early in the development of the Chernobyl accident, it becameobvious that the radioiodines were contributing significant thyroiddoses (Il90), especially to children, and the then Sovietauthorities made every effort not only to minimise doses, butalso to record the thyroid doses as accurately as possible. Theresults of these measurements and dose reconstruction assessmentsindicated that some groups in the population received high dosesto their thyroids, and that an increase in thyroid abnormalities,including cancer, was a very real possibility in the future. Thiswas particularly true for children in the contaminated regionsin Belarus, northern Ukraine and the Bryansk and Kaluga regionsof the Russian Federation. These were not inconsequential thyroiddoses and, as early as 1986, it was predicted by experts fromthe Soviet Union that the thyroid would be the target organ mostlikely to show evidence of radiation effects, especially an increasedincidence of benign and malignant tumours. It was known from previous studies of largely external irradiationof the thyroid that an increase in thyroid tumours tended to appearsix to eight years following irradiation, and continue for morethan twenty years after exposure, particularly in children. Whatwas not expected was that thyroid abnormalities would alreadybecome detectable about four years after the accident. At thesame time, the current conventional wisdom was that internal radioiodineexposure was less carcinogenic than external irradiation of thethyroid. It was estimated that the incidence of thyroid cancersin children, defined as those diagnosed between the ages of 0and 14, might increase by about 5 per cent, and in adults by about0.9 per cent over the next 30 years. As will be seen, a substantialincrease has already been detected in the more contaminated regions.A determined effort was made to estimate doses, record the data,initiate medical examinations and follow the cohorts already identifiedas being most at risk. In Ukraine, more than 150,000 examinations were conducted by specialdosimetric teams, and a realistic estimate of the collective thyroiddose of 64,000 person-Sv has been made, leading to a projectionof 300 additional thyroid cancers (Li93a). In the contaminatedregions of Russia, namely Bryansk, Tula and Orel, it was predictedthat an excess thyroid cancer total of 349 would appear in a populationof 4.3 million (Zv93). This represents an increase of 3to 6 per cent above the spontaneous rate. A programme to monitor the thyroid status of exposed childrenin Belarus was set up in Minsk in May/June 1986. The highest doseswere received by the evacuated inhabitants of the Hoiniki rayon(district) in the Gomel oblast. In the course of this study, itwas noted that the numbers of thyroid cancers in children wereincreasing in some areas. For Belarus as a whole (WH90, Ka92,Wi94), there has been a significantly increasing trend inchildhood thyroid cancer incidence since 1990 (Pa94). Itwas also noted that this increase is confined to regions in theGomel and Brest oblasts, and no significant increase has beennoted in the Mogilev, Minsk or Vitebsk areas where the radioactiveiodine contamination is assessed to have been lower. Over 50 percent of all the cases are from the Gomel oblast. For the eight years prior to 1986, only five cases of childhoodthyroid cancer were seen in Minsk, which is the main Belarussiancentre for thyroid cancer diagnosis and treatment on children(De94). From 1986 to 1989, 2 to 6 cases of thyroid cancerin children were seen annually in Minsk. In 1990, the number jumpedto 29, to 55 in 1991, then to 67 in 1992. By the end of 1994 thetotal had reached over 300 in Belarus. Nearly 50 per cent of theearly (1992) thyroid cancers appeared in children who were agedbetween one and four years at the time of the accident. The histology of the cancers has shown that nearly all were papillarycarcinomata (Ni94) and that they were particularly aggressive,often with prominent local invasion and distant metastases, usuallyto the lungs. This has made the treatment of these children lesssuccessful than expected, whether undertaken in Minsk or in specialisedcentres in Europe. In all, about 150,000 children in Belarus hadthyroid uptake measurements following the accident. Other datafrom Ukraine and Russia show a similar, but not as pronounced,increase in the incidence of childhood thyroid cancer since 1987. The increase in Belarus was confirmed by the final report of anEC Expert Panel (EC93) convened in 1992 to investigatethe reported increase. In 1992 the incidence of childhood thyroidcancer in Belarus as a whole was estimated to be 2.77 per 100,000,whereas in the Gomel and Brest oblasts it was 8.8 and 4.76 respectively.This increased incidence was not confined to children, as a largernumber of adult cases was registered in Belarus and in Ukraine(WH94). There is some difficulty in comparing the numbers quoted by thehealth authorities of the former Soviet Union with previous incidencestatistics, as previous data collection was not sufficiently rigorous.However, in Belarus all cases of childhood thyroid cancer havebeen confirmed since 1986 by international review of the histology,and, because of more rigid criteria for data collection, reliancecan be placed on accuracy and completeness. An attempt to reviewincidence estimates was made in the above-mentioned EC Report(EC93). These experts confirmed that the incidence of childhoodthyroid cancer (0-14 y) prior to the accident in Belarus (between0 and 0.14/100,000/y) was similar to that reported by other cancerregistries. This indicates that the data collection in Belaruswas adequate. They noted that it jumped to 2.25/100,000/y in 1991,about a twenty-fold increase. When this increase was first reported, it was very quickly pointedout (Be92) that any medical surveillance programme introducedwould apparently increase the incidence by revealing occult diseaseand rectifying misdiagnoses. While this may account for some ofthe increase (Ro92), it cannot possibly be the sole cause,as the increase is so large and many of the children presentednot with occult disease, but with clinical evidence of thyroidand/or metastatic disease. In fact, only 12 per cent of the childhoodthyroid cancers were discovered by ultrasound screening alonein Belarus (WH95). In addition, subsequent examinationby serial section of the thyroids of persons coming to autopsyin Belarus have confirmed that the number of occult thyroid cancersis similar to that found in other studies (Fu93) and showednone of the aggressive characteristics found in the childhoodcancers presenting in life (Fu92). The most recent published rates of childhood thyroid cancer (St95)show unequivocal increases as seen in Table 7. At the time ofwriting three children have died of their disease. Table 7. Number of cases and casesper million of childhood thyroid cancer (St95) 1981 - 85 1986 - 90 1991- 94 Area No. Rate (per million) No. Rate (per million) No. Rate (per million) Belarus Gomel Ukraine Five North Regions Russia Bryansk & Kaluga Regions 3 1 25 1 0 0.3 0.5 0.5 0.1 0 47 21 60 21 3 4 10.5 1.1 2 1.2 286 143 149 97 20 30.6 96.4 3.4 11.5 10 It can be concluded that there is a real, and large, increasein the incidence of childhood thyroid cancer in Belarus and Ukrainewhich is likely to be related to the Chernobyl accident. Thisis suggested by features of the disease, which differ somewhatfrom the so-called natural occurrence, as well as by its temporaland geographic distribution. As far as other thyroid disorders are concerned, no differencein Russia was detected by ultrasound examination, in the percentageincidence of cysts, nodules or autoimmune thyroiditis in the contaminatedversus the uncontaminated areas (Ts94). Following the accident,children in the Ukrainian contaminated regions exhibited a transientdose-dependent increase in serum thyroxine level, without overtclinical thyrotoxicosis, which returned to normal within 12 to18 months (Ni94). This was most marked in the youngestchildren. This finding cannot be regarded as an adverse healtheffect, as no abnormality was permanent. However, it may be apointer to future thyroid disease, especially when it may be associatedwith mild regional dietary iodine deficiency, and indicates theneed for continued monitoring. The majority of the estimates indicate that the overall healthimpact from these thyroid disorders will be extremely small andnot detectable when averaged over the population potentially atrisk. This viewpoint is widely held by the competent risk assessorswho have examined the potential effects of the accident. Other late health effects From data in the Russian National Medical Dosimetric Registry(RNMDR), the reported incidence of all types of disease has risenbetween 1989 and 1992 (Iv94). There has also been a reportedincrease in malignant disease which might be due to better surveillanceand/or radiation exposure. The crude mortality rate of the liquidatorsfrom all causes in the Russian Federation has increased from 5per 1,000 in 1991 to 7 per 1,000 in 1992. The crude death ratefrom respiratory cancer is reported to have increased significantlybetween 1990 and 1991, and for all malignant neoplasms between1991 and 1992. It is not clear what influence smoking has hadon these data, and the overall significance of these findingswill need to be established by further surveillance, especiallywhen there are distinct regional variations in the crude deathrate and the mortality rates from lung, breast and intestinalcancer are rising in the general population of the Russian Federation. From the dosimetric data in the RNMDR (Iv94), a predictedexcess 670 cancer deaths may occur in the exposed groups coveredby the Registry, peaking in about 25 years. This is about 3.4per cent of the expected cancer deaths from other causes. Datafrom the other national dose registries is not readily availablein the published literature. In view of the difficulties associated with these Registry data,such as the dose estimates, the influence of such confoundingfactors as smoking, the difficulty in follow-up, the possibleincrease in some diseases in the general population and also theshort time since the accident, it is not possible to draw anyfirm conclusions from these data at this time. The only inferencethat can be made is that these groups are the most exposed andthat, if any radiation effects are to be seen, they will occurin selected cohorts within these registries, which will requirelong-term future surveillance. A predicted increase of genetic effects in the next two generationswas 0.015 per cent of the spontaneous rate, and the estimatedlifetime excess percentage of all cancers as a result of livingin the strict control zones was 0.5 per cent, provided that alifetime dose limit of 350 mSv was not exceeded (Il90). Childhood leukemia incidence has not changed in the decade sincethe accident. There is no significant change in the level of leukemiaand related diseases in the contaminated (more than 555 kBq/m2)and noncontaminated territories of the three states (WH95).Other attempts through epidemiological studies have failed toestablish a link between radiation exposure from the Chernobylaccident and the incidence of leukemia and other abnormalities.No epidemiological evidence of an increase in childhood leukemiaaround Chernobyl (Iv93), in Sweden (Hj94) or therest of Europe (Pa92, Wi94) has been established. However,it may be prudent to withold final judgement on this issue fora few more years. Other studies Various reports (Pa93, Sc93, Se95, St93, Ve93) have beenpublished on the incidence of chromosome aberrations among peopleexposed both in the contaminated regions and in Europe. Some ofthese have shown little or no increase, while others have. Thismay reflect the wide variation in dose. However, there is a trendfor the incidence of chromosome aberrations to return to normalwith the passage of time. Other studies have not shown evidenceof lymphocytic chromosome damage (Br92). In East Germany one study found no rise in foetal chromosome aberrationsbetween May and December 1986. Chromosome aberrations are to beexpected in any exposed population, and should be regarded asbiological evidence of that exposure, rather than an adverse healtheffect. Another study in Germany suggesting a link between Down's syndrome(Trisomy 21) and the Chernobyl accident has been severely criticisedand cannot be accepted at face value because of the absence ofcontrol for confounding factors (Sp91), and it was notconfirmed by more extensive studies (Li93). Another studyin Finland (Ha92) showed no association of the incidenceof Trisomy 21 with radiation exposure from Chernobyl. There are no clear trends in data for birth anomalies in Belarusor Ukraine (Li93, Bo94). Two epidemiological studies inNorway concluded that no serious gross changes as to pregnancyoutcome were observed (Ir91), and that no birth defectsknown to be associated with radiation exposure were detected (Li92).In Austria, no significant changes in the incidence of birth defectsor spontaneous abortion rates which could be attributed to theChernobyl accident were detected (Ha92a). A review by the International Agency for Research on Cancer (IARC)showed no consistent evidence of a detrimental physical effectof the Chernobyl accident on congenital abnormalities or pregnancyoutcomes (Li93, EG88). No reliable data have shown anysignificant association between adverse pregnancy outcome or birthanomalies even in the most contaminated regions and the dosesindicate that none would be expected. There have been reports that have suggested that radiation exposureas a result of the accident resulted in altered immune reactions.While immune suppression at high whole-body doses is known tobe inevitable and severe, at the low doses experienced by thegeneral population it is expected that any detected alterationswill be minor and corrected naturally without any medical consequences.These minor changes may be indicative of radiation exposure, buttheir mild transitory nature is unlikely to lead to permanentdamage to the immune system. All immunological tests of radiationexposure are in their infancy, but tests such as stimulated immunoglobulinproduction by lymphocytes hold promise for the future as a meansof assessing doses below one Gy (De90). Psychological effects The severity of the psychological effects of the Chernobyl accidentappears to be related to the public's growing mistrust of officialdom,politicians and government, especially in the field of nuclearpower. Public scepticism towards authority is reinforced by itsdifficulty in understanding radiation and its effects, as wellas the inability of the experts to present the issues in a waythat is comprehensible. The impression that an unseen, unknowable,polluting hazard has been imposed upon them by the authoritiesagainst their will, fosters a feeling of outrage. Public outrage is magnified by the concept that their existingor future descendants are also at risk from this radiation pollution.This widespread public attitude was not confined to one country,and largely determined the initial public response outside theSoviet Union. The public distrust was increased by the fact thatthe accident that they had been told could not happen, did happen,and it induced anxiety and stress in people not only in the contaminatedareas but, to a lesser extent, all over the world. While stress and anxiety cannot be regarded as direct physicaladverse health effects of irradiation, their influence on thewell-being of people who were exposed or thought that they mighthave been, may well have a significant impact on the exposed population.Several surveys have shown that the intensity of the anxiety andstress are directly related to the presence of contamination.It should also be remembered that the stress induced by the accidentwas in addition to that produced in the general population bythe severe economic and social hardship caused by the break-upof the Soviet Union. Within the former Soviet Union Within the Soviet Union additional factors came into play to influencethe public reaction. It should be remembered that this accidentoccurred during the initial period of "glasnost" and"perestroika". After nearly seventy years of repression,the ordinary people in the Soviet Union were beginning openlyto express all the dissatisfaction and frustration that they hadbeen harbouring. Distrust and hatred of the central governmentand the Communist system could be expressed for the first timewithout too much fear of reprisal. In addition, nationalism wasnot repressed. The Chernobyl accident appeared to epitomise everythingthat was wrong with the old system, such as secrecy, witholdinginformation and a heavy-handed authoritarian approach. Oppositionto Chernobyl came to symbolise not only anti-nuclear and anti-communistsentiment but also was associated with an upsurge in nationalism. The distrust of officialdom was so great that even scientistsfrom the central government were not believed, and more reliancewas placed on local "experts" who often had very littleexpertise in radiation and its effects. The then Soviet Governmentrecognised this problem quickly, and tried to counteract the trendby inviting foreign experts to visit the contaminated areas, assessthe problems, meet with local specialists and publicise theirviews in open meetings and on television. These visits appearedto have a positive effect, at least initially, in allaying thefears of the public. In the contaminated Republics, anxiety andstress were much more prevalent and were not just confined tothe more heavily contaminated regions (WH90a). Severalsurveys conducted by Soviet (Al89) and other researchers(Du94) have shown that the anxiety induced by the accidenthas spread far beyond the more heavily contaminated regions. During this period there was severe economic hardship which addedto the social unrest and reinforced opposition to the officialsystem of government. Anti-nuclear demonstrations were commonplacein the larger cities in Belarus (Gomel and Minsk), and Ukraine(Kiev and Lvov) in the years following the accident (Co92).The dismissive attitude of some Soviet scientists and governmentofficials in describing the public reaction as "radiophobic"tended to alienate the public even further by implying some sortof mental illness or reaction which was irrational and abnormal.It also served as a convenient catch-all diagnosis which suggestedthat the public was somehow at fault, and the authorities wereunable to do anything about its manifestations. The concern of people for their own health is only overshadowedby their concern for the health of their children and grandchildren.Major and minor health problems are attributed to radiation exposureno matter what their origin, and the impact that the accidenthas had on their daily lives has added to the stress. Whole communitiesare facing or have faced evacuation or relocation. There are stillwidespread restrictions on daily life affecting schooling, work,diet and recreation. The accident has caused disruption of social networks and traditionalways of life. As most inhabitants of the contaminated settlementsare native to the area and often have lived there all their lives,relocation has in many cases, destroyed the existing family andcommunity social networks, transferring groups to new areas wherethey may well be resented or even ostracised. In spite of thesedrawbacks, about 70 per cent of the people living in contaminatedareas wished to be relocated (IA91). This may well be influencedby the economic incentives and improved living standards thatresult from relocation by the government. There are two additional circumstances and events which have tendedto increase the psychological impact of the accident, the firstof which was an initiative specifically designed to alleviatethese effects in Ukraine. This was the introduction of the compensationlaw in Ukraine in 1991. Some three million Ukrainians were affectedin some way by the post-accident management introduced, upon whichapproximately one sixth of the total national budget was spent(Du94). Different surveys have shown a general feelingof anxiety in all sectors of the population, but it was particularlyacute among those who had been relocated. People were fearfulof what the future might bring for themselves and their offspring,and were concerned about their lack of control over their owndestiny. The problem is that the system of compensation may well have exaggeratedthese fears by placing the recipients into the category of victims.This tended to segregate them socially and increased the resentmentof the native population into whose social system these "victims"had been injected without consultation. This had the effect onthe evacuees of increasing stress, often leading to withdrawal,apathy and despair. Locally, this compensation was often referredas a "coffin subsidy"! It is interesting to note thatthe 800 or so mostly elderly people who have returned to theircontaminated homes in the evacuated zones, and hence receive nocompensation, appear to be less stressed and anxious, in spiteof worse living conditions, than those who were relocated. Itshould be pointed out that compensation and assistance are notharmful in themselves, provided that care is taken not to inducean attitude of dependence and resignation in the recipients. The second factor which served to augment the psychological impactof the accident was the acceptance by physicians and the publicof the disease entity known as "vegetative dystonia".This diagnosis is characterised by vague symptoms and no definitivediagnostic tests. At any one time, up to 1,000 children were hospitalisedin Kiev, often for weeks, for treatment of this "disease"(St92). The diagnosis of vegetative dystonia appears tobe tailor-made for the post-accident situation, assigned by parentsand doctors to account for childhood complaints and accepted byadults as an explanation for vague symptoms. There is great pressure on the physicians to respond to theirpatients' needs in terms of arriving at an acceptable diagnosis,and "Vegetative Dystonia" is very convenient as it willfit any array of symptoms. Such a diagnosis not only justifiesthe patients' complaints by placing the blame for this "disease"on radiation exposure, it also exonerates the patient from anyresponsibility, which is placed squarely on the shoulders of thoseresponsible for the exposure - the Government. When the need forextended hospitalisation is added, the justification to acceptthis as a real disease is enhanced. It can be understood why thereis an epidemic of this diagnosis in the contaminated areas. Outside the former Soviet Union Psychological effects in other countries were minimal comparedwith those within the former Soviet Union, and were generallyexhibited more as concerned social reactions rather than psychosomaticsymptoms. In the contaminated regions of the former Soviet Union,many people were convinced that they were suffering from radiationinduced disease, whereas in the rest of the world, where contaminationwas much less, news of the accident appeared to reinforce anti-nuclearperceptions in the general population. This was evidenced, forexample, by the demonstrations on 7 June 1986 demanding the decommissioningof all nuclear power plants in the Federal Republic of Germany(Ze86). While in France public support for nuclear powerexpansion dropped since the accident, 63 per cent of the populationfelt that French nuclear power reactors operated efficiently (Ch90).The minimal impact of the Chernobyl accident on French publicopinion was probably due to the fact that about 75 per cent oftheir electrical power is derived from nuclear stations, and inaddition, France was one of the least contaminated European countries. The Swedish public response has been well-documented (Dr93,Sj87). In the survey, the question was asked: "Withthe experience that we now have, do you think it was good or badfor the country to invest in Nuclear Energy?" Those thatresponded "bad" jumped from 25 per cent before, to 47per cent after Chernobyl. The accident probably doubled the numberof people who admitted negative attitudes towards nuclear power(Sj87). This change was most marked among women, who, itwas felt, regarded nuclear power as an environmental problem,whereas men regarded it as a technical problem which could besolved. Media criticism of the radiation protection authoritiesin that country became more common, with the charge that the officialpronouncements on the one hand said that the risk in Sweden wasnegligible, and yet on the other, gave instructions on how itcould be reduced. The concept that a dose, however small, shouldbe avoided if it could be done easily and cheaply, was not understood. This sort of reaction was common outside the former Soviet Union,and while it did not give rise to significant psychosomatic effects,it tended to enhance public apprehension about the dangers ofnuclear power and foster the public's growing mistrust of officialbodies. In addition, public opinion in Europe was very sceptical of theinformation released by the Soviet Union. This mistrust was reinforcedfurther by the fact that the traditional sources of informationto which the public tended to turn in a crisis, the physiciansand teachers, were no better informed and often only repeatedand reinforced the fears that had been expressed to them. Addedto this were the media, who tended to respond to the need to print"newsworthy" items by publishing some of the more outlandishclaims of so-called radiation effects. The general public was confused and cynical and responded in predictablebut extreme ways such as seeking induced abortions, postponingtravel and not buying food that might conceivably be contaminated.Another global concern that was manifested, was the apprehensionover travel to the Soviet Union. Potential travellers sought advicefrom national authorities on whether to travel, what precautionsto take and how they could check on their exposure. Many people,in spite of being reassured that it was safe to travel, cancelledtheir trip, just to be on the safe side, exhibiting their lackof confidence in the advice they received. As has been seen, governments themselves were not immune fromthe influence of these fears and some responded by introducingmeasures such as unnecessarily stringent intervention levels forthe control of radionuclides in imported food. Thus, in the worldas a whole, while the individual psychological effects due toanxiety and stress were probably minimal, the collective perceptionand response had a significant economic and social impact. Itbecame clear that there was a need to inform the public on radiationeffects, to provide clear instructions on the precautions to betaken so that the public regains some level of personal control,and for the authorities to recognise the public's need to be involvedin the decisions that affect them. In summary, it can be stated that: Thirty-one people died in the course of the accident orsoon after and another 137 were treated for the acute radiationsyndrome. Extensive psychological effects are apparent in the affectedregions of the former Soviet Union, manifested as anxiety andstress. Severe forms induce a feeling of apathy and despair oftenleading to withdrawal. In the rest of the world these individualeffects were minimal. In the last decade, there has been a real and significantincrease in childhood and, to a certain extent, adult carcinomaof the thyroid in contaminated regions of the former Soviet Union(Wi940) which should be attributed to the Chernobyl accidentuntil proven otherwise. In children, the thyroid cancers are: largely papillary and particularly aggressive in natureoften self presenting with local invasion and/or distant metastases, more prevalent in children aged 0 to 5 years at the timeof the accident, and in areas assessed to be the more heavilycontaminated with iodine-131, apparently characterised by a shorter latent period thanexpected and, still increasing. There has been no increase in leukemia, congenital abnormalities,adverse pregnancy outcomes or any other radiation induced diseasein the general population either in the contaminated regions orin Western Europe, which can be attributed to this exposure. Itis unlikely that surveillance of the general population will revealany significant increase in the incidence of cancer. WT03-B20-74IA006-000055-B004-454http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl2/c0e.html 138.80.61.12 19970221174239 text/html 28878HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:12:33 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 28706Last-modified: Tue, 09 Jul 1996 02:25:47 GMT EXECUTIVE SUMMARY Introduction On 26 April, 1986, the Chernobyl nuclear power station, in Ukraine,suffered a major accident which was followed by a prolonged releaseto the atmosphere of large quantities of radioactive substances.The specific features of the release favoured a widespread distributionof radioactivity throughout the northern hemisphere, mainly acrossEurope. A contributing factor was the variation of meteorologicalconditions and wind regimes during the period of release. Activitytransported by the multiple plumes from Chernobyl was measurednot only in Northern and in Southern Europe, but also in Canada,Japan and the United States. Only the Southern hemisphere remainedfree of contamination. This had serious radiological, health and socio-economic consequencesfor the populations of Belarus, Ukraine and Russia, and to someextent they are still suffering from these consequences. Althoughthe radiological impact of the accident in other countries wasgenerally very low, and even insignificant outside Europe, thisevent had, however, the effect of enhancing public apprehensionall over the world on the risks associated with the use of nuclearenergy. This is one of the reasons explaining the renewed attention andeffort devoted during the last decade to the reactor safety studiesand to emergency preparedness by public authorities and the nuclearindustry. This also underlies the continuing attention of thepublic opinion to the situation at Chernobyl. The forthcoming tenth anniversary of the accident appears, therefore,the right moment to review the status of our knowledge of theserious aspects of the accident impact, to take stock of the informationaccumulated and the scientific studies underway, as well as toassess the degree to which national authorities and experts haveimplemented the numerous lessons that the Chernobyl accident taughtus. This brief report, prepared under the aegis of the Committee onRadiation Protection and Public Health (CRPPH) of the OECD NuclearEnergy Agency, presents a collective view by OECD radiation protectionexperts on this matter. The accident The Unit 4 of the Chernobyl nuclear power plant was to be shutdownfor routine maintenance on 25 April 1986. On that occasion, itwas decided to carry out a test of the capability of the plantequipment to provide enough electrical power to operate the reactorcore cooling system and emergency equipment during the transitionperiod between a loss of main station electrical power supplyand the start up of the emergency power supply provided by dieselengines. Unfortunately, this test, which was considered essentially toconcern the non-nuclear part of the power plant, was carried outwithout a proper exchange of information and co-ordination betweenthe team in charge of the test and the personnel in charge ofthe operation and safety of the nuclear reactor. Therefore, inadequatesafety precautions were included in the test programme and theoperating personnel were not alerted to the nuclear safety implicationsand potential danger of the electrical test. This lack of co-ordination and awareness, resulting from an insufficientlevel of "safety culture" within the plant staff, ledthe operators to take a number of actions which deviated fromestablished safety procedures and led to a potentially dangeroussituation. This course of actions was compounded by the existenceof significant drawbacks in the reactor design which made theplant potentially unstable and easily susceptible to loss of controlin case of operational errors. The combination of these factors provoked a sudden and uncontrollablepower surge which resulted in violent explosions and almost totaldestruction of the reactor. The consequences of this catastrophicevent were further worsened by the graphite moderator and othermaterial fires that broke out in the building and contributedto a widespread and prolonged release of radioactive materialsto the environment. Dispersion and deposition of radionuclides The release of radioactive materials to the atmosphere consistedof gases, aerosols and finely fragmented nuclear fuel particles.This release was extremely high in quantity, involving a largefraction of the radioactive product inventory existing in thereactor, and its duration was unexpectedly long, lasting for morethan a week. This duration and the high altitude (about 1 km)reached by the release were largely due to the graphite fire whichwas very difficult to extinguish. For these reasons and the concomitant frequent changes of winddirection during the release period, the area affected by theradioactive plume and the consequent deposition of radioactivesubstances on the ground was extremely large, encompassing thewhole Northern hemisphere, although significant contaminationoutside the former Soviet Union was only experienced in part ofEurope. The pattern of contamination on the ground and in foodchains was,however, very uneven in some areas due to the influence of rainfallduring the passage of the plume. This irregularity in the patternof deposition was particularly pronounced at larger distancesfrom the reactor site. Reactions of national authorities The scale and severity of the Chernobyl accident had not beenforeseen and took most national authorities responsible for publichealth and emergency preparedness by surprise. The interventioncriteria and procedures existing in most countries were not adequatefor dealing with an accident of such scale and provided littlehelp in decision-making concerning the choice and adoption ofprotective measures. In addition, early in the course of the accidentthere was little information available and considerable politicalpressure, partially based on the public perception of the radiationdanger, was being exerted on the decision-makers. In these circumstances, cautious immediate actions were felt necessaryand in many cases measures were introduced that tended to err,sometimes excessively so, on the side of prudence rather thanbeing driven by informed scientific and expert judgement. Within the territory of the former Soviet Union, short-term countermeasureswere massive and, in general, reasonably timely and effective.However, difficulties emerged when the authorities tried to establishcriteria for the management of the contaminated areas on the longterm and the associated relocation of large groups of population.Various approaches were proposed and criteria were applied overthe years. Eventually, criteria for population resettlement orrelocation from contaminated areas were adopted in which radiationprotection requirements and economic compensation considerationswere intermingled. This was and continues to be a source of confusionand possible abuse. The progressive spread of contamination at large distances fromthe accident site caused considerable concern in many countriesoutside the former Soviet Union and the reactions of the nationalauthorities to this situation were extremely varied, ranging froma simple intensification of the normal environmental monitoringprogrammes, without adoption of specific countermeasures, to compulsoryrestrictions concerning the marketing and consumption of foodstuffs. Apart from the objective differences of contamination levels andregulatory and public health systems between countries, one ofthe principal reasons for the variety of situations observed inthe different countries stems from the different criteria adoptedfor the choice and application of intervention levels for theimplementation of protective actions. These discrepancies werein some cases due to misinterpretation and misuse of internationalradiation protection guidelines, especially in the case of foodcontamination, and were further enhanced by the overwhelming roleplayed in many cases by non-radiological factors, such as socio-economic,political and psychological, in determining the countermeasures. This situation caused concern and confusion among the public,perplexities among the experts and difficulties to national authorities,including problems of public credibility, as well as a waste ofefforts and unnecessary economic losses. These problems were particularlyfelt in areas close to international borders due to differentreactions of the authorities and media in bordering countries.However, all these issues were soon identified as an area whereseveral lessons should be learned and international efforts wereundertaken to harmonise criteria and approaches to emergency management. Radiation dose estimates Most of the population of the Northern hemisphere was exposed,to various degrees, to radiation from the Chernobyl accident.After several years of accumulation of dosimetric data from allavailable sources and dose reconstruction calculations based onenvironmental contamination data and mathematical models, it isnow possible to arrive at a reasonable, although not highly accurate,assessment of the ranges of doses received by the various groupsof population affected by the accident. The main doses of concern are those to the thyroid due to externalirradiation and inhalation and ingestion of radioactive iodineisotopes, and those to the whole body due to external irradiationfrom and ingestion of radioactive caesium isotopes. Accordingto current estimates, the situation for the different exposedgroups is the following: Evacuees - More than 100,000 persons were evacuated,mostly from the 30-km radius area around the accident site, duringthe first few weeks following the accident. These people receivedsignificant doses both to the whole body and the thyroid, althoughthe distribution of those doses was very variable among them dependingon their positions around the accident site and the delays oftheir evacuation. Doses to the thyroid ranging from 70 millisieverts to adults upto about 1,000 millisieverts (i.e., 1 sievert) to youngchildren and an average individual dose of 15 millisieverts [mSv]to the whole body were estimated to have been absorbed by thispopulation prior to their evacuation. Many of these people continuedto be exposed, although to a lesser extent depending on the sitesof their relocation, after their evacuation from the 30-km zone. "Liquidators" - Hundreds of thousandsof workers, estimated to amount up to 800,000 and including alarge number of military personnel, were involved in the emergencyactions on the site during the accident and the subsequent clean-upoperations which lasted for a few years. These workers were called"liquidators". A restricted number, of the order of 400, including plant staff,firemen and medical aid personnel, were on the site during theaccident and its immediate aftermath and received very high dosesfrom a variety of sources and exposure pathways. Among them wereall those who developed acute radiation syndrome and requiredemergency medical treatment. The doses to these people rangedfrom a few grays to well above 10 grays to the whole body fromexternal irradiation and comparable or even higher internal doses,in particular to the thyroid, from incorporation of radionuclides.A number of scientists, who periodically performed technical actionsinside the destroyed reactor area during several years, accumulatedover time doses of similar magnitude. The largest group of liquidators participated in clean-up operationsfor variable durations over a number of years after the accident.Although they were not operating anymore in emergency conditionsand were submitted to controls and dose limitations, they receivedsignificant doses ranging from tens to hundreds of millisieverts. People living in contaminated areas of the former SovietUnion - About 270,000 people continue to live in contaminatedareas with radiocaesium deposition levels in excess of 555 kilobecquerelsper square metre [kBq/m2], where protection measures still continueto be required. Thyroid doses, due mainly to the consumption ofcow's milk contaminated with radioiodine, were delivered duringthe first few weeks after the accident; children in the Gomelregion of Belarus appear to have received the highest thyroiddoses with a range from negligible levels up to 40 sieverts andan average of about 1 sievert for children aged 0 to 7. Becauseof the control of foodstuffs in those areas, most of the radiationexposure since the summer of 1986 is due to external irradiationfrom the radiocaesium activity deposited on the ground; the whole-bodydoses for the 1986-89 time period are estimated to range from5 to 250 mSv with an average of 40 mSv. Populations outside the former Soviet Union - Theradioactive materials of a volatile nature (such as iodine andcaesium) that were released during the accident spread throughoutthe entire Northern hemisphere. The doses received by populationsoutside the former Soviet Union are relatively low, and show largedifferences from one country to another depending mainly uponwhether rainfall occurred during the passage of the radioactivecloud. These doses range from a lower extreme of a few microsievertsor tens of microsieverts outside Europe, to an upper extreme of1 or 2 mSv in some European countries. The latter value is ofthe same order as the annual individual exposure from naturalbackground radiation. Health impact The health impact of the Chernobyl accident can be described interms of acute health effects (death, severe health impairment),late health effects (cancers) and psychological effects liableto affect health. The acute health effects occurred among the plant personnel andthe persons who intervened in the emergency phase to fight fires,provide medical aid and immediate clean-up operations. A totalof 31 persons died as a consequence of the accident, and about140 persons suffered various degrees of radiation sickness andhealth impairment. No members of the general public suffered thesekinds of effects. As far as the late health effects are concerned, namely the possibleincrease of cancer incidence, in the decade following the accidentthere has been a real and significant increase of carcinomas ofthe thyroid among the children living in the contaminated regionsof the former Soviet Union, which should be attributed to theaccident until proved otherwise. There might also be some increaseof thyroid cancers among the adults living in those regions. Fromthe observed trend of this increase of thyroid cancers it is expectedthat the peak has not yet been reached and that this kind of cancerwill still continue for some time to show an excess above itsnatural rate in the area. On the other hand, the scientific and medical observation of thepopulation has not revealed any increase in other cancers, aswell as in leukaemia, congenital abnormalities, adverse pregnancyoutcomes or any other radiation induced disease that could beattributed to the Chernobyl accident. This observation appliesto the whole general population, both within and outside the formerSoviet Union. Large scientific and epidemiological research programmes,some of them sponsored by international organisations such asthe WHO and the EC, are being conducted to provide further insightinto possible future health effects. However, the population doseestimates generally accepted tend to indicate that, with the exceptionof thyroid disease, it is unlikely that the exposure would leadto discernible radiation effects in the general population abovethe background of natural incidence of the same diseases. In thecase of the liquidators this forecast should be taken with somecaution. An important effect of the accident, which has a bearing on health,is the appearance of a widespread status of psychological stressin the populations affected. The severity of this phenomenon,which is mostly observed in the contaminated regions of the formerSoviet Union, appears to reflect the public fears about the unknownsof radiation and its effects, as well as its mistrust towardspublic authorities and official experts, and is certainly madeworse by the disruption of the social networks and traditionalways of life provoked by the accident and its long-term consequences. Agricultural and environmental impacts The impact of the accident on agricultural practices, food productionand use and other aspects of the environment has been and continuesto be much more widespread than the direct health impact on humans. Several techniques of soil treatment and decontamination to reducethe accumulation of radioactivity in agricultural produce andcow's milk and meat have been experimented with positive resultsin some cases. Nevertheless, within the former Soviet Union largeareas of agricultural land are still excluded from use and areexpected to continue to be so for a long time. In a much largerarea, although agricultural and dairy production activities arecarried out, the food produced is subjected to strict controlsand restrictions of distribution and use. Similar problems of control and limitation of use, although ofa much lower severity, were experienced in some countries of Europeoutside the former Soviet Union, where agricultural and farm animalproduction were subjected to restrictions for variable durationsafter the accident. Most of these restrictions have been liftedseveral years ago. However, there are still today some areas inEurope where restrictions on slaughter and distribution of animalsare in force. This concerns, for example, several hundreds ofthousands of sheep in the United Kingdom and large numbers ofsheep and reindeer in some Nordic countries. A kind of environment where special problems were and continueto be experienced is the forest environment. Because of the highfiltering characteristics of trees, deposition was often higherin forests than in other areas. An extreme case was the so-called"red forest" near to the Chernobyl site where the irradiationwas so high as to kill the trees which had to be destroyed asradioactive waste. In more general terms, forests, being a sourceof timber, wild game, berries and mushrooms as well as a placefor work and recreation, continue to be of concern in some areasand are expected to constitute a radiological problem for a longtime. Water bodies, such as rivers, lakes and reservoirs can be, ifcontaminated, an important source of human radiation exposurebecause of their uses for recreation, drinking and fishing. Inthe case of the Chernobyl accident this segment of the environmentdid not contribute significantly to the total radiation exposureof the population. It was estimated that the component of theindividual and collective doses that can be attributed to thewater bodies and their products did not exceed 1 or 2 percentof the total exposure resulting from the accident. The contaminationof the water system has not posed a public health problem duringthe last decade; nevertheless, in view of the large quantitiesof radioactivity deposited in the catchment area of the systemof water bodies in the contaminated regions around Chernobyl,there will continue to be for a long time a need for careful monitoringto ensure that washout from the catchment area will not contaminatedrinking-water supplies. Outside the former Soviet Union, no concerns were ever warrantedfor the levels of radioactivity in drinking water. On the otherhand, there are lakes, particularly in Switzerland and the Nordiccountries, where restrictions were necessary for the consumptionof fish. These restrictions still exist in Sweden, for example,where thousands of lakes contain fish with a radioactivity contentwhich is still higher than the limits established by the authoritiesfor sale on the market. Potential residual risks Within seven months of the accident, the destroyed reactor wasencased in a massive concrete structure, known as the "sarcophagus",to provide some form of confinement of the damaged nuclear fueland destroyed equipment and reduce the likelihood of further releasesof radioactivity to the environment. This structure was, however,not conceived as a permanent containment but rather as a provisionalbarrier pending the definition of a more radical solution forthe elimination of the destroyed reactor and the safe disposalof the highly radioactive materials. Nine years after its erection, the sarcophagus structure, althoughstill generally sound, raises concerns for its long-term resistanceand represents a standing potential risk. In particular, the roofof the structure presented for a long time numerous cracks withconsequent impairment of leaktightness and penetration of largequantities of rain water which is now highly radioactive. Thisalso creates conditions of high humidity producing corrosion ofmetallic structures which contribute to the support of the sarcophagus.Moreover, some massive concrete structures, damaged or dislodgedby the reactor explosion, are unstable and their failure, dueto further degradation or to external events, could provoke acollapse of the roof and part of the building. According to various analyses, a number of potential accidentalscenarios could be envisaged. They include a criticality excursiondue to change of configuration of the melted nuclear fuel massesin the presence of water leaked from the roof, a resuspensionof radioactive dusts provoked by the collapse of the enclosureand the long-term migration of radionuclides from the enclosureinto the groundwater. The first two accident scenarios would resultin the release of radionuclides into the atmosphere which wouldproduce a new contamination of the surrounding area within a radiusof several tens of kilometres. It is not expected, however, thatsuch accidents could have serious radiological consequences atlonger distances. As far as the leaching of radionuclides from the fuel masses bythe water in the enclosure and their migration into the groundwaterare concerned, this phenomenon is expected to be very slow andit has been estimated that, for example, it will take 45 to 90years for certain radionuclides such as strontium90 to migrateunderground up to the Pripyat River catchment area. The expectedradiological significance of this phenomenon is not known withcertainty and a careful monitoring of the evolving situation ofthe groundwater will need to be carried out for a long time. The accident recovery and clean-up operations have resulted inthe production of very large quantities of radioactive wastesand contaminated equipment which are currently stored in about800 sites within and outside the 30-km exclusion zone around thereactor. These wastes and equipment are partly buried in trenchesand partly conserved in containers isolated from groundwater byclay or concrete screens. A large number of contaminated equipment,engines and vehicles are also stored in the open air. All these wastes are a potential source of contamination of thegroundwater which will require close monitoring until a safe disposalinto an appropriate repository is implemented. In general, it can be concluded that the sarcophagus and the proliferationof waste storage sites in the area constitute a series of potentialsources of release of radioactivity that threatens the surroundingarea. However, any such releases are expected to be very smallin comparison with those from the Chernobyl accident in 1986 andtheir consequences would be limited to a relatively small areaaround the site. On the other hand, concerns have been expressedby some experts that a much more important release might occurif the collapse of the sarcophagus should induce damage in theUnit 3 of the Chernobyl power plant, which currently is stillin operation. In any event, initiatives have been taken internationally, andare currently underway, to study a technical solution leadingto the elimination of these sources of residual risk on the site. Lessons learned The Chernobyl accident was very specific in nature and it shouldnot be seen as a reference accident for future emergency planningpurposes. However, it was very clear from the reactions of thepublic authorities in the various countries that they were notprepared to deal with an accident of this magnitude and that technicaland/or organisational deficiencies existed in emergency planningand preparedness in almost all countries. The lessons that could be learned from the Chernobyl accidentwere, therefore, numerous and encompassed all areas, includingreactor safety and severe accident management, intervention criteria,emergency procedures, communication, medical treatment of irradiatedpersons, monitoring methods, radioecological processes, land andagricultural management, public information, etc. However, the most important lesson learned was probably the understandingthat a major nuclear accident has inevitable transboundary implicationsand its consequences could affect, directly or indirectly, manycountries even at large distances from the accident site. Thisled to an extraordinary effort to expand and reinforce internationalco-operation in areas such as communication, harmonisation ofemergency management criteria and co-ordination of protectiveactions. Major improvements were achieved in this decade and importantinternational mechanisms of co-operation and information wereestablished, such as the international conventions on early notificationand assistance in case of a radiological accident, by the IAEAand the EC, the international nuclear emergency exercises (INEX)programme, by the NEA, the international accident severity scale(INES), by the IAEA and NEA and the international agreement onfood contamination, by the FAO and WHO. At the national level, the Chernobyl accident also stimulatedauthorities and experts to a radical review of their understandingof and attitude to radiation protection and nuclear emergencyissues. This prompted many countries to establish nationwide emergencyplans in addition to the existing structure of local emergencyplans for individual nuclear facilities. In the scientific andtechnical area, besides providing new impetus to nuclear safetyresearch, especially on the management of severe nuclear accidents,this new climate led to renewed efforts to expand knowledge onthe harmful effects of radiation and their medical treatment andto revitalise radioecological research and environmental monitoringprogrammes. Substantial improvements were also achieved in thedefinition of criteria and methods for the information of thepublic, an aspect whose importance was particularly evident duringthe accident and its aftermath. Conclusion The history of the modern industrial world has been affected onmany occasions by catastrophes comparable or even more severethan the Chernobyl accident. Nevertheless, this accident, duenot only to its severity but especially to the presence of ionisingradiation, had a significant impact on human society. Not only it produced severe health consequences and physical,industrial and economic damage in the short term, but, also, itslong-term consequences in terms of socio-economic disruption,psychological stress and damaged image of nuclear energy, areexpected to be long standing. However, the international community has demonstrated a remarkableability to apprehend and treasure the lessons to be drawn fromthis event, so that it will be better prepared to cope with achallenge of this kind, if ever a severe nuclear accident shouldhappen again. WT03-B20-75IA006-000055-B006-6http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl2/c08.html 138.80.61.12 19970221174850 text/html 11757HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:18:56 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 11585Last-modified: Tue, 09 Jul 1996 02:25:47 GMT Chapter VIII LESSONS LEARNED The accident did not affect each country in the same way and adifferent emphasis was placed on various aspects of the accidentwith particular reference to the circumstances of that country.Thus, countries remote from the accident, with no domestic nuclearpower programmes or neighbouring reactors, tended to emphasisefood control and information exchange as their major thrust forimprovement. Whereas those countries which were contaminated bythe accident, and had their own nuclear power programmes and/orreactors in neighbouring countries, drew extensive lessons fromthe way the accident developed and was treated. For these reasons,not all the lessons learned were applied universally with thesame emphasis. Operational aspects The Chernobyl accident was one of a kind, and, although it highlighteddeficiencies in emergency preparedness and radiation protection,it should not be seen as the reference accident for future emergencyplanning purposes (Bu91). It was very clear from the initial reactions of the competentnational authorities that they were unprepared for an accidentof such magnitude and they had to make decisions, as the accidentevolved, on criteria that could have been established beforehand.This also meant that too many organisations were involved in thedecision-making, as no clear-cut demarcations had been agreedand established. Areas of overlapping responsibility and jurisdictionneeded to be clearly established prior to any accident. A permanentinfrastructure needed also to be in place and maintained for anyefficient implementation of protective measures. Such an infrastructurehad to include rapid communications systems, intervention teamsand monitoring networks. Mobile ground monitoring teams were required,as was aerial monitoring and tracking of the plume. Many countriesresponded to this need by establishing such monitoring networksand reorganising their emergency response. Logistic problems associated with intervention plans, such asstable iodine distribution (Sc94, NE95a) and evacuationobviously needed to be in place and rehearsed long before theaccident, as they are too complex and time-consuming to be implementedduring the short time available during the evolution of the accident.Intervention actions and the levels at which they should be introducedneeded to be agreed, preferably internationally, and incorporatedinto the emergency plans so that they could be immediately andefficiently implemented. The accident also demonstrated the need to include the possibilityof transboundary implications in the emergency plans, as it hadbeen shown that the radionuclide release would be elevated andthe dispersion of contamination more widespread. The concern,raised by the experience of Chernobyl, that any country couldbe affected not only by nuclear accidents occurring on its territorybut also by the consequences of accidents happening abroad, stimulatedthe establishment of national emergency plans in several countries. The transboundary nature of the contamination prompted the internationalorganisations to promote international cooperation and communication,to harmonise actions (NE88, IA94, IC90, IC92, NE93, NE89, NE90,NE89b, WH88, WH87, IA89b, IA92, IA91a, IA89c, IA87a, IA94a, EC89a,EC89b) and to develop international emergency exercises suchas those organised by the OECD/NEA in its INEX Programme (NE95).A major accomplishment of the international community were theagreements reached on early notification in the event of a radiologicalaccident and on assistance in radiological emergencies throughinternational Conventions in the frame of the IAEA and the EC(EC87, IA86b, IA86c). Furthermore, in order to facilitate communication with the publicon the severity of nuclear accidents, the International NuclearEvent Scale INES was developed by the IAEA and the NEA and iscurrently adopted by a large number of countries. The accident provided the stimulus for international agreementon food contamination moving in trade, promoted by the WHO/FAO,as there is a need to import at least some food in most countries,and governments recognised the need to assure their citizens thatthe food that they eat is safe. Monitoring imported food was oneof the first control measures instituted and continues to be performed(FA91, EC89c, EC93a). This event also clearly showed that all national governments,even those without nuclear power programmes, needed to developemergency plans to address the problem of transboundary dispersionof radionuclides. Of necessity these plans had to be internationalin nature, involving the free and rapid exchange of informationbetween countries. It is essential that emergency plans are flexible. It would befoolish to plan for another accident similar to Chernobyl withoutany flexibility, as the only fact that one can be sure of is thatthe next severe accident will be different. Emergency plannersneed to distil the general principles applicable to various accidentsand incorporate these into a generic plan. The accident emphasised the need for public information and publicpressure at the time clearly demonstrated this need. A large numberof persons who are knowledgeable about the technique of providinginformation, are needed to establish a credible source of informationto the public before an accident, so that clear and simple reportscan be disseminated continuously in a timely and accurate form(EC89). Emergency plans also need to include a process by which largenumbers of people could have their exposure assessed, and thosewith high exposures differentiated. The accident also highlightedthe need for the prior identification of central specialised medicalfacilities with adequate transportation to treat the more highlyexposed individuals. Refinement and clarification of international advice was needed(Pa88). The recommendations for intervention in an accidentcontained in ICRP Publication 40 were not clearly understood whenthey came to be applied, and the Commission reviewed this advicein Publication 63 (IC92). This guide placed emphasis onthe averted dose as the parameter against which an interventionmeasure should be assessed. It was also made clear that an interventionhad to be "justified" in as far as it produced moregood than harm, and that where a choice existed between differentprotection options, "optimisation" was the mechanismto determine the choice. Emphasis was also placed on the needto integrate all protective actions in an emergency plan, andnot to assess each one in isolation, as one may well influencethe efficacy of another. Scientific and technical aspects Prior to the accident, it was felt that the flora and fauna ofthe environment were relatively radioresistant and this was supportedby the fact that no lethal radioecological injuries were notedafter the accident except in pine forests (600 ha) and small areasof birch close to the reactor. A cumulative dose of less than5 Gy has no gross effect even in the most sensitive flora of ecologicalsystems, but there are still ecological lessons to be learnedespecially on the siting of nuclear power reactors (Al93). Plant foliar and root uptake is being studied, as are resuspensionand weathering. The transfer coefficients at all stages of thepathways to human exposure are being refined. Following the accident,an assessment of the models used at thirteen sites to predictthe movement of iodine-131 and caesium-137 from the atmosphereto food chains (Ho91) indicated that models commonly usedtended to overpredict by anything up to a factor of ten. The extensivewhole-body monitoring of radioactivity in persons undertaken inconjunction with the measurement of ground and food contaminationallowed refinement of the accuracy of the models for human doseassessment from the exposure through different pathways. The methodsand techniques to handle contamination of food, equipment andsoil have been improved. Meteorological aspects, such as the relationship between depositionand precipitation and greater deposition over high ground andmountains, have been shown to be important especially in the developmentof more realistic models (NE96a). The importance of synopticscale weather patterns used in predictions was established, anddifferent models have been developed to predict deposition patternsunder a wide variety of weather conditions. The chemico-physicalchanges in the radioactive gases and aerosols transported throughthe atmosphere are being studied to improve the accuracy of transportmodels. Other impacts of the accident on model refinement include theimprovement in understanding the movement of radionuclides insoil and biota, pathways and transfer factors; the effect of rainstormsand the influence of mountains and the alignment of valleys ondeposition patterns; particulate re-suspension; long range pollutiontransport mechanisms; and the factors which influence depositionvelocities (NE89a, NE96). Uniform methods and standards were developed for the measurementof contaminating radionuclides in environmental samples. In the case of high exposures the importance of symptomatic andprophylactic medical and nursing procedures, such as antibiotics,anti-fungal and anti-viral agents, parenteral feeding, air sterilisationand barrier nursing was demonstrated, as were the disappointingresults of bone-marrow transplantation. In addition, the accident led to an expansion of research in nuclearsafety and the management of severe nuclear accidents. On the other hand, there is a need to set up sound epidemiologicalstudies to investigate potential health effects, both acute andchronic. In the Chernobyl case, the lack of routinely collecteddata, such as cancer registry data that are reliable enough, ledto difficulty in organising appropriate epidemiological investigationsin timely manner. There appears to be a need for developing andmaintaining a routine health surveillance system within and aroundnuclear facilities. In summary, besides providing new impetus to nuclear safetyresearch, especially on the management of severe nuclear accidents,the Chernobyl accident stimulated national authorities and expertsto a radical review of their understanding of, and attitude toradiation protection and nuclear emergency issues. This led to expand knowledge on radiation effects and theirtreatment and to revitalise radioecological research and monitoringprogrammes. emergency procedures, and criteria and methods forthe information of the public. Moreover, a substantial role in these improvements was playedby multiple international co-operation initiatives, includingrevision and rationalisation of radiation protection criteriafor the management of accident consequences, as well as reinforcementor creation of international communication and assistance mechanismsto cope with the transboundary implications of potential nuclearaccidents. WT03-B20-76IA006-000055-B004-481http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl2/c01.html 138.80.61.12 19970221174254 text/html 13892HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:13:11 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 13720Last-modified: Tue, 09 Jul 1996 02:25:47 GMT Chapter I THE SITE AND ACCIDENT SEQUENCE The Site At the time of the Chernobyl accident, on 26 April 1986, theSoviet Nuclear Power Programme was based mainly upon two typesof reactors, the WWER, a pressurised light-water reactor, andthe RBMK, a graphite moderated light-water reactor. While theWWER type of reactor was exported to other countries, the RBMKdesign was restricted to republics within the Soviet Union. The Chernobyl Power Complex, lying about 130 km north of Kiev,Ukraine (Figure 1), consisted of four nuclear reactors of theRBMK-1000 design, Units 1 and 2 being constructed between 1970and 1977, while Units 3 and 4 of the same design were completedin 1983 (IA86). Two more RBMK reactors were under constructionat the site at the time of the accident. Figure 1. The site of the Chernobyl nuclear power complex (modif. from IA91) To the South-east of the plant, an artificial lake of some 22km2, situated beside the river Pripyat, a tributary of the Dniepr,was constructed to provide cooling water for the reactors. This area of Ukraine is described as Belarussian-type woodlandwith a low population density. About 3 km away from the reactor,in Pripyat, there were 49,000 inhabitants. The town of Chernobyl,which had a population of 12,500, is about 15 km to the South-eastof the complex. Within a 30-km radius of the power plant, thetotal population was between 115,000 and 135,000. The RBMK-1000 reactor The RBMK-1000 (Figure 2) is a Soviet designed and built graphitemoderated pressure tube type reactor, using slightly enriched(2 per cent uranium-235) uranium dioxide fuel. It is a boilinglight water reactor, with Figure 2. The RBMK reactor direct steam feed to the turbines, without an intervening heat-exchanger.Water pumped to the bottom of the fuel channels boils as it progressesup the pressure tubes, producing steam which feeds two 500-MW(e)[megawatt electrical] turbines. The water acts as a coolant andalso provides the steam used to drive the turbines. The verticalpressure tubes contain the zirconium-alloy clad uranium-dioxidefuel around which the cooling water flows. A specially designedrefuelling machine allows fuel bundles to be changed without shuttingdown the reactor. The moderator, whose function is to slow down neutrons to makethem more efficient in producing fission in the fuel, is constructedof graphite. A mixture of nitrogen and helium is circulated betweenthe graphite blocks largely to prevent oxidation of the graphiteand to improve the transmission of the heat produced by neutroninteractions in the graphite, from the moderator to the fuel channel.The core itself is about 7 m high and about 12 m in diameter.There are four main coolant circulating pumps, one of which isalways on standby. The reactivity or power of the reactor is controlledby raising or lowering 211 control rods, which, when lowered,absorb neutrons and reduce the fission rate. The power outputof this reactor is 3,200 MW(t) [megawatt thermal] or 1,000 MW(e),although there is a larger version producing 1,500 MW(e). Varioussafety systems, such as an emergency core cooling system and therequirement for an absolute minimal insertion of 30 control rods,were incorporated into the reactor design and operation. The most important characteristic of the RBMK reactor is thatit possesses a "positive void coefficient". This meansthat if the power increases or the flow of water decreases, thereis increased steam production in the fuel channels, so that theneutrons that would have been absorbed by the denser water willnow produce increased fission in the fuel. However, as the powerincreases, so does the temperature of the fuel, and this has theeffect of reducing the neutron flux (negative fuel coefficient).The net effect of these two opposing characteristics varies withthe power level. At the high power level of normal operation,the temperature effect predominates, so that power excursionsleading to excessive overheating of the fuel do not occur. However,at a lower power output of less than 20 per cent of the maximum,the positive void coefficient effect is dominant and the reactorbecomes unstable and prone to sudden power surges. This was amajor factor in the development of the accident. Events leading to the accident (IA86, IA86a) The Unit 4 reactor was to be shutdown for routine maintenanceon 25 April 1986. It was decided to take advantage of this shutdownto determine whether, in the event of a loss of station power,the slowing turbine could provide enough electrical power to operatethe emergency equipment and the core cooling water circulatingpumps, until the diesel emergency power supply became operative.The aim of this test was to determine whether cooling of the corecould continue to be ensured in the event of a loss of power. This type of test had been run during a previous shut-down period,but the results had been inconclusive, so it was decided to repeatit. Unfortunately, this test, which was considered essentiallyto concern the non-nuclear part of the power plant, was carriedout without a proper exchange of information and co-ordinationbetween the team in charge of the test and the personnel in chargeof the operation and safety of the nuclear reactor. Therefore,inadequate safety precautions were included in the test programmeand the operating personnel were not alerted to the nuclear safetyimplications of the electrical test and its potential danger. The planned programme called for shutting off the reactor's emergencycore cooling system (ECCS), which provides water for cooling thecore in an emergency. Although subsequent events were not greatlyaffected by this, the exclusion of this system for the whole durationof the test reflected a lax attitude towards the implementationof safety procedures. As the shutdown proceeded, the reactor was operating at abouthalf power when the electrical load dispatcher refused to allowfurther shutdown, as the power was needed for the grid. In accordancewith the planned test programme, about an hour later the ECCSwas switched off while the reactor continued to operate at halfpower. It was not until about 23:00 hr on 25 April that the gridcontroller agreed to a further reduction in power. For this test, the reactor should have been stabilised at about1,000 MW(t) prior to shut down, but due to operational error thepower fell to about 30 MW(t), where the positive void coefficientbecame dominant. The operators then tried to raise the power to700-1,000 MW(t) by switching off the automatic regulators andfreeing all the control rods manually. It was only at about 01:00hr on 26 April that the reactor was stabilised at about 200 MW(t). Although there was a standard operating order that a minimum of 30 control rods was necessary to retain reactor control, in thetest only 6-8 control rods were actually used. Many of the controlrods were withdrawn to compensate for the build up of xenon whichacted as an absorber of neutrons and reduced power. This meantthat if there were a power surge, about 20 seconds would be required to lower the control rods and shutthe reactor down. In spite of this, it was decided to continuethe test programme. There was an increase in coolant flow and a resulting drop insteam pressure. The automatic trip which would have shut downthe reactor when the steam pressure was low, had been circumvented.In order to maintain power the operators had to withdraw nearlyall the remaining control rods. The reactor became very unstableand the operators had to make adjustments every few seconds tryingto maintain constant power. At about this time, the operators reduced the flow of feedwater,presumably to maintain the steam pressure. Simultaneously, thepumps that were powered by the slowing turbine were providingless cooling water to the reactor. The loss of cooling water exaggeratedthe unstable condition of the reactor by increasing steam productionin the cooling channels (positive void coefficient), and the operatorscould not prevent an overwhelming power surge, estimated to be 100 times the nominal power output. The sudden increase in heat production ruptured part of the fueland small hot fuel particles, reacting with water, caused a steamexplosion, which destroyed the reactor core. A second explosionadded to the destruction two to three seconds later. While itis not known for certain what caused the explosions, it is postulatedthat the first was a steam/hot fuel explosion, and that hydrogenmay have played a role in the second. The accident The accident occurred at 01:23 hr on Saturday, 26 April 1986,when the two explosions destroyed the core of Unit 4 and the roofof the reactor building. In the IAEA Post-Accident Assessment Meeting in August 1986 (IA86),much was made of the operators' responsibility for the accident,and not much emphasis was placed on the design faults of the reactor.Later assessments (IA86a) suggest that the event was dueto a combination of the two, with a little more emphasis on thedesign deficiencies and a little less on the operator actions. The two explosions sent a shower of hot and highly radioactivedebris and graphite into the air and exposed the destroyed coreto the atmosphere. The plume of smoke, radioactive fission productsand debris from the core and the building rose up to about 1 kminto the air. The heavier debris in the plume was deposited closeto the site, but lighter components, including fission productsand virtually all of the noble gas inventory were blown by theprevailing wind to the North-west of the plant. Fires started in what remained of the Unit 4 building, givingrise to clouds of steam and dust, and fires also broke out onthe adjacent turbine hall roof and in various stores of dieselfuel and inflammable materials. Over 100 fire-fighters from thesite and called in from Pripyat were needed, and it was this groupthat received the highest radiation exposures and suffered thegreatest losses in personnel. These fires were put out by 05:00hr of the same day, but by then the graphite fire had started.Many firemen added to their considerable doses by staying on callon site. The intense graphite fire was responsible for the dispersionof radionuclides and fission fragments high into the atmosphere.The emissions continued for about twenty days , but were muchlower after the tenth day when the graphite fire was finally extinguished. The graphite fire While the conventional fires at the site posed no special firefightingproblems, very high radiation doses were incurred by the firemen.However, the graphite moderator fire was a special problem. Verylittle national or international expertise on fighting graphitefires existed, and there was a very real fear that any attemptto put it out might well result in further dispersion of radionuclides,perhaps by steam production, or it might even provoke a criticalityexcursion in the nuclear fuel. A decision was made to layer the graphite fire with large amountsof different materials, each one designed to combat a differentfeature of the fire and the radioactive release. Boron carbidewas dumped in large quantities from helicopters to act as a neutronabsorber and prevent any renewed chain reaction. Dolomite wasalso added to act as heat sink and a source of carbon dioxideto smother the fire. Lead was included as a radiation absorber,as well as sand and clay which it was hoped would prevent therelease of particulates. While it was later discovered that manyof these compounds were not actually dropped on the target, theymay have acted as thermal insulators and precipitated an increasein the temperature of the damaged core leading to a further releaseof radionuclides a week later. By May 9, the graphite fire had been extinguished, and work beganon a massive reinforced concrete slab with a built-in coolingsystem beneath the reactor. This involved digging a tunnel fromunderneath Unit 3. About four hundred people worked on this tunnelwhich was completed in 15 days, allowing the installation of theconcrete slab. This slab would not only be of use to cool thecore if necessary, it would also act as a barrier to prevent penetrationof melted radioactive material into the groundwater. In summary, the Chernobyl accident was the product of a lackof "safety culture". The reactor design was poor fromthe point of view of safety and unforgiving for the operators,both of which provoked a dangerous operating state. The operatorswere not informed of this and were not aware that the test performedcould have brought the reactor into explosive conditions. In addition,they did not comply with established operational procedures. Thecombination of these factors provoked a nuclear accident of maximumseverity in which the reactor was totally destroyed within a fewseconds. WT03-B20-77IA006-000057-B014-622http://lacebark.ntu.edu.au:80/j_mitroy/sid101/acid2/backgrnd.html 138.80.61.12 19970221183917 text/html 5749HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:09:33 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5578Last-modified: Tue, 09 Jul 1996 02:25:50 GMT State of the Environment Norway - Acid rain GRID-Arendal: State of the Environment Norway - Acid rain Acid rain: Direct effects of SO2 and NOx SO2 and NOx have adverse effects on human health. Investigations show that the effects may reduce the function of the lungs in astmathics, increase the resistance of the respiratory passages ( healthy and astmathics) and increase the receptiveness for infection of the respiratory passages.Population groups in heavily affected areas have a larger number of acute- and chronic respiratory diseases. SO2 and NOx have adverse effects on vegetation. Heavy concentrations of NOx has proven to inhibit vegetation growth, especially when ozone is present. (Synergistic effects) SO inhibits photosynthesis in green plants, leading to reduced plant growth. The chemistry of acid in the soil. Indirect effects. The effects of acid rain are substantial. However, soil and water have a natural buffer capacity. This means that nature has a natural ability to cope with acid rain up to a certain level: the critical load. The problems are most severe where the soil contains only a small amount of Calcium, Potassium and Magnesium. These elements constitute the basic buffer capacity of the soil. When the critical load has been exceeded, the metal cations and other plant nutrients are washed out. This explains why there is often a significant amount of aluminium in water coming from areas damaged by acid precipitation. The acid in acid rain is due to Hydrogen ions (H+). The buffer capacity of soil implicates that the soil absorbs the H+ ions, while other positive ions are liberated. In other words; the H+ ions replace the other ions bound in the soil structure. (Colloids)The problem become apparent when there are more H+ ions absorbed than can be neutralised.The positive ions �washed out� by H+ ions are called cations. The most common cations are: Calsium (Ca++), Magnesium (Mg++), Potassium (K+), Aluminium (Al+++), Manganese (Mn++) and Ammonium (NH4+). How long it would take for an ecosystem to regain its normal chemical and biological status if emissions were to cease is uncertain. The chemistry of the soil and its capacity for recovery will be decisive. The soils store of base cations would gradually be built up through the inflow from weathering and from the atmosphere: a process that may be assumed to need several decades, in some cases centuries. Effects of the washing out of metal ions Aluminium The washing out of metal ions has lead to damages of forests, lakes, watercourses and fish. Sources for potable water may become poisoned by high concentrations of copper, lead and aluminium. Aluminium causes the most severe problems to the water in Southern Norway. Death of fish Death of fish is the best known effect of acidification, and is therefore a reliable and frequently used indicator. There are two reasons for the death of fish. When the acidification reaches a certain level, the fry die. Different species of fish have different tolerance levels to acidic water. The most sensitive species are trout and salmon. The presence of aluminium in the water is another common cause of fish death. Aluminium is toxic to fish, because it prevents a fish from absorbing salts and destroys the gills. Aluminium is the major cause of fish death in the southern part of Norway. From 1960 to 1990, areas with damages to fish stock were quintupled. Of the 13 000 investigated stocks of fish in the Southern part of Norway during this period, 19 per cent were lost, while a reduction in the number of fish was registered in 21 per cent of the stocks. Damage to vegetation Damage to vegetation and the dying of forests have long been associated with acid rain. The washing out of metal ions results in limited plant nutrients. (Nitrogen, Phosphorous, Potassium, Magnesium etc)In many cases, a plant�s ability to absorb these nutrients depends on the level of acidification in the soil. Vegetation exposed to acid rain will therefore absorb less of these essential nutrients. The result is poorer quality of a plant�s proteins, stunted growth and exposure to diseases. Green plants are the foundation of the food chain, and a reduction in plant production will have consequences higher up in the food chain. Animals and human beings may experience less valuable nutrition. Compared to other countries, the situation in Norway is better than in Central Europe. In e.g. the Czech Republic, 71 per cent of the forest has been damaged due to acid rain. The situation is also severe where the supply of potable water is concerned.(Source: Dr. Josef Krecek, Inst. of Applied Ecology, Agricultural University of Prague). IndicatorsAcid rain: Forests of BohemiaWT03-B20-78IA006-000055-B005-396http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl2/c07.html 138.80.61.12 19970221174810 text/html 14706HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:18:25 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 14534Last-modified: Tue, 09 Jul 1996 02:25:47 GMT Chapter VII POTENTIAL RESIDUAL RISKS The Sarcophagus In the aftermath of the accident several designs to encase thedamaged reactor were examined (Ku95). The option whichwas chosen provided for the construction of a massive structurein concrete and steel that used as a support what remained ofthe walls of the reactor building (Ku95). By August 1986 special sensors monitoring gamma radiation andother parameters were installed in various points by using cranesand helicopters. These sensors had primarily the function of assessingthe radiation exposure in the areas where the work for the constructionwas to be carried out. An outer protective wall was then erected around the perimeterand other walls in the turbine building, connected to the reactorUnit 3 building through an intermediate building, the so-called"V" building, and a steel roof completed the structure.The destroyed reactor was thus entombed in a 300,000-tonne concreteand steel structure known as the "Envelope" or "Sarcophagus".This mammoth task was completed in only seven months, in November1986. Multiple sensors were placed to monitor such parameters as gammaradiation and neutron flux, temperature, heat flux, as well asthe concentrations of hydrogen, carbon monoxide and water vapourin air. Other sensors monitor the mechanical stability of thestructure and the fuel mass so that any vibration or shifts ofmajor components can be detected. All these sensors are undercomputer control. Systems designed to mitigate any changing adverseconditions have also been put into place. These include the injectionof chemicals to prevent nuclear criticality excursions in thefuel and pumping to remove excess water leaking into the Sarcophagus(To95). An enormous effort was required to mount the clean-up operation;decontaminating ground and buildings, enclosing the damaged reactorand building the Sarcophagus was a formidable task, and it isimpressive that so much was achieved so quickly. At that timethe emphasis was placed on confinement as rapidly as possible.Consequently, a structure which would effectively be permanentwas not built and the Sarcophagus should rather be seen as a provisionalbarrier pending the definition of a more radical solution forthe elimination of the destroyed reactor and the safe disposalof highly radioactive materials. In these conditions, to maintainthe existing structure for the next several decades poses verysignificant engineering problems. Consultations and studies byan international consortium are currently taking place to providea permanent solution to this problem. The fuel in the damaged reactor exists in three forms, (a) aspellets of 2 per cent enriched uranium dioxide plus some fissionproducts essentially unchanged from the original forms in thefuel rods, (b) as hot particles of uranium dioxide a few tensof microns in diameter or smaller particles of a few microns,made of fuel fused with the metal cladding of the fuel rods, and(c) as three extensive lava-like flows of fuel mixed with sandor concrete. The amount of dispersed fuel in the form of dustis estimated to amount to several tons (Gl95). The molten fuel mixture has solidified into a glass-like materialcontaining former fuel. The estimates of the quantity of thisfuel are very uncertain. It is this vitrified material that islargely responsible for the very high dose rates in some areas(Se95a). Inside the reactor envelope, external exposureis largely from caesium-137, but the inhalation of fuel dust isalso a hazard. As was noted earlier, a small special group ofscientists who have worked periodically inside the Sarcophagusfor a number of years have accumulated doses in the estimatedrange of 0.5 to 13 Gy (Se95a). Due to the fact that thesedoses were fractionated over a long time period, no deterministiceffects have been noted in these scientists. Since the beginningof 1987 the intensity of the gamma radiation inside the structurefell by a factor 10. The temperature also fell significantly.Outside the Sarcophagus, the radiation levels are not high, exceptfor the roof where dose rates up to 0.5 Gy/h have been measuredafter the construction of the Sarcophagus. These radiation levelson the roof have now decreased to less than 0.05 Gy/h. Nine years after its erection, the Sarcophagus structure, althoughstill generally sound, raises concerns for its stability and long-termresistance and represents a standing potential risk. Some supportsfor the enclosure are the original Unit 4 building structureswhich may be in poor condition following the explosions and fire,and their failure could cause the roof to collapse. This situationis aggravated by the corrosion of internal metallic structuresdue to the high humidity of the Sarcophagus atmosphere provokedby the penetration of large quantities of rain water through thenumerous cracks which were present on the roof and were only recentlyrepaired (La95). The existing structure is not designedto withstand earthquakes or tornados. The upper concrete biologicalshield of the reactor is lodged between walls, and may fall. Thereis considerable uncertainty on the condition of the lower floorslab, which was damaged by the penetration of molten materialduring the accident. It this slab failed, it could result in thedestruction of most of the building. A number of potential situations have been considered which couldlead to breaches in the Sarcophagus and the release of radionuclidesinto the environment. These include the collapse of the roof andinternal structures, a possible criticality event, and the long-termmigration of radionuclides into groundwater. Currently, the envelope is not leaktight even if its degree ofconfinement has been recently improved. Although the current emissionsinto the environment are small, not exceeding 10 Gbq/y for caesium-137and 0.1 GBq/y for plutonium and other transuranic elements, disturbanceof the current conditions within the Sarcophagus, such as thedislodgement of the biological shield could result in more significantdispersion of radionuclides (To95). The dispersion in thiscase would not be severe and would be confined to the site providedthat the roof did not collapse. However, collapse of the roof,perhaps precipitated by an earthquake, a tornado or a plane crash,combined with collapse of internal unstable structures could leadto the release of the order of 0.1 PBq of fuel dust, contaminatingpart of the 30-km exclusion zone (Be95). More improbable worst case scenarios would result in higher contaminationof the exclusion zone, but no significant contamination is expectedbeyond that area. Perhaps the situation causing most concern isthe effect that the collapse of the Sarcophagus might have onthe reactor Unit 3, which is still producing power and whose buildingis connected to the Sarcophagus through the "V" Building,which is not very stable. Currently, criticality excursions are not thought to be likely(IP95). Nevertheless, it is possible to theorise (Go95,Bv95) on hypothetical accident scenarios, however remote,which could lead to a criticality event. One such scenario wouldinvolve a plane crash or earthquake with collapse of the Sarcophagus,combined with flooding. An accident of this type could releaseabout 0.4 PBq of old fuel dust and new fission products to theatmosphere to contaminate the ground mainly in the 30-km zone. Leakage from the Sarcophagus can also be a mechanism by whichradionuclides are released into the environment. There are currentlyover 3,000 m3 of water in various rooms in the Sarcophagus (To95).Most of this has entered through defects in the roof. Its activity,mainly caesium-137, ranges from 0.4 to 40 MBq/L. Studies on thefuel containing masses indicate that they are not inert and arechanging in various ways. These changes include the pulverisationof fuel particles, the surface breakdown of the lava-like material,the formation of new uranium compounds, some of which are solubleon the surface, and the leaching of radionuclides from the fuelcontaining masses. Studies to date indicate that this migrationmay become more significant as time passes. Another possible mechanism of dispersion of radioactivity intothe environment may be the transport of contamination by animals,particularly birds and insects, which penetrate and dwell in theSarcophagus (Pu92). Finally, the possibility of leachingof radionuclides from the fuel masses by the water in the enclosureand their migration into the groundwater has been considered.This phenomenon, however, is expected to be very slow and it hasbeen estimated that, for example, it will take 45 to 90 yearsfor certain radionuclides, such as strontium-90, to migrate undergoundup to the Pripyat river catchment area. The expected radiologicalsignificance of this phenomenon is not known with certainty anda careful monitoring of the evolving situation of the groundwaterwill need to be carried out for a long time. Radioactive waste storage sites The accident recovery and clean-up operations have resulted inthe production of very large quantities of radioactive wastesand contaminated equipment. Some of these radioactive wastes areburied in trenches or in containers isolated from the groundwaterby clay or concrete screens within the 30-km zone (Vo95).A review of these engineered sites concluded that, provided theclay layer remained intact, their contribution to groundwatercontamination would be negligible. On the other hand, 600 to 800waste trenches were hastily dug in the immediate vicinity of theUnit 4 in the aftermath of the accident. These unlined trenchescontain the radioactive fallout that had accumulated on trees,grass, and in the ground to a depth of 10-15 cm and which wasbulldozed from over an area of roughly 8 km2. The estimated activityamount is now of the order of 1 PBq, which is comparable to thetotal inventory stored in specially constructed facilities nextto Unit 4. Moreover, a large number of contaminated equipment,engines and vehicles are also stored in the open air. The original clean-up activities are poorly documented, and muchof the information on the present status of the unlined trenchesnear Unit 4 and the spread of radioelements has been obtainedin a one-time survey. Some of the findings of the study (Dz95)are that: the water table in the vicinity of Unit 4 has risen by 1 to1.5 m in a few years to about 4 m from the ground level and maystill be rising (apparently this is due mostly to the construction,in 1986, of a wall 3,5 km long and 35 m deep around the reactorto protect the Kiev reservoir from possible spread of contaminationthrough the underground water, as well as to the ceasing of drainageactivities formerly connected with the construction of new unitson the site). in the most targeted study area 32 of 43 explored trenchesare periodically or continually flooded; in that area the upper unconfined aquifer is contaminatedeverywhere with strontium-90 to levels exceeding 4 Bq/L. Caesiumand plutonium are less mobile and contamination from these elementsis confined to the immediate vicinity of the disposal trenches; the relative mobility of the strontium-90 is especially importantin that, from the closest trenches, it might reach the PripyatRiver in 10 to 20 years; It is clear that large uncertainties remain which require a correspondinglylarge characterisation effort. For instance, at present, mostdisposal sites are unexplored, and a few are uncharted; monitoringfor groundwater movement is insufficient and the interpretationof the hydrologic regime is complicated by artificial factors(pumping, mitigative measures, etc.); the mechanisms of radionuclideleaching from the variety of small buried particles are not wellunderstood. The problem of the potential spread of radioelements to the Pripyatriver is especially important in that the latter may act as ashortcut for the dispersion of additional radioactive elementsoutside the 30-km exclusion zone. In summary, the sarcophagus was never intended to be a permanentsolution to entomb the stricken reactor. The result is that thistemporary solution may well be unstable in the long term. Thismeans that there is the potential for collapse which needs tobe corrected by a permanent technical solution. The accident recovery and clean-up operations have also resultedin the production of very large quantities of radioactive wastesand contaminated equipment which are currently stored in about800 sites within and outside the 30-km exclusion zone around thereactor. These wastes are partly conserved in containers and partlyburied in trenches or stored in the open air. In general, it has been assessed that the Sarcophagus and theproliferation of waste storage sites in the area constitute aseries of potential sources of release of radioactivity that threatensthe surrounding area. However, any accidental releases from thesarcophagus are expected to be very small in comparison with thosefrom the Chernobyl accident in 1986 and their radiological consequenceswould be limited to a relatively small area around the site. Onthe other hand, concerns have been expressed by some experts thata more important release might occur if the collapse of the Sarcophagusshould induce damage in the Unit 3 of the Chernobyl power plant. As far as the radioactive wastes stored in the area aroundthe site are concerned, they are a potential source of contaminationof the groundwater which will require close monitoring until asafe disposal into an appropriate repository is implemented. Initiatives have been taken internationally, and are currentlyunderway, to study a technical solution leading to the eliminationof these sources of residual risk on the site. WT03-B20-79IA006-000055-B005-66http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl2/c03.html 138.80.61.12 19970221174408 text/html 15010HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:14:25 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 14838Last-modified: Tue, 09 Jul 1996 02:25:47 GMT Chapter III REACTIONS OF NATIONAL AUTHORITIES The scale and severity of the Chernobyl accident with its widespreadradioactive contamination had not been foreseen and took by surprisemost national authorities responsible for emergency preparedness.No provisions had been made for an accident of such scale and,though some radiation protection authorities had made criteriaavailable for intervention in an accident, these were often incompleteand provided little practical help in the circumstances, so thatvery few workable national guidelines or principles were actuallyin place. Those responsible for making national decisions weresuddenly faced with an accident for which there were no precedentsupon which to base their decisions. In addition, early in thecourse of the accident there was little information available,and considerable political pressure, partially based on the publicperception of the radiation danger, was being exerted on the decision-makers.In these circumstances, cautious immediate action was felt necessary,and measures were introduced that tended to err, sometimes excessivelyso, on the side of prudence rather than being driven by informedscientific and expert judgement. Within the former Soviet Union The town of Pripyat was not severely contaminated by the initialrelease of radionuclides, but, once the graphite fire started,it soon became obvious that contamination would make the townuninhabitable. Late on 26 April it was decided to evacuate thetown, and arrangements for transport and accommodation of theevacuees were made. The announcement of evacuation was made at11:00 hr the following day. Evacuation began at 14:00 hr, andPripyat was evacuated in about two and one half hours. As measurementsdisclosed the extensive pattern of deposition of radionuclides,and it was possible to make dose assessments, the remainder ofthe people in a 30-km zone around the reactor complex were graduallyevacuated, bringing the total evacuees to about 135,000. Other countermeasures to reduce dose were widely adopted (Ko90).Decontamination procedures performed by military personnel includedthe washing of buildings, cleaning residential areas, removingcontaminated soil, cleaning roads and decontaminating water supplies.Special attention was paid to schools, hospitals and other buildingsused by large numbers of people. Streets were watered in townsto suppress dust. However, the effectiveness of these countermeasuresoutside the 30-km zone was small. An attempt to reduce thyroiddoses by the administration of stable iodine to block radioactiveuptake by the thyroid was made (Me92), but its successwas doubtful. The Soviet National Committee on Radiation Protection (NCRP) proposeda 350-mSv lifetime dose intervention level for the relocationof population groups (Il87). This value was lower by afactor of 2 to 3 than that recommended by the International Commissionon Radiological Protection (ICRP) for the same countermeasure.Nevertheless, this value proposed by the NCRP was strongly criticisedas being a very high level. The situation was further complicatedby the political and social tension in the Soviet Union at thattime. As a result, the NCRP proposal was not adopted by the SupremeSoviet. Later, a special Commission was established which developednew recommendations for intervention levels. These recommendationswere based on the levels of ground contamination by the radionuclidescaesium-137, strontium-90 and plutonium239. As has been mentionedabove, large areas were contaminated mainly by caesium-137 anda ground contamination level by this radionuclide of 1,480 kBq/m2was used as the intervention criterion for permanent resettlementof population, and of 555 to 1,480 kBq/m2 for temporary relocation. People who continued to live in the heavily contaminated areaswere given compensation and offered annual medical examinationsby the government. Residents of less contaminated areas are providedwith medical monitoring. Current decisions on medical actionsare based on annual doses. Compensation is provided for residentswhose annual dose is greater than 1mSv. The use of locally producedmilk and mushrooms is restricted in some of these areas. Relocationis considered in Russia for annual doses above 5 mSv. As is mentioned in the section on psychological effects, in ChapterV, the Soviet authorities did not foresee that their attemptsto compensate those affected by the accident would be misinterpretedby the recipients and increase their stress, and that the labelof "radiophobia" attributed to these phenomena was notonly incorrect, but was one that alienated the public even more.Some of these initial approaches have been recognised as beinginappropriate and the authorities are endeavouring to rectifytheir attitude to the exposed population. Outside the former Soviet Union The progressive spread of contamination at large distances fromthe accident site has caused considerable concern in Member countries,and the reactions of national authorities to this situation havebeen extremely varied, ranging from a simple intensification ofthe normal environmental monitoring programmes, without adoptionof any specific countermeasures, to compulsory restrictions concerningthe marketing and consumption of foodstuffs. This variety of responseshas been accompanied by significant differences in the timingand duration of the countermeasures. In general, the most widespread countermeasures were those whichwere not expected to impose, in the short time for which theywere in effect, a significant burden on lifestyles or the economy.These included advice to wash fresh vegetables and fruit beforeconsumption, advice not to use rainwater for drinking or cooking,and programmes of monitoring citizens returning from potentiallycontaminated areas. In reality, experience has shown that eventhese types of measures had, in some cases, a negative impactwhich was not insignificant. Protective actions having a more significant impact on dietaryhabits and imposing a relatively important economic and regulatoryburden included restrictions or prohibitions on the marketingand consumption of milk, dairy products, fresh leafy vegetablesand some types of meat, as well as the control of the outdoorgrazing of dairy cattle. In some areas, prohibitions were placedon travel to areas affected by the accident and on the importof foodstuffs from the Soviet Union and Eastern European countries.In most Member countries, restrictions were imposed on the importof foodstuffs from Member as well as non-Member countries. The range of these reactions can be explained primarily by thediversity of local situations both in terms of uneven levels ofcontamination and in terms of national differences in administrative,regulatory and public health systems. However, one of the principalreasons for the variety of situations observed in Member countriesstems from the criteria adopted for the choice and applicationof intervention levels for the implementation of protective actions.In this respect, while the general radiation protection principlesunderlying the actions taken in most Member countries followingthe accident have been very similar, discrepancies arose in theassessment of the situation and the adoption and application ofoperational protection criteria. These discrepancies were furtherenhanced by the overwhelming role played in many cases by non-radiologicalfactors, such as socio-economic, political and psychological,in determining the countermeasures. This situation caused concern and confusion among the public,perplexities among the experts and difficulties to national authorities,especially in maintaining their public credibility. This was,therefore, identified as an area where several lessons shouldbe learned from the accident and efforts directed towards betterinternational harmonisation of the scientific bases and co-ordinationof concepts and measures for the protection of the public in caseof emergency. Nowhere was this problem better illustrated than by the way thatcontaminated food was handled. In some countries outside the SovietUnion the main source of exposure to the general population wasthe consumption of contaminated food. Mechanisms to handle locallyproduced as well as imported contaminated food had to be put inplace within a few weeks of the accident. National authoritieswere in an unenviable position. They had to act quickly and cautiouslyto introduce measures to protect the "purity" of thepublic food supply and, what is more, they had to be seen to beeffective in so doing. This inevitably led to some decisions whicheven at the time appeared to be over-reactions and not scientificallyjustified. In addition, dissenting opinions among experts addedto the difficulties of the decision-makers. Some countries without nuclear power programmes and whose ownfood was not contaminated, argued that they did not need to importany "tainted" food and refused any food containing anyradionuclides whatsoever. This extreme and impracticable measuremight well have been regarded as an example of how well the authoritiesof those countries were protecting the health of their population.Sometimes this attitude appeared to promote a neighbourly rivalrybetween countries to see which could set the more stringent standardsfor food contamination, as though, by so doing, their own citizenswere more protected. The result was that often slightly contaminatedfood was destroyed or refused importation to avoid only trivialdoses. In 1986, the EC imposed a ban on the importation of food containingmore than 370 Bq/kg of radiocaesium for milk products and 600Bq/kg for any other food, regardless of the quantity consumedin the average European diet. Thus, food items with a trivialconsumption (and dose), such as spices, were treated the sameas items of high consumption such as vegetables. However, thesevalues were later relaxed for some food items in order to removeinconsistent treatment of food groups. In some special circumstances, decisions had to be made basedon the local situation. For example, in some Northern Europeancommunities, reindeer meat is a major component of the diet; dueto the ecological circumstances, these animals tend to concentrateradiocaesium, which will then expose the populations which dependon them. Special countermeasures, such as pasturing reindeer inareas of lower contamination, were introduced in some countriesto avoid this exposure. The variety of solutions led to confusion and made any internationalconsensus on Derived Intervention Levels for food extremely difficultto achieve, and it was only with the WHO/FAO Codex AlimentariusMeeting in Geneva in 1989 that any agreement was reached on guidelinevalues for the radioactivity of food moving in international trade (Table 2). Table 2. Codex Alimentarius Guideline values for food moving in international trade (FA91). FOODS FOR GENERAL CONSUMPTION Radionuclide Level (Bq/kg) americium-241, plutonium-239 strontium-90 iodine-131, caesium-134, caesium-137 10 100 1,000 INFANT FOODS AND MILK americium-241, plutonium-239 iodine-131, strontium-90 caesium-134, caesium-137 1 100 1,000 It should be remembered that these guideline values were developedto facilitate international trade in food, and should be regardedas levels "below regulatory concern". Levels above thesedo not necessarily constitute a health hazard, and if found, thecompetent national authority should review what action shouldbe taken. Often the national authorities were not able accurately to predictthe public response to some of their advice and pronouncements.For example, in some European countries, soon after the accidentthe public were advised to wash leafy vegetables. The nationalauthority felt that this was innocuous advice as most people washedtheir vegetables anyway, and they were unprepared for the publicresponse which was to stop buying these vegetables. This resultedin significant economic loss to local producers which far outweighedany potential benefit in terms of radiological health. In some countries, the public was told that the risks were verysmall but, at the same time, were given advice on how to reducethese low risks. It was very difficult to explain this apparentlycontradictory advice, and the national authority came under criticismfrom the media (Sj87). Outside the Soviet Union, the initialconfusion led to inconsistent and precipitate actions which, althoughunderstandable, were sometimes ill-advised and unjustified. However, it should be emphasised that great progress has beenmade since this early confusion. As a result of the actions ofthe international organisations to harmonise intervention criteriaand the willingness of countries to cooperate in this endeavour,a firm groundwork for uniform criteria based on accepted radiationprotection principles has been established, so that relative consistencycan hopefully be expected in their implementation in the eventof a possible future nuclear accident. In summary, the Chernobyl accident took authorities by surpriseas regards extent, duration and contamination at long distance.As no guidelines were available for such an accident, little informationwas available and great political and public pressure to do somethingwere experienced, overprecautious decisions were often taken inand outside the Soviet Union. The psychological impact of someofficial decisions on the public were not predicted and variableinterpretations or even misinterpretations of ICRP recommendations,especially for intervention levels for food, led to inconsistentdecisions and advice. These added to public confusion and provokedmistrust and unnecessary economic losses. However, there wereexceptions and very soon international efforts started to harmonisecriteria and approaches to emergency management. WT03-B20-80IA006-000055-B005-355http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl2/c06.html 138.80.61.12 19970221174749 text/html 20047HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:18:06 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 19875Last-modified: Tue, 09 Jul 1996 02:25:47 GMT Chapter VI AGRICULTURAL AND ENVIRONMENTAL IMPACTS Agricultural impact All soil used anywhere for agriculture contains radionuclidesto a greater or lesser extent. Typical soils (IA89a) containapproximately 300 kBq/m3 of potassium-40 to a depth of 20 cm.This radionuclide and others are then taken up by crops and transferredto food, leading to a concentration in food and feed of between50 and 150 Bq/kg. The ingestion of radionuclides in food is oneof the pathways leading to internal retention and contributesto human exposure from natural and man-made sources. Excessivecontamination of agricultural land, such as may occur in a severeaccident, can lead to unacceptable levels of radionuclides infood. The radionuclide contaminants of most significance in agricultureare those which are relatively highly taken up by crops, havehigh rates of transfer to animal products such as milk and meat,and have relatively long radiological half-lives. However, theecological pathways leading to crop contamination and the radioecologicalbehaviour of the radionuclides are complex and are affected notonly by the physical and chemical properties of the radionuclidesbut also by factors which include soil type, cropping system (includingtillage),climate, season and, where relevant, biological half-lifewithin animals. The major radionuclides of concern in agriculturefollowing a large reactor accident are iodine-131, caesium-137,caesium-134 and strontium-90 (IA89a). Direct depositionon plants is the major source of contamination of agriculturalproduce in temperate regions. While the caesium isotopes and strontium-90 are relatively immobilein soil, uptake of roots is of less importance compared with plantdeposition. However, soil type (particularly with regard to claymineral composition and organic matter content), tillage practiceand climate all affect propensity to move to groundwater. Thesame factors affect availability to plants insofar as they controlconcentrations in soil solution. In addition, because caesiumand strontium are taken up by plants by the same mechanism aspotassium and calcium respectively, the extent of their uptakedepends on the availability of these elements. Thus, high levelsof potassium fertilisation can reduce caesium uptake and limingcan reduce strontium uptake. Within the former Soviet Union The releases during the Chernobyl accident contaminated about125,000 km2 of land in Belarus, Ukraine and Russia with radiocaesiumlevels greater than 37 kBq/m2, and about 30,000 km2 with radiostrontiumgreater than 10 kBq/m2. About 52,000 km2 of this total were inagricultural use; the remainder was forest, water bodies and urbancentres (Ri95). While the migration downwards of caesiumin the soil is generally slow (Bo93), especially in forestsand peaty soil, it is extremely variable depending on many factorssuch as the soil type, pH, rainfall and agricultural tilling.The radionuclides are generally confined to particles with a matrixof uranium dioxide, graphite, iron-ceramic alloys, silicate-rareearth, and silicate combinations of these materials. The movementof these radionuclides in the soil not only depends on the soilcharacteristics but also on the chemical breakdown of these complexesby oxidation to release more mobile forms. The bulk of the fissionproducts is distributed between organomineral and mineral partsof the soil largely in humic complexes. The 30-km exclusion zonehas improved significantly partly due to natural processes andpartly due to decontamination measures introduced. There were also large variations in the deposition levels. During1991 the caesium-137 activity concentrations in the 0-5 cm soillayer ranged from 25 to 1,000 kBq/m3 and were higher in naturalthan ploughed pastures. For all soils, between 60 and 95 per centof all caesium-137 was found to be strongly bound to soil components(Sa94). Ordinary ploughing disperses the radionuclidesmore evenly through the soil profile, reducing the activity concentrationin the 0-5 cm layer and crop root uptake. However, it does spreadthe contamination throughout the soil, and the removal and disposalof the uppermost topsoil may well be a viable decontaminationstrategy. The problem in the early phase of an accident is that the countermeasuresdesigned to avoid human exposure are of a restrictive nature andoften have to be imposed immediately, even before the levels ofcontamination are actually measured and known. These measuresinclude the cessation of field work, of the consumption of freshvegetables, of the pasturing of animals and poultry, and alsothe introduction of uncontaminated forage. Unfortunately, thesemeasures were not introduced immediately and enhanced the dosesto humans in Ukraine (Pr95). Furthermore, some initial extreme measures were introduced inthe first few days of the accident when 15,000 cows were slaughteredin Ukraine irrespective of their level of contamination, whenthe introduction of clean fodder could have minimised the incorporationof radiocaesium. Other countermeasures, such as the use of potassiumfertilisers, decreased the uptake of radiocaesium by a factorof 2 to 14, as well as increased crop yield. In some podzolic soils, lime in combination with manure and mineralfertilisers can reduce the accumulation of radiocaesium in somecereals and legumes by a factor of thirty. In peaty soils, sandand clay application can reduce the transfer of radiocaesium toplants by fixing it more firmly in the soil. The radiocaesiumcontent of cattle for human consumption can be minimised by astaged introduction of clean feed during about ten weeks priorto slaughter. A policy of allocating critical food productionto the least contaminated areas may be an effective common sensemeasure. In 1993, the concentration of caesium-137 in the meat of cowsfrom the Kolkhoz in the Sarny region, where countermeasures couldbe implemented effectively, tended to be much lower than thatin the meat from private farms in the Dubritsva region (Pr95).The meat of wild animals which could not be subjected to the samecountermeasures had a generally high concentration of radiocaesium.Decontamination of animals by the use of Prussian Blue boli wasfound to be very effective where radiocaesium content of feedis high and where it may be difficult to introduce clean fodder(Al93). Depending on the local circumstances, many of theabove mentioned agricultural countermeasures were introduced toreduce human exposure. Since July 1986, the dose rate from external irradiation in someareas has decreased by a factor of forty, and in some places,it is less than 1 per cent of its original value. Nevertheless,soil contamination with caesium-137, strontium-90 and plutonium-239is still high and in Belarus, the most widely contaminated Republic,eight years after the accident 2,640 km2 of agricultural landhave been excluded from use (Be94). Within a 40-km radiusof the power plant, 2,100 km2 of land in the Poles'e state naturereserve have been excluded from use for an indefinite duration. The uptake of plutonium from soil to plant parts lying above groundgenerally constitutes a small health hazard to the populationfrom the ingestion of vegetables. It only becomes a problem inareas of high contamination where root vegetables are consumed,especially if they are not washed and peeled. The total contentof the major radioactive contaminants in the 30-km zone has beenestimated at 4.4 PBq for caesium-137, 4 PBq for strontium-90 and32 TBq for plutonium-239 and plutonium-240. However, it is not possible to predict the rate of reduction asthis is dependent on so many variable factors, so that restrictionson the use of land are still necessary in the more contaminatedregions in Belarus, Ukraine and Russia. In these areas, no liftingof restrictions is likely in the foreseeable future. It is notclear whether return to the 30 km exclusion zone will ever bepossible, nor whether it would be feasible to utilise this landin other ways such as grazing for stud animals or hydroponic farming(Al93). It is however, to be recognised that a small numberof generally elderly residents have returned to that area withthe unofficial tolerance of the authorities. Within Europe In Europe, a similar variation in the downward migration of caesium-137has been seen, from tightly bound for years in the near-surfacelayer in meadows (Bo93), to a relatively rapid downwardmigration in sandy or marshy areas (EC94). For example,Caslano (TI) experienced the greatest deposition in Switzerlandand the soil there has fallen to 42 per cent of the initial caesium-137content in the six years after the accident, demonstrating theslow downward movement of caesium in soil (OF93). There,the caesium-137 from the accident has not penetrated to a depthof more than 10 cm, whereas the contribution from atmosphericnuclear weapon tests has reached 30 cm of depth. In the United Kingdom, restrictions were placed on the movementand slaughter of 4.25 million sheep in areas in southwest Scotland,northeast England, north Wales and northern Ireland. This wasdue largely to root uptake of relatively mobile caesium from peatysoil, but the area affected and the number of sheep rejected arereducing, so that, by January 1994, some 438,000 sheep were stillrestricted. In northeast Scotland (Ma89), where lambs grazedon contaminated pasture, their activity decreased to about 13per cent of the initial values after 115 days; where animals consumeduncontaminated feed, it fell to about 3.5 per cent. Restrictionson slaughter and distribution of sheep and reindeer, also, arestill in force in some Nordic countries. The regional average levels of caesium-137 in the diet of EuropeanUnion citizens, which was the main source of exposure after theearly phase of the accident, have been falling so that, by theend of 1990, they were approaching pre-accident levels (EC94).In Belgium, the average body burden of caesium137 measured inadult males increased after May 1986 and reached a peak in late1987, more than a year after the accident. This reflected theingestion of contaminated food. The measured ecological half-lifewas about 13 months. A similar trend was reported in Austria (Ha91). In short, there is a continuous, if slow, reduction in the levelof mainly caesium-137 activity in agricultural soil. Environmental impact Forests Forests are highly diverse ecosystems whose flora and fauna dependon a complex relationship with each other as well as with climate,soil characteristics and topography. They may be not only a siteof recreational activity, but also a place of work and a sourceof food. Wild game, berries and mushrooms are a supplementarysource of food for many inhabitants of the contaminated regions.Timber and timber products are a viable economic resource. Because of the high filtering characteristics of trees, depositionwas often higher in forests than in agricultural areas. When contaminated,the specific ecological pathways in forests often result in enhancedretention of contaminating radionuclides. The high organic contentand stability of the forest floor soil increases the soil-to-planttransfer of radionuclides with the result that lichens, mossesand mushrooms often exhibit high concentrations of radionuclides.The transfer of radionuclides to wild game in this environmentcould pose an unacceptable exposure for some individuals heavilydependent on game as a food source. This became evident in Scandinaviawhere reindeer meat had to be controlled. In other areas, mushroomsbecame severely contaminated with radiocaesium. In 1990, forest workers in Russia were estimated to have receiveda dose up to three times higher than others living in the samearea (IA94). In addition, some forest-based industries,such as pulp production which often recycle chemicals, have beenshown to be a potential radiation protection problem due to enhancementof radionuclides in liquors, sludges and ashes. However, harvestingtrees for pulp production may be a viable strategy for decontaminatingforests (Ho95). Different strategies have been developed for combatting forestcontamination. Some of the more effective include restrictionof access and the prevention of forest fires. One particularly affected site, known as the "Red Forest"(Dz95), lies to the South and West close to the site. Thiswas a pine forest in which the trees received doses up to 100Gy, killing them all. An area of about 375 ha was severely contaminatedand in 1987 remedial measures were undertaken to reduce the landcontamination and prevent the dispersion of radionuclides throughforest fires. The top 10-15 cm of soil were removed and dead treeswere cut down. This waste was placed in trenches and covered witha layer of sand. A total volume of about 100,000 m3 was buried,reducing the soil contamination by at least a factor of ten. These measures, combined with other fire prevention strategies,have significantly reduced the probability of dispersion of radionuclidesby forest fires (Ko90). The chemical treatment of soilto minimise radionuclide uptake in plants may be a viable optionand, as has been seen, the processing of contaminated timber intoless contaminated products can be effective, provided that measuresare taken to monitor the by-products. Changes in forest management and use can also be effective inreducing dose. Prohibition or restriction of food collection andcontrol of hunting can protect those who habitually consume largequantities. Dust suppression measures, such as re-forestationand the sowing of grasses, have also been undertaken on a widescale to prevent the spread of existing soil contamination. Water bodies In an accident, radionuclides contaminate bodies of water notonly directly from deposition from the air and discharge as effluent,but also indirectly by washout from the catchment basin. Radionuclidescontaminating large bodies of water are quickly redistributedand tend to accumulate in bottom sediments, benthos, aquatic plantsand fish. The main pathways of potential human exposure may bedirectly through contamination of drinking-water, or indirectlyfrom the use of water for irrigation and the consumption of contaminatedfish. As contaminating radionuclides tend to disappear from waterquickly, it is only in the initial fallout phase and in the verylate phase, when the contamination washed out from the catchmentarea reaches drinking-water supplies, that human exposure is likely.In the early phase of the Chernobyl accident, the aquaeous componentof the individual and collective doses from water bodies was estimatednot to exceed 1-2 per cent of the total exposure (Li89).The Chernobyl Cooling Pond was the most heavily contaminated waterbody in the exclusion zone. Radioactive contamination of the river ecosystems (Figure 8) wasnoted soon after the accident when the total activity of waterduring April and early May 1986 was 10 kBq/L in the river Pripyat,5 kBq/L in the Uzh river and 4 kBq/L in the Dniepr. At this time,shortlived radionuclides such as iodine-131 were the main contributors.As the river ecosystem drained into the Kiev, then the Kanev andKremenchug reservoirs, the contamination of water,sediments, algae,molluscs and fish fell significantly. In 1989, the content of caesium-137 in the water of the Kiev reservoirwas estimated to be 0.4 Bq/L, in the Kanev reservoir 0.2 Bq/L,and in the Kremenchug reservoir 0.05 Bq/L. Similarly, the caesium-137content of Bream fish fell by a factor of 10 between the Kievand Kanev reservoirs, and by a factor of two between the Kanevand Kremenchug reservoirs to reach about 10 Bq/kg (Kr95).In the last decade, contamination of the water system has notposed a public health problem. However, monitoring will need tobe continued to ensure that washout from the catchment area whichcontains a large quantity of stored radioactive waste will notcontaminate drinking-water. A hydrogeological study of groundwater contamination in the 30-kmexclusion zone (Vo95) has estimated that strontium-90 isthe most critical radionuclide, which could contaminate drinking-waterabove acceptable limits in 10 to 100 years from now. Outside the former Soviet Union, direct and indirect contaminationof lakes has caused and is still causing many problems, becausethe fish in the lakes are contaminated above the levels acceptedfor sale in the open market. In Sweden, for instance, about 14,000lakes (i.e., about 15 per cent of the Swedish total) hadfish with radiocaesium concentrations above 1,500 Bq/kg (the Swedishguideline for selling lake fish) during 1987. The ecological half-life,which depends on the kind of fish and types of lakes, ranges froma few years up to some tens of years (Ha91). In the countries of the European Union, the content of caesium-137in drinking-water has been regularly sampled and reveals levelsat, or below, 0.1 Bq/L from 1987 to 1990 (EC94), whichare of no health concern. The activity concentration in the waterdecreased substantially in the years following the accident duelargely to the fixation of radiocaesium in the sediments. Figure 8. Water bodies possibly affected by the radioactive contamination from the Chernobyl accident In summary, Many countermeasures to control the contamination of agriculturalproducts were applied with varying levels of efficacy. Nevertheless,within the former Soviet Union large areas of agricultural landare still excluded from use and are expected to continue to beso for a long time. In a much larger area, although agriculturaland farm animal activities are carried out, the food producedis subject to strict controls and restrictions of distributionand use; Similar problems, although of a much lower severity, wereexperienced in some countries of Europe outside the former SovietUnion, where agricultural and farm animal production were subjectedto controls and limitations for variable durations after the accident.Most of these restrictions have been lifted several years ago.However, there are still some areas in Europe where restrictionson slaughter and distribution of animals are in force. This concerns,for example, several hundreds of thousands of sheep in the UnitedKingdom and large numbers of sheep and reindeer in some Nordiccountries. Produce from forests may continue to be a radiologicalprotection problem for a long time. At present drinking water is not a problem. Contaminationof groundwater, especially with strontium-90, could be a problemfor the future in the catchment basins downstream of the Chernobylarea. Contaminated fish from lakes may be a long-term problemin some countries. WT03-B20-81IA006-000055-B006-53http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl2/c09.html 138.80.61.12 19970221174935 text/html 34963HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:19:24 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 34791Last-modified: Tue, 09 Jul 1996 02:25:47 GMT EXPLANATION OF TERMS Activity Quantity of a radionuclide. It describes the rate at which spontaneousnuclear transformations (i.e., radioactive decay) occur in it.It is measured in becquerels (Bq), where 1 Bq equals one nucleartransformation per second. Several multiples of the becquerel (Bq) are used throughout thetext. They are the following: exabecquerel (EBq) = 1018 Bq petabecquerel (PBq) = 1015 Bq terabecquerel (TBq) = 1012 Bq gigabecquerel (GBq) = 109 Bq megabecquerel (MBq) = 106 Bq kilobecquerel (kBq) = 103 Bq Collective dose Total dose over a population group exposed to a given source.It is represented by the product of the average dose to the individualsin the group by the number of persons comprising the group. Itis measured in person-sieverts (person-Sv). Dose A general term denoting a quantity of radiation. Depending onits application it can be qualified as "absorbed dose","equivalent dose" and "effective dose". Absorbed dose Quantity of energy imparted by radiation to a unit mass of mattersuch as tissue. Absorbed dose is measured in grays (Gy), where1 Gy equals 1 joule of energy absorbed per kilogramme of matter.One gray produces a different intensity of biological effectson tissue depending on the type of radiation (alpha, beta, gamma,neutrons). One common submultiple of the gray, the milligray,is often used. One milligray (mGy) is equal to 10-3 Gy. Effective dose Weighted sum of the "equivalent doses" to the variousorgans and tissues multiplied by weighting factors reflectingthe differing sensitivities of organs and tissues to radiation.The weighting factor for each organ or tissue expresses the fractionalcontribution of the risk of death or serious genetic defect fromirradiation of that organ or tissue to the total risk from uniformirradiation of the whole body. Effective dose is measured in sieverts(Sv). Some submultiples of the sievert are used throughout thetext. They are the following: millisievert (mSv) = 10-3 Sv microsievert (Sv) = 10-6 Sv Equivalent dose Quantity obtained by multiplying the "absorbed dose"in an organ (e.g., thyroid) or tissue by a factor representingthe different effectiveness of the various types of radiationin causing harm to the organ or tissue. This factor, whose valuevaries between 1 and 20 depending on the type of radiation, hasbeen introduced in order to allow grouping or comparing biologicaleffects due to different radiations. Equivalent dose is measuredin sieverts (Sv). One sievert produces the same biological effect,irrespective of the type of radiation. Health effects Acute radiation syndrome A clinical scenario characterized by a complex of acute deterministiceffects affecting various organs and body functions in the irradiatedperson. Deterministic effects (also called acutehealth effects) Early deleterious radiation effects on living tissues (e.g., body,organ or tissue death, cataracts), which generally occur onlyabove a threshold of dose and whose severity depends on the levelof dose absorbed. They become generally evident within a shorttime from the irradiation (hours, days or weeks, depending onthe dose received). Throughout the text the doses producing Deterministiceffects are expressed in grays (Gy). Genetic effects (also called hereditary effects) Stochastic effect which occur in the progeny of the exposed person. Stochastic effects (also called late healtheffects) Late deleterious radiation effects (e.g., leukaemia, tumours)whose severity is independent of dose and whose probability ofoccuring is assumed to be proportional to the dose received. Itis also assumed that there is no threshold dose below which stochasticeffects will not occur. The stochastic effects occur, therefore,at doses lower than those producing deterministic effects andmay manifest themselves after a long time (years, decades) fromthe irradiation. Throughout the text the doses producing stochasticeffects are expressed in sieverts (Sv). Intervention level The value of a quantity (dose, activity concentration) which,if exceeded or predicted to be exceeded in case of an accident,may require the application of a given protective action. LIST OF ACRONYMS AUDR Soviet All-Union Dose Registry CRPPH NEA Committee on Radiation Protection and Public Health DNA Desoxyribonucleic acid EC European Commission ECCS Emergency Core Cooling System FAO Food and Agriculture Organisation GSF Forschungszentrum für Umwelt und Gesundheit IAEA International Atomic Energy Agency IARC International Agency for Research on Cancer ICRP International Commission on Radiological Protection INES International Nuclear Event Scale INEX NEA Nuclear Emergency Exercises Programme INSAG International Nuclear Safety Advisory Group IPHECA International Programme on the Health Effects of the Chernobyl Accident IPSN Institut de Protection et de Sécurité Nucléaire JAERI Japan Atomic Energy Research Institute NAZ Nationale Alarmzentrale NCRP Soviet National Committee on Radiation Protection NEA OECD Nuclear Energy Agency OECD Organisation for Economic Cooperation and Development RNMDR Russian National Medical Dosimetry Registry SKI Swedish Nuclear Power Inspectorate UNSCEAR United Nations Scientific Committee on the Effects of Atomic Radiation WHO World Health Organisation REFERENCES Al89 Yu.A. Aleksandrovskij, "Psychoneurotic Disorders Associatedwith the Chernobyl Accident", Medical Aspects of the ChernobylAccident, TECDOC 516, IAEA, Vienna, 1989. Al93 R.M. Aleksakhin, "Radioecological Lessons of Chernobyl",Radiat. Biol. Ecol., 33: 3-80, 1993. An88 R. Lynn et al., "Global Impact of the Chernobyl ReactorAccident", Science, 242:1513-1519, 1988. Ba93 M.I. Balonov, "Overview of Doses to the Soviet Populationfrom the Chernobyl Accident and the Protective Actions Applied",The Chernobyl Papers, 1:23-45, Ed. S.E. Merwinand M.I. Balonov, Research Enterprises, Richland, WA, 1993. Ba94 R.M. Barkhudarov et al., "Characterization of IrradiationLevels of the Population in the Controlled Areas within the FirstFour Years after the Chernobyl NPP Accident", Institute ofBiophysis, Moscow, 1994. Be87 G. Bengsston, "Radiation Doses in Europe after theChernobyl Accident", Med. Oncol. Tumor. Pharmacother,4(3-4): 33-137, 1987. Be90 S.N. Bergichev et al., "Radioactive Releases Due tothe Chernobyl Accident", Fission Product Transport Processes,Ed. J.T. Rogers, Hemisphere, 1990. Be91 S.T. Bedyaev et al., "The Chernobyl Source Term",Proc. Seminar on Comparative Assessment of the EnviromentalImpact of Radionuclides Released during Three Major Nuclear Accidents:Kyshtym, Windscale, Chernobyl, EVR-13574, CEC, pp. 71-91,1991. Be92 V. Beral, G. Reeves et al., "Childhood Thyroid Cancerin Belarus", Nature, 359:680-681, 1992. Be94 "Eight Years after Chernobyl", The Republicof Belarus Information Bulletin [published by TRIC, 22001,17 Moscowskaya, Minsk, Belarus], 1994. Be95 V.P. Beskorovajnyj et al., "Radiation Effects of Collapseof Structural Elements of the Sarcophagus", SarcophagusSafety '94: Proceedings of an International Conference, ZelenyMys, Chernobyl, Ukraine, (14-18 March 1994), pp. 196-202,OECD/NEA, Paris, 1995. Bo93 G.C. Bonnazzola et al., "Profiles and Downward Migrationof 137Cs and 106Ru Deposited on Italian Soils After the ChernobylAccident", Health Physics, 64(5):479-484,1993. Bo94 J. Boice and M. Linet, "Chernobyl, Childhood Cancer,and Chromosome 21", BMJ, Editorial, 309:139-140,1994. Br88 J.A. Broadway et al., "Estimates of the Radiation Doseand Health Risks to the United States Population Following theChernobyl Nuclear Plant Accident", Health Physics,55(3):533-539, 1988. Br92 H. Braselmann et al., "Chromosome Analysis in a PopulationLiving in an Area of Germany With the Highest Fallout from theChernobyl Accident", Mutation Research, 283:221-225,1992. Bu91 W. Burkart et al., "Assessing Chernobyl's RadiologicalConsequences", Nuclear Europe Worldscan, 1(3-4):27-30, 1991. Bu93 Y.P. Buzulukov and Y.L. Dobrynin, "Release of RadionuclidesDuring the Chernobyl Accident", The Chernobyl Papers,1:321, Eds. S.E. Merwin and M.I. Balonov, Research Enterprises,Richland, WA,1993. Bv95 A.A. Borovoy, "On the Nuclear Safety of Ukritiye",Personal communication, 1995. Ca87 R.S. Cambrai et al., "Observations on Radioactivityfrom the Chernobyl Accident", Nuclear Energy, 26:77,1987. Ch90 J.-P. Chaussade, "Public Confidence and Nuclear Energy",IAEA Bull., 2:7- 10, 1990. Co92 D.L. Collins, "Behavioural Differences of IrradiatedPersons Associated with the Kyshtym, Cheliabinsk and ChernobylNuclear Accidents", Military Medicine, 157(10):548-552,1992. De87 "Health and Environmental Consequences of the ChernobylNuclear Power Plant Accident", A Report to the US Departmentof Energy, DOE/ER-0332, 1987. De90 A. Dehos, "Haematological and Immunological Indicatorsfor Radiation Exposure", Kerntechnik, 55(4):211-218, 1990. De94 E.P. Demidchik et al., "Thyroid Cancer in Children Afterthe Chernobyl Accident: Clinical and Epidemiological Evaluationof 251 cases in the Republic of Belarus", Proc. of NagasakiSymposium on Chernobyl: Update and Future, pp. 21-30, Ed.Shigenobu Nagataki, Elsevier, 1994. De95 L. Devell et al., "The Chernobyl Reactor Accident SourceTerm: Development of a Consensus View", CSNI Report in preparation,OECD/NEA, Paris. Dr93 B.M. Drottz-Sjoberg and L. Persson, "Public Reactionto Radiation: Fear, Anxiety or Phobia?", Health Physics,64 (3):223-231, 1993. Du94 G.H. Dubreuil, « Un premier bilan des effets psychiqueset sociaux de l'accident de Tchernobyl», Radioprotection,29(3):363 - 376, 1994. Dz95 S.P. Dzepo et al., "Hydrogeological Effects of the PrincipalRadioactive Waste Burial Sites Adjacent to the Chernobyl NPP",(Review) Sarcophagus Safety 94, Proceedings of an InternationalConference, Zeleny Mys, Chernobyl, Ukraine, 14-18 March 1994,pp. 370-382, OECD/NEA, Paris, 1995. EC87 "Council Decision of 14 December 1987 on Community arrangementsfor the early exchange of information in the event of a radiologicalemergency", European Commission, 1987. EC89 "Council Directive of 27 November 1989 on informingthe general public about health protection measures to be appliedand steps to be taken in the event of a radiological emergency",European Commission, 1989. EC89a "Council Regulation (Euratom) No. 3954/87 of 22 December1987 laying down maximum permitted levels of radioactive contaminationof foodstuffs and of feedingstuffs following a nuclear accidentor any other case of radiological emergency", European Commission,1989. EC89b "Commission Regulation (Euratom) No. 944/89 of 12 April1989 laying down maximum permitted levels of radioactive contaminationin minor foodstuffs following a nuclear accident or any othercase of radiological emergency", European Commission, 1989. EC89c "Council Regulation (EEC) No. 2219/89 of 18 July 1989on the special conditions for exporting foodstuffs and feedingstuffsfollowing a nuclear accident or any other case of radiologicalemergency", European Commission, 1989. EC92 "Report on the Meeting to Discuss Reports of an Excessof Thyroid Cancer in the Minsk Region", Organised by theEC, Neuherberg, 1992. EC93 "Thyroid Cancer in Children Living Near Chernobyl",EC Expert Panel Report (EUR 15248), EC, 1993. EC93a "Council Regulation (EEC) No. 737/90 of 22 March 1990on the conditions governing imports of agricultural products originatingin third countries following the accident at the Chernobyl nuclearpower station", European Commission, 1993. EC94 "Environmental Radioactivity in the European Community1987-1988-1989-1990", DGXI. EUR 15699, EC, 1994. EG88 "Preliminary Evaluation of the Impact of the ChernobylRadiological Contamination on the Frequency of Central NervousSystem Malformations in 18 Regions of Europe", The EurocatWorking Group, Paediatr. Perinat. Epidemiol, 2(3):253-264,1988. Er94 V. Erkin et al., "External Doses from Chernobyl Fall-out:Individual Dose Measurements in the Brjansk Region of Russia",Rad. Prot. Dosim., 51(4):265-273, 1994. FA91 Joint FAO/WHO Food Standards Programme, Codex Alimentarius,Vol. 1 (1991), section 6.1, "Levels for Radionuclides",1991. Fu92 A. Furmanchuk et al., "Pathomorphological Findings inThyroid Cancers of Children from the Republic of Belarus: A Studyof 86 Cases Occurring Between 1986 (post-Chernobyl) and 1991",Histopathology, 21:401-408, 1992. Fu93 A.W. Furmanchuk et al., "Occult Thyroid Carcinomas inthe Region of Minsk, Belarus. An Autopsy Study of 215 Patients",Histopathology, 23:319-325, 1993. Gl95 V.N. Glygalo et al., "The Current State of the Regulationson the Safety of Unit 4 at the Chernobyl NPP", SarcophagusSafety '94: Proceedings of an International Conference, ZelenyMys, Chernobyl, Ukraine, (14-18 March 1994), pp. 37-42, OECD/NEA,Paris, 1995. Go93 V.Yu. Golikov et al., "Estimation of External GammaRadiation Doses to the Population After the Chernobyl Accident",The Chernobyl Papers, 1:247-288, Ed. S.E. Merwinand M.I. Balonov, Research Enterprises, Richland, WA, 1993. Go95 N.V. Gorbacheva et al., "Hypothetical Accidents in theSarcophagus", Sarcophagus Safety '94: Proceedings of anInternational Conference, Zeleny Mys, Chernobyl, Ukraine, (14-18March 1994), pp. 203-209, OECD/NEA, Paris, 1995. Go95a G.M. Goulko et al., "Estimation of Thyroid Doses forthe Evacuees from Pripyat", Submitted to Radiat. Environ.Biophys., 1995. Gu89 P.H. Gudiksen et al., "Chernobyl Source Term, AtmosphericDispersion, and Dose Estimation", Health Physics,57(5):697 - 706, 1989. Ha91 L. Håkansson, "Radioactive Caesium in Fish inSwedish Lakes After Chernobyl - Geographical Distributions, Trends,Models and Remedial Measures", The Chernobyl Fallout inSweden, pp. 239-281, Ed. L. Moberg, Stockholm, 1991. Ha92 T. Harjulehto-Mervalaa et al., "The Accident at Chernobyland Trisomy 21 in Finland.", Mutation Research, 275:81-86,1992. Ha92a M.C.H.Haeusler et al., "The influence of the Post-ChernobylFallout on Birth Defects and Abortion Rates in Austria",Amer. J Obstet. Gynecol, 167:1025-1031, 1992. Hj94 U. Hjalmars et al., "Risk of Acute Childhood Leukaemiain Sweden After the Chernobyl Reactor Accident", BMJ,309(16):154, 1994. Ho91 "The use of Chernobyl Fallout Data to Test Model Predictionsof the Transfer of 131I and 137Cs from the Atmosphere throughAgricultural Food Chains", Report CONF-910434-7, F.O.Hoffman Oak Ridge National Lab., TN (USA), 1991. Ho94 M. Hoshi et al., "137Cs Concentration Among Childrenin Areas Contaminated with Radioactive Fallout from the ChernobylAccident: Mogilev and Gomel Oblasts, Belarus", HealthPhysics, 67(3):272-275, 1994. Ho95 E. Holm, "Fluxes and Technological Enhancement of Radionuclidesin the Forest Industry", Proc. of NEA Workshop on theAgricultural Issues Associated with Nuclear Emergencies, (June1995), OECD/NEA, Paris, 1996. Hu87 P. Hull, "Preliminary Dose Assessment of the ChernobylAccident", BNL Report 38550, Brookhaven National Laboratory, 1987. Hu88 P. Hull, "Update and Comparison of Dose Estimates followingthe Chernobyl Accident", Transactions of the AmericanNuclear Society; Off-site Consequences and Related Insightsfrom Chernobyl, 15:163-169, 1988. IA86 Post-Accident Review Meeting on the Chernobyl Accident,IAEA, Vienna, 1986. IA86a "Summary Report on the Post-Accident Review Meetingon the Chernobyl Accident", Safety Series No. 75 INSAG-1,IAEA, Vienna, 1986. IA86b IAEA Convention on Early Notification of a Nuclear Accident,IAEA, Vienna, 1986. IA86c IAEA Convention on Mutual Assistance in the Event ofa Nuclear Accident or Radiological Emergency, IAEA, Vienna,1986. IA87a "Techniques and Decision-Making in the Assessment ofOff-Site Consequences of an Accident in a Nuclear Facility",Safety Series No. 86, IAEA, Vienna, 1987. IA89 "Medical Aspects of the Chernobyl Accident", TECDOC-516,IAEA, Vienna, 1989. IA89a "Radioactive Fallout in Food and Agriculture",TECDOC-494, IAEA, Vienna, 1989. IA89b "Cleanup of Large Areas Contaminated as a Result ofa Nuclear Accident", Technical Report Series No. 300,IAEA, Vienna, 1989. IA89c "Principles and Techniques for Post-Accident Assessmentand Recovery in a Contaminated Environment of a Nuclear Facility,Safety Series No. 97, IAEA, Vienna, 1989. IA91 IAEA, "The International Chernobyl Project-Assessmentof Radiological Consequences and Evaluation of Protective Measures",Report by an International Advisory Committee, IAEA, Vienna, 1991. IA91a "Planning for Cleanup of Large Areas Contaminated asa Result of a Nuclear Accident, Technical Report Series No.327, IAEA, Vienna, 1991. IA92 "Disposal of Waste from the Cleanup of Large Areas Contaminatedas a Result of a Nuclear Accident, Technical Report SeriesNo. 330, IAEA, Vienna, 1992. IA94 "Guidelines for Agricultural Countermeasures Followingan Accidental Release of Radionuclides", IAEA/FAO TechnicalReport Series No. 363, IAEA, Vienna, 1994. IA94a "Intervention Criteria in a Nuclear or Radiation Emergency",Safety Series No. 109, IAEA, Vienna, 1994. IC90 ICRP, "1990 Recommendations of the International Commissionon Radiological Protection", ICRP Publication 60,Annals of the ICRP, 21(1-3), 1991. IC92 ICRP, "Principles for Intervention for Protection ofthe Public in a Radiological Emergency", ICRP Publication63, Annals of the ICRP, 22(4), 1992. Il87 L.A. Ilyin and A.O. Pavlovskij, "Radiological Consequencesof the Chernobyl Accident in the Soviet Union and Measures Takento Mitigate Their Impact", IAEA Bulletin 4, 1987. Il90 L.A.Ilyin et al., "Radiocontamination Patterns and PossibleHealth Consequences of the Accident at the Chernobyl Nuclear PowerStation, J. Radiol. Prot., 10(1):3-29, 1990. IP95 IPSN, « Tchernobyl 9 ans après », Dossierde presse, Institut de protection et de sûreté nucléaire(IPSN), Mission Communication, 1995. Ir91 L.M. Irgens et al., "Pregnancy Outcome in Norway AfterChernobyl", Biomed. & Pharmacother, 45:33-241,1991. Iv93 E.P. Ivanov et al., "Child Leukaemia after Chernobyl",Letter to Nature 365:702, 1993. Iv94 V.K. Ivanov et al., "Planning of Long-term Radiationand Epidemiological Research on the Basis of the Russian NationalMedical and Dosimetric Registry", Proceedings of NagasakiSymposium on Chernobyl: Update and Future, pp. 203-216, Ed.Shigenobu Nagataki, Elsevier, 1994. Iv95 N.P. Ivanova et al., "Population Doses in Russia fromPlutonium Fallout Following the Chernobyl Accident", Rad.Prot. Dosim, 58(4):255-260, 1995. Ka92 V.S.Kasakov et al. and Baverstock et al., "Thyroid CancerAfter Chernobyl", Nature, 359:21-22, 1992. Ko90 V.I. Komarov, "Radioactive Contamination and Decontaminationin the 30 km zone surrounding the Chernobyl Nuclear Power Plant",IAEA-SM-306/124, 2:3-16, 1990 Kr95 I.I. Kryshev, "Radioactive Contamination of AquaticEcosystems Following the Chernobyl Accident", J. Environ.Radioactivity, 27(3):207-219, 1995. Ku95 V.A. Kurnosov et al., "Design of Shelter - Experienceof Planning and Construction in 1986", Sarcophagus Safety'94: Proceedings of an International Conference, Zeleny Mys, Ukraine(14-18 March 1994), pp. 243-250, OECD/NEA, Paris, 1995. La95 J.M. Lavie, Personal communication, 1995. Li89 L.A. Likhtarev et al., "Radioactive Contamination ofWater Ecosystems and Sources of Drinking Water", MedicalAspects of the Chernobyl Accident, TECDOC 516, IAEA, Vienna,1989. Li92 R.T. Lie et al., "Birth Defects in Norway by Levelsof External and Food-based Exposure to Radiation from Chernobyl",Am. J. Epidemiol., 136:377-388, 1992. Li93 J. Little, "The Chernobyl Accident, Congenital Anomaliesand Other Reproductive Outcomes", Paediatr. Perinat. Epidemiolog.,7(2):121-151, 1993 Li93a I.A. Likhtarev et al., "Exposure Doses to Thyroid ofthe Ukrainian Population After the Chernobyl Accident", HealthPhysics, 64:594-599, 1993. Li94 I.A. Likhtarev et al., "Retrospective Reconstructionof Individual and Collective External Gamma Doses of PopulationEvacuated after the Chernobyl Accident", Health Physics,66(6):43-652, 1994. Ma89 C.J. Martin et al., "Caesium-137, Cs-134 and Ag-110min Lambs Grazing Pasture in NE Scotland Contaminated by ChernobylFallout", Health Physics, 56:(4):459-464, 1989. Ma91 J. Malone et al., "Thyroid Consequences of ChernobylAccident in the Countries of the European Community", J.Endocrinol..Invest., 14:701-717, 1991. Me92 F.A. Mettler et al., "Administration of Stable Iodineto the Population Around the Chernobyl Nuclear Power Plant",J. Radiol. Prot., 12(3):159-165, 1992 Mo87 M.Morrey et al., "Preliminary Assessment of the RadiologicalImpact of the Chernobyl Reactor Accident on the Population ofthe European Community", A Report to the Commission of theEuropean Community, 1987. NE87 "The Radiological Impact of the Chernobyl Accident inOECD Countries", OECD/NEA, Paris, 1987. NE88 G. Boeri and C. Viktorsson, "Emergency Planning Practicesand Criteria in the OECD Countries After the Chernobyl Accident:A Critical Review", OECD/NEA, Paris, 1988. NE89 "Nuclear Accidents: Intervention Levels for Protectionof the Public", OECD/NEA, Paris, 1989. NE89a "The Influence of Seasonal Conditions on the RadiologicalConsequences of a Nuclear Accident", Proceedings of anNEA Workshop, Paris September 1988, OECD/NEA, Paris, 1989. NE89b "Emergency Planning in Case of Nuclear Accident: TechnicalAspects", Proceedings of a Joint NEA/CEC Workshop, Brussels,June 1989, OECD/NEA, Paris, 1989. NE90 "Protection of the Population in the Event of a NuclearAccident", OECD/NEA, Paris, 1990. NE93 "Off-site Nuclear Emergency Exercises", Proceedingsof an NEA Workshop, The Hague, November 1991, OECD/NEA, Paris,1993. NE94 "Radiation Protection Today and Tomorrow: A CollectiveOpinion of the Committee on Radiation Protection and Public Health",OECD/NEA, Paris, 1994. NE95 "INEX1: An International Nuclear Emergency Exercise",OECD/NEA, Paris, 1995. NE95a "Short-Term Countermeasures After a Nuclear Emergency",Proceedings of an NEA Workshop, June 1994, Stockholm, OECD/NEA,Paris, 1995. NE96 "Agricultural Issues Associated with Nuclear Emergencies",Proceedings of an NEA Workshop, June 1995, OECD/NEA, Paris,1996. NE96a "Emergency Data Management", Proceedings ofan NEA Workshop, Zurich, September 1995, OECD/NEA, Paris,1996. Ni94 Y. Nikiforuk and D.R. Gnepp, "Paediatric Thyroid CancerAfter the Chernobyl Disaster", Cancer, 74(2):748-766,1994. OF93 "Environmental Radioactivity and Radiation Exposurein Switzerland- 1993", Federal Office of Public Health, Bern,1993. Pa88 H.G. Paretzke, "The Impact of the Chernobyl Accidenton Radiation Protection", Health Physics, 55(2):139-143,1988. Pa89 N. Parmentier and J-C. Nénot, "Radiation DamageAspects of the Chernobyl Accident", Atmospheric Environment,23:771-775, 1989. Pa92 D.M. Parkin et al., "Childhood Leukaemia Following theChernobyl Accident", The European Childhood Leukaemia-LymphomaIncidence Study (ECLIS), DM Parkin/ob/mg-1089, 1992. Pa93 L. Padovani et al., "Cytogenetic Study in Lymphocytesfrom Children Exposed to Ionizing Radiation after the ChernobylAccident", Mutation Research, 319:55-60,1993. Pa94 W. Paile and S. Salomaa, "Radiogenic thyroid cancerin Belarus: Fact or fiction?", Note: J. Radiolog. Prot.,14(3):265-269, 1994. Pe88 G. Pershagen, "Health Effects of Chernobyl", BMJ,297:1488-1489, 1988. Pr91 A. Prisyazhiuk et al., "Cancer in the Ukraine, PostChernobyl", Letter to the Lancet, 338:1334-1335,1991. Pr95 B.S. Prister et al., "Agricultural Aspects of Consequencesof the Accident on the Chernobyl NPP in the Ukraine",Proc. of NEA workshop on The Agricultural Issues Associated withNuclear Emergencies, (June 1995), OECD/NEA, Paris, 1996. Pu92 E.E. Purvis, III and M. Goldman, "Statement to the U.S.Senate, Committee on Environment and Public Works", Los AlamosTechnical Associates Inc., July 1992. Re87 P. Reizenstein, "Carcinogenicity of Radiation DosesCaused by the Chernobyl Fall-out in Sweden, and Prevention ofPossible Tumors", Med. Oncol. Tumor. Pharmacother,4(1):1-5, 1987. Ri94 "Estimation of the Individual and Collective Doses ReceivedDuring Post-Accident Period by the Inhabitants of Belarus",Research Institute of Radiation Medicine, Minsk, 1994. Ri95 J.I. Richards et al., "Standards and Criteria Establishedby International Organisations for Agricultural Aspects of RadiologicalEmergency Situations", Proc. of NEA workshop on The AgriculturalIssues Associated with Nuclear Emergencies, (June 1995), OECD/NEA,Paris, 1996. Ro92 E. Ron et al., Letter to the Editor, "Thyroid CancerIncidence", Nature, 360:113, 1992. Sa94 B. Salbu et al., "The Mobility of 137Cs and 90Sr inAgricultural Soils in the Ukraine, Belarus and Russia 1991",Health Physics, 67(5):518-528, 1994. Sc93 W. Scheid et al., "Chromosome Aberrations in Human LymphocytesApparently Induced by Chernobyl Fallout", Note to HealthPhysics, 64(5):531-534, 1993. Sc94 K. M. Schlumberger, "Thyroid Cases after Chernobyl:Importance of Prophylaxis", Radioprotection, 29(3):397-404,1994. Se95 A.V. Sevan'kaev et al., "Chromosomal Aberrations inLymphocytes of Residents of Areas Contaminated by RadioactiveDischarges from the Chernobyl Accident", Rad. Prot. Dosim.,58(4):247-254, 1995. Se95a A.V. Sevan'kaev et al., "High Exposures to RadiationReceived by Workers Inside the Chernobyl Sarcophagus", Rad.Prot. Dosim., 59(.2):85-91, 1995. Si94 A.R. Sich, "Chernobyl Accident Management Actions",Nuclear Safety, 35(1), 1994. Si94a A.R. Sich, "Chernobyl Thesis", Letter to the Editor,Science, 265:859-861, 1994. Sj87 L. Sjoberg and B.M. Drottz, "Psychological Reactionsto Cancer Risks After the Chernobyl Accident", Med. Oncol.Tumor. Pharmacother, 4(3-4):259-271, 1987. So95 G. Souchkevitch, "Participants in the Clean-up OperationsFollowing the Chernobyl Disaster: State of Health and MedicalMonitoring in Belarus, Russia and Ukraine", Personal communication,1995. Sp91 K. Sperling et al., "Frequency of Trisomy 21 in GermanyBefore and After the Chernobyl Accident", Biomed. &Pharmacother. 45:255-262,1991. St92 E.R. Stiehm, "The Psychologic Fallout From Chernobyl",AJDC, 146:761-762, 1992. St93 G. Stephan and U. Oestreicher, "Chromosome Investigationof Individuals Living in Areas of Southern Germany Contaminatedby Fallout from the Chernobyl Reactor Accident", MutationResearch, 319:189-196, 1993. St95 Stsjazhko V.A. et al., "Childhood Thyroid Cancer SinceAccident at Chernobyl", Letter to the Editor, BMJ, 310(6982):801,1995. Ta94 T. Ito et al., "Activated RET Oncogene in Thyroid Cancersof Children from Areas Contaminated by Chernobyl Accident",Lancet, 344:259, 1994. To95 V.K. Tolstonogov, "Current State of the Sarcophagusand Safety Problems", "Sarcophagus Safety '94",Proceedings of an International Conference, Zeleny Mys, Chernobyl,Ukraine, (14-18 March 1994), pp. 13-36, OECD/NEA, Paris, 1995. Ts94 A.F. Tsyb et al., "Disease Incidences of the Thyroidand Their Dose Dependence in Children Affected as a Result ofthe Chernobyl Accident", Proc. of Nagasaki Symposium onChernobyl: Update and Future, pp. 9-19, Ed. Shigenobu NagatakiElsevier, 1994. UN88 United Nations Scientific Committee on the Effects of AtomicRadiation (UNSCEAR), Report to the United Nations, 1988. US91 "Radiation Maps in the Territory of the European Partof the USSR as of December 1990. Densities of Area Contaminationby Caesium-137, Strontium-90, and Plutonium-239, 240", USSRState Committee on Hydrometeorology, Minsk, SCH, 1991 Ve93 L Verschaeve et al., "Chromosome Aberrations of Inhabitantsof Byelorussia: Consequence of the Chernobyl accident", MutationResearch, 287:253-259, 1993. Vo95 I. F. Vovk et al., "Geological and Hydrogeological Featuresof the ChAES 30-kilometer Zone and Possibilities for the Deepor Shallow Burial of Radioactive Wastes", "SarcophagusSafety '94", Proceedings of an International Conference,Zeleny Mys, Chernobyl, Ukraine, (14-18 March 1994), pp. 341-357,OECD/NEA, Paris, 1995. Wa87 E.A. Warman, "Soviet and Far-Field Radiation Measurementsand an Inferred Source Term from Chernobyl.", TP87-13,Stone and Webster Engineering Corp, Boston, MA. 1987. WH87 "Nuclear Accidents: Harmonisation of the Public HealthResponse", WHO Regional Office for Europe, Copenhagen, 1989. WH88 "Derived Intervention Levels for Radionuclides in Food",WHO, Geneva, 1988. WH90 WHO, "The Effects on the Thyroid of Exposed PopulationsFollowing the Chernobyl Accident", A Report on a Symposium:Chernikov, 3-6 December 1990, EUR/ICP/CEH/101, WHO Regional Officefor Europe, Copenhagen, 1990. WH90a Report of a WHO Working Group on Psychological Effects ofNuclear Accidents held in Kiev, USSR, 28 May-1 June 1990, EUR/ICP/CEH093(S) 7236r, WHO Regional Office for Europe, Copenhagen, 1990. WH94 WHO, "Report of the Expert Meeting on Thyroid CancerAfter the Chernobyl Accident", Kiev, Ukraine, 18-21 October,1993, WHO, Geneva, 1994. WH95 WHO, "Report on the Pilot Phase of the InternationalProgramme on the Health Effects of the Chernobyl Accident",(IPHECA), WHO, Geneva, 1995. Wi94 D. Williams, "Chernobyl, eight years on", Nature,371:556, 1994. Wi94a D. Williams, "Thyroid Cancer in United Kingdom Childrenand in Children Exposed to Fall-out from Chernobyl", Proc.of Nagasaki Symposium on Chernobyl: Update and Future, pp.89-94, Ed. Shigenobu Nagataki Elsevier, 1994. Ze86 J. Bischoffet al., "It Can't Go On Like That. From Brokdorfto Wackersdorf: Will the Revived Resistance Against Atomic EnergyEnd in a Dead Circle of Violence", Zeit, 41(25):25-29,1986. Zv93 I.A. Zvonova and M.I. Balonov, "Radioiodine Dosimetryand Prediction of Consequences of Thyroid Exposure of the RussianPopulation following the Chernobyl Accident", The ChernobylPapers, 1:71-125, Eds. S.E. Merwin & M.I. Balonov,Research Enterprises, Richland, WA, 1993.WT03-B20-82IA005-000051-B016-366http://lacebark.ntu.edu.au:80/j_mitroy/sid101/conserve/npfdsm.html 138.80.61.12 19970221151229 text/html 121903HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:41:53 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 121730Last-modified: Tue, 09 Jul 1996 02:26:11 GMT Societal Benefits of Energy Efficiency in New England Societal Benefits of Energy Efficiency in New England November 8, 1995 Authors: Bruce Biewald (Project Manager), Stephen Bernow, William Dougherty, Max Duckworth,Irene Peters, Alexander Rudkevich, Karen Shapiro, Timothy Woolf Contents.. 1. INTRODUCTION 1.1 Competitive Forces in the Electric Industry 1.2 Trends in Environmental Regulation 2. BENEFITS OF NEW ENGLAND'S DSM PROGRAMS 2.1 Avoided Emissions of Criteria Air Pollutants and Greenhouse Gases 2.2 Heavy Metals 2.3 Water Use Impacts 2.4 Land Use Impacts 2.5 Radiological Impacts of Nuclear Power 2.6 Electricity Transmission and Distribution Impacts 2.7 Job Creation and Economic Development 2.8 Government Subsidies for Energy Resources 2.9 The Costs of Relying Upon Imported Oil 2.10 Resource Depletion 2.11 Benefits to Low-Income Customers 2.12 The Costs Associated with Outstanding Bills 2.13 DSM Spillover and Market Transformation 2.14 Additional Benefits of DSM 3. SUMMARY, CONCLUSIONS AND RECOMMENDATIONS 3.1 Valuation Methodology for Monetizing Non-Price Factors 3.2 Illustration of Potential Impacts of DSM Savings 3.3 Recommended Research Priorities 4. REFERENCES 5. FOOT NOTES 1. INTRODUCTION Demand Side Management (DSM) is one of the most cost-effectiveresources available to utilities today. DSM represents an importantpolicy choice for avoiding adverse impacts on health, environment,and public amenity that result from the production of electricalpower -- impacts that include the societal costs of energy useassociated with the entire fuel cycle, not simply the impactsat the point of generation. Furthermore, energy efficiency programstargeted to low-income customers can provide important societalbenefits beyond those related to the customer's energy consumption.DSM can also have a beneficial impact upon direct utility coststhrough decreasing arrears and by decreasing exposure to risksof future environmental regulation. Other social benefits associatedwith job creation, government subsidies, oil import costs, anddepletion of natural resources are also important. This report aims to comprehensively identify, and quantify whereappropriate, the societal benefits associated with DSM programsin New England. 1 Among the issues that are exploredin this report are societal benefits of avoided generation, effectson local economies, hidden costs of fossil fuel dependence, effectson low-income customers, and additional benefits of DSM programs.The range of sources examined includes published documents fromacademic institutions, state and federal government, utility planningstudies, a variety of unpublished information (e.g., utility RFPprotocols, testimony, commission orders) and personal communications.Wherever possible, New England-specific data is used. 1.1 Competitive Forces in the Electric Industry The Energy Policy Act of 1992 brought about increased wholesalecompetition where electric utilities have been able to choosebetween purchases from Qualifying Facilities (QFs), IndependentPower Producers (IPPs), cogenerators, other utilities sellingbulk power, and from Exempt Wholesale Generators (EWGs). Utilitiescould face increased retail competition as well,whereby consumers (e.g., an industrial facility) or groups ofconsumers could contract with whatever suppliers they wish, includingutilities outside the conventional service territory and/or IPPs. Increasing competition in the electric industry will require arethinking of the means to achieve energy and environmental objectives.Some of the results presented in this report are directly affectedby the profound changes affecting the electric industry. Initiativesby state legislatures in New England, notably Rhode Island andNew Hampshire, have succeeded in expanding the agenda from wholesalecompetition, which currently exists in limited form, to the possibilityof retail competition where all classes of customers could negotiategeneration contracts. For example, in Massachusetts, the Departmentof Public Utilities has opened its own investigation into electricindustry restructuring, stating its intention to "... takesteps to allow a greater reliance on competition and customerchoice for retail customers in order to increase efficiency andreduce costs, while still providing safe, reliable and least-costservice."2 These changes emphasize the need for New England utilities toplace an even greater emphasis on obtaining the right the mixof resources to meet customer demand. Examining the societal benefitsassociated with DSM programs is a means of addressing, not onlythe risk of future regulations, but the uncertainty associatedwith restructuring the electric industry. The results of our analysissuggest that the non-price benefits of DSM currently unaccountedfor in the planning process can be quite large. While many of the restructuring proposal substitute reliance onmarkets for regulation claim they will secure significant economicbenefits for customers, fundamental concerns have emerged thatcertain elements of these reform proposals could discourage continuedinvestments in DSM programs and detract from the environmentalprotection gains achieved in recent years. If retail competition were implemented, it could significantlyaffect the level of New England DSM programs and their impacts.Retail competition in New England could fundamentally alter planningprocesses, by (a) introducing significant uncertainty into theshort- and long-term planning, (b) altering utilities' prioritiesand objectives, (c) possibly undermining opportunities for cost-effectiveDSM as a result of uncertainty and competitive pressures, and(d) limiting consideration of environmental and social externalities.It will be important to find ways to ensure that the economicand environmental goals of IRP, and the progress that has alreadybeen made thus far, are preserved in new protocols and conditionsaffecting planning. A range of mechanisms are available that could be used to encouragecontinued investments in DSM in a restructured industry, and therebytake into account DSM's non-price factors to some degree. Theserange from market-based approaches such as emission trading schemesand Performance Based Ratemaking (PBR), to command and controloptions such as additional environmental standards and regulations,emission caps, pollution taxes, efficiency credits, and DSM resourceportfolio requirement. There may also be various combinationsof mechanisms. At a minimum, a systems benefits charge shouldbe established to allow utilities assurance for recovering thecosts of their DSM programs, and minimize the risks associatedwith competition. 1.2 Trends in Environmental Regulation Over the years, an extensive array of state and federal regulationhas been enacted that deal with air and water pollution, solidwaste disposal, and nuclear power operation. These range fromspecific control technologies to the enforcement of emission standardsand requirements for accident prevention. Generally, federal environmentallegislation does not preempt states from enforcing their own standardsfor advancing environmental quality. No matter how the electric industry is restructured, additionalenvironmental standards and regulations are expected to keep pacewith developments in toxicological and environmental research.While regulations aim to proscribe action deemed to have unacceptablyhigh damages to the environmental or human health, they frequentlylag behind either confirmation of the particular adverse effectsor significant exposure to the pollutant. This raises the distinctpossibility that today's allowable residual emissions from powergeneration may be challenged in the future as advances in medicalknowledge about dose-response functions and sensitive populationsdevelops. Utility planning must find a way to take account ofthe risks of future environmental regulations while awaiting theiroutcome. The impact of future environmental regulation may take a varietyof forms. Additional restrictions of a given power plant may reduceits level of operation or require investment in additional controlequipment. A tax on carbon dioxide emissions would affect thecost of production from all fossil plants.3 In addition,since resource decisions are always made in the context of anutility system with a mix of resources, these uncertainties needto be considered not only for a single resource option being evaluated,but in a system-wide context for existing and alternative futureresource options. The exact nature of these impacts is hard to predict, but thefacilities most likely to be affected are those burning fossilfuels. For example, the Clinton Administration's Climate ChangeAction Plan has established specific goals affecting carbondioxide emissions, and has recognized the need to go much furtherthan the current voluntary program to contribute to climate stabilizationover the longer term. Also, pursuant to Title III of the 1990Clean Air Act Amendments, the USEPA has initiated a study to examinethe health risks of air toxics emitted from fossil power plants- the results of which could lead to costly emission control equipment.4 In Massachusetts, for example, the importance of accounting forthe risk of future regulation has been emphasized by a recentcourt decision on the use of externalities in electric resourceplanning. Even while striking down the DPU rule that requiredutilities to apply environmental adders to bids from new powerplants in resource solicitations, the Massachusetts Supreme JudicialCourt (SJC) ruled that the DPU has the right to order a utilityto avoid a resource that may be subject to expensive emissionsrestrictions in the future. The SJC ruled that the DPU lackedthe statutory authority to consider societal impacts, whileaffirming that risks to ratepayers associated with thecosts of future regulations is a valid consideration. The FERC recently ruled in California regarding the avoided costpayments made to QF's under PURPA, and addressed how non-pricefactors and externalities may be applied to these payments.5 PURPA's avoided cost standard can be construed as "allowingconsideration of non-price factors essential to integrated resourceplanning."6 In particular, Commissioner Masseyemphasized that the FERC decision "does not preclude statesfrom considering environmental costs in determining avoided costsunder PURPA."7 What is not clear is how thesenon-price factors can be taken into account. FERC clearly pointedto state tax laws as an acceptable mechanism. 1.3 Approach of This Report An important objective of this report is to express values forthe benefits of DSM in specific quantitative and monetary termswherever possible. This approach should not be interpreted asa position that monetization is the only, or even the best, methodfor incorporating externalities and other policy factors intoelectric power system planning. In some instances, the valuationof environmental resources in monetary terms may be reasonableand appropriate. The dollar value can then applied through a taxor an "externality adder" in order to appropriatelyincorporate that impact into utility planning. In other cases,valuation may be difficult, impossible, or simply inappropriate,and setting targets may be a better policy approach. In such instances,where a policy target might be set for a state, regionor entire country, an "externality adder" might be auseful instrument for the implementation of that policy. However, some policy goals simply do not fit the category of "externality".These can, for example, relate to equity between individuals,classes of customers, citizens of different states, or membersof different generations. Such equity considerations have to someextent been, and should continue to be, recognized in utilityplanning and DSM screening. Such distributional issues do notreadily lend themselves to economic valuation. For example, "willingnessto pay", a key concept for economic analysis, may be constrainedby low-income customers' ability to pay. Similarly, future generationsare not available to express their views. Nonetheless, quantitativeanalysis of distributional issues can be illuminating, and policyfactors for incorporating equity considerations into utility planningmay take a quantitative form. Over the past several years, the range of "other non-pricefactors affecting the costs or benefits of the electrical service(e.g., reliability, fuel diversity)" has come under increasingscrutiny. DSM represents an important policy choice for avoidingadverse impacts on health, environment, and public amenity thatresult from society's need for electrical power -- impacts thatinclude the societal costs of energy use associated with the entirefuel cycle, not simply the impacts at the point of generation.Furthermore, DSM programs targeted to low-income customers canprovide important societal benefits beyond those related to thecustomer's energy consumption. DSM can also have a beneficialimpact upon direct utility costs through decreasing arrears andby decreasing exposure to risks of future environmental regulation.In addition, DSM programs have other social associated with jobcreation, government subsidies, oil import costs, and depletionof natural resources. To quantify the benefits of DSM programs in New England, we haveexamined total DSM programs for regional utilities that have madesubstantial investments in DSM. Table 1.1 shows data for the threeutilities in New England with the largest 1993 expenditures onenergy efficiency programs. In our study, we considered DSM programsto be equivalent to 100 MW of generating capacity. This is slightlyless that 101 MW of DSM savings that the Boston Edison Company(BECo), has developed through 1993. It can be considered as alower bound figure when compared to the 267 MW which the ConnecticutLight & Power achieved in 1993. Table 1.1: New England Utilities with the largest 1993 expenditures on DSM Utility Change in Annual Sales (GWh) Change in Ann. Peak Cap. (MW) Cost (million $) Connecticut Light & Power 1,160 267 41 Mass. Electric Company 549 147 53 Boston Edison Company 382 101 57 On the other hand, the 100 MW of DSM savings is essentially ahypothetical quantity of savings that we have chosen for our illustrativecalculations. We make the simplifying assumption that the 100MW of DSM savings is made up of a similar mix of DSM programsas New England's current and future programs. This assumptionimplies that the 100 MW will have a "load factor" ofroughly 60 percent, and associated energy savings of roughly 525GWh. For each non-price factor, two scenarios are specified, alow impact scenario and a high impact scenario, details of whichare given in the subsections that follow. 2. BENEFITS OF NEW ENGLAND'S DSM PROGRAMS 2.1 Avoided Emissions of Criteria Air Pollutants and Greenhouse Gases Criteria air pollutants include those that have been designatedby the EPA, under the 1990 Clean Air Act Amendments, with NationalAmbient Air Quality Standards (NAAQS) set to protect human healthand welfare. The list of criteria pollutants includes sulfur dioxide(SO2), volatile organic compounds (VOC), nitrogen oxide(NOX), total suspended particulates (TSP), carbon monoxide(CO), lead, and others. Greenhouse gases include those pollutants which increase the heat-trappingcapacity of the earth's atmosphere and contribute to global warming.The primary greenhouse gases emitted from power plants includecarbon dioxide (CO2), methane (CH4), nitrousoxide (N2O). In 1990, the Massachusetts Department of Public Utilities (DPU)adopted a policy requiring utilities to add monetary values offive criteria pollutants and three greenhouse gases to the costsof new generation facilities, as part of their integrated resourceplans. The monetary values chosen by the DPU for these air pollutants,in terms of dollars per ton emitted, are presented in Table 2.1.These values were subsequently thrown out by the Supreme JudicialCourt which ruled that the DPU lacked the statutory authorityto consider societal impacts, but affirmed the DPU's rolein ordering that utilities consider the risk of future regulations. Table 2.1: Massachusetts Adopted Values for Air Emissions ($1994) Pollutant Air Emission Value ($/ton) Sulfur oxides 1,784 Nitrogen oxides 7,557 Particulates 4,618 Volatile organic compounds 6,192 Carbon monoxide 1,008 Carbon dioxide 25 Methane 252 Nitrous oxide 4,618 Source: DPU 91-131, page 91, adjusted to 1994$ The monetary values of the criteria air pollutants have been estimatedusing the "regulator's revealed preference" methodology,sometimes referred to as the control cost approach. The monetaryvalues of the greenhouse gases have been estimated using the costsof maintaining a target amount of CO2 emissions, aswell as the relative global warming potential of the remainingtwo pollutants relative to carbon dioxide.8 The purpose of our analysis here is to apply the MassachusettsDPU monetary values to generation resources on the NEPOOL system,in order to provide a rough indication of the magnitude of thecost of these pollutants, and to compare these pollutants withother non-price factors. Accordingly, the DPU values for eachcriteria pollutant and greenhouse gas have been converted intomonetary costs by assuming that DSM programs displace energy fromoil and natural gas plants on the NEPOOL margin.9The results are summarized in Table 2.2, which indicates thatthe total costs of these air pollutants combined can be as highas 5.45 cents/kWh. Table 2.2: Benefits from reductions in NEPOOL's air pollutant and greenhouse gas emissions due to DSM programs (1994$) Pollutant Pollutant Upstream Generation Total Type Name (c/kWh) (c/kWh) (c/kWh) Criteria Nitrogen oxides 0.202 1.662 1.86 Air Sulfur oxides 0.026 1.124 1.15 Pollutants Particulates 0.002 0.190 0.19 Carbon monoxide 0.001 0.017 0.02 Greenhouse Carbon dioxide 0.028 2.069 2.10 Gases Methane 0.044 0.001 0.04 Nitrous Oxide N/A 0.078 0.08 Total 0.30 5.14 5.45 For New England, we estimate that for the high impact scenariothe potential impacts of 100 MW of DSM for the NEPOOL margin is$27 million, for generation-related greenhouse gas and criteriaair pollutant emissions. If upstream emissions are considered,DSM programs save an additional $1.5 million. In the low impactscenario, if the DSM were to displace generation from a new gascombined cycle plant with steam water injection and selectivecatalytic reduction, the benefits would amount to $6.9 million,representing only the emissions associated with generation. Ifupstream emissions are considered, DSM programs save an additional$5.4 million. 2.2 Heavy Metals During combustion of coal and oil, heavy metals which occur naturallyin coal and oil deposits are released into the air, either ina gaseous phase or bound to particles. Once released into theenvironment, metals are persistent.10People may be exposed to these metals through several pathways,including inhalation. Through atmospheric deposition, metals canbe deposited on soil, in lakes, and in streams. Contaminated soilmay present a health risk due to ingestion of soil (especiallyfor children) and ingestion of crops which may take up the metal.Metal deposition in lakes and streams may impact fish, humans,and other species consuming these fish. In general, two types of health impacts may occur - acute andchronic. Acute health impacts, which may be reversible, are associatedwith short term exposures to high concentrations of metals. However,fossil fuel combustion can also result in long term exposure tolower concentrations. These can cause chronic health effects whichtypically are not reversible. To date, very few air toxics have been regulated in the UnitedStates. Title III of the U.S. Clean Air Act Amendments (CAAA)of 1990 lists 189 hazardous air pollutants which are to be regulatedon a source-by-source basis. Sources for each air pollutant willbe listed and regulations are still to be promulgated for eachair pollutant from each source. The CAAA directs the EPA administratorto perform a study of the public health hazards associated withhazardous air emissions from electric generation facilities, andto regulate these emissions if deemed necessary by the EPA study. Until recently, little attention has been given to the study ofair toxics. To date we are aware of only two methodologies forestimating the monetary values of air toxics. The first methoddevelops the marginal cost of control for arsenic, chromiumand lead by assessing the marginal cost of complying with Massachusettsregulations pertaining to those three pollutants, including standardsfor arsenic emissions from copper smelters, chromium emissionsfrom cooling towers, and lead emissions from secondary lead smelters.(Chernick and Caverhill 1991). The second methodology is to develop relative toxicitiesof each heavy metal by comparing their potential health impactsto another heavy metal whose marginal cost of control is alreadyknown. This methodology assumes that society's willingness topay to avoid damages from heavy metals is proportional to theirrelative toxicities. For example, the monetary value of lead canbe determined using the marginal cost of complying with Massachusettsregulations, as described above. The monetary value of anothermetal which is twice as toxic as lead should be assigned twicethe monetary value of lead. We recommend inferring relative toxicities from the MassachusettsAllowable Ambient Limits (AAL) for the heavy metals. The AAL valuesare an annual average exposure concentration designed to protecthuman health. They are established by the Massachusetts Departmentof Environmental Protection. The AAL values are preferable tothe alternative "reference doses" determined by theEPA, because they consider both non-cancer and cancer health effects. The resulting monetary values of heavy metals (in dollars perpound of pollutant) are presented in Table 2.3. We present a rangeof low and high values, derived by applying the relative toxicitiesto arsenic, chromium and lead. The significant breadth of therange for many heavy metals is an indication of the uncertaintyembodied in these values. Table 2.3 - Recommended Values (1994 $/lb pollutant) Air Toxic Low High Arsenic 920 920 Beryllium 359 94,488 Cadmium 143 37,795 Trivalent Chromium 0 55 Hexavalent Chromium 1,430 1,430 Copper 0 70 Lead 540 540 Manganese 55 1,404 Mercury 14 3,779 Nickel 1 210 Selinium 0 70 Accordingly, these values have been converted into monetary costsby assuming that DSM programs displace energy from oil and naturalgas plants on the NEPOOL margin. The results are summarized inTable 2.4, which indicates that the upper range costs of theseair toxics can be as high as 0.28 cents/kWh. For New England, we estimate that for the high impact scenariothe potential impacts of 100 MW of DSM for the NEPOOL margin is$1.5 million. For the low impact scenario, the lower bound ofair toxic values was used, yielding a cost of about $0.1 million. 2.3 Water Use Impacts Electricity generation affects water resources at the point offuel mining and extraction, at the point of fuel combustion, andat the point of waste disposal. Water use impacts can compromisethe integrity of aquatic systems, the viability of their biota,as well as the health and amenity of the human populations thatthey serve. Fuel mining and extraction activities can have adverse water qualityimpacts as a result of water drainage from mines, leaching anderosion of solid wastes from mining and cleaning plants, slurrydewatering, and modification of aquifers. Mining activities canaffect both surface and groundwater supplies. For coal mines,potentially severe problems are created by acid drainage fromboth underground and surface mines and from coal waste storagepiles. For uranium mines, the construction of dikes creates aloss of aquatic habitat. More significantly, drainage from uraniummine tailings can increase the concentration of dissolved radiumin surface waters; sometimes leading to acute short-term poisoning,and sometimes leading to chronic long-term poisoning of aquaticpopulations. Table 2.4: Benefits from reductions in NEPOOL's air toxic emissions due to DSM programs (1994$) Societal Cost (cents/kWh) Air Toxic Low High Arsenic 0.005 0.005 Beryllium 0.000 0.021 Cadmium 0.000 0.070 Trivalent Chromium 0.000 0.001 Hexavalent Chromium N/A N/A Copper N/A N/A Lead 0.009 0.009 Manganese N/A N/A Mercury 0.000 0.002 Nickel 0.001 0.170 Selenium N/A N/A Total 0.015 0.278 For oil and gas exploration, offshore and terminal operationsresult in some discharge to surrounding waters. Tidal marshes,coastal wetlands, river swamps, and sheltered bays support organismsthat may be affected by low levels of hydrocarbons. Brine separationand transport incidents may also have impact on local water bodies. At the point of generation, power plants can have three typesof direct impacts on water systems: impacts on aquatic populationsfrom water intake systems, thermal discharges into water bodies,and solids and toxic chemical discharges into water bodies. Most of the waste impacts result from the cooling process in thermalpower plants. Therefore, water impacts will depend largely uponthe type of cooling system used. In once-through coolingsystems, water is taken from a local body of water, passed throughsteam condensers, and discharged at an elevated temperature. Inclosed-cycle cooling, cooling water is circulated throughcooling towers to transfer heat to the atmosphere, and then recirculatedthrough the plant. Closed cycle cooling systems tend to have lowerwater impacts than once-through. The primary impacts on aquatic populations from water intake systemsinclude (a) entrapment, when fish and crustaceans are affectedby the currents that are created as large volumes of water aredrawn into power plants; (b) impingement, when large aquaticorganisms are trapped on the intake screens that are used to preventthem from entering the power plant; and (c) entrainment,when those aquatic organisms small enough to pass through theintake screens enter the cooling pipes and are drawn through thecooling system. Impingement losses can be considerable. A studyof the Great Lakes estimated that fish kills in excess of 40 millionper year result from the 90 once-through thermal electric stationson the shores of the Great Lakes. (Ottinger et al., 1990) Thermal discharges of cooling water from power plants can haveadverse ecosystem impacts in a variety of ways. Organisms respondto changes in the aquatic environment with both behavioral andphysiological responses, including death. Submerged vegetationcan be increased or decreased in the nearby waters, thereby changingthe associated fauna, reproductive sites, and nesting areas oforganisms. New organisms may be attracted by the thermal effluent,while some existing organisms may be repelled by them, resultingin further modifications to the nearby habitat. Unanticipatedplant shutdowns or emergency responses can cause sudden changesto cooling water usage, resulting in thermal shock to the nearbyaquatic environment. Solids and toxic chemical discharges into water bodies resultfrom numerous operations within the plant, including: once-throughcooling processes, cooling tower blowdown, bottom ash and flyash transport water, chemical metal cleaning wastes, ion exchangewater treatment systems, and discharges from wet air pollutioncontrol devices. Many types of water pollutants have been detectedin power plant effluent, with major classes including: dissolvedsolids; chlorine residual; chlorinated organic compounds; corrosionproducts such as copper and zinc; residual anti-corrosion/anti-scalingwater treatment chemicals; and grease and oil. To our knowledge,no study has assessed the external cost of these water pollutantsfrom power plants. Finally, power plants can have water use impacts at the pointof waste disposal. Waste streams associated with electricity generationare disposed of in landfills or surface ponds. Once landfilled,ash constituents may enter groundwater if the landfill lacks aliner, leachate collection and monitoring systems. Water runofffrom the landfill may cause contamination of local surface waters.Similarly, water overflow in surface ponds may also carry pollutantsinto surface waters. Water use impacts from all stages of electricity production tendto be highly site-specific. In addition, relatively few studieshas been performed which attempt to quantify, or to monetize,the environmental impacts of water usage. In theory, it wouldbe possible to develop monetary values of water impacts, usingthe regulators' revealed preference methodology, for water intakeimpacts, thermal discharge impacts, chemical discharge impacts,and waste disposal impacts. However, we are aware of only one study that has developed monetaryvalues for any of these impacts. The Pace study estimated thecontrol costs of fish impingement resulting from cooling waterintake. Using an assumed fish impingement rate of 0.00061 fish/kWh,the study estimated mitigation costs that ranged from 0.005 to0.02 cents/kWh. To our knowledge, monetary values have not beendeveloped for the many other water use impacts described above. For New England, we have estimated the potential impacts of 100MW of DSM savings for water intake impacts only, because we havenot found monetary values of other water use impacts that aresufficiently comprehensive. Under a high impact scenario, thetotal cost is about $1.1 million; under the low impact scenario,the total cost is about $0.3 million. 2.4 Land Use Impacts Electric power facilities can have a variety of land use impacts,including those from fuel extraction and mining, impacts fromthe power plants themselves, and impacts as a result of wastedisposal. Mining activities can create impacts on terrestrial biota andother land resources, e.g., forced migration and threats to endangeredspecies. Surface mining can have environmental and economic impactsas a result of the removal of land from its use as farmland, forestor rangeland. In addition, spoil from improperly controlled minescan result in erosion and landslides, while unreclaimed coal highwallscan be a major aesthetic problem. In some cases, noise and visualimpacts extend beyond mine boundaries, and affect wildlife habitatsand recreational activities in a larger area. The amount of land used for the construction and operation ofpower plants varies significantly by the type of plant. Coal plantstypically require significantly more land space due to coal preparationand on-site waste disposal. According to the U.S. DOE, naturalgas and oil fired generators tend to require roughly 0.1 squareyards per MWh of generation, while conventional coal plants requireapproximately 10 times that amount (1.2 sq. yards/MWh). Nuclearplants also require a great deal of land space. Pressurized WaterReactors use slightly more than conventional coal (1.5 sq. yards/MWh),while Boiling Water Reactors require nearly twice that amount(2.7 sq. yards/MWh) (US DOE 1983). Hydroelectric facilities can also create land-use impacts, dependingupon the type and configuration of the installation, the modeof operation, the environmental characteristics of the site, andthe use of the surrounding lands. Flooding of land behind a damcan inundate large areas of forest, farmland, and even towns,destroying a wide range of wildlife habitats and flora. Adversebiological effects on fish life can be caused by the obstructionof downstream sediment movement, the buildup of anaerobic gases,obstruction of spawning pathways, and increased salt and mineralcontents. In addition, socio-economic impacts can occur as previousrecreational and commercial waters are impacted -- although creationof the reservoir may also present alternative recreational andcommercial opportunities. Energy distribution facilities such as gas or oil pipelines canhave a host of potential impacts on land use. The constructionand maintenance of pipelines and related facilities may affectfarming, forestry, and tourism, and may impair the aestheticsand general amenity of life in the neighboring communities. Inaddition, local employment might rise as a result of constructionand maintenance of pipelines, but at the cost of displacing ordisrupting existing communities. Several pipeline projects havebeen stopped in part because they interfered with aboriginal communities;while other projects have required payments ranging from fourto eight percent of the value of the project to protect tribalfisheries and other traditional assets. Finally, the land required for the disposal of solid wastes producedby generating facilities may be substantial. For example, a 500MW coal plant requires approximately 300 acres of land for disposingof solid waste -- equal to over one-third of the land requiredfor the location of the plant itself. Unlike many of the other non-price factors reviewed in this report,part of the value of the land that is used for electricity generationis reflected in the price that the utility must pay for the land.However, a number of external costs remain. Converting open spaceto an industrialized application involves a range of aesthetic,recreational and ecosystem diversity impacts. In addition, plantsites can have health, environmental, aesthetic, and noise impactswhich lower the value of surrounding properties. Furthermore,the volume of combustion wastes from fossil fuel plants createspressure on landfill space, and accelerates the need for additionallandfill areas. The on-going controversy regarding the U.S. DOE'sinability to find a publicly-acceptable site for the disposalof nuclear wastes indicates the degree of land use impacts thatextend beyond the price of the land. The land-use impacts from every stage of electricity productionare highly site-specific, and will depend largely upon the facilitytype, the surrounding geography, wildlife characteristics anddemographics. In addition, there have been very few studies performedto develop monetary values of land-use impacts. Therefore, wedo not recommend that any particular value be used at this timein New England. 2.5 Radiological Impacts of Nuclear Power Emissions of radionuclides occur at each stage of the nuclearfuel cycle. Some of the radionuclides are very long-lived andwill continue to decay over thousands of years, suggesting thattoday's nuclear power plants may result in impacts that will lastover many generations. Other radionuclides are very short-lived,but may nonetheless result in intense exposure due to the typeof radioactivity emitted. Uranium tailings piles, associated with the mining and extractionof nuclear fuels, release a variety of radionuclides into theatmosphere. Of principle concern is radon (Rn-222) -- an inertgas with a short half-life (less than four days), but which isreleased into the atmosphere virtually forever because one ofits precursors (Th-230) has a half-life of 80,000 years. Dependingupon the future condition of the tailing pile, adverse healtheffects may arise from radon and its daughters as they are inhaled,deposited, and retained in the respiratory system. A variety of radionuclides are routinely emitted from the operationof nuclear generating stations as well. The emissions are routinelyproduced by nuclear reactions occurring in the fuel, in the moderatoror heat transport systems, and in structural components. Radioactiveemission rates vary depending on structural design characteristics,power rating, emission control systems, and waste handling methods.The general public is exposed to radiation through a variety ofair and water pathways.11 The most uncertain factor in estimating externality costs fromnuclear power generation is the risk of possible catastrophicradionuclide releases due to accidents. Opinions diverge greatlyregarding the frequency of a catastrophic release, the exposureof local, regional and global populations, and the dose-responserelationships. Some recent studies conclude that external costsassociated with possible reactor accidents may account for thelargest single contribution to total nuclear externality costs. Nuclear reactors could suffer an accident for a number of reasons.Much of the analysis has focused on the possibility of a lossof coolant accident (LOCA) which involves the rupture of a largecooling pipe. However, many other accident scenarios are possible,as well as multiple event combinations leading to a major accident.The worst-case scenario occurs when severe core damage resultsin the release of much of the radionuclide inventory into thecontainment and eventually to the environment. Finally, nuclear power plants produce nuclear waste which canalso release radionuclides into the environment. Low-level wasteswill remain hazardous for hundreds of years. Occupational exposureto radionuclides can result from workers handling, packaging,and storing the wastes. Moreover, long-term exposure may resultfrom radioactive effluent from waste buried in trenches and in-groundcontainers. Another potential source of exposure is associatedwith the possibility of accidents during handling, transport,and final disposal. In the US, accidents during transport andhandling have produced contamination beyond the boundaries oflow-level waste sites. Long-term options for permanent storageof low-level wastes are still under review. High-level wastes consist primarily of spent fuel generated bythe nuclear fission process, and can remain highly radioactivefor thousands of years. High-level wastes are also subject tooccupational and accident-related risks. High-level wastes arecurrently stored on the site of the generation facilities, pendingthe development of a temporary or permanent storage site. Estimating the direct physical impacts of damages due to radionuclideemissions is a complex task replete with uncertainties, scientificdisagreements, and unresolved issues. Impacts will depend upona variety of factors, including the actual level of emissionsinto air, water and soil; the transport of radionuclides throughthose media, based on climatological and topographical conditions;the exposure of receptor areas or populations, and the dose-responserelationship of those populations. The primary emphasis in the literature that we reviewed was onthe human health impacts of radionuclide releases. Bioaccumulationof radionuclides in many other kinds of living organisms can alsobe significantly affected by the nuclear fuel cycle. Accordingly,the results presented here should not be seen as encompassingthe full range of environmental and health impacts of radionuclidereleases. Most of the studies we reviewed determined monetary values ofradionuclide emissions using the "damage cost" methodology.One of the central assumptions in this approach is the dollarvalue of a human life. While a broad range of values have beenused, the Pace study recommends a "middle ground" valueof a statistical human life of $4 million (Ottinger et al., 1990).Alternatively, it is reasonable to assume $10,000 for a dose commitmentof one person-rem.12 This value is used by the U.S.Nuclear Regulatory Commission as a guideline to determine if arisk-reducing plant modification is economically justified. Table 2.5 summarizes the results of several estimates of the dollarcosts of radionuclide emissions from uranium mining, routine reactoroperations, risk of nuclear accident, decommissioning and disposalof nuclear wastes. This table indicates the wide range of estimatesthat exist for each of the stages of the nuclear fuel cycle. Italso indicates that the risk of nuclear accidents, and the long-termradon releases from unstabilized tailings piles, could potentiallyhave the greatest environmental impacts. In New England, for the low impact scenario, these avoided costswill be zero, because nuclear fuel production, generation anddisposal will not be affected by reducing the energy generatedon the NEPOOL margin. However, in the high impact scenario wherewe assume that the DSM will avoid nuclear generation, the avoidedcosts could be as high as $1.2 billion, dwarfing the other non-pricebenefits of DSM. Most of this impact is due to the cost of long-termradon releases from tailings piles that we have assumed will beunstable. 2.6 Electricity Transmission and Distribution Impacts There are a number of adverse health and environmental impactsassociated with electricity transmission and distribution (T&D)systems. Over the past decade, concern over siting of new T&Dlines has become one of the more controversial topics of the electricityindustry. Transmission towers are required to have a right-of-way, whichis an open space on either side of the line, used primarily forsafety, protection and maintenance. Construction and maintenanceof the right-of-way may cause local adverse impacts on soil, drainagepatterns, flora and fauna. New techniques in right-of-way managementhave served to decrease considerably the impact on wildlife habitats.However, powerline construction practices vary widely; carefuloperations are able to mitigate land impacts, while poorly executedoperations may impose significant environmental impacts. The maintenance and operation of a transmissionline and its right-of-way can have a negative effect on localsurface water quality and aquatic ecology. Where service roadscross wetlands and surface waters, soil erosion can induce elevatedturbidity, sedimentation, and toxic effects from herbicide runoff.Land values in rural areas can also be affected by the sitingof transmission lines, due to the interference posed to agriculturalproduction, irrigation schemes, and aerial pesticide applications. In addition, small amounts of ozone and substantially smallerlevels of nitrogen oxides are produced along the length of thetransmission line by a phenomenon known as corona, a "highenergy discharge" which occurs when air ionizes on the conductorsurface. Transmission lines also result in negative aestheticand noise impacts. The noise impacts of transmission lines areresult of the corona effect on high voltage transmission lines. A part of the value of the land that is used for electricity T&Dis reflected in the price that the utility must pay for the land.However, a number of external costs remain. Converting open spaceto an industrialized application involves a range of aesthetic,recreational and ecosystem diversity impacts. In addition, T&Dlines can have a number of impacts which lower the value of surroundingproperties. Much of the present controversy about T&D systems is causedby the concern about adverse biological effects of exposure toelectromagnetic fields (EMF). Although the epidemiological dataremains inconclusive, several studies suggest that exposure toEMF may cause increased cancer risk. The increased risk is especiallypronounced for childhood leukemia.13 When viewed asa whole, the literature suggests that EMF may not necessarilybe a cancer initiator, but it may be a cancer promoter. To our knowledge, there has not yet been any study which developsmonetary values for the land-use impacts of T&D systems. Thereare reasonable approaches to estimating monetary values, suchas assessing the reductions in market values of the land usedand the surrounding properties. However, such techniques have not yet been applied. Similarly, we are not aware of any study that has developed monetaryvalues for EMF impacts. In fact, the very existence of EMF impactsis still heavily debated. Nonetheless, some analysts and policy-makershave proposed a policy of "prudent avoidance" as a guidingapproach in T&D planning and siting. The underlying principlebehind this approach is that the scientific evidence suggeststhat EMF health risks are credible, even if they have not beenfully quantified or demonstrated. As a result, it is prudent totake reasonable steps to avoid EMF health risks. Some Public ServiceCommissions such as Wisconsin now require utilities, when proposingtransmission line projects, to consider the number of personspotentially exposed to EMF, and to use low-EMF design structureswhere practical. Because of the lack of information available regarding monetaryvalues for T&D impacts, we do not recommend that any particularvalue be used at this time in New England. However, we do recommendthat the policy of prudent avoidance be applied for all transmissionand siting considerations. 2.7 Job Creation and Economic Development Demand-side management can contribute to economic developmentand job creation in a number of ways. First, implementing DSMprograms creates jobs in a variety of trades, such as carpenters,plumbers, contractors, architects, engineers, and the many jobsrelated to the production of energy efficiency products. Second, DSM reduces the cost of electricity, and thereby createsadditional jobs through "respending" effects. Residentialcustomers will have lower electricity bills, which provides themwith more disposable income to spend on goods and services, thusexpanding local markets. Commercial and industrial customers willhave lower production costs on average, and will therefore bemore competitive. Businesses and industries may pass on thesecost savings to customers by lowering their product prices, thusexpanding their market shares. They may also pass on some of thecost savings in the form of profits to investors, resulting inincreased disposable income, which may further stimulate economicactivity and growth. Third, DSM stimulates the production of goods and services thatare well-placed for future markets, and creates a manufacturingbase and fosters skills in an commercial area that may supportthe local economy in the long-term. Fourth, DSM can improve thequality of power delivered to businesses and industries, by reducingoutages, voltage surges, and bad frequency control. Finally, byimproving environmental quality in general, DSM can act as a stimulusfor attracting new businesses and residents. Most of the literature on this topic focuses on the first twoeffects -- the creation of jobs through implementation of DSMand through respending effects. In almost all cases these jobimpacts are estimated using an input-output (I-O) model. I-O modelscan compute the increase in output, income, and employment inall sectors of the economy that are affected when money is spenton a specific product.14 Typically, an I-O analysis is applied to compare two projects;for example, a new power plant versus a package of DSM projects.The employment effect of DSM is determined by first taking thedifference between gross jobs created by the DSM and the grossjobs created by the alternative power plant. The net employmenteffect of DSM is then determined by adding in the jobs createdby the respending effect. The same approach could be used to comparetwo alternative resource plans, each with their own combinationof DSM and supply-side options. Table 2.6 summarizes some results of selected DSM I-O employmentstudies. For each study, the table presents gross, respending,and net employment effects of DSM relative to different supply-sideresources. Table 2.6 also presents the gross employment effectsin terms of jobs per million dollars spent to acquire a resource.Finally, Table 2.8 presents a ratio of jobs created by DSM overjobs created by supply-side resources. This ratio includes therespending effects in the DSM jobs, and therefore captures thenet DSM employment effect relative to supply-side resources. As demonstrated in Table 2.6, most studies find that gross employmenteffects from DSM are roughly comparable to gross employment effectsof supply-side resources, and that the respending effect accountsfor the primary difference in employment impacts. The DSM/supplyjob ratio suggests that when the respending effect is taken intoaccount, DSM programs tend to generate roughly 1.5 to 4 timesmore jobs than supply-side resources. In addition, the results in Table 2.6 indicate the extent to whichestimates of employment impacts will differ widely depending uponthe type and size of the DSM program, and the size and type ofthe alternative supply-side resource. The range of results presentedin Table 2.6 demonstrates the many variables and uncertaintiesinvolved in estimating job impacts, and suggest that caution beused in directly applying the results. The number of jobs presented in the table above does not capturea variety of qualitative aspects of job creation. Employment inthe energy efficiency sector has a number of advantages over employmentin traditional utility construction and maintenance. For example,conservation employment tends to be more evenly distributed overtime, providing a more sustained stimulus to the economy. As anotherexample, DSM employment utilizes labor with generally applicableskills (e.g., carpenters, electricians, architects) that are oftenavailable in the local economy. In contrast, utility plant constructionrequires highly specialized labor that often has to be imported. Estimating the potential job impacts of DSM programs would requirean in-depth economic analysis of the specific programs and thetypes of supply-side resources that they would avoid.Nonetheless, it is useful to present a simple, illustrative calculationof the amount of employment that might be generated by a hypothetical100 MW of DSM, as presented in Table 2.2. For our low impact scenario, we assume that the gross employmentof DSM is equivalent to the gross employment of the avoided supply-sideresource, which means that the only employment impacts will resultfrom respending effects. We then assume that New England DSM programswill result in respending employment of roughly 5 jobs per million(i.e., the lowest end of the range from Table 2.6), and that thetypical New England utility continues to spend roughly $50 millionper year for the next ten years. These assumptions imply thatour hypothetical DSM programs will result in the creation of approximately2,500 local jobs over those ten years. To the extent that theseprograms are more cost-effective, additional jobs will be created. In the high impact scenario, we assume that DSM does not avoidany new supply-side capacity, but only avoids marginal energyon the NEPOOL system. As a result, there will be significantlyless gross employment from the supply-side, because the DSM isonly avoiding the fuel and operations and maintenance costs ofsupply-side resources. We therefore assume that the gross employmentfrom the supply-side is roughly 10 jobs per million dollars spent(the low end of the range from Table 2.6), and that the DSM grossemployment will be roughly 20 jobs per million dollars spent (takenfrom the low to middle end of the range presented in Table 2.6).Again assuming that roughly $50 million per year in DSM expendituresfor the next ten years, there would be a total of 5,000 more job-yearscreated form the gross DSM employment than the gross supply-sideemployment. Finally, we assume that the respending effect willresult in 20 jobs per million dollars spent, resulting in an additional10,000 job-years of employment. The total employment impact ofthis high scenario would then be 15,000 job-years over the nextten years. It is also important to consider whether, and how,employment impacts should be accounted for in evaluating DSM programs.In general, employment impacts are not necessarily an externalitybecause they are accounted for in prices and in the economic marketplacein general. Therefore, employment impacts cannot be simply addedto the "total cost to society" in the way that, forexample, environmental externalities can. However, there are manyinstances where policy-makers decide that improving local economicdevelopment is desirable. Accordingly, the job creation impactscan be considered a "policy factor," which is somehowqualitatively accounted for when evaluating resource options.Clearly, the creation of 2,500 to 10,000 job-years has importantpublic policy implications that should be accounted for in evaluatingDSM programs. Table 2.6: Summary of results of selected studies of DSM employment impacts Author Charles River Charles River Jaccard & Sims Clark, et al. Goodman, et al. Geller, et al. Goodman, et al. Date 1984 1984 1991 1992 1992b 1992 1993 Region Washington Washington British Columbia Maine Quebec United States Florida Supply Option Coal Nuclear Hydro Fluid Bed Coal Hydro Mix Mix of Fuels Supply Gross Employment 15,939 34,155 3,641 1,883 62,499 1,468,000 65,116 DSM Gross Employment 28,462 28,462 6,357 4,212 75,225 1,093,000 86,620 DSM Respending Employment 30,739 42,124 9,144 2,770 19,572 1,462,000 35,436 Net DSM Employment 43,262 36,431 11,860 5,099 32,298 1,087,000 56,940 DSM - Jobs per Million 60.6 60.6 13.5 9.0 17.8 18.3 na Supply - Jobs per Million 31.6 29.9 3.0 4.9-5.9 11.8-24.8 11.4 na DSM/Supply Job Ratio 3.7 2.1 4.3 3.7 1.5 1.7 1.9 Notes: These results are presented for illustrative purposes. It is difficult to compare across avariety of studies because of the methods used and the particular DSM and supply projects considered. The DSM and Supply "jobs per million" dollars spent are for gross employment impacts only.DSM/Supply Job Ratio equals (DSM gross employment + DSM respending employment)/supply gross employment. All the results listed, except for Goodman 1993, are taken as cited in Goodman 1992c. In addition, it is important to note that as New England utilitiesimplement DSM programs whose costs are close to avoided costs,the employment effects from respending (the primary employmentbenefits) will be significantly reduced. If DSM programs are implementedthat are significantly less than avoided costs, than employmentimpacts should play a greater role in evaluating different resourceoptions. 2.8 Government Subsidies for Energy Resources The U.S. energy market is supported by a host of subsidies thataffect the market prices and the mix of energy sources. For example: A variety of federal tax benefits are offered in the formof preferential tax rates, tax exemptions, tax deferrals, taxcredits, and measures that reduce taxable income. Tax benefitsare probably the most pervasive form of energy subsidy. Government agency programs support different energy resourcesby providing research and development, loan guarantees and othergrants, and maintenance of energy-related operations that supplyproducts and services to energy producers at low cost. Some regulatory interventions constitute preferential treatmentof certain energy sources. For example, the Price-Anderson Actof 1959 limits the liability of nuclear power accidents, significantlyreducing insurance or liability costs for nuclear power plants.Other interventions include price and import/export regulations,and government procurement guidelines. State and local governments also provide subsidies, primarilyby exempting energy products from a sales tax, reducing incomeand property taxes of utilities, and subsidizing the productionof raw energy materials. A summary of the results of the most recent, comprehensive assessmentof federal subsidies for energy resources is provided in Table2.7, which presents high and low estimates of annual federal subsidiesin 1989. (ASE and Koplow 1993) The high case estimates indicatethat federal energy subsidies were roughly $42 billion, with $21billion of that going towards tax breaks, and $18 billion goingtowards federal agency programs. In general, nuclear and fossil fuels receive most of the federalsubsidies. The authors estimate that nuclear and coal fuels receivedroughly 24 percent of the total federal subsidies each, whileoil and renewable energy receive roughly 2 percent each.15 As indicated in Table 2.7, the Price Anderson Act isestimated to be the second largest single component of federalsubsidies to energy resources. It would be difficult to identify any federal or state energysubsidies that could be avoided by DSM programs on the NEPOOLsystem. Nonetheless, these subsidies do represent a cost associatedwith certain resource types. It is therefore useful to estimatethe magnitude of that cost on a per unit (i.e., cents/KWh) basis,in order to compare this cost across different resource types. For example, Table 2.7 indicates that total federal subsidiesin 1989 amounted to roughly $25 to $42 billion. The nuclear electricityshare of this was 24 percent, and nuclear electricity generationin 1989 was 529 billion kWh. This implies a range of nuclear subsidiesof roughly 1.1 to 1.9 cents/kWh (in 1994$). These subsidies aresignificant, given that the cost of nuclear fuel is roughly 1cent/kWh. Similarly, the coal electricity share of the total was 24 percent,and U.S. coal-fired electricity generation in 1989 was 1,454 billionkWh, implying a range of coal subsidies of roughly 0.4 to 0.7cents/kWh (in 1994$). These subsidies are also significant, giventhat the production costs of coal are roughly 1.5 to 2 cents/kWh. It is also useful to estimate the potential impact of these subsidiesin the context of avoided costs of the hypothetical 100 MW ofDSM savings, and the associated 525 GWh of energy savings. Forthe low impact scenario, we assume that the DSM avoids coal generationentirely, and we assume the high estimate of coal subsidies, resultingin roughly $3.8 million of federal subsidies. For the high impactscenario, we assume that DSM avoids nuclear generation entirely,and we assume the high estimate of nuclear subsidies, resultingroughly $10.2 million of federal subsidies.16 It is important to note that subsidies for energy facilities arenot externalities, because they actually represent a transferof costs from utility ratepayers to taxpayers. Nevertheless, reducingfederal subsidies can imply a positive welfare effect. Federalenergy subsidies may be appropriate when there are positive externaleffects associated with the product being supported. However,when this is not the case, subsidization leads to an inefficientallocation of resources and hence a loss in welfare. 2.9 The Costs of Relying Upon Imported Oil In the wake of the oil crises of the 1970's, some analysts haveclaimed that there are large, measurable costs to dependence onoil, beyond those reflected in the current price. The argumentis that the concentration of ownership of world oil supplies causesvulnerability to future supply shocks. Use of fuels subject tosudden disruptions then adds costs and risks to the economy asa whole.17 Click to display large table To the extent that the consumption of foreign oil increases U.S. security needs, oil consumption givesrise to externalities. A study by Oak Ridge National Laboratoryestimates an annual cost to the U.S. economy due to the monopolizationof the world oil industry, of roughly $100 billion to $400 billionfrom 1972 to 1991. (Greene and Leiby 1993) This cost includesmonopoly-induced transfer of U.S. wealth to foreign oil producers,loss of potential output due to the increased price of oil, andmacroeconomic adjustment costs precipitated by oil price shocks. In addition, some analysts argue that military costs attributableto oil dependence, and protection of U.S. interests in oil producingcountries, may equal billions of dollars per year. Furthermore,the Strategic Petroleum Reserve, costing about $1 billion annually,is another cost to the U.S. government of reducing the risk ofoil supply. On the other hand, studies by Resources for the Future indicatethat the magnitudes of costs of reliance on imported oil are notwell understood. These studies find that the evidence does notsupport a conclusion that the market fails to internalize therisks of oil supply shocks. (Bohi and Toman 1992). It would be difficult to identify any particular cost of relyingupon foreign oil that would be avoided by DSM programs on theNEPOOL system. Nonetheless, these costs do represent a cost associatedwith the use of oil. It is therefore useful to estimate the magnitudeof that cost on a per-unit (i.e., cents/kWh) basis, in order tocompare this cost with the cost of electricity in general. For this purpose, we assume a cost of relying upon imported oilof $2.56 per barrel of oil, based on the studies of Broadman andHogan. (Broadman and Hogan 1988; Broadman and Hogan 1986) Assuminga heat rate of 10,000 for residual oil units on the NEPOOL margin,this implies a cost of 0.4 cents/kWh. In New England, for the high impact scenario, we assume that theDSM avoids oil generation entirely, resulting in roughly $2.2million for the costs of imported oil. For the low impact scenario,we assume that DSM avoids energy on the NEPOOL margin (which is96 percent oil), resulting roughly $2.1 million of federal subsidies. 2.10 Resource Depletion The fossil and nuclear fuels used to generate electricity in NewEngland represent a finite energy resource. While fossil fueldeposits appear to be plentiful and inexpensive in the short-term,the time is approaching when these fuels may become economicallyscarce, and society will be forced to switch to higher-cost energyalternatives. Given the importance of fossil fuels to nearly allaspects of our economy, these costs could be enormous. In addition,there may be no viable substitutes for fossil fuels in some applications,resulting in even higher costs to society. The predominant school of thought regarding resource depletionis the neoclassical approach, which affirms that technologicalprogress stemming from human ingenuity will overcome any imminentresource scarcity. The price of a resource is assumed to comprisetwo components: a marginal cost reflecting the cost of exploration,extraction and production, and a societal cost representing depletion.Thus, in the neoclassical model, the increasing price of a naturalresource, such as oil or gas, that is growing scarce triggersmechanisms that serve to augment the resource base: increasedexploration, recycling, substitution, as well as technologicalinnovation. There are, however, several reasons why the short-term marketprice non-renewable energy resources may not include the societalcost component associated with depletion: i) markets do not operatewith perfect efficiency; ii) there is never perfect informationregarding the resource base; iii) the price of a resource is oftenaffected by extraneous factors such as political incentives, internationalagreements, and collusion; and iv) resource production is ofteninfluenced by short-term political action which may expedite resourcedepletion, as seen in the Russian oil industry. The neoclassical model is sharply contrasted by the biophysicalapproach to resource scarcity, which accounts explicitly for theuse of low-entropy energy and matter in both the energy resourceproduction process and in the economy as a whole. The biophysicalmodel regards high-quality energy and matter, which are elementsof natural capital, as the fundamental constituents of the labor,capital and technological progress that drive our economy. Theenergy-return-on-investment (EROI) index (the ratio ofthe energy embodied in resources to the direct and indirect energyinput required to extract and refine them) used in the biophysicalmodel shows signs of depletion for oil and gas. The biophysical model to resource scarcity appears more consistentwith the goals of sustainable development and intergenerationalequity. Sustainable development strives to ensure that subsequentgenerations have the same amount of natural capital (fuels, speciesdiversity, waste assimilation through the Earth's biogeochemicalcycles, etc.) at their disposal as previous ones. Intergenerationalequity promotes the equal treatment of all generations. The assignment of a monetary value to the costs of resource depletionrequires a number of assumptions and inputs, many of which areclouded by uncertainty and imperfect information. These inputsrange from estimates of current reserves and undiscovered reservesto the costs of technological change, and from the future demandfor resources to the appropriate discount rate. While there isa vast body of literature on the subject of resource depletion,there have been few studies that have determined the monetaryvalue of the costs of resource depletion for nuclear or fossilfuels. One such study, by Hohmeyer, advocates penalizing the extractionof fossil fuels by imposing a reinvestment surcharge, thatis explicitly included in the market price. Hohmeyer's methodology,while requiring several estimates of future quantities which areopen to debate, is certainly consistent with the aims of sustainabledevelopment and intergenerational equity. While the depletion of oil causes concern in today's society,electricity production is only of secondary importance in thisissue, since it is responsible for only 4 percent of U.S. oilconsumption. Furthermore, most of the oil that is burned in powergeneration is residual oil, which is a by-product of oil production.However, if environmental sustainability policies begin to influenceoil consumption over the medium-term, electricity (and residualoil) consumption may assume a more prominent role in the depletionand the long-term costs of oil. Consequently, the resource depletioncosts of oil-fired electricity are likely to be low in the short-term,but more pronounced in the long-term. Coal is a major fuel for electric utilities in U.S., with electricutilities consuming a very significant portion of U.S. coal (86%).Consequently, reduction of electricity use may have a very significantimpact on coal production and eventually on coal depletion. However,U.S. coal reserves are relatively plentiful. The current reserves-to-productionratio, based on the Department of Energy estimates of recoverablereserves, exceeds 150 years. Consequently, the depletion costsof coal-fired electricity are likely to be low in the short-term,but from a long-term sustainability perspective, coal depletioncould have a significant environmental externality cost associatedwith it. Natural gas also plays an important role in power generation.Power generators, including independent producers, were responsiblefor 14% of all natural gas consumed in the U.S. in 1992. The Departmentof Energy projects that in 2010, power generation will be responsiblefor over 20 percent of gas use in the U.S., given the increasingreliance being placed upon gas. The natural gas industry currentlyoperates at a reserves-to-production ratio of 9 years. AlthoughU.S. gas reserves are significant, the National Petroleum Councilprojects that by 2010, the gas industry will operate at a reserves-to-productionratio of about 7 years. As a result, the costs associated withnatural gas depletion could be significant, both in the short-termand long-term. 2.11 Benefits to Low-Income Customers DSM programs targeted to low-income customersin New England offer a variety of societal benefits to those customersand the community in general. DSM programs can improve the amenitiesand comforts available to low-income customers, because such customersare often unable to afford the full amount of electricity services,and their electricity service is sometimes disconnected due toinability to pay their bills. Some of the benefits of low-incomeDSM programs include the following: Reduced health impacts, ranging from the common cold to mortalitycaused by hypothermia, as a result of customers being able toproperly heat their living spaces. (Brockway 1993a) Reduced dangers of fires and hazardous fumes from alternativeheating sources such as stoves and electric heaters. (Colton 1993)18 Reduced forced mobility and homelessness, as a result of customersbeing more able to make payments for rent and mortgages. (Brockway1993b; Colton 1993) Customers are more able to afford expenditures which improvethe quality of their health, such as better nutrition and preventivehealth measures. (Cambridge Systematics 1994b) Support for, and cooperation with, other low-income servicessuch as fuel assistance and Community Action Programs. A host of less tangible benefits, such as improved goodwillbetween the customers and the utility, and improved self-esteemof low-income households. (Brockway 1993b) A few studies have evaluated the extent to which particular low-incomeDSM programs result in some of the benefits described above. Forexample, a study of the U.S. Department of Energy's WeatherizationAssistance Program found that the program provided safety repairsto heating systems in seven percent of the homes, as well as additionalsafety repairs (e.g., carbon monoxide tests, window and door locks,smoke detectors) in 27 percent of the homes. (Oak Ridge 1993)The same study found that the Weatherization Assistance Program(which reached 198,000 dwellings) prevented at least 4,000 forcedchanges in occupancy in its first year. (Oak Ridge 1993) As anotherexample, a survey of participants in low-income DSM programs inNew York found that the participants had lower rates of colds,influenza or bronchitis, and reported a much lower rate of heatingwith the stove or oven. The recent Oak Ridge study is the only effort we have seen toattempt to place a dollar value on the non-energy benefits oflow-income DSM programs described above. The authors found that,despite a good deal of anecdotal evidence of the non-energy benefits,there was little evidence to support the assignment of dollarvalues to the benefits. Nevertheless, the literature indicates that non-energy benefitsof low-income customers do exist. In addition, public utilitycommissions in Kansas, Pennsylvania, Wisconsin, New York and Texashave acknowledged that such benefits exist. The Texas Public ServiceCommission has explicitly allowed utilities to implement low-incomeDSM programs that exceed the Total Resource Cost Test, on thebasis of the non-energy benefits. However, the Commission hasnot specified by how much the utilities should be ableto exceed the test. We recommend that the non-energy benefits of low-income DSM programsbe accounted for by applying an adder of 10 percent to the avoidedcost (i.e., the cost savings) of each program. Such an adder isjustified because it is clear that non-price benefits of DSM programsdo exist and are significant, even if they have not yet been adequatelymonetized. Obviously, such an adder is approximate, but we believethat it is the most appropriate number available at this time.Using a rough adder offers the advantages of being easily applicable,of scaling with the size of the DSM program, and of providingNew England utilities with an important planning benchmark.19 In order to make an illustrative calculation of the potentialimpacts of the low-income DSM adder, we assume an avoided costof roughly 3 cents/kWh in New England. The hypothetical 100 MWof DSM would result in 525 GWh of energy savings, which wouldresult in avoided cost savings of roughly $16,125,000 per year.For our low and high impact scenarios, we assume that low-incomeDSM program savings represent 1 percent and 10 percent of totalDSM program savings, respectively.20 The 10 percentadder would then represent roughly $16,125 in additional benefitsin the low impact scenario, and roughly $161,250 in additionalbenefits in the high impact scenario. 2.12 The Costs Associated with Outstanding Bills DSM programs can also enable a utility to reduce costs associatedwith outstanding electric bills. By reducing the cost of electricityservice, and by improving customer payment practices, DSM programscan increase some customer's ability to make timely payments ontheir monthly bills.21 As a result, the utility canreduce a variety of costs associated with payment-troubled customers,including: the lost time value of money associated with arrears,credit and collection expenses, bad debt, the costs of implementingpayment plans, and regulatory expenses associated with payment-troubledcustomers. (Colton 1993) A number of studies have estimated the ability of DSM programsto reduce customer arrears, usually by monitoring the resultsof actual low-income DSM programs. These studies have shown thatDSM programs can reduce customer arrears by anywhere from $3 to$176 per customer, with most studies showing savings on the orderof $30-$40 per customer, depending upon the DSM program and thecustomer's payment history.22 In addition, two studieshave indicated that the variable costs associated with creditand collection expenses are approximately $68 to $85 per customer.(Colton 1994b and Central Maine Power 1993) Some public service commissions (e.g., Texas) have found thatDSM programs can reduce the costs of outstanding electric bills,and that utilities should therefore account for this benefit oflow-income DSM programs. However, we do not know of any commissionthat has required utilities to quantify the extent of this benefit. The magnitude of costs associated with outstanding bills can besignificant. For example, in 1990 for BECo: average bills that were 60 days or more in arrears amountedto roughly $31.6 million. The total number of customers given a termination notice was533,392. The total number that actually were terminated was 12,388-- roughly two percent of all residential customers. Total residential charges to bad debt was roughly $8.8 million(NARUC 1992). The literature indicates that well-designed DSM programs can helputilities to reduce some of the costs of outstanding bills. Therefore,we recommend that a 5 percent adder be included with the benefitsof low-income programs, as an approximation of the savings fromreducing the costs of outstanding bills. Obviously, such an adderis approximate, but we believe that it is the most appropriatenumber available at this time. Using a rough adder offers theadvantages of being easily applicable, of scaling with the sizeof the DSM program, and of providing New England utilities withan important planning benchmark.23 In order to make an illustrative calculation of the potentialimpacts of the 5 percent adder, we assume an avoided cost of roughly3 cents/kWh in New England. The hypothetical 100 MW of DSM wouldresult in 525 GWh of energy savings, which would result in avoidedcost savings of roughly $16,125,000 per year. For our low andhigh impact scenarios, we assume that low-income DSM program savingsrepresent 1 percent and 10 percent of total DSM program savings,respectively. The 5 percent adder would then represent roughly$9,675 of additional benefits in the low impact scenario, androughly $96,750 of additional benefits in the high impact scenario. 2.13 DSM Spillover and Market Transformation DSM can sometimes result in "spillover" effects, i.e.,impacts beyond just the specific end-uses targeted and the particularprogram participants. Spillover effects can occur in three ways:(a) when participants adopt non-program DSM measures, (b) whennon-participants adopt program measures, and (c) when non-participantsadopt non-program measures. In addition, some DSM programs resultin "market transformation," where the commercializationand market adoption of energy efficiency equipment and practicesare advanced to the point where they reach non-participants, andeven other service territories. (Cambridge Systematics 1994a) DSM programs specifically designed to promote market transformationhave received increased interest lately, in part because theymay be more appealing to utilities if they are exposed to increasedretail competition. Market transformation programs may be seenas a relatively low-cost way of providing customer services, andmay enable utilities to move away from customer-specific programsto those that are based on a more comprehensive market approach.(Feldman 1994; Kitchin 1993) In general, spillover effects can be seen as the opposite of thefree-rider effect (i.e., when a customer benefits from a DSM program,even though he or she would have adopted the efficiency measurewithout the program). It has become general practice for utilitiesto subtract the savings associated with free-riders from theirDSM program estimates, but not the costs. In the same way, itis appropriate for utilities to add the savings associated withspillover effects. Until recently, there has been little attention paid to estimatingthe magnitude of spillover effects from different DSM programs.(Cambridge Systematics 1994a) It is difficult to generalize aboutthe magnitude of spillover effects, because they will be highlydependent upon the specific DSM program, the specific end-usemeasure, and even the particular market conditions in the utility'sservice territory and surrounding regions. Nevertheless, given the size of the New England service territoryand the extent of its DSM programs, some of these programs arelikely to result in some level of spillover effects. Market transformationimpacts are likely to be the most significant type of spillovereffects, because they tend to be long-lasting and to affect morethan specific participating or non-participating customers. Accordingly,we recommend that all DSM programs that are expected to transformthe market for energy efficiency products or services be creditedwith a 20 percent adder to its energy (kWh) and capacity (KW)savings. For example, this credit could be applied to Residentialand Commercial/Industrial New Construction programs. Obviously, such an adder is approximate, but we believe that itis the most appropriate number available at this time. The 20percent adder should also be seen as a minimum planning objective.In other words, because of the many benefits of market transformationprograms, utilities should be designing their programs with theintent of achieving market transformation. The literatureon this subject suggests that well-designed market transformationprograms should be able to achieve at least an additional 20 percentof typical DSM savings. In order to make an illustrative calculation of the potentialimpacts of the 20 percent adder, we assume an avoided cost ofroughly 3 cents/kWh. The hypothetical 100 MW of DSM would resultin 525 GWh of energy savings, which would result in avoided costsavings of roughly $16,125,000 per year. For our low and highimpact scenarios, we assume that market transformation DSM programsavings represent 12 percent and 50 percent of total DSM programsavings, respectively.24 The 20 percent adder wouldthen represent roughly $387,000 and roughly $1,612,500 of additionalbenefits in the low and impact scenarios, respectively. 2.14 Additional Benefits of DSM DSM can often provide benefits to participating customers, beyondthose associated with reduced electricity bills. These benefits,are often referred to as "non-energy" benefits, include,for example, reduced water and sewer bills as a result of measures to reduceelectric water heater consumption; reduced lighting maintenance costs for commercial customerswho install long-lived fluorescent lighting; reduced air conditioning costs as a result of installing fluorescentlamps that produce less heat; and reduced heating costs (oil, gas or electric) as a result ofbuilding shell measures installed to reduce air conditioning needs. In 1988, the Massachusetts Department of Public Utilities ruledthat in evaluating the cost-effectiveness of DSM resources, utilitiesshould include "any known, quantifiable, and significantend-user benefits... resulting from the installation of a particulardemand-side option."25 Commonwealth Electric Company (ComElectric) recently applied thisconcept in a request for proposals (RFP) to purchase DSM resourcesfrom Energy Service Companies and customers. The RFP allowed sellersof DSM to identify non-energy benefits that could be used in rankingthe different proposals. ComElectric received proposals whichclaimed several types of non-energy benefits. At the time of draftingthis study the proposals from the energy service companies wereconsidered confidential, and are therefore unavailable. The "value tests" proposed in the last few years byBenjamin Hobbs (1991), Barakat & Chamberlain (Chamberlainet. al. 1993) and others expand the traditional "total resourcecost" approach to cost-benefit analysis of DSM. These testsattempt to include impacts of DSM upon participants through estimatesof changes in consumer surplus. Thus, for example, if after theinstallation of a DSM measure, the participant consumes more "energyservice", there may be an increase in welfare to the customer.26 DSM programs can also have direct environmental and health benefitsbeyond the environmental impacts of avoided electricity generation.For example, a Tellus study for the Electric Power Research Institutefound that utility-sponsored replacement of old fluorescent lightballasts is likely to provide net environmental benefits becausethe PCBs in the old ballasts are more likely to be handled properlyby an organized program than if left to the normal replacementcycle. Similarly, refrigerator replacement in a DSM program caninclude a systematic means of capturing CFCs in the refrigerantand perhaps even the foam that might otherwise be released tothe atmosphere. (Tellus 1992) The non-energy and direct environmental benefits of DSM will bemeasure-specific, and also customer specific. In addition, theliterature on the subject is quite limited. Therefore, it is difficultto even make an illustrative estimate of the potential impactof New England DSM. The best way to quantify these benefits wouldbe to follow Commonwealth Electric's example of requiring EnergyService Companies to develop quantitative estimates as a partof their DSM program proposals. 3. SUMMARY, CONCLUSIONS AND RECOMMENDATIONS The research summarized in the previous chapter leads us to certainspecific conclusions and recommendations for evaluating the non-pricebenefits of DSM programs in New England. In this chapter, we willdiscuss the inferences from our analysis for valuation methodologyas well as the potential benefits from DSM under low/high impactscenarios. In the last section, we offer recommendations for futureresearch priorities. 3.1 Valuation Methodology for Monetizing Non-Price Factors Table 3.1 presents a broad summary of the information available,the methods used, and the range of results for monetizing non-pricefactors. The first column describes the extent to which the particularimpact of interest can be quantified. For example, it isrelatively easy to quantify the amount of NOx thatis emitted from power plants, as well as the environmental impactsof those emissions in terms of increased acid rain or ground-levelozone. Therefore, we indicate that the "quantification information"for NOx is "good." For other pollutants,such as lead, the amount of emissions from power plants are lesswell understood at this time, but the potential impacts on humanhealth are relatively well understood; so lead is rated as "fair." Click to display large table The second column in Table 3.1 presents the methods that are appliedto monetize, or otherwise account for, the impact of each non-pricefactor. For some factors (e.g., air pollutants), economic approachessuch as "regulator's revealed preferences" are used.For other non-price factors (e.g., radiological impacts), damagecosts are used, based on quantified environmental and health impacts.Some factors (e.g., low-income DSM benefits) could be accountedfor using approximate "adders," which are based on ourreview of DSM analyses provided in the literature that we reviewed. The third column of Table 3.1 presents our conclusions regardingthe current ability to monetize the costs or benefits of eachnon-price factor. For example, for certain criteria air pollutantsit is relatively easy to quantify their environmental impacts,and to identify their costs based on regulator's revealed preferences.Therefore, we conclude that the "monetization ability"is "good" for such pollutants. On the other hand, forother non-price factors (e.g., the benefits of low-income DSM),monetary estimates have not been developed in much depth. We concludethat, at present, such factors have "poor" or "fair"monetization ability. The fourth column in Table 3.1 presentsthe range of monetary values (in c/kWh) of each non-price factor,where appropriate. The values presented are based on the costsassociated with the relevant type of resource. For example, themonetary values for criteria air pollutants are based on marginalemissions from NEPOOL; the values for heavy metals are based onemissions from coal plants; and the values of radiological impactsare based on nuclear plants. In a few cases, it may not be possible or appropriate to accountfor non-price factors in monetary terms. For example, the benefitsof low-income DSM have been clearly demonstrated in the literature;however, there has been little success in quantifying those benefitsin monetary terms. Therefore, we propose that for such non-pricefactors a simple percentage adder to the direct avoided cost benefitsof the programs be applied. These recommendations are summarizedin the fifth column in Table 3.1. Finally, the last column in Table 3.1 describes what typeof benefit or cost the particular non-price factor represents.For example, environmental impacts (e.g., radiological impacts)are considered externalities which can be directly added to thetotal resource cost of a particular electricity resource. Otherimpacts (e.g., job creation) may not technically be an externalitywhich can be added to the total resource costs, but may be consideredanyway as an important "policy factor." Other factors(e.g., spillover effects of DSM) represent utility savings, andcan be accounted for similarly to traditional utility savings. The results in Table 3.1 indicate that criteria air pollutantsand greenhouse gases represent the largest costs, out of all thecosts that were monetized. Heavy metals can alias have a significantcost, depending upon the source of generation. The costs associatedwith radiological impacts span an enormous range, and are quiteuncertain at this time. Monetary values for land, water and transmissionand distribution were not available from the literature that wereviewed, although the impacts can be potentially large. Monetaryvalues were also not available for the benefits of low-incomeDSM, the benefits relating to outstanding bills, and DSM spillovereffects; however, we recommend that approximate percentage addersbe used to account for these benefits when evaluating resourceoptions. 3.2 Illustration of Potential Impacts of DSM Savings The results of our analysis suggest that the non-price benefitsof DSM currently unaccounted for in the planning process can bequite large. On the basis of the research summarized in the previouschapter, we estimate that the lower and upper bound of the totalannual non-priced of these programs benefits (excluding nuclear)are $19 million and $45 million, respectively. as shown in Figure3.1. The basis for these results are explained further by Table3.2 which presents illustrative calculations of the potentialimpacts of each non-price factor, in monetary terms, of 100 MWof DSM savings on the NEPOOL system. This indicates the potentialmagnitude of the societal benefits of each non-price factor, underlow impact and high impact scenarios. The actual costs and benefits of eachnon-price factor will depend not only upon the monetary valuesassigned to them, but also to the specific type and design ofDSM program and the particular supply-side resource that the programdisplaces. Performing such calculations for specific DSM programsand supply-side resources is beyond the scope of this study. However,it is nonetheless informative to estimate the costs and benefitsbased on generic DSM programs and generic supply-side resources,using as much NEPOOL information as is readily available. As aresult, the impacts presented in Table 3.2 should not be seenas definitive estimates of costs and benefits to New England utilities,but rather as approximations to illustrate the range of potentialimpacts. Click to display large table For our low impact scenario, we simply assume that the DSM savingsavoids the likely supply-side resource which would have the lowestimpact for each particular non-price factor. For example, forcriteria air pollutants, DSM that avoids generation and capacityfrom new natural gas combined cycle units will have relativelylow benefits, because gas units emit relatively low amounts ofsuch pollutants.27 For our high impact scenario, we simply assume that the DSM savingsavoids the likely supply-side resource which would have the highestimpact for each particular non-price factor. In this case, forradiological impacts we assume that the DSM savings avoid capacityand generation from a nuclear plant, because this obviously wouldresult in the greatest benefits. Similarly, for heavy metals weassume that the DSM savings avoid generation from a coal plant,because coal plants emit the most significant levels of heavymetal pollutants. Figure 3.1: Breakdown of societal benefits associated withNew England's DSM programs under low and high impact scenarios Table 3.2 demonstrates that reductions in criteria air pollutantsthrough our hypothetical DSM could result in savings of roughly$7 to $17 million. Reductions in heavy metals could result insavings of roughly $0.3 to $1.5 million. The estimated savingsfrom avoiding radiological impacts are dominated by the costsassociated with unstabilized uranium mill tailings piles. Table3.2 also demonstrates that our recommended percentage adders forlow-income DSM, benefits relating to outstanding bills, and DSMspillover will not necessarily be large relative to other externalities,but may nonetheless be important relative to the size of the specificDSM program budgets. These recommended DSM adders may be moreimportant for the messages that they send to utility planners,rather than the actual amount of money involved. Actual DSM savings are likely to have impacts somewhere betweenthe low and high scenarios that we present. More specific andfocused estimates of the potential impacts should be determinedthrough more detailed future studies. In addition, since differentavoided resources are used to calculate the impact of each groupof non-price factors, these results should not be added acrossdifferent non-price factors. 3.3 Recommended Research Priorities In the following list, we summarize the basis for each of theseresearch recommendations. Criteria air pollutants and greenhouse gases: Giventhat much of the information necessary to monetize these impactsis already available, we recommend a low research priority. Heavy Metals: While additional research could significantlyreduce the uncertainty associated with current estimates of monetaryimpacts of heavy metals. We conclude that reduced uncertaintyabout the monetary impacts of heavy metals may not have a significantimpact on DSM planning in the short-term.28 Therefore,we recommend a low to medium research priority Water Use Impacts: We recommend a medium to high researchpriority because the environmental costs could be significant,and the research necessary to monetize the impacts is relativelylow. Land Use Impacts: We recommend a high researchpriority because of the lack of research into this area. Radiological Impacts of Nuclear Power: We recommenda high research priority for the uranium fuel supply, the riskof nuclear accident, and the costs of waste disposal. These costsare not yet fully understood, and could have important impactson electricity planning in general. Future research regardingthe emissions from routine reactor operations and decommissioningshould have a medium priority. Electricity Transmission and Distribution: We recommenda high research priority because very little research hasbeen done to date, and additional research can resolve some issueswhich may have significant implications for electricity planningin general. Job Creation and Economic Development: Given the publicpolicy implications of DSM employment impacts, and the size ofNew England's investments in DSM, we believe that further researchin this area should be given a medium priority. We do not recommenda high priority primarily because employment impacts are not necessarilyan externality. On the other hand, it is important to more fullyunderstand the general economic development benefits that DSMoffers by lowering energy costs to businesses and households. Government Subsidies for Energy Resources: We recommenda low research priority, primarily because subsidies arenot an externality, and DSM programs are unlikely to have a significanteffect on federal energy subsidies. The Costs of Relying Upon Imported Oil: We recommenda medium research priority because a significant portionof New England's fuel mix. We also recommend that the risks associatedwith reliance on foreign oil be appropriately addressed in utilities'planning models. Resource Depletion: Despite the fact that there arefew, if any, signs of fossil fuel scarcity in the short-term,we recommend that New England utilities consider the possiblefuture effects of oil and gas depletion. Regional utilities canact by implementing an aggressive DSM regimen, promoting supply-sideenergy efficiency, and investing in renewable energy researchand development. Further research should focus on applying Hohmeyer'ssustainability initiative to the U.S., and on obtaining improvedoil and gas reserve estimates. Benefits to Low-Income Customers: While there currentlyexists a significant amount of research regarding the existenceof low-income DSM benefits, there is very little information availableto develop monetary values of these benefits. Therefore, moreresearch is warranted on this important topic. On the other hand,many of the low-income benefits could be represented with the10 percent adder to avoided costs. Therefore, we recommend thatthis topic receive a medium research priority. The Costs Associated with Outstanding Bills: Becauseof the potential reductions in costs to utilities, more researchis warranted on this topic. On the other hand, many of the benefitscould be represented with the 5 percent adder to avoided costs.Therefore, we recommend that this topic receive a medium researchpriority. DSM Spillover and Market Transformation: Given thepotential magnitude of additional savings available from spillovereffects, we recommend that this topic be given a high researchpriority. There are currently a number of studies (in Massachusetts,California, Wisconsin, and nationally) being undertaken to addressthe potential magnitude of DSM spillover effects. Additional Benefits of DSM: The non-energy and directenvironmental impacts of DSM programs could provide significantbenefits to New England utilities and their customers. However,we recommend that this topic area be given a relatively low researchpriority, because of the potential for utilizing Energy ServiceCompanies to conduct customer-specific and measure-specific estimates. Beyond these research priorities, we note that thecontinued implementation of DSM programs by New England Utilitiesshould be a policy priority. In considering various proposalsfor restructuring the electric industry in New England, it willbe important to keep in view the full range of non-price benefits,environmental. economic, and societal, that are associated withenergy efficiency. 4. REFERENCES Alliance to Save Energy (ASE) and D. Koplow, 1993. FederalEnergy Subsidies: Energy, Environmental, and Fiscal Impacts,April. Broadman and Hogan, 1988. "Is an Oil Tariff Justified? AnAmerican Debate: the Numbers Say Yes," Energy Journal,July. Broadman and Hogan, 1986. Oil Tariff Policy in an UncertainMarket, Energy and Environmental Policy Center, Harvard University. Bohi, D.R., and M.A. Toman, Resources for the Future, 1992. EnergySecurity Externalities and Fuel Cycle Comparisons, Washington,D.C., February. Brockway, N., 1993b. Direct Testimony and Exhibits ConcerningRevenue Requirements, application of Texas Utilities ElectricCompany for authority to change rates, Docket No. 11735, presentedon behalf of the Low-Income Intervenors, April. Brockway, N., 1993a. Direct Testimony and Exhibits ConcerningRate Design, application of Texas Utilities Electric Companyfor authority to change rates, Docket No. 11735, presented onbehalf of the Low-Income Intervenors, May. Cambridge Systematics, 1994b. ESEERCO Free Ridership Study:Final Report, Empire State Electric Energy Research Corporation,New York. Cambridge Systematics, 1994a. DSM Program Spillover Effects:Review of Empirical Studies and Recommendations for MeasurementMethods, prepared for Southern California Edison, with CaliforniaDSM Measurement Advisory Committee, June. Central Maine Power, 1993. The Electric Lifeline Program: ImpactEvaluation, Planning and Budgets/Evaluation and Assessment,July. Chamberlain, John, Patricia Herman and Melanie Mauldin. 1993.The Value Test: Economic Efficiency and Demand-Side Management.Barakat & Chamberlain, Inc. March. Chernick, P. and Caverhill, E., 1991. Joint Testimony on Behalfof Boston Gas Company, Docket No. D.P.U. 91-131, Oct. 4. Colton, R., National Consumer Law Center, 1993. The EnergyEfficiency Advocate's Interest in Affordable Low-Income EnergyRates, prepared for Hudson County Legal Services Corp., December. Colton, R., National Consumer Law Center, 1994a. IdentifyingSavings Arising From Low-Income Programs, March. Feldman, S., 1994. "Market Transformation: Hot Topic or HotAir?" Proceedings of the ACEEE 1994 Summer Study on EnergyEfficiency in Buildings, Draft, June. Greene, D.L. and P.N. Leiby, Oak Ridge National Laboratory, 1993.The Social Costs to the U.S. of Monopolization of the WorldOil Market, 1972-1991, for the U.S. Department of Energy,January. Hobbs, B.F. 1991. "The Most-Value" Test: Economic Evaluationof Electricity Demand-Side Management Considering Customer Value."The Energy Journal, 12:2:67-91. Kitchin, D., 1993. "The Impact of Market Transformation onDSM Evaluation Techniques," Proceedings of the 1993 InternationalEnergy Program Evaluation Conference, Chicago, IL, August. Oak Ridge 1993. National Impacts of the Weatherization AssistanceProgram in Single-Family and Small Multi-Family Dwellings.Also prepared by Martin Marietta Energy Systems, Inc., ORNL/CON-326. Ottinger et al., 1990. Environmental Costs of Electricity,Pace University Center for Environmental Legal Studies, OceanaPublications, Inc., New York. Tellus. 1992. The Environmental Impacts of Demand-Side ManagementMeasures. prepared for Electric Power Research Institute.EPRI TR-101573, Research Project 3121-05. final report December1992. 5. FOOT NOTES This report is based on a similar, but more detailedreport by Tellus for the Boston Edison Company Settlement Board.That report is available in a two-volume set that includes a SummaryReport and a Technical Report, and can be obtained by contactingTellus. Massachusetts Department of Public Utilities,Investigation by the Department on its own Motion into ElectricIndustry Restructuring, DPU 95-30, page 3. Some utilities are already accounting for therisk of future regulation of carbon emissions. For example, inits 1994 IRP filing, the Kansas City Power & Light Companyassumes there is a 60% chance of a $100/ton of carbon tax, 30%for a $30/ton of carbon tax, and 10% for a $500/ton of carbontax. USEPA, 1995. Study of Hazardous Air PollutantEmissions from Electric Utility Steam Generating Units Pursuantto Section 112(n)(1)(A) of the Clean Air Act -- Scientific PeerReview Draft,. June, Draft. FERC Order on Petitions for Enforcement ActionPursuant to Section 210(h) of PURPA, issued February 23, 1995,Dockets Nos. EL96-16-000 and EL95-19-000. FERC, op. cit., 1995, Concurring Opinion of CommissionerMassey, page 2.. FERC, op. cit., 1995, Concurring Opinion of CommissionerMassey, page 1. Recent work by the IPCC shows that the globalwarming potential of methane is 24.5 relative to carbon dioxide,or 2.45 times greater than what was known at the time the DPUestablished their externality values. NEPOOL marginal generation is comprised mostlyof residual oil steam boilers and distillate oil combustion turbines(95%) and the remainder natural gas steam boilers and Combustionturbines. The main metals emitted from fossil fuel combustioninclude: arsenic, beryllium, cadmium, chromium (both trivalentand hexavalent), copper, lead, manganese, mercury, nickel, andselenium. Major radionuclides that are emitted to air includetritium, particulates, carbon-14, noble gases, and iodine-131.Major radionuclides that are emitted to water include tritiumand gross beta gamma emitters. In addition, other radionuclidesare classified as particulate radioactivity because they attachto dust particles and become airborne with the dust. These include:fission products (cesium-134, cesium-137, and strontium-90), andactivation products (zinc, cobalt, manganese) produced in theprimary heat transport, moderator, or auxiliary systems. A person-rem is a measure of the total radiationdose received by a group of persons. It is important to note that EMF are presentin many uses of electricity, such as appliances in the home oroffice. In our discussion here, the focus will be on EMF exposuredue to T&D lines. For example, I-O analyses show how the installationof insulation not only affects the contractor making the installation,but also the insulation manufacturer, and all the business thatprovide materials and services to the insulation manufacturer.I-O analyses use a simple model of the economy but offer greatsectoral and regional detail. The percentages presented here are for the portionsof the federal subsidies that went towards the production of electricitywith these fuels. Electricity production in total representedroughly 61 percent of all of the federal subsidies. Recent changes to the federal budget have resultedin less subsidies to conventional energy resources and greatersubsidies to renewable resources and energy efficiency. On theother hand, the subsidies provided in one year results in benefitsto certain resource types for many years. In addition, it wouldbe appropriate to also account for the subsidies provided to energyefficiency. However, these are estimated to be quite small in1989. (ASE and Koplow) This point applies to gas as well. While gasresources are located in politically less sensitive areas thanoil, and gas ownership is less concentrated, its price tends tomove together with that of oil. Defects in, or mishandling of, heating equipmentcauses more residential fires than any know cause (Oak Ridge 1993).In 1988 and 1989, 46 percent and 38 percent of fire deaths werea result of four causes related to loss of utility service (openflame, portable heaters, electrical wiring and cooking equipment)(ECA/IPPS 1991). The 10 percent adder should be applied to allof BECo's programs that serve a significant number of low-incomecustomers. For those programs where low-income customers makeup only a portion of the total participants (e.g., the ResidentialMultifamily program), BECo should scale down the 10 percent adderaccording to the portion of low-income customers that are servedwithin the program. In 2004 the energy savings from the Public HousingAuthority program were roughly 1 percent of the savings from BECo'stotal DSM programs. The high case scenario is based on the premisethat (a) BECo has more low-income customers than those in thePublic Housing Authority program, and that (b) BECo is more aggressiveabout reaching and providing DSM savings to those customers. Most of the literature on this subject focuseson DSM programs targeted to low-income customers. However, somestudies have indicated that low-income customers (or customersreceiving energy assistance) make up roughly 30 to 40 percentof payment troubled customers -- suggesting that other customersmay also be able to improve their payment practices with effectiveDSM programs. (Quaid and Pigg 1991) It is difficult to compare the results of thevarious studies, because the they will be significantly affectedby the methodologies used in the study of the program, the designof the DSM program itself, and the different levels of arrearsexperienced by customers and utilities. The 6 percent adder should be applied to allof BECo's programs that serve a significant number of low-incomecustomers. For those programs where low-income customers makeup only a portion of the total participants (e.g., the ResidentialMultifamily program), the 6 percent adder should be scaled downaccording to the portion of low-income customers that are servedwithin the program. In 2004 the energy savings from the residentialand commercial new construction programs are roughly 12 percentof the savings from BECo's total DSM programs. The high case scenariois based on the premise that (a) BECo's other programs will resultin additional market transformation impacts, and that (b) BECois more aggressive about designing and implementing market transformationprograms. Massachusetts Department of Public Utilities,Investigation into the pricing and ratemaking treatment to beafforded new electric generating facilities which are not QualifyingFacilities, D.P.U. 83-36-F, November 30, 1988, page 21. The valuation of this benefit is problematic,in that it is not a simple observable cost. Rather, its valuationdepends upon the slope of a hypothetical demand curve and variousother controversial assumptions. It is possible that there will be even lowerenvironmental benefits by avoiding other resources such as renewableenergy facilities. We have chosen to focus primarily on energyfrom the NEPOOL margin and new natural gas facilities, becausethese are the predominant resources likely to be displaced byDSM over the short- and medium-term planning horizon. Depending upon the outcome of the EPA's investigationof heavy metals, if regulators were to determine that air toxicemissions should be accounted for in the economic dispatch ofgeneration facilities, then this area should be assigned a highresearch priority. For information contact: Bruce Biewald at Tellus Institute Tellus Institute Home Page Table of Contents WT03-B20-83IA006-000057-B015-681http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/maplist.html 138.80.61.12 19970221184708 text/html 1963HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:17:11 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1792Last-modified: Tue, 09 Jul 1996 02:26:03 GMT A LIST of AVAILABLE MAPS. Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). A LIST of AVAILABLE MAPS. The following maps are presented in this database: Example of Aerogamma Survey Map. Cs contamination of 30-km zone around ChNPP ( Recalculated to 1 May 1994, Related Coordinates). Pu contamination of 30-km zone around ChNPP ( Recalculated to 1 May 1994, Related Coordinates). Sr contamination of 30-km zone around ChNPP ( Recalculated to 1 May 1994, Related Coordinates). Cs contamination of the Europe part of USSR. Cs contamination of Russia. State monitoring net of an environment. Map of 30-km zone with Pu spot ( > 0.1 Ci/km**2). Other illustrative materials are presented in list of pictures, list of charts and list of relations. For more information about each picture, map or chart see introduction to "Polyn" project and contents of data base. Any comments and questions are welcome by Khramtsov Pavel (e-mail address: dobr@kiae.su ) . Access to Data Bases is supported by Relcom (Reliable Communications Asociation). WT03-B20-84IA005-000051-B017-278http://lacebark.ntu.edu.au:80/j_mitroy/sid101/gh1/globalwarming.html 138.80.61.12 19970221151645 text/html 125246HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:46:48 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 125073Last-modified: Tue, 09 Jul 1996 02:25:43 GMT Global warming is good for you Global Warming:A Boon for Humans and Other Animals* Thomas Gale MooreSenior Fellow Hoover Institution				Abstract     Contrary to the doom and gloom scenarios that     environmentalists propound, both evidence and theory suggest     that global warming would in general be beneficial for     mankind.  Simple logic indicates that most of modern man's     activities would be unaffected by warming of 5 to 9 degrees     Fahrenheit.  Agriculture and some services might actually     benefit.  Moreover, past history shows two periods that were     significantly warmer than today and during both eras mankind     flourished.  The first epoch, which has been dubbed by     climatologists "The Climatic Optimum," brought temperatures     which were as warm as the median prediction for the next     century.  During this period, Homo Sapiens shifted from     surviving in small tribes through hunting and gathering to     settled farming communities and from the stone age to the     bronze age.  During the second warming, "The Little Climate     Optimum," Europe enjoyed the High Middle Ages and went on     one of the largest building sprees ever recorded.Why Global Warming Would be Good for You     Climate extremes would trigger meteorological chaos ---     raging hurricanes such as we have never seen, capable of     killing millions of people; uncommonly long, record-breaking     heat waves; and profound drought that could drive Africa and     the entire Indian subcontinent over the edge into mass     starvation. ...  Even if we could stop all greenhouse gas     emissions today, we would still be committed to a     temperature increase worldwide of two to four degrees     Fahrenheit by the middle of the twenty-first century. It     would be warmer then than it has been for the past two     million years. Unchecked it would match nuclear war in its     potential for devastation.1          - Senate Majority Leader George J. MitchellSenator Mitchell's forecast and his history are both wrong. Warmerperiods bring benign rather than more violent weather. Mildertemperatures will induce more evaporation from oceans and thus morerainfall---where it will fall we cannot be sure but the earth as awhole should receive greater precipitation. Meteorologists now believethat any rise in sea levels over the next century will be at most afoot or more, not twenty.2 In addition, Mitchell flunks history:around 6,000 years ago the earth sustained temperatures that wereprobably more than four degrees Fahrenheit hotter than those of thetwentieth century, yet mankind flourished. The Sahara desert bloomedwith plants, and water loving animals such as hippopotamuses wallowedin rivers and lakes. Dense forests carpeted Europe from the Alps toScandinavia. The Midwest of the United States was somewhat drier thanit is today, similar to contemporary western Kansas or easternColorado; but Canada enjoyed a warmer climate and more rainfall.Raising the specter of disaster as well, Vice President Al Gore hascalled the threat of global warming "the most serious problem ourcivilization faces."3 In fact, he has styled those who dispute it as"self-interested" and compared them to spokesmen for the tobaccoindustry who have questioned the relation of smoking to cancer. ButGore is misinformed; many disinterested scientists, includingclimatologists with no financial interest other than preventingwasteful expenditures of society's limited resources, question theevidence and the models that underlie the warming hypothesis.In fact, the evidence supporting the claim that the earth has grownwarmer is shaky; the theory is weak; and the models on which theconclusions are based cannot even replicate the current climate. It isasserted, for example, that over the last hundred years the averagetemperature at the earth's surface has gone up by 0.5 degreesCentigrade or about 1 degree Fahrenheit. Given the paucity of data inthe Southern Hemisphere, the evidence that in the United States, withthe best records, temperatures have failed to rise; the British navalrecords that find no significant change in temperatures at sea sincethe mid- 1800s; and that the reported increases occurred mainly priorto 1940 -- before the rapid rise in CO2 -- the public is entitled tobe wary. Moreover, even the National Academy of Sciences is skepticalof the validity of the computer models and warns that the modeling ofclouds -- a key factor -- is inadequate and poorly understood.4The dire forecasts of global warming hinge on a prediction that humanactivity will provoke a continued upsurge in atmospheric carbondioxide. Many environmentalists believe that the burning of fossilfuels, the release of methane from agricultural activities, and theescape of other chemicals into the air over the next few decades willlead to an effective doubling of greenhouse gases sometime in the nextcentury. Studies of carbon (CO and CO2) in the atmosphere show that attimes in the last 8,000 years the level has been substantially higherthan it is today and greater than it is likely to reach any timesoon. Although fluctuations in CO2 correlate with climate shifts, therecord cannot distinguish whether they followed the temperaturechanges or preceded them.6 Theory suggests either is possible.What is well known is that climate changes. The world has shifted fromperiods that were considerably warmer--during the Mesozoic era whenthe dinosaurs thrived the earth appears to have been about 18 degreesFahrenheit warmer than now--to spells that were substantially colder,such as the Ice Ages when huge glaciers submerged much of the NorthernHemisphere.  One paleoclimatologist estimated that, during thePrecambrian period, the polar regions were about 36 degrees Fahrenheitcolder than they are in the contemporary world.  During the lastinterglacial, about 130,000 years ago or about when modern man wasfirst exploring the globe, the average temperature in Europe was atleast 2 to 5 degrees Fahrenheit warmer than at present.Hippopotamuses, lions, rhinoceroses and elephants roamed the Englishcountryside. Areas watered today by the monsoons in Africa and eastAsia enjoyed even more rainfall then. Indeed during the last 12,000years, that is since the end of the last glacial period, the globe hasalternated between times substantially warmer and epochs that werenoticeably cooler.An examination of the record of the last twelve millennia reveals thatmankind prospered during the warm periods and suffered during the coldones. Transitions from a warm to a cold period or vice-versa weredifficult for people who lived in climates that were adverselyaffected yet benefited others who inhabited regions in which theweather improved. On average, however, humans gained during thecenturies in which the earth enjoyed higher temperatures. In writingabout the effect of climate change on human development, Senator andnow Vice-President Al Gore admits: 	The archaeological and anthropological records indicate 	that each time the ice retreated [during the ice ages], 	the primitive peoples of the Eurasian landmass grew more 	populous and their culture more advanced.  Then, 40,000	years ago, the so-called cultural explosion of tools and 	jewelry may have coincided with an unusually warm millen-	nium in Europe.10		Expected Effects of Global WarmingAlthough most of the forecasts of global warming�s repercussions havebeen dire, an examination of the likely effects suggests little basisfor that gloomy view. Climate affects principally agriculture,forestry, and fishing. Manufacturing, most service industries, andnearly all extractive industries are immune to climateshifts. Factories can be built in northern Sweden or Canada or inTexas, Central America, or Mexico.  Banking, insurance, medicalservices, retailing, education and a wide variety of other servicescan prosper as well in warm climates (with air-conditioning) as incold (with central heating). A few services, such as transportationand tourism, may be more susceptible to weather. A warmer climate willlower transportation costs: less snow and ice will torment truckersand automobile drivers; fewer winter storms - bad weather in thesummer has less disruptive effects and is over quickly - will disruptair travel; a lower incidence of storms and less fog will make watertransport less risky. Hotter temperatures will leave mining and theextractive industries largely unaffected; they might even benefit oildrilling in the northern seas and mining in the mountains. A warmerclimate could, however, change the nature and location of tourism.Many ski resorts, for example, might face less reliably cold weatherand shorter seasons. Warmer conditions would mean that fewernortherners would feel the need to vacation in Florida or theCaribbean. On the other hand, new tourist opportunities might developin Alaska, northern Canada and other locales at higher latitudes or inupper elevations.A rise in world-wide temperatures will go virtually unnoticed byinhabitants of the advanced industrial countries. In his 1991 addressto its members, the President of the American Economic Associationasserted: "I conclude that in the United States, and probably Japan,Western Europe and other developed countries, the impact on economicoutput [of global warming] will be negligible and unlikely to benoticed." As modern societies have developed a larger industrial baseand become more service oriented, they have grown less dependent onfarming, thus boosting their immunity to temperature variations. Warmer weather means, if anything, fewer power outages andless frequent interruptions of wired communications.Only if warmer weather caused more droughts or lowered agriculturaloutput would even Third World countries suffer. Should the worldwarm--and there is little evidence or theory to support such aprognostication--the hotter temperatures would enhance evaporationfrom the seas, producing more clouds and more precipitation world-wide. Although some areas might become drier, others would becomewetter. Judging from history, Western Europe would retain plentifulrainfall, while North Africa and the Sahara might gain moisture. TheMidwest of the United States might suffer from less precipitation andbecome more suitable for cattle grazing than farming. On the otherhand, the Southwest would likely become wetter and better for crops.A warmer climate would produce the greatest gain in temperatures atnorthern latitudes and much less change near the equator. Not onlywould this foster a longer growing season and open up new territoryfor farming but it would mitigate harsh weather. The contrast betweenthe extreme cold near the poles and the warm moist atmosphere on theequator drives storms and much of the earth�s climate. This differencepropels air flows; if the disparity is reduced, the strength of windsdriven by equatorial highs and Arctic lows will be diminished.As a result of more evaporation from the oceans, a warmer climateshould intensify cloudiness. More cloud cover will moderate daytimetemperatures while acting at night as an insulating blanket to retainheat. The Intergovernmental Panel on Climate Change has found exactlythis pattern both for the last 40 years, indeed for the whole of thetwentieth century.12 For the Northern Hemisphere in summer months,daytime high temperatures have actually fallen; but in the fall,winter and spring, both the maximum and especially the minimumtemperatures (nighttime) have climbed.Warmer nighttime temperatures, particularly in the spring and fall,create longer growing seasons, which should enhance agriculturalproductivity. Moreover, the enrichment of the atmosphere with CO2 willfertilize plants and make for more vigorous growth. Agriculturaleconomists studying the relationship of higher temperatures andadditional CO2 to crop yields in Canada, Australia, Japan, northernRussia, Finland, and Iceland found not only that a warmer climatewould push up yields, but also that the added boost from enriched CO2would enhance output by 17 percent.13 Researchers have attributed aburgeoning of forests in Europe to the increased CO2 and thefertilizing effect of nitrogen oxides.14 Professor of ClimatologyRobert Pease writes that we may now be living in an �icehouse� worldand that a warming of about two degrees Celsius, which is what hismodel indicates,	may actually make the earth more habitable. The higher 	temperatures combined with more carbon dioxide will favor 	plant and crop growth and could well provide more food for 	our burgeoning global populations. Geologic history reveals 	that warmer global temperatures produce more, not less, 	precipitation, a fact reflected by a recent scientific 	investigation that shows the Greenland ice-cap to be 	thickening, not melting.  So much for the catastrophic 	prediction that	our coastlines will be flooded by a rise 	in sea level from polar	meltwaters.15The United States Department of Agriculture in a cautious reportreviewed the likely influence of global warming on crop production andworld food prices. The study, which assumed that farmers fail to makeany adjustment to mitigate the effects of warmer, wetter, or drierweather--such as substituting new varieties or alternative crops,increasing or decreasing irrigation--concludes that:	The overall effect on the world and domestic economies 	would be small as reduced production in some areas would 	be balanced by gains in others, according to an economic 	model of the effects of climate change on world agricultural 	markets. The model ... estimates a slight increase in world 	output and a decline in commodity prices under moderate 	climate change conditions.16 [Emphasis added.]Economists Robert Mendelsohn, William D. Nordhous, and Daigee Shawresearched the relationship of climate to land values in the UnitedStates.17 After holding land quality, the proximity to urban areas andthe nearest coast, and income per capita constant, they found thatclimate explained over two-thirds of the value of crop lands.  Theyconcluded that for the lower-48 states, a rise in average temperatureof about 5 degees Fahrenheit and an 8 percent increase in rainfallstemming from global warming would, depending on their model, reducethe value of output between 4 and 6 percent or boost the value ofoutput slightly. This result ignored the effect of increased CO2 onfarm output. It is also consistent with the Department of Agriculturestudy that suggests the U.S. might see a slight fall in output whilethe rest of the world increased production.Forestry is another sector that is potentially subject to change dueto an increase in world temperatures. Canadian agricultural economistshave examined the effect of a doubling of CO2 on forestryproduction. They concluded that increased carbon dioxide would boostproductivity by 20 percent and that overall the harvest of timber inCanada would climb by about 7.5 percent.18			Historical EvidenceHistory provides the best evidence for the effect of climate change onhumans, plants and animals, but a few researchers have challenged itsrelevance. David Rind, a climate modeler and NASA scientist, hasquestioned the applicability of past warming episodes to the modernissue of climatic alteration caused by increased CO2 concentrations.19He attributes the origin of past periods of warmth and cold to shiftsover time in the orbital position of the earth which impose more orless energy on the poles, as contrasted to a general world-widewarming that might result from the addition of man-made greenhousegases. [See Appendix A on factors determining climate]. He also arguesthat the swiftness in warming that would occur following increasedlevels of CO2 is unprecedented in history. On the latter point, heignores other research, such as that by a German academic, BurkhardFrenzel, who writes, "During the Holocene, very rapid changes ofclimate occurred. According to dendroclimatology [tree ring analysisapplied to climatology], they often lasted about 20 to 30 years, or[were] even as brief as 2 to 3 years."20 Other climate historians havefound that a rapid cooling in the late glacial period--about 11,000years ago--took about 100 to 150 years to complete and realized about5 degree Fahrenheit variation in temperature within 100 years, morethan is being forecast for the next century.21Although changes in the earth's orbital position may easily haveplayed a role in warming the earth after the last Ice Age, the effectwas world-wide rather than concentrated in northern latitudes. Iceretreated in the Southern as well as in the Northern Hemisphere.Moreover, in the subsequent warming, from around 7,000 to 4,000 yearsago, the climate around the world appears to have improved. Althoughthe evidence for warming in the Southern Hemisphere is weaker, even ifhigher temperatures had been localized in one hemisphere or onecontinent, the effect on human beings would still tell us about thebenefits or costs of climatic change. Dr. Rind argues that greenhousewarming would raise winter as well as summer temperatures while pastwarmings, driven by orbital mechanics, have raised summer temperaturesalone. Even though his models suggest that these past warmings shouldhave boosted temperatures solely in June, July, and August, theevidence, albeit a little tenuous for the three thousand year periodof Climatic Optimum, supports warmer winters. For the Little ClimateOptimum that coincided with the High Middle Ages, researchers havefound strong support for mild winters.Moreover, at a recent conference the Russians have put forward thehypothesis that past climate changes support the proposition that thecause of the warming or cooling is irrelevant; the pattern has beenthe same.22 This conclusion, disputed by some, is based on a largenumber of past shifts in average weather conditions dating backmillions of years. The Russians contend that the climate modelsoverstate the amount of temperature change at the equator andunderstate it at the poles.			Measurement of Human Well-beingSince statistics on the human condition are unavailable except for themost recent centuries, I shall use indirect methods to demonstrate theinfluence of climate on man�s well-being. A growth in the population,major construction projects, a significant expansion in arts andculture, all indicate that society is prosperous. If the population isexpanding, food must be plentiful, disease cannot be overwhelming, andliving standards must be satisfactory. In addition, if building, art,science, and literature are vigorous, the civilization must beproducing enough goods and services to provide a surplus available forsuch activities. Renaissance Florence was rich; Shakespeare flourishedin prosperous London; wealthy Vienna provided a welcome venue forHaydn, Schubert, Mozart, and Beethoven.Clearly climate is far from the only influence on man'swell-being. Governments that extort too much from their peopleimpoverish their countries. A free open economy stimulates growth andprosperity. War and diseases can prove catastrophic. On the otherhand, a change in climate has frequently been a cause of war or aidedthe spread of disease. A shift to more arid conditions, for example,impelled the Mongols to desert their traditional lands to invadericher areas. A cold wet climate can also confine people to closequarters, which can abet contagion. Moreover, a shift towards a poorerclimate can lead to hunger and famine, which make disease morevirulent.Throughout history climatic changes probably forced technologicalinnovations and adaptations. The shift from warm periods into Ice Agesand back again likely accelerated the evolution of modern man. Eachshift would have left small groups of hominoids isolated and subjectto pressures to adapt to new weather conditions. These shifts,especially to the more adverse conditions created by the spread ofextreme cold, would put strong selection pressure on the humanforebears that ultimately led to modern man. Even after Homo Sapiensstarted spreading across the earth, climate shifts fostered newtechnologies to deal with changed circumstances.The influence of climate on human activities has declined with thegrowth in wealth and resources. Primitive man and hunter-gatherertribes were at the mercy of the weather, as are societies which arestill almost totally bound to the soil. A series of bad years can bedevastating. If, as was the usual case until very recently,transportation is costly and slow, even a regionalized drought or anexcess of rain can lead to disaster, although crops may be plentiful ashort distance away. Thus variation in the weather for early man had amore profound influence on his life and death than do fluctuations intemperature or rainfall in modern times when economies are moredeveloped. Since the time of the Industrial Revolution, climate hasbasically been confined to a minor role in human activity.				Climate HistorySince its origins, the earth has experienced periods significantlywarmer than the modern world - some epochs have been even hotter thanthe most extreme predictions of global warming - and times much colderthan today. Today�s cool temperatures are well below average for theglobe in its more than four billion year history.23 During one of thewarmest such eras the dinosaurs roamed the earth and a rich ecologicalworld flourished.Studies of climate history show as was mentioned above that sharpchanges in temperatures over brief periods of time have occurredfrequently without setting into motion any disastrous feedback systemsthat would lead either to a runaway heating that would cook the earthor a freezing that would eliminate all life. In addition, carbondioxide levels have varied greatly. Ice core data exhibit fluctuatinglevels of CO2 that do not correspond to temperature changes.24 Mostpast periods display a positive relationship between CO2 andtemperature, however, with a relationship roughly corresponding tothat of the Global Climate Models.25 During interglacial periods highlatitudes enjoyed temperatures that were about 5 to 11 degreesFahrenheit warmer than today.26 Middle latitudes experiencedtemperatures only about 4 to 5 degrees Fahrenheit warmer. These warmerperiods brought more moisture to the Northern Hemisphere with theexception during the Holocene of central North America. At the time ofthe medieval warm period, temperatures in Europe, except for the areaaround the Caspian Sea basin, were 1 to 3 degrees Fahrenheit higherand rainfall more plentiful than today.27This historical evidence is consistent with only some of the forecastsof the computer climate models. Most climate estimates indicate that adoubling of CO2 would generate greater rainfall in middle latitudes,and history shows that warm climates do produce more wet weather.28 Ashas been found in the historical record, land temperatures shouldincrease more than water thus strengthening monsoons. The models alsopredict that sea-surface temperatures in the tropics would be higherwith increased CO2 but evidence from the past evinces no suchrelationship.29Carbon dioxide concentrations may have been up to sixteen times higherabout 60 million years ago without producing runaway greenhouseeffects.30 Other periods experienced two to four times current levelsof CO2 with some warming. Scientists have been unable to determinewhether the warming preceded or followed the rises in carbondioxide. For virtually all of the period from around 125 million toabout 75,000 years ago, CO2 levels were markedly higher than now.The prevailing view among climatologists is that the Climatic Optimum--9,000 to 4,000 years ago--resulted from orbital mechanics whichincreased summer radiation in the Northern Hemisphere, althoughwinters received less heat than they do in the modern world.31 Thewarmer summers melted the northern glaciers over severalmillennia. Warmer lands in the interior of northern continents andcooler oceans expanded the monsoons further north to bring greaterrainfall to the Sahara, Arabia and southern and eastern Asia.32 Northof the monsoon area, the climate was drier than today. Anatolia,Northwestern Africa, parts of China and northern Japan experiencedless rainfall.33 By 4000 B.C., however, a slackening of the tradewinds had produced warmer Atlantic ocean water off northwesternAfrica, and as a consequence the Middle East, including Greece andmodern Turkey, were enjoying more reliable rain.If orbital variations produced the Climatic Optimum, the SouthernHemisphere should have been cooler. Between 10,000 B.C. and 7000 B.C.,however, winter temperatures (June, July, August) below the equatorwarmed to higher levels than now while summer temperatures (December,January, February) were cooler than the modern world.34 Rainfall overSouth America, Australia and New Zealand apparently was lighter thanthe present. Although the Southern Hemisphere moved out of the Ice Agewith the Northern Hemisphere, its climate since then has not trackedwell weather patterns north of the equator.35 Data based on vegetationsuggest that annual temperatures in New Zealand were coldest between20,000 and 15,000 years ago, warmed subsequently and peaked between10,000 and 8,000 years before the present - somewhat earlier than theydid in the Northern Hemisphere.36 Temperatures appear to have beenfalling over the last 7,500 years. By 1500 B.C., the climate was quitesimilar to today�s.37Whether the whole globe warmed or not during the period 7,000 to 4,000years ago is really irrelevant to the question of how hottertemperatures affect humans. If the Northern Hemisphere warmed, andthere is good evidence that it did, then comparing how people survivedin that portion of the globe provides information about how higherglobal temperatures would influence mankind.Modern man apparently evolved into his current genotype between 40,000and 200,000 years ago, probably in Africa during an Ice Age.38 Around150,000 years ago the extent of ice coverage reached a maximum,followed around 130,000 years before the present (YBP) by a rapiddeglaciation.39 The warm interglacial era, during which temperaturesmay have exceeded those forecast under a doubling of greenhouse gases,lasted about 15,000 years until the onset of renewed glaciation at115,000 YBP. Over the next 100,000 years the glaciers fluctuated withthe climate, but at no time did the average temperature equal thelevel of the previous interglacial epoch or reach the warmth of thelast 10,000 years.40In the thousands of years of the last Ice Age preceding the currentwarm epoch, man existed as a hunter-gatherer in a world that lookedquite different from today�s.  Herds of large animals such as bison,mammoths, and elk roamed a largely treeless savanna in Europe. Thesebeasts made easy prey for human hunters that enjoyed as a consequencea rich diet of wild animal meat plus, in season, local fruits andvegetables. It was during the Ice Age that the level of the ocean fellsufficiently that Asian peoples were able to migrate across what isnow the Bering Strait but then was dry land. Most archaeologists datethe first arrivals of humans in the Americas from around 15,000 yearsago, although some have claimed evidence for an earlier arrival. Nodoubt the lower sea levels during the Ice Age also facilitated thearrival of the aborigines in Australia some 35,000 years ago.Climatologists consider that the last Ice Age ended about 12,000 to10,000 years ago when the glaciers covering much of North America,Scandinavia and northern Asia began to retreat to approximately theircurrent positions. In North America the glacial covering lasted longerthan in Eurasia because of topographic features that delayed thewarming. Throughout history warming and cooling in different regionsof the world have not been exactly correlated because of the influenceof oceans, mountains, prevailing winds, and numerous otherfactors. Nevertheless, across the Northern Hemisphere largetemperature shifts have occurred roughly together - perhaps in someareas they have lagged other zones by a century or more. Thecorrespondence between warming and cooling in the Northern Hemisphereand that in the Southern is less well known and may be less wellcorrelated because of the predominance of water south of the equatorand the existence of Antarctica.Human progress, a few improvements in hunting tools and some cave art,was incredibly slow during the Ice Age - a period whose length dwarfsthe centuries since.  Over the last 12 millennia of interglacialwarmth, however, modern man has advanced rapidly. The growth intechnology and living standards required a climate that was morehospitable than existed throughout that frozen period.During the last Ice Age humans survived through hunting andgathering. Initially archeologists believed that these tribes, whichtypically consisted of 15 to 40 people, eked out a precariousexistence.41 Many modern archeologists, however, feel, based onstudies of the few bands of hunter-gatherers that survived into thetwentieth century, that they normally found plentiful foods in theirforays and would rarely have been hungry.  Modern primitive people,however, may not have been typical of earlier groups. The ones thatdid face food pressures would have adopted farming while those thatfound plentiful supplies in their environment would be less concernedwith new ways of acquiring sustenance.42 Food pressures could havearisen from either a change in climate that made previous ways-of-lifeuntenable or an expansion of population in the region that began tooverwhelm the natural supply.As the earth warmed with the waning of the Ice Age, the sea level roseas much as 300 feet; hunters in Europe roamed through modern Norway;agriculture developed in the Middle East. For about 3,000 to 4,000years the globe enjoyed what historians of climate call the ClimaticOptimum period--a time when average world temperatures--at least inthe Northern Hemisphere - were significantly hotter than today. At itsheight between 4000 B.C. and 2000 B.C., H.H. Lamb, a leading climatehistorian, judges that the world was 4 to 5 degrees Fahrenheit warmerthan the twentieth century.43 During the relatively short period sincethe end of glaciation the climate has experienced periods of stabilityseparated by "abrupt transition."44 Lamb calculates that at itscoldest, during the Mini Ice Age, the temperature in central Englandfor January was about 4.5 degrees Fahrenheit colder than today.45 Healso concludes that in the central and northern latitudes of Europeduring the warmest periods, rainfall may have been 10 to 15 percentgreater than now and during the coldest periods of the Mini Ice Ages,5 to 15 percent less.46 On the other hand, cooler periods usuallysuffered from more swampy conditions because of less evaporation.If modern humans originated 200,000 years ago, why did they notdevelop agriculture for the first 190,000 years? Even if Homo Sapiensoriginated only 40,000 years ago, people waited 30,000 years to growtheir first crops--an innovation which yielded a more reliable andample food supply. Farming developed first in the Middle East, rightafter the end of the last Ice Age--a coincidence? The evidencesuggests that from 11,000 to 9,000 years ago the climate became warmerand wetter in the Middle East shifting the ecology from steppe to openwoodland.47 This led to the domestication of plants and animals,probably because the warmer, wetter weather made farmingpossible. From its origins around 8000 B.C., agriculture spreadnorthward, appearing in Greece about 6000 B.C., Hungary 5000 B.C.,France 4500 B.C. and Poland 4250 B.C.48 Is it chance that thisnorthward spread followed a gradual warming of the climate that madeagriculture more feasible at higher latitudes?As Anthropologist Mark Cohen writes, "If, as the archaeological recordindicates, hunting and gathering was such a successful mode ofadaptation over such a long period of time, and if most humanpopulations are as conservative as anthropologists have observed themto be, we are faced with answering the question why this form ofadaptation was ever abandoned."49 He gives estimates of the efficiencyof hunting and gathering that indicate that the latter was moreefficient than farming--at least for large game. He reports that whenlarge animals are available, hunting brings 10,000 to 15,000kilocalories per hour of hunting. However, if large animals areunavailable--because the environment is poor or because they have allbeen killed--hunting of small game will return only a few hundred to1,500 kilocalories per hour devoted to the effort. Collecting andprocessing small seeds from such plants as wild wheat may produce only700 to 1,300 kilocalories for each hour. Shellfish collection canproduce 1,000 to 2,000 kilocalories per hour of work. On the otherhand subsistence farming produces 3,000 to 5,000 kilocalories per hourdevoted to agriculture.50 This connotes that hunting large animals,when and if they are available, is the most economical method ofsubsistence, but if these beasts are exterminated or if the humansmove to areas without such species, domestication of plants andanimals can produce more food for the effort than any other strategy.Moreover hunter-gatherers can only survive if the density of theirpopulation is low. Too many mouths would strain the environment andpreclude survival. Once, humans developed farming which could supportlarger families and a denser population, however, the number of peopledid explode. Primitive tribes, dependent on hunting, scavenging, andcollecting edibles to survive, had to hold their populations belowwhat they would individually have preferred or nature kept them incheck through periodic food shortages. A number of twentieth centuryhunter-gatherers have practiced infanticide and induced abortions torestrict the number and spacing of their children.51 Constant travelby nomads may increase infant mortality, maternal mortality andproduce more miscarriages than a sedentary life and thus have kept thenumbers in check. In any case farming solved a major problem forprimitive peoples. Once people settled down into fixed abodes, thepopulation apparently ballooned.Although many people view the current world�s huge population withalarm, most ecologists take the size of the population of a species asan indicator of its fitness. By this criterion, the domestication ofplants and animals improved greatly Homo Sapiens fitness. This essayis not the place to discuss the capacity of the globe to sustain thenumber of people expected to populate the world in the next century,but certainly anything that produced greater numbers of peoplethousands of years ago must have been beneficial for mankind.Over history the number of humans has been expanding at ever morerapid rates.  Around 25,000 years ago, the world's population may havemeasured only about 3 million.52 Fifteen thousand years later, around10,000 B.C., the total had grown by one-third to 4 million. It took5,000 more years to jump one more million, but in the 1,000 yearsafter 5000 B.C. it added another million. Except for a few disastrousperiods, the number of men, women and children has mounted withincreasing rapidity. Only in the last few decades of the twentiethcentury has the escalation slowed. Certainly there have been goodtimes when man did better and poor times when people suffered--although in most cases these were regional problems. However, as thefollowing chart shows, in propitious periods, that is, when theclimate was warm, the population swelled faster than during lessclement eras.This chart is based on a paper by economist Michael Kremer who arguesthat, until the Industrial Revolution, existing technology limited thesize of the population.53 As innovators discovered new techniques andinvented new tools, more people could be fed and housed and thepopulation expanded. Moreover, the greater the number of people, themore innovations would be hit upon. He assumed that every individualhad an equal but very small probability of uncovering a new techniqueor device and that the probability of being an innovator wasindependent of the size of the population.  Therefore the number ofinventions would be proportional to the number of people. Thus as theworld population expanded--slowly at first--the rate oftechnological innovation escalated and hence the rate of growth of thepopulation that could be sustained. Only in recent times hastechnological change become so rapid that it has run ahead ofpopulation growth, leading to a rising standard of living, which inturn has reduced the birth rate.                                                             		-----------------------------			CHART MISSING		-----------------------------Source: Michael Kremer (August 1993): Table 1 and the Author.Kremer's hypothesis signifies that for most of history the rate ofpopulation growth should be proportional to the size of thepopulation. To link his model and data with climate change, I startedwith his estimate of the world�s people in 10,000 B.C. and calculatedthe rate of growth of the population over the next 5,000 years. Foreach subsequent period, I also computed the rate of increase innumbers of people. Comparing these expected rates with actual growthrevealed eras in which the number of humans has expanded faster thanpredicted and periods during which the world�s people has grown moreslowly. The chart then shows the centuries in which the growth rate ofthe globe�s populace has exceeded or fallen short of the rate expectedunder this simple model. As can be seen, warm periods have doneconsiderably better than cold periods in terms of human expansion. Thewarmest period since the end of the last Ice Age produced the highestrate of population growth compared to what would have been expected--in this era agriculture was spreading. Moreover, the Mini Ice Age,which saw the coldest temperatures in the last 10,000 years, underwentthe slowest relative population expansion. This chart demonstratesthat mankind has prospered in warm periods and the hotter the better!Another measure of the well-being of humans is how long they live. Thelife of the hunter-gatherer was not as rosy as some have contended.Life was short--skeleton remains from before 8000 B.C. show that theaverage age of death for men was about 33 and that of women 28.54Death for men was frequently violent, while many women must have diedin childbirth. Since women died so young, they had only aroundthirteen years in which to bear children. Anthropologists haveestimated that on average they could have given birth to less thanfive live babies, assuming that they bore a child every 22 months.55An infant and childhood mortality rate of about 60 percent would havekept the population stagnant.Table 1 below shows some relevant data. The warmest periods, theNeolithic, Bronze Ages and England in the thirteenth centuries enjoyedthe longest life spans of the entire record. The shortening of livesfrom the late thirteenth to the late fourteenth centuries with theadvent of much cooler weather is particularly notable. Moreover, therise in life expectancies during the warm period could easily explainthe population explosion that took place during that period.				Table 1		Life Expectancy at Various Periods 			Mesolithic people in Europe	31.5	Neolithic, Anatolia		38.2	Bronze Age, Austria		38	Classical Greece		35	Classical Rome			32	England 1276 A.D.		48	England 1376-1400		38	Source: Lamb [1977]: 264 from Comfort [1969].	Good childhood nutrition is reflected in taller adults. Skeletonremains collected over wide areas of Eurasia from the period whenroving bands shifted from eating large animals and a few plants tosmaller prey and a much wider variety of foods attest to a decline inheight for both men and women of about five centimeters (twoinches).56 The shorter stature came at the end of the Ice Age whenlarge animals were disappearing.  Some archaeologists have found thataverage age of death for adults also declined during thistransitionary period.57 Studies of bone chemistry from Middle Easternskeletons indicate a reduction in meat consumption. The new dietalthough more dependent on grains, fruits, and vegetables must havebeen less nutritious than the old. As large game animals disappearedwith the end of the Ice Age, humans widened the variety of plants intheir diet, increasingly consuming vegetable matter that they hadignored for thousands of years either because it was less nutritious,more difficult to secure and process, or less tasty.			Table 2		Average Height of Icelandic Males				Period (A.D.)		Mean Height 		Medieval Warmth		874-1000		68"	1000-1100		68"		Mini Ice Age		1650-1796		66"	1700-1800		66"		Modern World		1952-1954		70"	Source: Lamb [1977]: 264 from Bergthorsson [1962].		Research on American Indians before the arrival of Europeans alsoreveals a decline in health between early periods and later.58 Theevidence for the Americas is more mixed, however, than forEurope. Based on the Eurasian studies and those of North Americanaborigines it seems safe to conclude that health and nutrition weredeclining before the advent of agriculture and that it may be thatagriculture was invented to stave off further decreases in foodavailability. The absence of agriculture for most North Americanpeoples may have reflected that their nutrition fell less than it didin Europe.In southern Europe, the shift to agriculture coincides with areduction in skeleton size of 3 centimeters (1.2 inches) for men and 4centimeters (1.6 inches) for women.59 Although some otherarchaeological studies have found that agriculture led to shorterpeople, a few have found the reverse. In Israel, for example, onestudy found that people grew taller with the domestication ofanimals.60 Overall the evidence supports the view that the diet mayhave become less nutritious with the shift from large animal huntingto food production but that its quality initially exceeded that ofmedieval Europe. Table 2 on heights, however, signifies that food wasmore plentiful and better during the medieval Period than during themini Ice Age.In summary, the evidence overwhelmingly supports the proposition thatduring warm periods, humans prospered. They multiplied more rapidly;they lived longer; and they apparently were healthier. We now turn toa closer examination of the two major warm epochs.			The First Climatic Optimum Around 9,000 to 5,000 years ago the earth was much warmer than today;perhaps 4 degrees Fahrenheit hotter, about the average of the variouspredictions for global warming after a doubling of CO2.61 Although theclimate cooled a bit after 3000 B.C., it stayed relatively warmer thanthe modern world until sometime after 1000 B.C., when chillytemperatures became more common. During this Climatic Optimum epoch,Europe enjoyed mild winters and warm summers with a storm belt far tothe north. Not only was the country less subject to severe storms, butthe skies were less cloudy and the days sunnier.Notwithstanding the less stormy weather, rainfall was more thanadequate to produce widespread forests. Western Europe, includingparts of Iceland and the Highlands of Scotland, was mantled by greatwoods.62 The timber, until average temperatures dipped temporarily forabout 400 years between 3,500 B.C. and 3,000 B.C., consisted ofwarmth-demanding trees, such as elms and linden in North America andoak and hazel in Europe. These species have never regained their oncedominant position in Europe and America. Not only did Europe enjoy abenign climate with adequate rainfall, but the Mediterranean littoral,including the Middle East, apparently received considerably moremoisture than it does today.63 The Indian subcontinent and China werealso much wetter during this Optimal period.64As a Senator, Al Gore, writing on the prospect of further globalwarming and its potential harm, contended that the temperature riseover the last century has led to increased drought in Africa.65 Tobolster his argument, he presented a chart which shows a drop inrainfall from 1930 to the early 1980s for portions of sub-SaharanAfrica. His conclusion, however, is based on a false premise: for mostof that period the earth was cooling, not warming! His chart actuallyimplies that further cooling would be undesirable. In fact, historydemonstrates and climatology attests that warming should drive themonsoon rains that originate near the equator farther north, possiblyas far as the Sahara, contributing to a moister not a drier climate!Compared to cooler periods in the last few thousand years, the Saharawas much wetter and more fertile during the Climatic Optimum.66 Cavepaintings from the epoch depict hippopotamuses, elephants, crocodiles,antelopes and even canoes.67 The water level in Lake Chad about 14degrees north of the equator in central Africa was some 30 to 40meters, that is, 90 to 125 feet higher, than it is today, indicatingmuch greater precipitation. Ruins of ancient irrigation channels inArabia, probably from the warmest millennia, derived their water fromsources well above current water supplies, indicating a wetterclimate.68 A warming would likely lead to similar conditions, not astrengthening of African drought. With the cooling that started after3000 B.C., North Africa dried up and the abundance of lifedisappeared.Research has shown, however, that some portions of the globe didsuffer from drier conditions. The Caspian Sea may have been at itslowest level in over 80,000 years during the warmest recent period--4,000 to 6,000 years ago--when it was some 20 to 22 meters--66 to 72feet--below its modern height.69The Southern Hemisphere seems to have flourished as well during thewarm millennia after the most recent Ice Age. Professor Lamb reportsthat the southern temperate zone enjoyed both warmer weather and moremoisture than it does currently.70 Scholars have found that Australiawas consistently wetter than today in both the tropical and temperateregions.71 Since the end of that epoch, the great deserts of Australiahave expanded and the climate has become both cooler anddrier. Apparently most of the other great desert regions of the worldenjoyed more rainfall during the Climatic Optimum than they donow. Lamb contends that the period of temperature maximum was also aperiod of moisture maximum in subtropical and tropical latitudes and agood period for forests in most temperate regions.72 During this warmera, Hawaii experienced more rainfall than in the twentieth century.73Even Antarctica enjoyed warmer weather, about 4 to 5 degreesFahrenheit higher, and during the summer in some of the mountains theweather was warm enough to produce running streams and lakes whichhave subsequently frozen.74 Nevertheless, the basic ice sheet remainedintact.As already mentioned, the invention of agriculture coincided with theend of the last Ice Age and the melting of the glaciers.Archaeologists have found the earliest evidence for husbandry andfarming in Mesopotamia around 9000 B.C.75 As the earth warmed, theMiddle East became wetter and the Iranian plateau shifted from an opendry plain with roving bands of game to a more wooded environment withless reliable food sources and a diminished supply of large animals.No one really knows how man first domesticated plants and animals, butthe coincidence in time and the forcing nature of climate changesuggest that the warmer wetter weather, especially in the mountains,may have encouraged new techniques.The transition from the Ice Age to a warmer climate that ledeventually to agriculture is best documented in Europe. During thecold period, most of Europe was a dry plain, an open savanna, in whichlarge herds of reindeer, mammoths, and bison roamed. As has been shownby the cave drawings in France and Spain, the population secured agood living by preying on these ungulates. As the climate warmed andas rainfall increased, forests spread north limiting the habitat forthese large mammals. This forced humans into following northward thedwindling herds or developing new sources of food. As the largeanimals disappeared the local people shifted to exploiting red deer,wild boar and smaller species. Those located near the seas or largerivers found seafood a plentiful source of substance. On the otherhand, people who made their living at the edge of the ocean faced seasthat were rising about 3 feet each century and which often drownedthem when high tides and storms washed over their primitive villages.The domestication of plants appears to have occurred around the worldat about the same time: from 10,000 YBP to 7,500 YBP.76 The earliestwell documented employment of agriculture arose in the MiddleEast. Planting of wheat and barley began in southwest Asia between8000 B.C. and 7000 B.C. In north China�s Shensi Province between 4500B.C. and 3500 B.C., peasants grew foxtail and millet and raisedpigs. Food production in this part of China extends back at least intothe sixth millennium B.C. In the Americas domestication of some grainsand chili peppers dates from between 7000 B.C.  and 6000 B.C.;anthropologists have documented maize in the Tehuacan Valley by 5700B.C. and production may have started earlier. In South America theevidence suggests that domestication of two species of beans and chilipepper as well in the Andean highlands arose 8,500 years ago. Maizeappears in this area only about 3000 B.C. In Africa the evidenceimplies the cultivation of plants after 3500 B.C. Domestication ofcattle occurred in the Sahara about 8,000 years before the present.77As Professors Ammerman and Cavalli-Sforza put it, "One of the fewvariables that would seem to be shared is timing: early experiments atplant domestication occurred in southwest Asia, east Asia, and CentralAmerica during the period between 8000 B.C. and 5500 B.C."78 Thecoincidence of the invention of agriculture with a general warming ofthe climate, an increase in rainfall, and a rise in carbon dioxidelevels, all of which would have made plant growth more vigorous andmore plentiful, cannot be accidental.Domestication of plants and animals represented a fundamental shift inman's involvement with nature. Prior to this humans simply took whatnature offered. People hunted or scavenged the local animals thathappened their way. Women gathered fruits and vegetables that grewwild in their territory. With farming and herding, mankind, for thefirst time, began to modify his environment. Humans determined whatwould be grown, which plants would survive in their gardens, whichanimals would be cultivated and bred, and which would be shunned oreliminated. "Homo Sapiens" ceased being simply another species thatsurvived by predation coupled with grazing and became a manager of hisenvironment.The shift from a hunter-gatherer existence to a sedentary one may bethe most important innovation in human existence. Prior to thischange, humans lived in small groups and moved frequently with theseasons to find new sources of meat, fruit and vegetables. Beingmobile meant carrying few goods and only those that were light and notfragile. Thus pottery, which is both heavy and easily breakable, wasnot part of their culture. Any musical instruments must have beensmall and portable. Many small children would have been a hindrance aswould elderly feeble individuals. Such small groups would have hadlittle opportunity to develop specialization. Virtually all males musthave participated in the hunt while all females, not giving birth orcaring for infants, must have helped gather edibles. These tribal orfamily groups could not have supported elaborate priesthoods,bureaucratic governmental structures, or even people who specializedin artistic, cultural or intellectual activities. As a consequencethese societies were probably quite egalitarian with only a few, suchas the chief or elder and perhaps a medicine man, that stood out fromthe rest.The development of agriculture and the establishment of fixedcommunities led to a population explosion and the founding ofcities. Agricultural societies produce enough surplus to support suchurban developments, including the evolution of trades and newoccupations. A large community could afford to have specialists whomade farm tools, crafted pots, and traded within the village andbetween the locals and outsiders. The people in today's Palestineestablished the first known city, Jericho, and thus the first steptowards specialization--which lies at the heart of economicadvancement - around 8000 B.C.79Farming required the development of property rights in lands, althoughinitially pastures may have been held in common. Even though in thebeginning farm holdings were probably fairly equally distributed, overtime some families must have acquired larger holdings thanothers. This increase in income inequality may offend modernsensibilities, but it provided a major benefit. A wealthy class or arich ruler could afford to maintain individuals who would createdesirable objects, such as art, elaborate pots, and musicalinstruments, and who could record eclipses, star movements or tradewith other centers.Man's taming of animals and plants represents a movement towardsestablishing property rights. In a hunter-gatherer's world no one ownsthe wild beasts or the fruit and grains until they are collected. Thiscan work satisfactorily only as long as demands for the resources arequite limited. But as the literature on the tragedy of the commonsshows, once pressures for more grow too large, the resource base canbe exhausted. In what is now called North America, many large species,such as horses, were apparently hunted to extinction. Domestication--privatization of animals and plants--became the answer to overhunting and over grazing.In Europe, the Optimum period produced an expansion of civilizationwith the construction of cities and a technological revolution. TheBronze Age replaced the New Stone Age.80 The more benign climate withless severe storms encouraged travel by sea.Trade flourished during this warm period. People from ancient Denmarkshipped amber along the Atlantic coast to the Mediterranean. As earlyas 2000 B.C., the Celts apparently were sailing from Cornwall andBrittany to both Scandinavia and southern Italy. Astrologicalmonuments built around this time, such as Stonehenge, indicate thatthe skies were less cloudy than now.81 With the glaciers in the Alpsduring the late Bronze Age being only about 20 percent of the size ofthe ice in the nineteenth century, merchants made their way throughthe Brenner Pass, the dominant link between northern and southernEurope. Northern Europeans exchanged tin for manufactured bronze fromthe south. Alpine people mined gold and traded it for goods craftedaround the Mediterranean. Baltic amber found its way to Scotland.During the warm period prior to 3000 B.C., China enjoyed much warmertemperatures. In particular midwinters were as much as 9 degreesFahrenheit hotter and rice was planted a month earlier than is nowcommon.82 Bamboo, valued for food, building material, writingimplements, furniture and musical instruments, grew much farthernorth--about 3 degrees in latitude--than is now possible.83 Chinesearchaeologists have found evidence in a district near Sian that theclimate 5,000 to 6,000 years ago was warmer and wetter than thepresent.Prior to around 2500 to 1750 B.C., northwestern India, which is nowvery dry, enjoyed greater rainfall than it does in the twentiethcentury.84 In the Indus Valley, the Harappas created a thrivingcivilization that reached its apogee during the warmest and wettestperiods, when their farmers were growing cereals in what is now adesert.85 The area was well watered with many lakes. This civilizationdisappeared around 1500 B.C. at a time when the climate becamedistinctly drier.86 The earth was cooling. Historians andarchaeologists also attribute the failure of this civilization to pooragricultural techniques that may have exacerbated drought.Virtually all change can make some worse off and the warming after thelast Ice Age is no exception. Although as the population explosionindicates most humans benefited, the growing warmth harmed somepeople, especially those who lived near the coast or who had earnedtheir living hunting large animals. As the ice sheets melted, the sealevel rose sharply and probably peaked around 2000 B.C.87 During themany centuries in which the waters mounted, storms often led to oceanflooding of coastal communities.  A few times each century, peoplewere forced to abandon well-established villages and move to higherground.		Cooler, More Varied, and Stormy Times From the end of the Optimum period of sustained warmth until around800 A.D. to 900 A.D., what we know of the world's climate and, inparticular, the European varied between periods of warmth andcold. Based on the height of the upper tree lines in middle latitudes'mountains, the temperature record following the peak warm periodaround 5000 B.C. demonstrates a more or less steady decline right upto the 20th century.88 As mentioned above, tree ring data for NewZealand indicate that after temperatures reached a maximum around 6000to 8000 B.C., the climate cooled in that part of the world.After 1000 B.C. the climate in Europe and the Mediterranean cooledsharply and by 500 B.C. had reached modern average temperatures.89 Theperiod from 500 B.C. to 600 A.D. was one of varied warmth, althoughcooler on average than the previous 4,500 years.  However, the climatebecame more clement and somewhat more stable from 100 B.C. to 400A.D., the period of the Roman Empire.90 The Italians grew grapes andolives farther north than they had prior to this period. During thesecenturies of varied weather, Classical Greece flourished and thendeclined; the Roman Empire spread its authority through much of whatis now Europe, the Middle East and North Africa, only to be overrun bybarbarians from central Asia whose eruption out of their homeland mayhave been brought on by a change in the climate.The cooler climate after the start of the last millennium B.C. appearsto have contributed to a southern migration of people from northernEurope.91 Archaeologists have also found evidence that Greeks adoptedwarmer clothing after 1300 B.C. The population living in the Alpsdiminished sharply with the cooler weather and miningceased. Classical historian Ray Carpenter attributes a depopulation ofGreece and Turkey between 1200 and 750 B.C. to long term drought thatmust have reflected the increased coolness of the climate.92Evidence for a cooler Mediterranean climate from 600 B.C. to 100B.C. comes from remains of ancient harbors at Naples and in theAdriatic which are located about one meter (three feet) below currentwater levels.93 Further support for lower sea levels has been found onthe North African coast, around the Aegean, the Crimea, and theeastern Mediterranean. Lower oceans imply a colder world leading to abuild-up of snow and ice at the poles and in major mountainglaciers. By 400 A.D., however, temperatures had warmed enough toraise water levels to about three feet above current elevations. Theancient harbors of Rome and Ravenna from the time of the Roman Empireare now located about one kilometer from the sea.94 Evidence existsfor a peak in ocean heights in the fourth century for points as remoteas Brazil, Ceylon, Crete, England, and the Netherlands, indicating aworld-wide warming.Changes in the climate in Eurasia appear to have played a major rolein the waves of conquering horsemen who rode out of the plains ofcentral Asia into China and Europe. Near the end of the Roman Empire,around 300 A.D., the climate began to warm and conditions in centralAsia improved apparently leading to a population explosion.95 Thesepeople, needing room to expand and a way to make a living, invaded themore civilized societies of China and the West. The medieval warmthalso seems to have also triggered an expansion from that area. Duringthis second optimum period, the homeland of the Khazars centeredaround the Caspian Sea enjoyed much greater rainfall than earlier orthan it does now. The increased prosperity in this area produced arapidly rising number of young men that provided the manpower forGenghis Khan to invade China and India and to terrorize Russia and theMiddle East.96After 550 A.D. until around 800, Europe suffered through a colder,wetter, and more stormy period. As the weather became wetter, peatbogs formed in northern areas.97 The population abandoned manylakeside dwellings while mountain passes became choked with ice andsnow, making transportation between northern Europe and the southdifficult. The Mediterranean littoral and North Africa dried up,although they remained moister than now.Inhabitants of the British Isles between the seventh and the ninthcenturies were often crippled with arthritis while their predecessorsduring the warmer Bronze Age period suffered little from such anaffliction. Although some archaeologists have attributed thedifficulties of the dark age people to harder work, the cold wetclimate between 600 and 1000 A.D. may have fostered such ailments.98During the centuries after the fall of the Roman empire and with thedeterioration of the climate, Greece languished. In 542 A.D., thepopulation was decimated by the plague, aggravated by cold dampconditions; the Black Death struck again between 744 and 747.99 As aconsequence the number of people was sharply reduced. Greece waspartially re-populated in the ninth and tenth centuries when theByzantine Emperors brought Greek settlers from Asia Minor back intothe area. For the first time in centuries Greek commerce andprosperity returned--probably due to an improved climate.100In the ninth century, land hunger and a rising population in Norwayand Sweden spurred the Scandinavians on to loot and pillage bysea. Their first descent was on the monastery of Lindisfarne innorthern England in 793. This was followed by raids on Seville inMuslim Spain in 844 and later farther into the Mediterranean.101 In870 they discovered Iceland and in the next century, Greenland. In 877they began an invasion of England and conquered from the north to thewhole of the midlands--all of which became a Danish overseas kingdomby the mid-tenth century. At the same time, they stormed France andthe king had to cede them Normandy as a fief. They also crossed theBaltic (known as Rus in that time) and sent traders south to Islam andByzantium.		The High Middle Ages and Medieval WarmthFrom around 800 A.D. to 1200 or 1300, the globe warmed considerablyand civilization prospered. This Little Climate Optimum generallydisplays, although less distinctly, many of the same characteristicsas the first climate optimum.102 Virtually all of northern Europe, theBritish Isles, Scandinavia, Greenland, and Iceland were considerablywarmer than at present. The Mediterranean, the Near East, and NorthAfrica, including the Sahara, received more rainfall than they dotoday.103 North America enjoyed better weather during most of thisperiod. China during the early part of this epoch experienced highertemperatures and a more clement climate. From Western Europe to China,East Asia, India, and the Americas, mankind flourished as neverbefore.Evidence for the medieval warming comes from contemporaneous reportson weather conditions, from oxygen isotope measurements taken from theGreenland ice, from upper tree lines in Europe, and from sea levelchanges. These all point to a more benign, warmer, climate with morerainfall but because of more evaporation less standing water. Not onlydid northern Europe enjoy more rainfall but the Mediterranean littoralwas wetter. An early twelfth century bridge with twelve arches whichstill exists over the river Oreto at Palermo exceeds the needs of thesmall trickle of water that flows there now.104 According to Arabgeographers two rivers in Sicily that are too small for boats werenavigable during this period.105 In England at the same time, medievalwater mills on streams that today carry too little water to turn themattest to greater rainfall. Although England apparently received morerainfall than in modern times, the warm weather led to more drying outof the land. Support for a more temperate climate in central Europecomes from the period in which German colonists founded villages. Asaverage temperatures rose people established towns at higherelevations. Early settlements were under 650 feet in altitude; thosefrom a later period were between 1,000 and 1,300 feet high; and thosebuilt after 1,100 were located above 1,300 feet.106H. H. Lamb counted manuscript reports of flooding and wet years inItaly.107 He discovered that starting in the latter part of the tenthcentury, the number of wet years climbed steadily, reaching a peakaround 1300. Over the same period northern Europe was enjoying warmerand more clement weather. Not only was the temperature higher than nowin Europe during the twelfth and thirteenth century but the populationenjoyed mild wet winters. In the Mediterranean it was moist as wellwith summer thunderstorms frequently reported.108Studies have shown that some areas became drier during thesecenturies. In particular, the Caspian Sea was apparently four meters--over 13 feet--lower from the ninth through the eleventh century thancurrently.109 After 1200 A.D. the elevation of the lake rose sharplyfor the next two or three hundred years.110 In the Asian steppes, warmperiods with fine summers and often little snow in the winter producedlake levels that were low by modern standards.111 A recent study oftree rings in California's Sierra Nevada and Patagonia concluded thatthe "Golden State" suffered from extreme droughts from around 900 to1100 and again from 1210 to 1350 while the tip of South America duringthe first 200 years also enjoyed little precipitation.112The timing of the medieval warm spell, which lasted no more than 300years, was not synchronous around the globe. For much of NorthAmerica, for Greenland and in Russia, the climate was warmer between950 and 1200.113 The warmest period in Europe appears to have beenlater, roughly between 1150 and 1300, although parts of the tenthcentury were quite warm. Evidence from New Zealand indicates peaktemperatures from 1200 to 1400. Data on the Far East is meager butmixed. Judging from the number of severe winters reported by centuryin China, the climate was somewhat warmer than normal in the ninth,tenth, and eleventh centuries, cold in the twelfth and thirteenth andvery cold in the fourteenth. Chinese scholar Chu Ko-chen reports thatthe eighth and ninth centuries were warmer and received more rainfall,but that the climate deteriorated significantly in the twelfthcentury.114 He found records, however, that show that the first halfof the thirteenth century was quite clement and very cold weatherreturned in the fourteenth century.115 Another historian found on thebasis of records of major floods and droughts that between the ninthand eleventh century China suffered much fewer of these calamitiesthan during the fourteenth through the seventeenth.116 The evidencefor Japan is based on records of the average April day on which thecherry trees bloomed in the royal gardens in Kyoto. From this record,the tenth century springs were warmer than normal; in the eleventhcentury they were cooler; the twelfth century experienced the latestsprings; the thirteenth century was average and then the fourteenthwas again colder than normal.117 This record suggests that the LittleClimate Optimum began in Asia in the eighth or ninth centuries andcontinued into the eleventh. The warm climate moved west, reachingRussia and central Asia in the tenth through the eleventh, and Europefrom the twelfth to the fourteenth. Some climatologists have theorizedthat the Mini Ice Age also started in the Far East in the twelfthcentury and spread westward reaching Europe in the fourteenth.118EuropeThe warm period coincided with an upsurge of population almosteverywhere, but the best numbers are for Europe. For centuries duringthe cold damp "dark ages" the population of Europe had been relativelystagnant. Towns shrank to a few houses clustered behind citywalls. Although we lack census data, the figures from Western Europeafter the climate improved show that cities grew in size; new townswere founded; and colonists moved into relatively unpopulated areas.Historians have failed to agree on why after the eleventh century, thepopulation soared. It might be more enlightening to ask why thepopulation remained stagnant for so long. As John Keegan, the eminentmilitary historian put it: "The mysterious revival of trade between1100 and 1300, itself perhaps due to an equally mysterious rise in theEuropean population from about 40,000,000 to about 60,000,000, in turnrevived the life of towns, which through the growth of a money economywon the funds to protect themselves from dangers beyond the walls".119Although it is impossible to document it, the change in the climatefrom a cold, wet one to a warm, drier climate--had more rainfall,but more evaporation reduced bogs and marshy areas--seems likely tohave played a significant role. In the eighth through the eleventhcenturies, most people spent considerable time in dank hovels avoidingthe inclement weather. These conditions were ripe for the spread ofdisease.  Tuberculosis, malaria, influenza, and pneumonia undoubtedlytook many small children and the elderly--those over 30.Written records confirm that the warmer climate brought drier andconsequently healthier conditions to much of Europe. Robert Bartlett,citing H.E. Hallam in Settlement and Society, quotes the people ofHolland who invaded Lincolnshire in 1189 that "because their own marshhad dried up, they converted them into good and fertileploughland."120 Moreover, prior to the twelfth century German settlerson the east side of the Elbe frequently named their towns with mar,which meant marsh, but later colonists did not use thatsuffix. Bartlett's explanation is that the term had gone out of use,but an alternative one is that the warmer climate had dried up themarshes.121With a more pleasant climate people spent longer periods outdoors;food supplies were more reliable. Even the homes of the peasants wouldhave become warmer and less damp. The draining or drying up of marshesand wetlands reduced the breeding grounds for mosquitoes that broughtmalaria. In all the infant and childhood mortality rate must havefallen spawning an explosion in population.From the ninth century, with a climate still quite cool, to theeleventh, medieval Europe was almost totally agricultural. The fewcities that existed consisted mainly of religious seats with theirsupport personnel. Even as late as the twelfth century, city dwellersmade up less than 10 percent of the population.122 Trade before theeleventh century was virtually non-existent.123 People were tied tothe land through custom and necessity. The great feudal estates grewwhat they ate and ate what they grew; they wove their own cloth andsewed their own clothes; they built what little furniture was needed.In a word they were almost entirely self-sufficient. The serfs thattilled the land had inherited rights to enough land to sustain afamily. Typically the older son would follow his father. Any othersons either joined the priesthood, became monks, vagabonds, or inlater centuries, mercenaries. Given the cold climate before theeleventh century, the lack of medical care, and a restricted dietfostering poor nutrition, few babies lived to adulthood. The problemof an excess of labor was, therefore, nonexistent. The truth is thatthe population was growing so slowly that a labor shortage persistedand the feudal nobility established laws prohibiting serfs fromleaving their land.Until the twelfth century when the weather became significantly morebenign, a Europe fettered by tradition remained cloistered inself-sufficient units. The next two centuries, however, witnessed aprofound revolution which, by the end of the fourteenth century,transformed the landscape into an economy filled with merchants,vibrant towns and great fairs. Crop failures became less frequent; newterritories were brought under control. With a more clement climateand a more reliable food supply, the population mushroomed. Even withthe additional arable land permitted by a warmer climate, theexpansion in the number of mouths exceeded farm output: food pricesrose while real wages fell. Farmers, however, did well with moreground under cultivation and low wages payable to farm hands.124The rise in the population may have contributed to the spread ofprimogeniture.  Prior to the twelfth century, infant and childmortality coupled with short life expectancy required parents to beflexible in designating their heirs. Robert Bartlett quotes anestimate for the eleventh century that the probability of a coupleraising sons to adulthood was only 60 percent.125Although the first sons born on the estates could follow theirfathers, other children, especially the men, had to find newopportunities. The crusades furnished an occasion for both the sons ofserfs and of the nobility to enrich themselves and even to find newland to cultivate. Others moved to virgin territory in eastern Europe,Scandinavia or previously forested or swampy areas.126 The Franks andNormans launched invasions of England, southern Italy, ByzantineGreece, and the eastern Mediterranean. In 1130 the Tancred deHauteville clan, a notable example, founded the Kingdom ofSicily. This family, a classic case of an "over-breeding, land-hungrylesser nobility," consisted of 12 sons from two mothers who,recognizing that their Norman property was inadequate, invadedsouthern Italy in search of land and riches.127During the High Middle Ages, the Germans advanced across the Elbe totake land from pagan Serbs. The spread of knights and soldiers out ofFrance and Germany demonstrates that the population was multiplyingmore rapidly in northern Europe than in southern. The rapid rise innumbers north of the Alps fits the improved climate scenario: globalor continental warming brought greater temperature change and morebeneficial weather to higher latitudes.The more skilled and enterprising who did not seek their fortune inforeign lands typically flocked to towns and urban centers, becominglaborers, artisans, or traders. Both those who moved to the new citiesand those who founded colonies were legally freed of feudalobligations. This new liberty, making risk taking and innovationpossible, was essential for those in commerce.The warmth of the Little Climate Optimum made territory farther northcultivable.  In Scandinavia, Iceland, Scotland, and the high countryof England and Wales, farming became common in regions which neitherbefore nor since have yielded crops reliably. In Iceland, oats andbarley were cultivated. In Norway, farmers were planting further northand higher up hillsides than at any time for centuries. Greenland was4 to 7 degrees Fahrenheit warmer than at present and settlers couldbury their dead in ground which is now permanently frozen. Scotlandflourished during this warm period with increased prosperity andconstruction.128 Greater crop production meant that more people couldbe fed, and the population of Scandinavia exploded.129 The rapidgrowth in numbers in turn propelled and sustained the Vikingexplorations and led to the foundation of colonies in Iceland andGreenland.The increasingly warm climate was reflected in a rising sealevel. People were driven out of the low lands and there was a largescale migration of men and women from these areas to places east ofthe Elbe, into Wales, Ireland, and Scotland. Flemish dikes to holdback the sea date at least from the early eleventh century. AlthoughPirenna and Bartlett attribute them to attempts to reclaim land fromthe sea to provide new areas for farming, the evidence points towardsa higher water level that farmers in the low countries had tobattle.130 The earliest texts setting out rights on the reclaimed landfail to mention any obligation to maintain the dikes, although laterones spell out the requirement, suggesting that the problem of holdingback the sea became worse over time. Robert Bartlett quotes from aWelsh chronicle on the influx of people from Flanders:	that folk, as is said, had come from Flanders, their land, 	which is situated close to the sea of the Britons, because the sea 	had taken and overwhelmed their land...after they had failed to 	find a place to live in - for the sea had overflowed the coast 	lands, and the mountains [sic] were full of people so that it was 	not possible for everyone to live together there because of the 	multitude of the people and the smallness of the land...131In addition to the land north of the Alps, the warm rainier climatebenefited southern Europe, especially Greece, Sicily and southernItaly. All of the Mezzogiorno in the Middle Ages did well.132 NicolasCheetham, a former British diplomat who authored a recent book on"Mediaeval Greece", reports that during the first half of the thirteenthcentury, the plains and valleys of the Peloponnese were fertile andplanted with a wide variety of valuable crops and trees. They producedwheat, olives, fruits, honey, cochineal for dyeing, flax for the linenindustry and, silk from the mulberry trees. The wealthy inConstantinople prized highly the wines, olives, and fruit fromGreece. Thessaly's grain fed the Byzantine Empire.133 Patras exportedtextiles and silk of very high quality.  Extensive forests, which werefull of game, supplied acorns for hordes of pigs. Herders raised sheepand goats in the mountain pastures, while in the valleys farmers kepthorses and cattle.134The Mediterranean flourished in the twelfth century. Christian andMoslem lands achieved great brilliance. Cordova, Palermo,Constantinople and Cairo all thrived, engendering great tolerance forcontending religions.135 Christian communities survived and prosperedin Moslem Cairo and Cordova. The Rulers of Byzantine countenanced theMoslems and often preferred them to "barbaric" Westerners.In the West, Charlemagne, creator of the Holy Roman Empire, may haveinaugurated the era of the High Middle Ages while Dante, writing TheDivine Comedy, may have closed it. In A History of Knowledge, CharlesVan Doren contended that: "the...three centuries, from about 1000 toabout 1300, became one of the most optimistic, prosperous, andprogressive periods in European history."136 All across Europe, thepopulation went on an unparalleled building spree erecting at hugecost spectacular cathedrals and public edifices. Byzantine churchesgave way to Romanesque, to be replaced in the twelfth century byGothic cathedrals. During this period construction began on the Abbeyof Mont-St-Michel (1017), St. Marks in Venice (1043), WestminsterAbbey in London (1045), the Cathedral of our Lady in Coutances (1056),the Leaning Tower at Pisa (1067), the Cathedral of Santiago deCompostela in northern Spain (1078), the Cathedral of Modena (1099),V�zelay Abbey in France (1130), Notre-Dame in Paris (1163),Canterbury in England (1175), Chartres (1194), Rouen's cathedral inFrance (1201), Burgos' cathedral in Castile (1220), the basilica ofSaint Francis in Assisi (1228), the Sainte Chapelle in Paris (1246),Cologne Cathedral (1248) and the Duomo in Florence (1298). Virtuallyall the magnificent religious edifices that we visit in awe today werestarted by the optimistic populations of the eleventh through thethirteenth centuries, although many were not finished forcenturies. In southern Spain, the Moors laid the cornerstone in 1248for perhaps the world�s most beautiful fortress, the Alhambra. TheFranks founded Mistra near ancient Sparta in the middle of thethirteenth century.It took a prosperous society to launch such major architecturalprojects. In Europe, building the cathedrals required a large andmostly experienced pool of labor. During the week of June 23 to June29, 1253, the accounts of the construction at Westminister Abbey, forexample, showed 428 men on the job, including 53 stonecutters, 49monumental masons, 28 carpenters, 14 glassmakers, 4 roofers, and 220simple laborers.137 Nearly half of all workers were skilledspecialists. Even during the slowest season in November, the Abbeyemployed 100 workers, including 34 stonecutters.  Masons andstonecutters earned the highest wages and usually hired a number ofworkers as assistants. Master craftsmen moved from job to job aroundEurope without any concern about national borders--the first trulyEuropean Community. Historians have found that only 5 to 10 percent ofthe masons and stonecutters were local people, but 85 percent of themen who quarried the stones--an unhealthy and arduous job � were fromthe vicinity.138 Thus during these centuries a European-wide marketflourished in skilled craftsmen.Economic activity blossomed throughout the continent. Banking,insurance, and finance developed; a money economy became wellestablished; manufacturing of textiles expanded to levels never seenbefore. Farmers were clearing forests, draining swamps and expandingfood production to new areas.139 The building spree mentioned abovewas made possible by low wages resulting from a population explosionand by the riches that the new merchant classes were creating. InEngland, virtually all of the churches and chapels which hadoriginally been built of wood were reconstructed in stone between thetwelfth and fourteenth centuries.140 With the clergy still opposingbuying and selling for gain, those who became wealthy oftenconstructed churches or willed their estate or much of it to religiousinstitutions as acts of redemption.141 They supplied much of the fundsto erect the great Gothic cathedrals.Starting in the eleventh century European traders developed greatfairs that brought together merchants from all over Europe. At theirpeak in the thirteenth century they were located on all the main traderoutes and served not only to facilitate the buying and selling of alltypes of goods but also functioned as major money markets and clearinghouses for financial transactions. The fourteenth century saw thewaning of these enterprises probably because the weather became sounreliable and poor that transport to and from these locations withgreat stocks of goods became impractical. Belgian historian HenriPirenne attributes their decline to war, which may indeed have playeda role, but the failure of crops and the increased wetness must havemade travel considerably more difficult.142 Wet roads were muddyrendering it arduous to transport heavy goods. Crop failures made forfamines and more vagabonds who preyed on travelers.During the High Middle Ages, technology grew rapidly. New techniquesexpanded the use of the water mill, the windmill, and coal for energyand heat. Sailing improved through the invention of the lateen sail,the sternpost rudder and the compass.  Governments constructed roadsand contractors developed new techniques for use of stone inconstruction. New iron casting techniques led to better tools andweapons. The textile industry began employing wool, linen, cotton, andsilk and, in the thirteenth century, developed the spinningwheel. Soap, an essential for hygiene, came into use in the twelfthcentury. Mining, which had declined since the Romans, at least partlybecause the cold and snow made access to mountain areas difficult,revived after the tenth century.Farmers and peasants in medieval England launched a thriving wineindustry south of Manchester. Good wines demand warm springs free offrosts, substantial summer warmth and sunshine without too much rain,and sunny days in the fall. Winters cannot be too cold - not belowzero Fahrenheit for any significant period. The northern limit forgrapes during the Middle Ages was about 300 miles above the currentcommercial wine areas in France and Germany. These wines were notsimply marginal supplies, but of sufficient quality and quantity that,after the Norman conquest, the French monarchy tried to prohibitBritish wine production.143 Based on average and extreme temperaturesin the most northern current wine growing regions of France andGermany compared to current temperatures in the former wine growingregions in England, Lamb calculates that the climate in springs andsummers were somewhere between 0.9 and 3.4 degrees Fahrenheit warmerin the Middle Ages.144Not only did the British produce wines during the Little ClimateOptimum but farmers grew grapes in East Prussia, Tilsit, and southNorway.145 Many areas cultivated in Europe were much further upmountains than is possible under the modern climate.  Together thesefactors suggest that the temperatures in central Europe were about1.8 to 2.5 degrees Fahrenheit higher than during the twentieth century.Europe's riches and a surplus of labor enabled and emboldened itsrulers to take on the conquest of the Holy Land through a series ofCrusades starting in 1096 and ending in 1291. The Crusades, stimulatedat least in part by a mushrooming population and an economic surpluslarge enough to spare men to invade the Muslim empire, capturedJerusalem in 1099--a feat not equaled until the nineteenth century. Amajor attraction of the first crusade was the promise of land in a"southern climate."146Even southern Europe around the Mediterranean enjoyed a more moistclimate than currently.147 In the reign of the Byzantine EmperorManuel I Comnenus, art and culture flourished and all the world lookedto Constantinople as its leader.148 Under the control of the Fatimidcaliphate, Egypt cultivated a "House of Science," where scholarsworked on optics, compiled an encyclopedia of natural history, with adepiction of the first known windmills, and described the circulationof the blood. In Egypt block-printing appeared for the first time inthe West.149 The caliphate turned Cairo into a brilliant center ofMoslem culture. In Persia, Omar Khayyam published astronomical tables,a revision of the Muslim calendar, a treatise on algebra and hisfamous Rub�iy�t.150As European commerce expanded, traders reached the Middle East,bringing back not only exotic goods, but new ideas and informationabout classical times. Drawing on fresh information about Aristotelianlogic, St. Thomas Aquinas defined medieval Christian doctrine in hisSumma Theologica. Possibly the oldest continuous university in theworld was founded in Bologna for the study of the law in 1000A.D. Early in the twelfth century a group of scholars under a licensegranted by the chancellor of Notre-Dame began to teach logic, thusinaugurating the University of Paris. Cambridge University traces itsfoundation to 1209 and Oxford to slightly later in the thirteenthcentury. Roger Bacon, one of the first to put forward the importanceof experimentation and careful research, studied and taught at Oxfordin the thirteenth century.Secular writing began to appear throughout northern Europe. In thetwelfth century the medieval epic of chivalry, the Chanson de Roland,was put into writing.  Between 1200 and 1220 an anonymous French poetcomposed the delightful and optimistic masterpiece, Aucassin etNicolette. An anonymous Austrian wrote in Middle High German theNibelungenlied.151The Arctic From the ninth through the thirteenth centuries agriculture spreadinto northern Europe and Russia where it had been too cold to producefood before. In the Far East, Chinese and Japanese farmers migratednorth into Manchuria, the Amur Valley and northern Japan.152 Asmentioned above, the Vikings founded colonies in Iceland andGreenland, a region that may have been more green than historians haveclaimed. It was also during this period that Scandinavian seafarersdiscovered "Vinland"--somewhere along the East Coast of NorthAmerica. The subsequent Mini Ice Age cut off the colonies in Greenlandfrom Europe, and they eventually died off. Even today, during thiswarm period of the late twentieth century, the British climateforecloses large-scale grape production and Greenland is unsuitablefor farming.The Eskimos apparently expanded throughout the Arctic area during themedieval warm epoch.153 Starting with Ellesmere Land around 900 A.D.,Eskimo bands and their culture spread from the Bering Sea into theSiberian Arctic. Two centuries later, these people migrated along thecoast of Alaska and into Greenland. During this period the Eskimos�main source of food came from whaling, which had to be abandoned withthe subsequent cooling. The Mini Ice Age forced the Thule Eskimossouth out of northern Alaska and Greenland. These hardy aborigines hadabandoned Ellesmere Land by the sixteenth century.At the same time that the Eskimos were moving north, Viking explorerswere venturing into Greenland, Vinland, and even the CanadianArctic. Scandinavian sailors found Iceland in 860, Greenland around930, and reached the shores of North America by 986.154 By the turn ofthe millennium, when the waters south-west of Greenland may have beenat least 7 degrees Fahrenheit warmers than now, Vikings were regularlyvisiting Vinland for timber.155 They were received with greathostility by the natives and eventually abandoned contact, althoughthe last trip may have occurred as late as 1347, when a Greenland shipwas blown off course.156 At the height of the warm period,Greenlanders were growing corn and a few cultivated grain. Somearchaeologists have found evidence that Vikings from Greenland mayhave visited remote portions of the Canadian archipelago and even mayhave sailed through the Northwest Passage to the West Coat of Americatraveling as far south as the Gulf of California. At least onescientists believes that this visit is the origin of the Aztec beliefin the visit of �fair� people from the East.157The Far EastAs noted above, the warming in the Far East seems to have precededthat in Europe by about two centuries. Chinese Economist Kang Chao hasstudied the economic performance of China since 200 B.C. In hiscareful investigation, he discovers that real earnings rose from theHan period (206 B.C. to 220 A.D.) to a peak during the Northern SungDynasty (961 A.D. to 1127).158 This coincides with other evidence oflonger growing seasons and a warmer climate. He explains the fall inworker productivity after the twelfth century as stemming frompopulation pressures, but a change in climate may have played asignificant role. Chao reports that the number of major floodsaveraged fewer than 4 per century in the warm period of the ninththrough the eleventh centuries while the average number was more thandouble that figure in the fourteenth through the seventeenth centuriesof the Mini Ice Age.159 Not only floods but droughts were less commonduring the warm period. The era of benign climate sustained about 3major droughts per century, while during the later cold period, Chinasuffered from almost 13 each hundred years.The wealth of this period gave rise to a great flowering of art,writing, and science. The Little Climate Optimum witnessed the highestrate of technological advance in Chinese history. During the 300 yearsof the Sung Dynasty, farmers invented 35 major farm implements--thatis, over 11 per century, a significantly higher rate of invention thanin any other era.160 In the middle of the eleventh century, theChinese invented movable type employing clay pieces.161During the Northern Sung Dynasty Chinese landscape painting with itsexquisite detail and color reached a peak never again matched.162 AdamKessler, curator of the Los Angeles County Museum of Natural Historydates the earliest Chinese blue-and-white porcelain to the twelfthcentury.163 The Southern Sung produced pottery and porcelainsunequaled in subtlety and sophistication. Literature, history andscholarship flourished as well. Scholars prepared two greatencyclopedias, compiled a history of China, and composed essays andpoems. Mathematicians developed the properties of the circle.Astronomers devised a number of technological improvements to increasethe accuracy of measuring the stars and the year.164Chinese civilization has waned and waxed for about 3500 years, yetduring all those centuries only once has it ventured to spread itsculture or its people beyond its normal borders. Beginning with thetwelfth century A.D., however, Chinese explorers and merchant shipsplied the Indian Ocean and landed on the East African coast.165Although favorable weather may not have motivated these voyages, whichended in the fourteenth century, they coincided with both the warmesttemperatures in much of the world and the time of the richest percapita incomes in China. Certainly the more clement climate meantfewer storms and easier sea travel.Japan also prospered during the Little Climate Optimum. In the HeianPeriod (794 A.D. to 1192) the arts thrived as emperors and empressescommissioned vast numbers of Buddhist temples. Murasaki Shikibu,perhaps the world's first female novelist, composed Japan's mostfamous book, The Tale of Genji. Other classical writers penned essays:Sei Shonagon--another court lady--wrote Makura-no-Soshi (the PillowBook). The Japanese aristocracy vied in composing the best poems. Allof this attests a prosperous economy with ample food stocks to supporta leisured and cultivated upper class.Over the four hundred years between 800 A.D. and 1200, the peoples ofthe Indian subcontinent prospered as well. Society was rich enough toproduce colossal and impressive temples, beautiful sculpture,elaborate carvings, many of which survive to this day.166 TheLingaraja Temple, one of the finest Hindu shrines, as well as theShiva Temple date from this period.167 Seafaring empires existed inJava and Sumatra, which reached its height around 1180. Ninth centuryJava erected the vast stupa of Borobudur; other temples--the Medut,Pawon, Kelasan and Prambanan--originate in this era. In the earlytwelfth century, the predecessors of the Cambodians, the Khmers, builtthe magnificent temple of Angkor Wat.168 In the eleventh centuryBurmese civilization reached a pinnacle. In or around its capital,Pagan, between 931 and 1284, succeeding kings competed in constructingvast numbers of sacred monuments and even a library.169 Today the areais a dusty plain littered with the crumbling remains of about 13,000temples and pagodas, built in a more hospitable era.Archaeologists studying the compositions of forests in New Zealandhave found that the South Island enjoyed a warmer climate between 700A.D. and 1400, about the time when Polynesians were colonizing theSouth Pacific Islands and the Maoris were settling in New Zealand.170Partially confirming that warming are data from Tasmania of tree ringswhich show a warm period from 940 to 1000 and another from 1100 to1190.171The AmericasLess is known about civilizations in the Americas during the LittleClimate Optimum or even how the prevailing weather changed. Much ofthe currently arid areas of North America were apparently wetterduring this epoch. The Great Plains areas east of the Rocky Mountains,the upper Mississippi Valley and the Southwest received more rainfallbetween 800 A.D. and 1200 than they do now.172 Radiocarbon dating oftree rings indicates that warmth extended from New Mexico to northernCanada. In Canada, forests extended about sixty miles north of theircurrent limit.173Starting around 800 to 900 A.D., the indigenous peoples of NorthAmerica extended their agriculture northward up the Mississippi,Missouri, and Illinois river basins. By 1000 they were farming insouthwestern and western Wisconsin and eastern Minnesota.174 They grewcorn in northwestern Iowa prior to 1200 in an area which is nowmarginal for rainfall.175 Indian settlements on the northern plains ofIowa were abandoned with colder drier weather that set in after 1150to 1200. After that time, the natives substituted bison hunting forgrowing crops. In general the land east of the Rocky Mountains enjoyedwetter conditions from 700 to 1200 and then turned drier as itexperienced greater intrusions of colder Arctic weather.The Anasazi civilization of Mesa Verde flourished during the warmperiod, but the cooling of the climate at the end of the medievalwarmth around 1280 probably led to its disappearance.176 This climaticshift brought drier conditions to much of the region, leading to aretreat from the territory and forcing the Pueblo Indians to shifttheir farming to the edge of the Rio Grande River.Around 900, the Chimu Indians in South America developed an extensiveirrigation system on Peru's coast to feed their capital of between100,000 to 200,000 souls--a huge number for the era.177 The Tolteccivilization, which occupied much of Mexico, reached its apogee in thethirteenth century.178 By 1200, the Aztecs had built the pyramid ofQuetzalcoatl near modern Mexico City.179 The Mayas' civilization,however, reached a peak somewhat earlier, before 1000, and declinedsubsequently for reasons that remain unclear. It is possible that thewarming after 1000 led to additional rainfall in the Yucatan, makingthe jungle too vigorous to restrain and causing a decline in farming,while at the same time improving agricultural conditions in theMexican highlands and farther north into what is now the southwesternUnited States.Thus warmer times brought benefits to most people and most regions,but not all.  As is always the case with a climate shift, the changesbenefited some while affecting other adversely. Change is disruptive;at the same time it produces new ideas and new ways of coping with theworld. Nevertheless, for most of the known world, the Little ClimateOptimum of the ninth through the thirteenth centuries broughtsignificant benefits to the local populations. Compared with thesubsequent cooling it was nirvana.The Mini Ice AgeThe Little Ice Age is even less well defined than the medieval warmperiod.  Climatologists are generally agreed that, at least forEurope, North America, New Zealand and Greenland, temperatures fellafter 1300 to around 1800 or 1850, although with many ups anddowns. There was a cold period in the first decade of the fourteenthcentury, another around 1430 and again in 1560. The end of this periodof increasingly harsh temperatures could have been as early as 1700,1850 or even 1900 for Tasmania.  The worst period for most of theworld occurred between 1550 and 1700.180 One reasonable interpretationof the data is that the world has been cooling since around 4500B.C. with a temporary upswing during the High Middle Ages.Europe and Asia cooled substantially from around 1300 to 1850,especially after 1400, with temperatures falling some 2 to 4 degreesFahrenheit below those of the twentieth century. This indicates thattemperatures may have dipped by as much as 9 degrees Fahrenheit in thetwo hundred years from 1200 to 1400, a drop of about the samemagnitude as the maximum rise forecast from a doubling of CO2. Thesefrigid times did bring hardships, and as the chart shows worldpopulation growth slowed. For much of these centuries, famine anddisease stalked Europe and Asia.Glaciers in North America and northern Europe peaked between the late1600s and 1730 to 1780. In the Alps these ice sheets reached theirmaximum between 1600 and 1650. The coldness came later below theequator where the glaciers reached their extreme between 1820 and1850.181Oxygen isotope ratios from oak trees in Germany document a steadydecline in average temperatures from 1350 to about 1800, with theexception of a few small upsurges and one strong temperature spike inthe first half of the eighteenth century.182 Since late in the 19thcentury they confirm a recovery to much higher levels. Icelandicrecords of sea ice attest to an increase between 1200 and the middleof the fourteenth century and then, starting in the latter half of thesixteenth century, a marked upswing in ice which appears to havepeaked around 1800.183 As H. H. Lamb points out, "in most parts of theworld the extent of snow and ice on land and sea seems to haveattained a maximum as great as, or in most cases greater than, at anytime since the last major Ice Age."184The Little Ice Age, especially the century and a half between 1550 and1700 - the exact timing varied around the globe - produced lowtemperatures throughout the year and considerable variation in weatherfrom year to year and from decade to decade.  It included some yearsthat were exceptionally warm.185 The polar cap expanded as did thecircumpolar vortex, driving storms and the weather to lowerlatitudes. Although much of Europe experienced greater wetness thanduring the earlier warm epoch, this dampness was more the product ofless evaporation due to the cold than an excess of precipitation.The cooling after the High Middle Ages can be seen in the lowering oftree lines in the mountains of Europe, changes in oxygen isotopemeasurement, and advances of the glaciers and of sea ice. This coolingdiminished the abundance and quality of wine production in France,Germany and Luxembourg as depicted in historical documents such asweather diaries and farm records.186 The ocean, which had reachedrelatively high levels both in the late Roman period and again duringthe High Middle Ages, fell to lower elevations in the seventeenth andnineteenth centuries.187 As a result of an expanded ice cap, thecircumpolar vortex, which funnels weather around the globe, movedsouth and spawned increasingly cold and stormy weather in middlelatitudes. With the exception of southern United States and centralAsia, both of which enjoyed more rainfall, this brought a worsening ofthe climate and disasters to people almost everywhere. During thecoldest period of the seventeenth century, snow fell in the highmountains of Ethiopia above 10,000 feet which today never seesnow. The subtropical monsoon rains decreased and receded farthersouth causing droughts in East Asia and parts of Africa.188The expansion of the circumpolar vortex produced some of the greatestwindstorms ever recorded in Europe. A terrible tempest destroyed theSpanish Armada in 1588. Fierce gales wracked Europe in December 1703and on Christmas Day 1717.189 The contrast between the cold northerntemperatures which moved south and the warm subtropical Atlanticundoubtedly generated a fierce jet stream. Although we lack anyinformation, this may also, have enhanced tornado activity on theplains of the United States.190The reduced temperatures had the following general effects: (1) Arcticsea ice expanded in the Atlantic eventually cutting off Greenland; (2)glaciers advanced in Iceland, Norway, Greenland, and the Alps; (3) theupper tree line in North America and central Europe lowered; (4)enhanced wetness spawned bogs, marshes, lakes, and floods; (5) riversand lakes froze more frequently; (6) the number and strength ofstorms, some of which were extraordinarily destructive, intensifiedsharply; (7) harvests failed engendering famine and higher prices forbasic foods; (8) peasants abandoned farms that no longer enjoyedreliable weather; (9) disease for both animals and humans spread.191As early as 1250, floating ice from the East Greenland ice cap washindering navigation between Iceland and Greenland.192 Over the nextcentury and a half the prevalence of icebergs became worse and by 1410sea travel between the two outposts of Scandinavia ceased. Based onthe ratio of isotopes of oxygen in teeth of ancient Norsemen,researchers have estimated that the climate in Greenland cooled byabout 3 degrees Fahrenheit between 1100 and 1450.193 For about 350years, from the third quarter of the fifteenth century to 1822, noships found their way to Greenland and the local populationperished.194The deteriorating climate in Europe was heralded by harvest failure inthe last quarter of the thirteenth century. Compounding theinsufficiency was a shift of land from farming, which because of thechange in climate was more chancy, to enclosure and sheep rearing.195Average yields, which were already low by modern standards, worsenedafter the middle of the thirteenth century.196 One of the first severebouts of cold wet weather afflicted Europe from 1310 to 1319, leadingto large scale crop failures.197 Food supplies deteriorated sharplygenerating famine for much of Europe in 1315-18 and again in 1321.198Harvest deficits and hunger preceded the Black Death by 40 years.199According to Lamb, for much of the continent, "the poor were reducedto eating dogs, cats and even children."200 This scanty food outputcontributed to a decline in population which was aggravated bydisease. The history of many villages shows that they were abandonedbefore the beginning of the plague not afterwards. By 1327, thepopulation in parts of England--especially those later devastated bythe plague--had fallen by 67 percent.201 People poorly nourished werequickly carried off by disease.  Between 1693 and 1700 in Scotland,seven out of the eight harvests failed and a larger percentage of thepopulation starved than died in the Black Death of 1348-50.202In two terrible years, 1347 and 1348, famine struck northern Italy,followed by the Black Death, which decimated most of those not alreadycarried away by lack of food.203 Bubonic plague spread across the Alpsafter 1348, killing in the next two years about one-third of northernEurope's people. Life expectancy fell by ten years in a little over acentury: from 48 years in 1280 to 38 years in the years 1376 to1400.204 Crops often failed; peasants abandoned many lands that hadbeen cultivated during the earlier warm epoch. Between 1300 and 1600the growing season shrank by three to five weeks with a catastrophicimpact on farming.205 In Norway and Scotland, the population declinedand villagers deserted many locales well before the plague reachedthose areas.206 The capitals of both Scotland and Norway moved southbefore both areas lost their autonomy.The cooling after 1300 may also have contributed to the bubonicplague, the greatest disaster to ever befall Europe. The diseaseappears to have originated around 1333 in China, shortly after majorrains and floods in 1332, which are reputed to have caused 7 milliondeaths, while disturbing wildlife and displacing plague-carryingrats.207 The Black Death then spread to central Asia around 1338-9,which, with the increased coldness, was also drying out. By 1348rodents carrying fleas infested with bubonic plague had marched orbeen carried from the Crimea into Europe. Historians have estimatedthat as many as one-third of all the people in Europe died in theraging epidemic that swept the continent.208 This outburst of theplague, like a similar one in the sixth century, occurred during aperiod of increasing coolness, storminess, and wet periods, followedby dry hot ones. The unpleasant weather is likely to have confinedpeople to their homes where they were more likely to be exposed to thefleas that carried the disease. In addition the inclement weather mayhave induced rats to take shelter in human buildings, exposing theirinhabitants to the bacillus.Not only did the cold facilitate the spread of the plague, but itcaused much other human suffering. In July of 1789, just prior to theFrench Revolution, wet weather and air temperatures between 59 and 85caused an ergot blight in the rye crop of Brittany and other parts ofFrance. This blight caused hallucinations, paralysis, abortions andconvulsions and came after a very cold winter that had created severefood shortages.209 Earlier in that century wet cold summers hadproduced two famine years in Europe.The end of the medieval warmth had devastating effects on populationsthat lived at the edge of habitable lands. For example, historianshave estimated the population of Iceland at the end of the eleventhcentury at about 77,000, and early in the fourteenth it still numberedover 72,000. By the end of the eighteenth century, after severalhundred years of coolness and stormy weather, the number of Icelandershad been cut in half to 38,000.210The poorer climate in Europe after the thirteenth century brought ahalt to the economic boom of the High Middle Ages. Innovation slowedsharply.211 Except for military advances, technological improvementsceased for the next 150 years. Population growth not only ended but,with starvation and the black death, fell. Without the drive ofadditional numbers of people, colonial enterprise ceased and no newlands were reclaimed nor towns founded. The economic slump of 1337brought on the collapse of the great Italian bank, Scali, leading toone of the first recorded major financial crises.212 Construction onchurches and cathedrals halted.The hardships of the fourteenth century induced a search forscapegoats. In 1290 after some years of crop failures, the king ofEngland expelled the Jewish population from the country. The Frenchking followed this example in 1306 and again in 1393.213 In 1349, theChristians of Brabant massacred the local Jews and expelled theremainder twenty-one years later.The Mini Ice Age at its coldest devastated the fishing industry. From1570 to 1640, during the most severe period, Icelandic documentsrecord an exceptionally high number of weeks with coastal seaice. Except for a few years, fishermen from the Faeroe Islandssuffered from a lack of cod from 1615 to 1828--cod needs water warmerthan 3.6 degrees Fahrenheit to flourish. During the worst periods,1685 to 1704, fishing off southwest Iceland failed totally.214 Inthe very icy year of 1695, Norwegian fishermen found no cod off theircoast. Lamb calculates that the sea around the Faeroe Islands wasprobably 7 to 9 degrees Fahrenheit colder than it has been over thelast century.215The Mini Ice Age brought hard times to Southern Europe as well. Severewinters and wet summers created shortages and famines in the south ofFrance and in Spain. The great variability in the weather madeagricultural output quite uncertain and contributed to a farmingcrisis in the Iberian Peninsula. Although one cannot know for surethat it was the weather, the whole of the Mediterranean littoraldeclined economically in the seventeenth century.216The cold had devastating effects elsewhere in the world. In China,frosts killed the orange trees in Kiangsi province between 1646 and1676.217 Per capita incomes fell as food prices rose. As alreadymentioned, cooler weather brought an end to the Anastazi Indian puebloculture, as well as ending native American farming in the upper middlewest.According to Nicolas Cheetham, in the second half of the thirteenthcentury warfare in Greece and the necessity of keeping a largemilitary establishment under arms reduced its previous prosperity. Wardoes exact a high toll on economies, but it seems extraordinarilycoincidental that economic troubles occurred at the time Europe wasexperiencing a deteriorating climate. In 1268, the King of Naples, ingratitude for military service send wheat, barley and cattle to thePeloponnese.218 Was this needed because of crop failures solely due tomilitary disruptions? Although not necessarily weather related, in1275 Geoffroy de Briel, a major figure in medieval Greece, died duringa military campaign of dysentery, a disease often exacerbated by coldwet conditions.219Notwithstanding the cooling climate and the ravages of disease after1300, European civilization recovered with the advent of theRenaissance in the fifteenth century. This burst of cultural activityrepresented a continuation, an expansion, and a deepening of theartistic and intellectual activity of the High MiddleAges. Ironically, the outpouring of art, science and literature thatmade up the Renaissance may have been sustained by the plague. Thecolder climate made agriculture more chancy, reduced the territoryavailable for farming, and cut yields. Yet without the one-third dropin Europe's population caused by the Black Death, food supplies wouldhave been too meager to support a large artistic and cultured classthat promoted and supported the arts. The reduced agricultural output,however, was still large enough to support the even more diminishedpopulation. In China, which experienced a slower decline in numbers,real wages fell and the people became increasingly impoverished.220But in Europe, as a result of such a terrible death rate over a veryshort period, real incomes for the survivors actually climbed.221From around 1550 to 1700 the globe suffered from the coldesttemperatures since the last Ice Age. Lamb estimates that in the 1590sand 1690s the average temperature was 3 degrees Fahrenheit below thepresent. Grain prices increased sharply as crops failed. Famines werecommon. The Renaissance had ended; Europe was in turmoil. TheContinent suffered from cold and rain, which produced poor growingconditions, food shortages, famines and finally riots in the years1527-29, 1590-97, and the 1640s. The shortages between 1690 to 1700killed millions and were followed by more famines in 1725 and 1816.222China, Japan, and the Indian subcontinent were also afflicted withsevere winters between 1500 and 1850-80. Despite the development of anew type of rice that permitted the cultivation of three crops a yearon the same land--up from two--the population of China, as well asthat of Korea and the Near East, declined for two centuries after1200, undoubtedly reflecting a deteriorating climate.223 Theabandonment of sea trade by the Chinese most likely resulted fromdeteriorating weather and less population pressure.		Costs and Benefits of Efforts to Mitigate WarmingIf mankind had to choose between a warmer or a cooler climate, humans,most other animals and, after adjustment, most plants would be betteroff with higher temperatures. Not all animals or plants would prosperunder these conditions; many are adapted to the current weather andmight have difficulty making the transition. Society might wish tohelp natural systems and various species adapt to warmer temperatures(or cooler, should that occur). Whether the climate will warm is farfrom certain; that it will change is unquestionable. The weather haschanged in the past and will no doubt continue to vary in thefuture. Human activity is likely to play only a small and uncertainrole in climate change. The burning of fossil fuel may generate anenhanced greenhouse effect or the release into the atmosphere ofparticulates may cause cooling. It may also be simply hubris tobelieve that Homo Sapiens can affect temperatures, rainfall and winds.As noted, not all regions or all peoples benefit from a shift to awarmer climate.  Some locales may become too dry or too wet; othersmay become too warm. Certain areas may be subject to high pressuresystems which block storms and rains. Other parts may experience thereverse. On the whole, though, mankind should benefit from an upwardtick in the thermometer. Warmer weather means longer growing seasons,more rainfall overall, and fewer and less violent storms. The optimalway to deal with potential climate change is not to strive to preventit, a useless activity in any case, but to promote growth andprosperity so that people will have the resources to deal with anyshift.It is much easier for a rich country such as the United States toadapt to any long term shift in weather than it is for poor countries,most of which are considerably more dependent on agriculture than therich industrial nations. Such populations lack the resources to aidtheir flora and fauna in adapting, and many of their farmers earn toolittle to survive a shift to new conditions. These agriculturallydependent societies could suffer real hardship if the climate shiftsquickly. The best preventive would be a rise in incomes, which woulddiminish their dependence on agriculture. Higher earnings wouldprovide them with the resources to adjust.The cost of trimming emissions of CO2 could be quite high. WilliamCline of the Institute for International Economics - a proponent ofmajor regulatory initiatives to reduce the use of fossil fuels - hascalculated that the cost of cutting emissions from current levels byone-third by 2040 as 31/2 percent of World Gross Product.224 Given hisassumption that cutbacks of CO2 emissions are done by the least costmethods and his bias, we can be certain that in the real world outlaysto slow warming would be considerably higher. In terms of theestimated level of world output in 1992, his estimate would amount toroughly $900 billion annually, an amount that could slow growth andimpoverish some who survive on the margin. These resources could bebetter spent on promoting investment and growth in the poorercountries of the world.Should warming become apparent at some time in the future and shouldit create more difficulties than benefits, policy makers would have toconsider preventive measures. Based on history, however, globalwarming is likely to be positive for most of mankind while theadditional carbon, rain, and warmth should also promote plant growththat can sustain an expanding world population. Global change isinevitable; warmer is better; richer is healthier.* A shorter version of this paper appeared as "Why Global Warming would be Good for You" in the Winter 1995 issue of The Public Interest.1Mitchell [1991]:  70-71.2Committee on Science, Engineering, and Public Policy [1991]. Policy Implications of Greenhouse Warming, p. 24.3Healy [1994].4Policy Implications of Greenhouse Warming, p. 18.5Crowley & North [1991]:, 70.6Crowley & North [1991]: 82-83.7Levenson [1989,]: 25.8Huggett [1991]: 74.9Crowley & North [1991]: 117.10Gore [1992]: 62 & 63.11Schelling [1992]: 6.12Folland et al. [1992]Climate Change 1992, Table C2, p. 152.13Parry et al. [1988] as summarized in Kane [1991]:  7.14Kauppi, et al. [1992]: 70-74.15Letter to the editor, Wall  Street Journal, February 16, 1990.16Kane et al. [1991]17Mendelsohn,[1994]: 753-771.18Van Kooten [1990]: 704.19Rind [1993]: 39-49.20Frenzel [1993]: 7.21Flohn [1983]: 404.22Broccoli [1994]: 282.23Giles[1990]: 23.24Frenzel [1993]: 8.25Crowley [1993]: 23.26Frenzel [1993]: 10.27Frenzel [1993]: 11.28Crowley [1993]: 21.29Crowley [1993]: 25.30Rind [1993]: 41.31Webb III, et al.. [1993]: 51732Webb III, et. al. [1993]: 521.33Webb III, et. al. [1993]: 523.34Webb III, et. al. [1993]: 525.35Morley and Dworetzky [1993]: 133-134.36McGlone et al. [1993]: 311.37McGlone et al. [1993]: 313.38Genetic support for the 200,000 years ago estimate comes from Vigilant,et al. [1991]: 1503-1507. The oldest human remains found by archeologists are from 40,000 years ago.  39Crowley & North [1991]: 116.40Crowley & North [1991]: 20.41Ammerman and Cavalli-Sforze [1984]: 4.42Boserup [1981]: 39-40.43Lamb [1968]: 6.44Wendland & Bryson [1974].45Lamb [1968]: 12.46Lamb (1988): 30.47Ammerman & Cavalli-Sforza [1984]: 28.48Ammerman & Cavalli-Sforza [1984]: 41.49Cohen.[1977]: 1. 50Cohen [1989]: 56.51Boserup [1981]: 34.52Kremer [1993]: 683.53Kremer [1993]: 681-716.54Boserup [1981]: 36-37.55Boserup [1981]: 38.56Cohen [1989]: 112.57Cohen [1989]: 113.58Cohen [1989]: 114-115.59Cohen [1989]: 119.60Cohen [1989]: 119.61Lamb (1988): 22.62Giles, [1990]: 133.63Claiborne [1970]: 324.64Lamb [1982]: 120.65Gore [1992]: 76.66Lamb [1988]: 21.67Giles [1982]: 115-116.68Lamb [1977]: 270.69Lamb [1977]: 133.70Lamb [1968]: 61.71Lamb [1982]: 131.72Lamb [1982]: 131.73Lamb [1968]: 61.74Lamb [1968]: 62.75Claiborne [1970]: 243.76Ammerman & Cavalli-Sforze [1984]: 16.77Ammerman & Cavalli-Sforza [1984]: 14-16.78Ammerman & Cavalli-Sforze [1984]: 16. 79Lamb [1977]: 256.80Lamb [1982]: 126.81Lamb [1977]: 254.82Limb [1982]: 124.83Ko-chen [1973]: 228 & 229.84Lamb [1977]: 251.85Lamb [1977]: 389.86Claiborne [1970]: 295.87Lamb [1977]: note 1, p. 257.88Lamb [1982]: Fig. 43, p. 118.89Lamb [1988]: 22.90Lamb [1988]: 23.91Lamb [1977]: 419.92Carpenter [1966]93Lamb [1977]: 257.94Lamb [1977]: 258.95 Claiborne [1970]: 344-347.96Lamb [1977]: 250.97Lamb [1968]: 63.98Lamb [1977]: 261.99Cheetham [1981]: 18 & 20.100Cheetham [1981]: 26.101Keegan [1993]: 288.102Lamb [1968]: 64.103Lamb [1968]: 64-65.104Lamb [1968]: 8.105Lamb [1977]: 271.106Bartlett [1993]: 162.107Lamb [1977]:, 427.108Lamb [1977]: 429.109Lamb [1977]: 133.110Lamb [1977]: 439.111Lamb [1977]: 136.112Stine [1994]: 546-549.113Lamb [1977]: 435.114Chu [1973]: 235.115Chu [1973]: 237 & 238.116Chao [1986]: 203.117Lamb [1977]: Tables 17.3 and 17.4, pp. 443 & 447.118Chu [1973]: 239-240.119Keegan [1993]: 149.120Bartlett [1993]: 155 from Hallam [1965]: 166.121Bartlett [1993]: 162.122Pirenne [n.d., c. 1938]: 59.123Pirenne [n.d., c. 1938]: 12.124Donkin, [1973]: 90.125Bartlett [1993]: 48.126Bartlett [1993]: Chapter 6.127Bartlett [1993]: 48.128Lamb [1977]: 437.129Claiborne [1970]: 348-364.130Pirenne [n.d.]: 76 and Bartlett [1993]: 114-115.131Bartlett [1993]: 115.132Cheetham [1981]: 37.133Cheetham [1981]: 28.134Cheetham [1981]: 85.135Cheetham,[1981]: 35-36.136Van Doren [1991]: 111.137Gimpel [1983]: Table p. 68.138Gimpel [1983]: 69.139Bartlett [1993]: 2.140Donking [1973]: 110-111.141Pirenne [n.d.]: 50.142Pierenne [n.d.]: 103.143Lamb [1977]: 277.144Lamb [1977]: 278-279.145Lamb [1977]: 279.146Keegan, [1993]: 291.147Lamb [1968]: 8.148Langer [1968]: 269.149Langer [1968]: 206 & 286.150Carruth [1993]: 161.151Carruth [1993]: 134, 170, & 171.152McNeill [1963]: 559.153Lamb [1977]: 248.154Lamb [1977]: 252.155Lamb [1988]: 159.156Lamb [1977]: 252.157Lamb [1977]: 252.158Chao [1986]: 219.159Chao [1986]: 203.160Chao [1986]: 195.161Carruth [1993]: 151.162Langer [1968]: 366.163Kesseler [1994]: A17.164Langer [1968]: 367.165Claiborne [1970]: 288.166McNeill [1963]: 559.167Carruth [1993]: 151.168Langer [1968]: 372.169Deland [1987]: 9, 29-32.170Lamb [1977]: 430-431.171Cook et al. [1991): 1267.172Lamb [1988]: 42.173Lamb [1988]: 42.174Lamb [1977]: 249.175Lamb [1982]: 177.176Gore [1992]: 78.177Carruth [1993]: 142-143.178Langer [1968]: 386.179Carruth [1993]: 168.180Lamb [1977]: 463.181Lamb [1988]: 166.182Lamb [1977]: Fig. 17.12, p. 450.183Lamb [1977]: Fig. 17.13, p. 452.184Lamb [1977]: 461-462.185Lamb [1977]: 465-466.186Lamb [1977]: 246.187Lamb [1977]: 432.188Fairbridge [1984]: 181-190.189Lamb [1988]: 158.190Lamb [1977]: 467.191Lamb [1977]: 451-452.192Lamb [1988]: 159.193Monastersky [1994]: 310.194Lamb [1988]: 159.195Lamb [1977]: 7.196Donkin [1973]: 91.197Lamb [1977]: 454. 198Donkin [1973]: 90.199Lamb [1977]: 266.200Lamb [1977]: 7.201Lamb [1977]: 454.202Lamb [1977]: 471.203Langer [1968: 317.204Lamb [1982]: 189.205Lamb [1988]: 32.206Lamb [1988]: 36.207Lamb [1977]: 456208Lamb [1977]: 262.209Lamb [1988]: 165.210Lamb [1977]: 265 taken from Thorarinsson [1961].211Gimpel [1983]: 150.212Gimpel [1993]: 151.213Pirenne [n.d.]: 134.214Lamb [1988]: 153-54, 155.215Lamb [1988]: 156 & 160.216Lamb [1977]: 469.217Lamb [1977]: 471.218Cheetham [1981]: 98 & 99.219Cheetham [1981]: 101.220Kremer, [1993]: Appendix A, p. 714 & Chao [1986]: Table 9.2, p. 218.221Rosenberg & Birdzell, Jr. [1986]: 54.222Ladurie [1971]: 64-79.223Carruth, [1993]: 166 & 168.224Cline [1992]: 8.WT03-B20-85IA006-000057-B013-359http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/re-pv.html 138.80.61.12 19970221183001 text/html 4893HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:59:48 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 4722Last-modified: Tue, 09 Jul 1996 02:25:53 GMT Renewable Energy: Photovoltaics Renewable Energy: Photovoltaics Photovoltaics (PV), the direct conversion of sunlight to electricity, has maderemarkable strides since its invention in the 1950s. Most locations in thiscountry are good for photovoltaics. PV systems are assembled by grouping cellsinto modules and, in turn, connecting modules into arrays. As the cost of PVmodules continues to decrease, the size and value of PV systems that are costeffective increase steadily. PV systems have no moving parts to wear out orbreak down, make no noise, and cause no pollution while generatingelectricity. Resource Land use for PV is reasonable; PV modules covering 0.3% of the land of thiscountry, one-fourth of the area occupied by roadways, could provide all of itselectricity. PV electrical output correlates well with the daily load pattern of manyutilities, which generally increases during daylight hours when people are moreactive and decreases again at night. This means that PV systems can helpalleviate peak loads and reduce the need for new power plants. Current Use, Cost, and Market Growth The combined efforts of industry and the U.S. Department of Energy (DOE) havereduced PV system costs by more than 300% since 1982. The PV market isestimated to be growing at 20% per year today. The number of U.S. companiesproducing PV panels has doubled since the late 1970s to about 20 today. The most frequently seen application of PV is in consumer products, using tinyamounts of direct current (dc) power, less than 1 watt (W). More than 1billion hand-held calculators, several million watches, and a couple of millionportable lights and battery chargers are all powered by PV cells. PV is rapidly becoming the power supply of choice for all remote andsmall-power, dc applications of 100 W or less. More than 200,000 homes worldwide depend on PV to supply all of theirelectricity. Most of these systems are rated about 1 kW and often supplyalternating current (ac) power. PV module production for terrestrial use has increased 500-fold in the past20 years. Worldwide PV module shipments in 1993 were 60 megawatts (MW).The U.S. share of this market is now one-third. Worldwide production of PVmodules includes 48% single-crystal silicon, 30% polycrystalline silicon, and20% amorphous silicon, mostly used in consumer products. The cost of larger PV systems (greater than 1 kW) is measured in "levelized"costs per kWh--the costs are spread out over the system lifetime and divided bykWh output. The levelized cost is now about $0.25 to $0.50/kWh. At thisprice, PV is cost effective for residential customers located farther than aquarter of a mile from the utility line. Reliability and lifetime are steadilyimproving; PV manufacturers guarantee their products for up to 20 years. The worldwide manufacturing capacity of PV modules will almost double by 1996from 60 MW to 100 MW. The increase will take place in all of the major PVtechnologies. Most of this increase in capacity is taking place in the UnitedStates; however, new manufacturing plants are also under construction inBrazil, China, Germany, Hong Kong, India, Italy, and the United Kingdom. The worldwide PV industry has grown from sales of less than $2 million in 1975to greater than $750 million in 1993. The companies with the largest increasein sales in the 1990s have been U.S. companies, reflecting their strong,competitive position. The U.S. just regained the lead over Japan in grossannual sales of PV. Technology Development All types of PV technologies are improving their performance. The efficiencyof existing commercial PV modules is 5% to 13%. Manufacturers are usingautomation and taking advantage of economies of scale in production andpurchasing to further lower costs. These costs are expected to be cut in halfwithin a decade. The United States is leading the world in developing andcommercializing high-performance and low-cost PV technology. References 1. Photovoltaics News, Photovoltaic Energy Systems, Inc., February1994. 2. Photovoltaics Program Overview: Fiscal Year 1993, DOE, February1994. 3. The Potential of Renewable Energy: An Interlaboratory White Paper,DOE, March 1990. WT03-B20-86IA005-000051-B019-233http://lacebark.ntu.edu.au:80/j_mitroy/sid101/wind2/windlecture.html 138.80.61.12 19970221152500 text/html 42682HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:54:43 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 42510Last-modified: Tue, 09 Jul 1996 02:25:47 GMT Current Technology: full document Current Wind Turbine Technology Richard Hales, Cranfield University This article may be freely copied and used for personal or academic purposes; copying or use, in whole or in part, for any other purpose is only allowed with the prior permission, in writing, from the author. Introduction PICTURE size 110kb Wind energy is currently viewed as one of the most promising of the renewable energy sources, with an enormous resource in the British Isles and a steadily maturing technology base in Europe and North America. Wind turbines deployed in single units, in small groups of four or five, and increasingly in wind farms of up to fifty machines, are becoming a feature on the landscape of many European countries, especially Denmark, the Netherlands, northern Germany, and now Spain and Britain. In three areas of California (Altamont, Tehachapi and San Gorgonio) wind farming developed on a massive scale in the 1980s with well over 15000 machines now installed. But what does this renewable technology involve, and how much of a contribution can it actually make to the UK electricity supply industry? This two hour lecture will give a picture of the present state of this technology and describe current trends in its application. Technology I The most distinctive feature of a wind turbine is its rotor, converting the fluid mechanical energy of the wind to rotational energy in the main shaft. Most modern machines work by developing aerodynamic lift from the air flow; this is created by the suction caused on the blade by the acceleration of the air over it, along the same principle as an aircraft wing. Any aerodynamic force is proportional to the square of the incident air speed. For the horizontal-axis wind turbine (HAWT), where the axis of the rotor is parallel to the wind direction, the angle of attack at any point along the blade is more-or-less constant for a given wind speed. Up to the stall point, the lift force created is proportional to this angle of attack. The rotor driving force is largely made up of the in-plane component of the lift along the blade, but the out-of-plane component causes a thrust load on the rotor, tending to bend the blades and blow the turbine over. Put very simply: it is the in-plane component of force which 'brings in the money' since it determines the driving torque, but the thrust load 'costs the money' since it determines the strength requirement and to a large extent the fatigue life of the machine. For the vertical-axis configuration (VAWT), where the rotor axis is perpendicular to the wind stream, the angle of attack at any point on the blade varies in time, so the aerodynamic power and loads are unsteady. Although this is a complication, the vertical-axis wind turbine does have the advantage of not requiring re-alignment with the wind when its direction changes. The power of a wind turbine is proportional to the swept area of its rotor, and at a typical site about 500W per square metre would be the upper limit of what could be expected from any machine. Machines vary in size from 600mm diameter, rated at about 50W, to 60m diameter, rated at about 3MW. The 'best' size for a turbine for a particular application will be a topic we shall return to later. The main components of a turbine are illustrated in theannotated drawing (size 84kb)- the numbers used for the mechanical components in this text refer to that drawing.A wide variety of materials have been used for blade manufacture, including glass fibre on a foam core, glass-reinforced polyester (GRP), wood/epoxy laminate, and steel. The number of blades (4) on a rotor is to some extent dependent on size, with two blades being generally preferred on larger machines, and three blades for machines under 500kW. For a two-bladed rotor there would be a cost reduction associated with the reduced rotor weight, and a reduced gearbox cost because of the higher running speed; but the three-bladed rotor produces slightly more power and 'looks better'. The increased tip speed of a two-blader might also cause increased aerodynamic noise. On some HAWTs the rotor is placed downwind of the tower to improve the yaw behaviour of the rotor - its ability to track the wind as its direction changes - and improve the clearance between tower and blades under load; but an upwind rotor does not have the disadvantage of operating through the disturbed air flow behind the tower, with the associated aerodynamic noise penalty that would bring. Rotational speed is roughly inversely proportional to rotor diameter, so as to give a blade tip speed somewhere between 50m/s (smaller machines) and 100m/s (larger machines). The rotor main shaft (2), which will often carry a disc brake (5), transmits the torque to the gearbox (not numbered in the drawing, but located under the brake hydraulic unit (7)). On larger machines (MW scale) a planetary gearbox is often used for reasons of weight and size, but for smaller (kW scale) machines a simpler parallel shaft gearbox is often preferred. The generator (6) will commonly be of the simple induction type, providing damping to the transmission system; the alternative synchronous type has the advantage of being excited from the grid, but demands precise rotor speed control for synchronising at start up. The function of the controller is to match the output of the wind turbine generator (WTG) to the wind available and to the power demand; an important function on a HAWT is to change the orientation of the turbine, via a yaw drive (8), to match the wind direction. All of this equipment will be mounted in a nacelle to provide protection from the elements, but access will always be required for maintenance. The rotor and nacelle are mounted atop a tower, nowadays often of tubular steel construction but many lattice towers can still be seen supporting earlier WTGs; guyed pole towers are an option for VAWTs and for smaller HAWTs. The height of the tower is a trade-off between the benefits of catching the 'better' wind at some height above ground level versus the increased bending moment at the base. The foundations must not only withstand the bending moment arising from the thrust load on the rotor, but also cope with any vibrations arising from the rotational frequency. On any given site, the solid rock from which the foundations are built may be many metres below the surface. These civil engineering aspects of the project then raise the whole question of WTG installation. For a larger machine (over say 100kW) the tower will be built up on site from sections fabricated elsewhere, but the nacelle (possibly weighing several tons) must be hoisted to the top and the blades mounted at the hub. This would require a large crane, which is usually expensive and, if the terrain is very rough, impossible to deploy. An alternative method of installation would require some kind of hoisting from the ground, necessitating 'hard points' on both the nacelle and on the site. The volume and weight of the WTG and installation equipment may be such as to demand the construction of access roads to the site, depending on the nature of the terrain and the distances involved; such a road network could add substantially to the total cost of a wind energy project. For operational convenience, the monitoring and control panel will be located at the base of the tower, within the shelter of the tubular tower itself. Power cables, possibly under ground, will lead from the base of the tower to a transformer station; in a wind farm consisting of a number of WTGs, a local network of distribution points will be installed, leading eventually to the electricity utility's nearest substation where the connection to the grid is made. Cabling costs to a very remote site could easily outweigh the benefit to income of even a most favourable wind regime. Although by no means essential equipment at a wind farm site, the use of anemometry to measure wind speed could give the operator useful performance indicators on individual turbines and the farm as a whole. This then has been a first look at what the technology involves; we shall return to the subject again after giving some attention to the wind itself, both as the bountiful resource and the hostile environment. The Wind The local wind speed is the most important parameter for a prospective site. The energy in the wind is proportional to the cube of the wind speed (its force - or more precisely its pressure - multiplied by its velocity), so the energy yield of the proposed WTG is most sensitive to the site wind speed. Since the wind is continually varying, we use the mean annual wind speed (MAWS) to give a broad indication of the site's potential. Generally the windy parts of Europe are the Western and Northern coastal regions, but of course the regional landscape and local topography affect this picture greatly. The mistral winds of southern France and the seasonal winds over the islands of Greece are a distinctive exception to this broad rule. The map (size 70kb)reproduced (from Ref 1) hereshows the MAWS distribution for the twelve member states of the European Community, together with the annual average energy density. The map is presented in terms of a 'flattened landscape', so the appropriate topographic conditions must be selected from the following table to give an indication of the regional wind resource.By means of this table, the available wind energy can be estimated at the height of 50m for four topographic conditions. (Regions where local concentration effects may occur are not indicated. Such regions are expected to exist in the United Kingdom, France, Portugal, Spain, Italy and Greece.) --+-------------+-------------+--------------+----------------  | Flat, open  |    At the   |  More than   |   Well-exposed  |  farmland   |  sea coast  |10km offshore |  hills & ridges--+-------------+-------------+--------------+----------------  | m/s  W/m/m  | m/s  W/m/m  | m/s   W/m/m  |  m/s    W/m/m==+=============+=============+==============+================A |  >8    >600 |  >9    >800 |  >10    >950 |   >12     >2000B | 7-8 400-600 | 8-9 500-800 | 9-10 650-950 | 10-12 1200-2000C | 6-7 250-400 | 7-8 300-500 | 8- 9 400-650 |  9-10  800-1200D | 5-6 150-250 | 6-7 200-300 | 7- 8 250-400 |  8- 9  600- 800E |  <5    <150 |  <6    <200 |  < 7    <250 |   < 8     < 600--+-------------+-------------+--------------+---------------- All the well-exposed hills and ridges of mainland Scotland, the Western Isles, the Orkney Isles, and the Shetland Isles have a MAWS greater than 12m/s and an energy density greater than 2kW per square metre. As will be discussed in the final chapter, current wind farm developments in the UK are concentrated on the good sites of Cornwall, Wales, Cumbria, and the East coast regions of England, with MAWSs greater than 7m/s. The 'technical potential wind energy resource' for the UK has been conservatively estimated (Ref 2) to be 45 TWh/y, around 15% of annual electricity supply. As has been mentioned earlier, the wind varies (in both speed and direction) with height above ground level, as in any boundary layer flow. This wind shear effect can be quite significant for a large WTG (more than 20m diameter), in providing a wind pressure gradient across the rotor disc; the loading on a turbine blade will fluctuate periodically as it rotates. The wind shear profile is often assumed to follow a 'one seventh power law', but the reality can be significantly different where for example a hill top causes a local acceleration of the wind. The wind speeds given in the European wind atlas are quoted for a reference height of 50m above ground, which is representative of the hub height of a MW scale wind turbine; standard UK Meteorological Office data is quoted for a height of 10m, which is therefore probably an under-estimate of the hub-height wind speed. Historical records of wind speed at the Cranfield control tower anemometer indicate a MAWS of around 4m/s which is rather less than the 6-7m/s suggested by the wind atlas. Variations of wind speed occur over a wide range of time scales. Some years are windier than others, and there may well be an historical trend of changing wind speeds. Seasonal variations are marked, even for the temperate climate of Northern Europe; the monsoons of SE Asia for example would place critical loadings on a WTG. The variations associated with the passage of weather systems over the site have a time scale of around three days. Diurnal variations also have an implication for wind power production: when the wind dies down in the late afternoon, other grid-connected sources are required to provide the power for the early evening peak demand. Our personal experience of the wind, standing on the top of an exposed hill for instance, tells us that second-by-second variations can be very considerable. This variability is caused by turbulent eddies in the flow, generated by thermal gradients and topographic features. Wind gusting superimposes random blade vibrations on top of the periodic oscillations due to wind shear; fluctuations in rotor torque impose severe demands on the wind turbine gearbox and could mean that the electrical power quality is unacceptable to the utility. For the estimation of structural excitation and fatigue damage, it is often useful to use a spectral approach to the wind variability, considered as continuous turbulence. PICTURE size 88kb It can be seen, therefore, that associated with high wind speeds are various harmful effects of great concern to the wind turbine manufacturer. To first order, turbulence levels are proportional to the wind speed - levels of 20% are not atypical for good sites. Sites in mountainous regions may have quite excellent average wind speeds, but the air is so churned up by the terrain itself that the turbulent loadings demand a costly 'over-design' to a number of critical components. Simply because the wind turbine is placed high up on a tower it will be exposed to the worst of driving rain. In Northern regions, and at high altitudes, the precipitation comes down as snow and forms ice. In coastal regions, the wind turbine must be protected against the corrosive effects of sea water. And at the other extreme, in the deserts of California, temperatures are very high, and sand erosion and dust penetration are important problems. These elements form the hostile environment in which a WTG must operate. Technology II The target lifetime for a wind turbine is generally 20 years. Since they are intended for installation in remote and exposed areas, maintenance will be both costly and limited. On-site inspections of many components can be carried out regularly, but failed components will be replaced rather than repaired. Sophisticated examination procedures cannot be carried out on-site, and specialist repairs (to the gearbox for example) must be carried out at the factory. Problems with the rotor (blades and hub) are likely to be costly since they will require access and possibly removal. Elaborate monitoring instrumentation (over and above basic operational status and power production) on every machine in a wind farm would add significantly to the total project cost. Component fatigue is obviously a major concern to a manufacturer, and very roughly speaking the stochastic loading associated with turbulence accounts for 50% of the fatigue damage on the rotor. But for a HAWT, particularly vulnerable components are the yaw drive and yaw brake which take a lot of punishment especially on turbulent sites. Continual wind veering means that for a large proportion of the time the rotor is misaligned to the wind, resulting in loss of power, increased blade bending moments, and torque at the yaw axis. It has been found counter-productive to make the yaw controller too active in following the wind direction changes, since that in itself can significantly increase fatigue damage. Any kind of imbalance in the rotor can also add to the vibratory loading on the yaw drive. The variability of the wind also raises the need for power control. Power cannot be allowed to increase much beyond the rated power of the machine because of the risk of damage to the gearbox and generator. There are three methods of power control in wide use. Full-blade pitch control provides a mechanism for varying the pitch angle of each blade so as to reduce the angle of attack in high winds; the hub of a pitch-controlled HAWT is quite complicated and the actuator system must have enough power to rotate the whole blade. A simpler solution is tip control where only the outer 20% (say) of the blade is rotated. The figure (size 21kb)shows power versus wind speed (m/s) of a tip-controlled rotor - the measured mean power over each 10 minute interval has been normalised by the rated power of 330kW. The hub and blade root details of a tip-controlled HAWT will be simpler than for full-blade pitch control; maintenance of the actuator and tip bearings of a tip-regulated blade can be done from an access door half way up the tower with the blade at bottom-dead-centre. Stall regulation on the other hand, relies on the inherent aerodynamic stalling of the aerofoil sections to limit the power in high winds, and so in principal is the simplest and cheapest method of power control to engineer. The next figure (size 21kb) shows a similarly-treated power curve for a stall-regulated HAWT, this time normalised for a maximum power of 125kW; actually this turbine is rated by the manufacturer at 108kW, which illustrates the problem of over-production for stall-regulated turbines. Stall-regulated rotors must also have small, pitchable tips fitted to provide aerodynamic braking in an emergency if the rotor should over-speed (when the mechanical brake would be unable to stop the rotor). Most small WTGs (under say 150kW) are stall-regulated; there is some interest in simplifying the rotors of larger machines by using stall-control, if only there was confidence that the power would actually be limited. Variable speed control aims to continuously vary the rotor speed with the wind speed so as to keep the ratio of the two constant for optimum rotor efficiency. The necessary AC-DC-AC inverter links have now been developed to make this a possibility. An innovative German manufacturer has now developed a variable speed WTG without gearbox, using a directly-coupled large rim generator. The point has been made earlier about the desirability of simplifying the rotor for the sake of reliability and maintainability. Indeed, the achievement of simplicity has become something of a creed in the wind industry, with the widespread view that the more complicated a component is made (often for the purpose of increasing energy capture), the less reliable and more difficult to maintain and repair it will be. By far the biggest proportion of reported component failures is to the electrical control system. Failure of the grid itself is not infrequent and will result in the WTG shutting down. Failures on the rotor are significant and, as has already been mentioned, are generally difficult to correct. Problems with the specialist equipment such as gearbox and generator, though not particularly frequent, are associated with a high level of down time. Where mechanical failures have occurred in great numbers, they were invariably the result of poor design detailing and/or manufacturing quality control. These type failures are most serious for the operator. One of the most (in)famous examples concerned the Alternegy GRP blades fitted to something like 1500 wind turbines in California; after a relatively short period of operation, some of these blades experienced root failures which were subsequently found (1986) to be associated with inadequate quality control (incorrect tightening of root bolts, excessive stiffness in the root bushings, and lack of root filling). Law suits were the inevitable outcome, and an extensive blade replacement programme. This, and a few other failures like it, are now seen to be the result, to a large extent, of the 'over-stimulation' of a juvenile wind energy industry by the tax credits offered for renewable energy plant in California in the early 1980s. Since that time, the technology has improved considerably, through a process of emulation of the best and rejection of the worst. Availability factors of 95% or more, and overall load factors of up to 40%, are now being claimed for the most successful machines. And many of the reasons for 'under-performance' of wind turbines are now better understood: poor positioning in relation to the local landscape and to other turbines (micro-siting) is the cause in many cases. Operation in the rain and blade soiling are also reasons for under-performance in various locations. Vertical-axis wind turbines have been mentioned only briefly so far. In general, they have not lived up to the expectations their proponents have raised; none of the supposed advantages over the HAWT configuration have been realised in practice, and they have suffered more than their share of vibration problems and type failures. Worldwide, there are now very few manufacturers actively producing vertical-axis machines. Indeed the House of Commons Public Accounts Committee has recently criticised the UK's VAWT development as a waste of resources. Economics Like any other renewable energy source, the fuel for a WTG is free, with no delivery charges to the site. So why is economics an issue? Well, the turbine must be bought, delivered to the site, installed and commissioned. This requires capital, and there is a cost associated with that capital, even if only the 'loss' of not getting any interest when you take the money out of your savings account to buy the wind turbine. Then there are costs incurred when the WTG is operated and maintained. All these costs are encapsulated by the concept of the cost of energy (CoE), which is an expression for the total generation cost per unit of electricity produced:                   G  =  C ( R + M ) / E where C are the initial capital costs, R is the annual charge rate on capital, M is the annual operation and maintenance costs as a fraction of the initial capital, and E is the annual energy yield given by:                   E  =  h W F where h is the number of hours in a year (8760), W is the rated power of the wind turbine, and F is the overall load factor. So the generation costs are:                   G  =  ( C / W ) ( R + M ) / ( h F ) This is the expression given for CoE in Ref 3, which is essentially that presented in the final chapter (Ref 4) of the most useful British text book.The ratio C / W describes the initial capital cost per kW of rated power, the sum R + M describes the annual costs of capital and maintenance as a fraction of the initial capital, and the product h F is the effective number of hours in the year when the WTG can be considered to be operating at rated power. The annual charge rate can be expressed in terms of the interest or discount rate r net of inflation, and the repayment term n years:                   R  =  r / { 1 - ( 1 + r ) ^ (-n) } The overall load factor may be expressed as the product of the capacity factor L of the wind turbine, reduced by factors representing machine availability A and other causes of loss of energy or efficiency a, such as inter-machine interference or transmission losses, which are not accounted for in the machine performance characteristics:                   F  =  L A a The capacity factor L is determined from the machine performance characteristics and the wind speed frequency distribution. Reasonable assumptions can be made for the performance characteristic as a function of wind speed (or wind speed normalised by the rated wind speed Ur ); the figure (size 29kb)(reproduced from Ref 4)shows a cubic curve over the range between the start-up wind speed and the rated speed, with zero power above the shut-down speed. For the wind speed probability, a Rayleigh distribution is usually assumed in the absence of any site-specific data: this gives an expression for the probability of exceeding a given speed as a function of the site MAWS Um . The same figure also shows such probability distributions for typical values of the ratio Ur / Um . The capacity factor is then determined as the integrated sum (over all possible wind speeds) of the power production multiplied by the wind speed probability. For a given wind turbine, this is a strong function of the site MAWS. The expression for the cost of energy can now be evaluated using some representative values: 2% for operation and maintenance costs, 95% for machine availability, and a factor of 90% for other losses. Assuming a turbine with a rated wind speed of 13.3m/s and a site with a MAWS of 7.8m/s (so that the ratio Ur / Um is equal to 1.71), the capacity factor turns out to be 37% which is representative of the first wind farm sites to be developed in the UK. If we further assume that the capital is borrowed at a real rate of return of 8% per annum, and repayed over a term of 8 years, then the annual charge comes to 17.4%. Finally, for a capital cost of �880 per kW of rated power, the cost of energy (over the period of the repayment) comes out as 6.3p/kWh. This number should not be taken as the definitive cost of wind-generated electricity, but merely an indicative value; the next figure (size 39kb)(again reproduced from Ref 4) shows the sensitivity of this CoE to variations in the assumed values (logarithmic scales). Wind speed is more significant than any other factor. We have earlier discussed the issues affecting machine availability; capital cost is a function of machine size, production volume, and site accessibility. The current trend is to use an alternative formulation for cost of energy based on costs per unit of rotor swept area, rather than costs per unit of rated power. Thus the CoE equation becomes:                   G  =  ( C / S ) R / (E / S)  +  M C / E where S is the rotor swept area, and the specific yield is given by the empirical relation               E / S  =  2.5 Um ^ 3 In a recent analysis of UK developments (the BWEA submission to the Renewable Energy Advisory Group (Ref 5)), the following table of capital costs was given as a linear function of site MAWS                              Wind        Capital                            speed         cost                           ( m/s )      ( �/m/m )      Very good site          8.5           420     Good site               7.5           350     Reasonable site         6.5           280 and a value of 2.5% was recommended for the O&M costs. Assuming a capital recovery factor R of .174 as before, and taking a hub-height wind speed of 7.8m/s, gives CoE of 6.2p/kWh which is very close to the previous value. In making cost comparisons with other sources of electricity, it is important to use similar assumptions concerning the capital recovery in each case. The generation of electricity by coal, gas and nuclear is a capital-intensive operation carried out by a large industrial company (National Power, PowerGen, and Nuclear Electric in the UK), and new generation plant is financed internally over a long time-scale. The Renewable Energy Advisory Group (Ref 5) has recently estimated that "a typical onshore wind turbine would generate electricity at a cost of about 7p/kWh assuming a 20 year life, 7.5m/s wind speed at hub height and a 15% required rate of return". There have been two distinct trends in the size of wind turbines. What may be described as "the Danish approach" involved stretching the simple and small-scale technology which had its roots in agricultural engineering; this led from machines of 22kW, upwards to 100kW and currently machines of around 300kW. The other approach started with an initial decision to develop large (MW) machines using aerospace engineering, but the prototypes were prone to failure and uneconomic - these "manufacturers have gradually reduced the size to find a machine which is commercially viable. The two approaches have now met at machines with rated capacities of approximately 300kW and rotor diameters of 25m." (Ref 6). A recent study on the potential of large machines (Ref 7) has concluded there will be little difference in the manufacturing cost (per annual kWh) of turbines with diameters in the range 30m to 80m; a related study (Ref 8) makes the same conclusion in relation to CoE over this diameter range, with the implication that "the best use of land" will be a decisive criterion for selecting the size of large wind turbines. An energy accounting approach to the cost of wind energy has been taken in Ref 9. A 'harvest factor' was defined as the ratio of displaced primary energy by energy generated from the WTG to the primary energy consumed in its manufacture; the best machines were shown to have a harvest factor of 4, which means that within one year, 4 times the primary energy input was displaced by the wind electricity generation. It is beyond the scope of this lecture to present comparative costs of the different supply technologies, since that would require the adoption of common standards in each case. But the opportunity must be taken to raise the issue of external costs, the costs which may be added to the financial costs to account for the external consequences of electricity production, including damage to the environment and health, the cost of clearing up polluted areas, defending oil routes, etc. Many countries are now introducing 'carbon taxes' which are a practical attempt to add something for external costs to the fossil-fuel sources. An estimate of the avoided cost of using wind energy is given in Ref 10 for both the Netherlands and UK. In the UK, that study finds a figure of 4.6p/kWh is saved by using wind energy, of which 1.9p/kWh is for the cost of emissions; these estimates may be conservative since they exclude the external costs associated with particulate and solid pollutants. Clearly estimates of this magnitude would make a significant difference to our interpretation of comparative costs, but the methodology needs to gain a broad acceptance before it can provide a rational basis to decision making. A recent critical survey (Ref 11) of published electricity generation costs and trends has concluded that a realistic carbon tax, scheduled for the year 2000, would add 1.2p to the cost of generating a kWh from coal, 0.8p for gas, and 0.5p for nuclear. Current Developments in the UK this section is most in need of updating: scheduled for January 1995 The situation of wind power (and, indeed, the other renewables) has changed dramatically in the UK since the turn of the decade. Prior to then, there were just a few, isolated and experimental, wind turbines scattered across England, Wales and Scotland. The UK's Department of Energy (now disbanded) began a modestly-funded R&D programme in the mid 1970's, concentrating most of its support for the large prototypes eventually built on the Island of Orkney, Scotland (the 60m diameter HAWT rated 3MW) and at Carmarthen Bay, Wales (the 25m diameter VAWT rated 130kW). From the late 1970s onwards, the electricity utility for England and Wales, the Central Electricity Generating Board (now disbanded) showed a steadily-increasing interest in wind energy, to the extent of developing the site at Carmarthen Bay as a demonstration centre for wind energy, and building a 55m diameter HAWT (rated 1MW) at Richborough in Kent. Then, at the end of the 1980s, the CEGB announced a major plan to develop wind farms, of about 25 machines each, at three named sites. But that plan was blown sideways by the Government's decision to privatise the UK electricity supply industry. The CEGB was split into four companies: National Power, PowerGen, Nuclear Electric and the National Grid Company. The 12 regional area boards, which distributed and sold on the electricity to the consumer, became independent companies, called Regional Electricity Companies, jointly owning the National Grid Company. Also at this time (late 1989 - early 1990), the real costs of nuclear power were emerging from the public inquiry into the proposed Sizewell B reactor (now started up), giving rise to the need for the Government to retain ownership of the nuclear power stations through Nuclear Electric. Simultaneously, a change to the rating system (local property tax) eliminated the unfair distinction between generation plant operated by the CEGB and plant operated by the independents. From this major upheaval, via the mechanism of the Non-Fossil Fuel Obligation, has emerged the Government stimulus which has had such a dramatic impact on renewables in the UK. The NFFO is a requirement placed upon the 12 RECs "to contract for a specified amount of generating capacity from non-fossil sources (ie nuclear and the renewables, including wind). ... The additional cost to the RECs resulting from satisfying NFFO orders is recovered by them via a levy on electricity sales. ... The first renewables order was made [in 1990] and the contracts made by the Non-Fossil Purchasing Agency to comply with it include nine for wind, of which five are wind farms" (Ref 12). A second order was placed in 1991, including a total of 49 wind projects, of which 38 are wind farms. These wind projects are listed in the attached table, and their locations indicated on the attached map. References 1.     E.L. Petersen, I. Troen, N.G. Mortensen         "The European wind energy resources",        ECWEC, Herning Dk, June 1988, pp103-109 2.     Department of Energy,        "Renewable energy in the UK: the way forward",        Energy Paper No 55, 1988 3.     BWEA         "Wind Power for the UK",        January 1987 4.     D.T. Swift-Hook       "Economics", Chapter 18 in              L.L. Freris  (ed)              "Wind Energy Conversion Systems",               Prentice Hall, 1990 5.     Department of Trade and Industry,        "Renewable Energy Advisory Group: Report to the               President of the Board of Trade",        Energy Paper No 60, November 1992 6.     EWEA         "Wind Energy in Europe - a Time for Action",        October 1991 7.     R. Harrison, G. Jenkins, A.N. Macrae         "Study on the next generation of large wind               turbines - Manufacturing cost analysis",        ECWEC, Madrid, September 1990, pp 433-437 8.     E. Hau         "Study on the next generation of large wind               turbines - Energy cost analysis and               conclusions",        ECWEC, Madrid, September 1990, pp443-447 9.     J. Schmid, H.P. Klein         "Database on existing wind turbines and               climates in the Community",        EC Wind Energy Contractors' Meeting,               Maastricht, June 1991, pp 47-54 10.    A.J.M. vanWijk, W.C. Turkenburg         "The value of wind energy",        BWEA13, Swansea, April 1991, pp1-9 11.    D.J. Milborrow         "Energy generation costs - now and for the               year 2000",        BWEA15, York, October 1993 12.    R.A. Meir         "The NFFO - a Department of Energy view",        BWEA13, Swansea, April 1991, pp11-13 13.    Department of the Environment         "Planning policy guidance note: renewable               energy",        PPG22, February 1993 up to Current Wind Turbine Technology - menu WT03-B20-87IA006-000055-B004-398http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/sources.html 138.80.61.12 19970221174158 text/html 4678HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:11:53 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 4507Last-modified: Tue, 09 Jul 1996 02:26:04 GMT REFERENCES Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). REFERENCES 1. USSR State Committee on the Utilization of Atomic Energy: "The Accident at the Chernobyl NPP and its Consequences"; IAEA Post-Accident Review Meeting, Vienna, 25-29 August 1986. 2. Belyaev S.T., Borovoy A.A., Demin V.F. et al. The Chernobyl Source Term. Proc. of Seminar on comparative assessment of the environmental impact of radionuclides released during three major nuclear accidents: Kyshtym, Windscale, Chernobyl. 1-5 October 1990, Luxembourg, Rep. EUR 13574, V1, pp.71-91. 3. Exposures from the Chernobyl Accident, ANNEX D of the "Sources, Effects and Risks of Ionizing Radiation", UNSCEAR Report to the General Assembly, UN, New-York, 1988. 4. Proceedings of the First International Workshop on Past Severe Accidents and Their Consequences, 30.10 - 03.11 1989, Sotchy, USSR. "NAUKA" Moscow, 1990. 5. Belyaev S.T., Borovoy A.A., and Dobrynin Yu.L. Information System PROBA: Monitoring of Area Radioactive Contaminations after the Chernobyl Accident". Atomnaya Energiya, 68(3) 197-201 (1990). 6. Warman E.A. Soviet and far-field radiation measurements and an inferred source term from Chernobyl. N.Y.Chapter - Health Physics Society Symposium on the Effects of the Nuclear Reactor Accident at Chernobyl, Brookhaven National Laboratory, April 1987. 7. Atmosphere Physics. Coll. vol. Institute of Physics, Acad. Sci. Lithuania, v. 14, Vilnus, Mokslas, 1989. 8. Medical Consequences of the Chernobyl Accident. Inform. Bull. Ukr. Resw. Centre of Radiation Medicine, Kiev, 1991. 9.Jaworowski Z. and Kownacka L. Tropospheric and stratosphericdistributions of radioactive iodine and cesium after the Chernobylaccident. J.Environ. Radioact. 1988 (6), 145-150. 10.Gudiksen P.H., Sullivan T.J., and Harvey T.F. The CurrentStatus of ARAC and Its Application to the Chernobyl Event. VCRL - 95562 Preprint, Lawrence Livermore National Laboratory, Oct. 1986. 11. ApSimon H.M., MacDonald H.F., and Wilson J.J.N. An InitialAssessment of the Chernobyl-4 Reactor Accident Release Source".J.Soc.Radiol.Prot., 6(3),1986. 12.Begichev S.N., Borovoy A.A., Burlakov E.V., et.al. Fuel of unit4 reactor at the Chernobyl NPP. Preprint Institute of AtomicEnergy - 5268/3, 1990. 13.Kirhner G. and Noack C. Core history and nuclide inventory ofChernobyl core at the time of accident. Nucl. Safety, 29 (1) 1-5(1988). 14.Izrael Yu.A., Vakulovskii S.M., et.al. Chernobyl: RadioactiveContamination of Natural Media. Leningrad, Gidrometeoizdat,1990. 15.The safety of Nuclear Power Plants. An assessment by aninternational group of senior nuclear safety experts. The UraniumInstitute. London 1988. 16.Gudiksen P.H., Harvey T.F. and Jange R.Chernobyl Source Term,Atmospheric Dispersion, and Dose Estimation. Health Physics, 5,(5), 697-706, 1989. 17.Warman E.Source Term and Emergency Response - Post ChernobylPerspective, Stone&Webster Engineering Corp., Boston, MA. 18.Begichev S.N., Borovoy A.A., Burlakov E.V., et.al. "RadioactiveRelease due to Chernobyl Accident. Proc Int. Sem. "FissionProducts Transport Processes in Reactor Accidents" May 22-26,Dubrovnik, Yugoslavia, 1989. 19.ICRP Publication 38. Radionuclide Transformation. Energy andIntensity of Emissions. Pergamon Press20.Mervin S., Balonov M.(eds.) The Chernobyl Papers, Vol.1, Doses to Populationand Early Health Effects Studies. Research Enterprises, Richland, WA, 1993.21.Chernobyl' - pyat' trudnykh let. IZDAT, M., 1992. For more information about data base contents see: Contents. Access to Data Bases is supported by Relcom (Reliable Communications Asociation). WT03-B20-88IA006-000055-B003-355http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/history.html 138.80.61.12 19970221173610 text/html 4977HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:06:18 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 4806Last-modified: Tue, 09 Jul 1996 02:26:03 GMT The causes of the accident and its progress. <Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). The causes of the accident and its progress. Untill the end of the day of 25 April 1986, operators of theforth unit of the Chernobyl NPP did some erroneous actions. Itresulted in unstable operation mode of the reactor expressed in the decreaseof its power with unacceptably small reactivity margin [1]. Thiscircumstance had combined with design drawbacks of reactors of thistype (positive reactivity coefficient and instability of operationat small power, imperfection of control and protection system [15]and with most serious violations of the safety operation rules(for instance, switching-off blocking of the reactor protectionsystem). After the second electroturbogenerator was turned offas it was planned before in order to make an electrotechnical experiment, all these factors had led to a practically instantaneous catastrophicincrease of thermal power on 26 April 1986 at 1 hour 23 min(Moscow time). As a result, steam explosion occured, thereactor and a part of the building were destroyed.Radioactive materials accumulated in the reactor core started to be thrown out into the environment Multiple fire sites were formed both inside the reactor halland on roofs of nearby buildings because of the explosion andthrowing out fragments heated up to high temperatures. By 5 o'clock on26 April 1986 the fire men smothered main fire sitesexcept the central reactor hall, where graphite continued toburn in the close vicinity of the destroyed reactor In subsequent days, about 5000 tons of materials, includingabout 40 tons of substances containing boron and absorbingneutrons, 2400 tons of lead, 1800 tons of sand and clay, 600 tonsof dolomite, trinatriumphosphate and polymerizing liquids werethrown into the reactor well from helicopters of air forces toextinguish burning graphite and suppress radioactive release. The flowof different substances continued untill the beginning ofJune 1986. It is not clear yet, whether the throwing of materials offthe reactor ruins reached its goal. According to the data ofsubsequent observations, only a small part of the thrown materialsgot into the reactor well, and they mainly formed hills with theheight up to 15 m in the central hall of the unit 4. Partial fillingof the channels formed at the explosion could take place. As thechannels were the ways for flows of hot air leaving the reactorself-heating of the reactor core ruins could take place. Theaction could also be in the form of mechanical destructions anddisplacements, and this also caused redistribution of the energybalance formed after the accident. After accident the object "Shelter" was constructed. Its construction was one of the most complicated building work in the world. In condition of high radiation this unique object was finished by November of 1986. "Shelter". Winter, 1986. Unfortunately, 30 persons (staff of block and fire brigade) died receiving high doses of radiation. The square of contaminated area has been more than 130000 sq. km (by Cs in USSR only). Near 4.9 millions people lived on this territory before the Accident. All population from 30-km zone was evacuated totally. A lot of people were relocated. Impact of the Chernobyl Accident on a Nuclear Energy Policy is tremendous. Some countries stopped national nuclear energy programs. A construction of new plants in USSR were frozen. Public opinion was directed against Nuclear Power Plants. Some plants was closed (Armenian NPP, for example. Nowadays, Armenia has decided to reactivate plant for energy production). Chernobyl Accident had initiated an international activity in the area of nuclear safety and nuclear emergency planning. Many countries started a development of decision support systems for nuclear accident case. See also: Contents, Release of Nuclides, Core Inventory, RBMK-1000 Access to Data Bases is supported by Relcom (Reliable Communications Asociation). WT03-B20-89IA006-000057-B015-571http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/coreinv.html 138.80.61.12 19970221184608 text/html 7037HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:16:27 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 6866Last-modified: Tue, 09 Jul 1996 02:26:02 GMT  Radionuclide composition of the  Chernobyl-4 reactor core. Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). Radionuclide Composition of the Chernobyl-4 Reactor Core. At present the radionuclide inventory of the reactor core is calculated with the help of special computer programs, as far as physical processes of nuclide formation and their characteristics are well known. For each particular reactor design, knowing fuel composition and parameters characterizing time variation of power generation, this problem has sufficiently accurate solution. Calculation error depends on particular situation and the radionuclide type. For instance, for calculations made in [9] this error is about 5% for Cs-137 and 20% for Pu-236. In [9] the radionuclide inventory of the core at the accidental moment is not presented in explicit form. However, as far as it presents the absolute value of the release of basic radionuclides (with the estimate of error +50%) and their part with respect to activity of nuclides in the reactor, the core inventory at the accidental moment can be easily reconstructed by calculations, and this was done in [18] (Table at the end of current topic). The authors of [1] used in their calculations the averaged value burn-up equal to 10.3 MW*day/kg of uranium. There is a number of later works, for instance, [12, 14, 16], the authors of which calculate the same radionuclide inventory with different accuracy. There are no serious differences in the general approach. However, in a number of works made soon after the accident, their authors had to use approximate initial data, because they had no accurate data on fuel burn-up and enrichment. Thus, employees of Bremen university sampled soil in the region of Munchen, measured the ratio of activity of reference isotopes and estimated the average fuel burn-up at the moment of the accident as (12.85-0.15) MW*day/kg [13]. The authors from the Livermore Laboratory [16] used underestimated data on fuel burn-up and time of reactor operation This explains somewhat lower results as compared with [1, 12]. Specialists of the USSR hydrometeorological service [14] relied mainly on the data of [1], but corrected some of them with the results of their own field measurements. The most accurate value of average burn-up is presented in [12] and is 10.9 MW*day/kg. It is essential that there were fuel assemblies with different burn-up in the reactor. Therefore, the usage of the average burn-up value for calculation without accounting for nonlinearity of radionuclide formation in separate assemblies also causes considerable inaccuracy. These processes of individual burn-up and generation of radionuclides in each of 1659 fuel assemblies were taken into account only in the work "Fuel in the reactor of unit 4 at the Chernobyl NPP" [12], which gives the most accurate, to our opinion, characteristics of radionuclide inventory of the core. Unfortunately, this work considers only the inventory of basic long-lived isotopes. Further refinement of calculations connected with accounting for inhomogeneity of neutron field in the reactor weakly told on the values presented in table placed below, excluding the data on inventory of transuranium elements. Radionuclide inventory in the unit 4 reactor at the Chernobyl NPP as of 26 April 1986 according to the data of different works.  +--------+--------+--------------------------------------------+ |Nuclide | T-1/2  |Activity [MCi] according to different works |   +--------+--------+-------------+--------+------+------+-------+ |        |  [19]  |  [1]([18])  | [13]   | [16] | [14] | [12] | +        +--------+-------------+--------+------+------+-------+ | Kr-85  | 10.7y  |     0.9     | ------ | 0.55 | ---- | ----- | | Sr-89  | 50.5d  |    63       |  90    |  80  |  52  | ----- | | Sr-90  | 29.1y  |     5.5     |  6.2   | 4.2  | 5.2  |  5.9  | | Zr-95  | 65.0d  |   130       |  154   | 134  | 130  | ----- | | Mo-98  | 66.0d  |   150       | ------ | 137  | 130  | ----- | | Ru-103 | 39.3d  |   130       |  128   | 116  | 130  | ----- | | Ru-106 | 368d   |    56       |   33   |  29  |  52  |   23  | | I-131  | 8.04d  |    86       |   66   |  82  |  90  | ----- | | Te-132 | 78.2h  |    73       |   56   | 109  | 120  | ----- | | Xe-133 | 5.24d  |   170       | ------ | 193  | ---- | ----- | | Cs-134 | 2.06y  |     5.0     |    4.2 | 3.7  | 4.0  |  4.1  | | Cs-137 | 30y    |     7.7     |    8.2 | 5.6  | 7.2  |  7.0  | | Ba-140 | 12.7d  |   130       |  133   | 140  | 130  | ----- | | Ce-141 | 32.5d  |   150       | ------ | ---- | 130  | ----- | | Ce-144 | 284d   |    88       |  113   |  82  |  90  |  106  | | Pu-238 | 87.7y  |   0.027     |  0.022 | ---- | 0.026| 0.025 | | Np-239 | 2.35y  | 720(1340*)  |  560   | ---- | 1300 | 1570  | | Pu-239 | 24065y |   0.023     |  0.024 | ---- | 0.023| 0.04  | | Pu-240 | 6537y  |   0.033     |  0.05  | ---- | 0.033| 0.04  | | Pu-241 | 14.4y  |     4.7     |  5.7   | ---- | 4.6  | 5.0   | | Am-241 | 432y   |   ------    | 4.2E-3 | ---- | 0.024|0.0037 | | Pu-242 | 3.8E5y |   6.7E-5    | 6.7E-5 | ---- | ---- |5.6E-5 | | Cm-242 | 163d   |   0.7(0.4*) |  ------| ---- | 0.7  | 0.83  | | Am-243 | 7380y  |   ------    | 1.7E-4 | ---- | ---- |1.5E-4 | | Cm-243 | 28.5y  |   ------    |  ----  | ---- | 0.001| ----- | | Cm-244 | 18.1   |   ------    | 4.0E-4 | ---- |2.6E-3|4.8E-3 | +--------+--------+-------------+--------+------+------+-------+ * Figures in brackets are corrected according for individual burn-up of different fuel assemblies. See also: RBMK-1000, Release of nuclides, Contents. WT03-B20-90IA006-000057-B004-151http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/rfstage.html 138.80.61.12 19970221191555 text/html 984HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:46:17 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 814Last-modified: Tue, 09 Jul 1996 02:26:03 GMT First Stage of Radio nuclides Release. Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). First Stage of Radionuclides Release. At the first stage the mechanical release of the dispersed fuel took place as a result of the initial explosion. Its radionuclide composition corresponds to fuel inventory of the core with enrichment by volatile nuclides of iodine, tellurium, and cesium (Load picture for illustration if it is needed). See also: Core Inventory, RBMK reactor, Contents. WT03-B20-91IA006-000057-B004-183http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/rsstage.html 138.80.61.12 19970221191607 text/html 1456HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:46:28 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1285Last-modified: Tue, 09 Jul 1996 02:26:03 GMT Second Stage of Radio nuclides Release Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). Second Stage of Radionuclides Release. At the second stage, from 26 April till 2 May(picture of release), the power of the release fell, obviously because of the measures undertaken for extinguishing of burning graphite, and formation of the filtrating layer. The dependence of the release power on time Q(t) at this stage can be presented in the first approximation as Q(t) = 0.25.exp(-0.28.t), day , (1) where 0.25, day , is the part of the total release corresponding to the first day; t, days, is the time after the explosion. At this stage the release consisted of fine-dispersed particles of fuel dust entrained by the flow of hot air and of graphite burning products. The radionuclide composition is also close to the fuel one. See also: RBMK-1000, Contents. WT03-B20-92IA006-000057-B004-209http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/rtstage.html 138.80.61.12 19970221191616 text/html 1423HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:46:38 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1252Last-modified: Tue, 09 Jul 1996 02:26:04 GMT Third stage of radio nuclides release. Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). Third Stage of Radionuclides Release. The next, the third stage of the accident is characterized by fast increase of the release power(picture of stages), at first with predominance of the volatile component, especially of iodine, and then the composition became again close to the fuel one. The approximation of the release power in this period is the following: Q(t) = 0.09.exp(0.35.(t-5)), day . (2) The release at this stage was attributed to fuel heating over 2000 C by the residual heat release. As a result of temperature-dependent migration of decay products and chemical conversions of uranium dioxide, the decay products were released in the form of aerosols or of the particles of graphite burning products. See also: RBMK-1000, Core Inventory, Contents. WT03-B20-93IA006-000057-B014-295http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/speech.html 138.80.61.12 19970221183539 text/html 8459HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:05:33 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 8288Last-modified: Tue, 09 Jul 1996 02:25:53 GMT Energy Awareness Month Speech Energy Awareness Month--Speech Energy Awareness--Why Should We Care? For the fifteenth year in a row, the Department of Energy (DOE) has declaredthat October is Energy Awareness Month. Some people may ask why? Prices arereasonable. There are no gas lines or fuel shortages. Why should you care? The reason is our children, their children, and the generations that follow.The United States and other nations are making decisions today that will affectthe global economy and environment for centuries to come. In 1860, when Abe Lincoln was running for President, the United States wasusing the equivalent of 111 million barrels of oil annually. Today, we usethat much oil in less than a week. Energy--where we get it, how we use it affects virtually every aspect of ourlives--our health, our budgets, our national economy, our national security, ourlifestyles. We think it's worthwhile once a year to stop and think about this pervasiveinfluence on our lives. To understand the role of energy today, and properly plan for its future, itis important to understand its past. Since the first Native Americans, we have constantly sought new sources ofenergy. We first used our own physical energy, then harnessed the strength ofanimals. We tapped the power of streams and rivers to operate machines, caught thewind to pump water and to sail our ships. We found ways to use fossil fuels for energy, and today they account for 85%of the energy used in the United States. We learned to harness the energy from nuclear power, which provides 20% ofour electricity. We also are learning ways to more efficiently use renewable energy: solar,hydro, wind, and thermal. Renewables today account for 8% of our nation'senergy. Energy surrounds us, yet it is seldom noticed. It is hidden behind walls andunderground, or is designed to prevent us from having to think about energywithin our cars. Our homes, cars and consumer products often are designed toprevent us from having to think about energy. For example, we all know that automobiles use a lot of energy, but do you knowhow much of the energy contained in your car's gasoline actually turns thewheels? The answer is only 13%. The rest of the energy is wasted. Another example: the United States has 5% of the world's population. Do youknow what percent of the world's energy we use? The answer: 25%. One more quiz question: we know that motor vehicles cause air pollution. Doyou know how much we spend for health care related to air pollution? Theanswer: $50 billion each year. By one estimate, that pollution is responsiblefor 120,000 premature deaths each year in the United States. Though we may take it for granted, energy costs amount to a significantportion of the dollars we spend. We don't realize the cost, because the energycosts of our products are imbedded in them--we don't see the energy costs formanufacturing, transporting, or marketing. Last year, we spent $51 billion on imported oil. By the end of this decade,the amount of U.S. dollars leaking away to oil-producing nations overseas isexpected to grow to $94 billion. Today, we spend almost $500 billion annually in the U.S. to light and heatour homes and offices, run our transportation systems, and operate our farmsand factories. Even modest energy savings can produce new tax-free disposable income for ourfamilies, new capital for our businesses, and lower overhead to make ourindustries more competitive. But money is not the only reason we should be aware of energy. Theproduction and consumption of energy causes more environmental damage than anyother single human activity. To give just one example: the energy that runs our appliances, furnaces,water heaters, air conditioners, and lighting produces two tons of carbondioxide each year for every man, woman, and child in the United States. This October, the world stands at a crossroads in regard to energy use and,by extension, the quality of our natural environment and our lives. During this decade, the United States is expected to spend as much as $200billion on new electric generating capacity. Worldwide, nations will spend an estimated $1 trillion in the next few yearsto bring electric power to the billions of people who don't have it. The choices we make about where that energy comes from and how it is producedand consumed will affect the world environment for a long, long time to come. That's why DOE established this year's Energy Awareness Month theme as"Energy--Our Future Is Today." The goal of Energy Awareness Month this year is not only to promote a greaterpublic understanding and awareness of energy sources, but to encourage all ofus to take the long view--to consider the kind of future we want to create forour children, and the children being born in other nations around the world. Today, policy makers in Washington and in the international community oftenrefer to this long-range view as "sustainable development." The goal of sustainable development is to meet the needs of today'sgeneration without impairing the ability of tomorrow's generations to meettheir needs. In sustainable development, we engage in economic development in ways that donot harm the environment. In other words, we find ways to achieve botheconomic development and environmental health. Our future depends on our ability to make peace between economic developmentand the environment. And how we produce and consume energy will be the key toour success or failure. To understand the need for sustainable development, let's take just a momentto look at this global view--at the big picture. 25% of the population lives in developed countries like the United States.They consume 75% of the energy worldwide. Today, an estimated two billion people around the world do not haveelectricity. In the years ahead, there will be a major push among nations tomeet their energy needs. China and India--the world's two most populous nations--are among the manyunderdeveloped countries undertaking aggressive economic development programsand planning major investments in energy systems. The world investment in energy, with its attendant impact on environment,will keep growing. The world's population is expected to shoot from 5.6 billionto 8 billion in the next quarter-century. Driven by this continuing populationgrowth and by economic development, global energy consumption is expected toincrease as much as 50% by the year 2010. The United States must lead the world in developing and using energyefficiently, and in making the transition to clean, renewable resources likesolar, wind, hydro, biomass, and geothermal energy. One thing is for certain: we can use the energy sources we have moreefficiently and get more power for our money. We can and are finding newsources of energy, and we can reduce the impact of energy on the environment. As Energy Secretary Hazel O'Leary puts it: "There has never been a greateropportunity for the United States and other nations around the world toharmonize the needs of people with the protection of the environment. If wedon't produce and use energy correctly today, we risk damage to our environmenttomorrow...The discovery and use of sustainable energy resources advanceeconomic growth, energy security and environmental health at the same time." As we celebrate Energy Awareness Month, we all can renew our commitment tousing energy resources wisely for the generations that will follow. WT03-B20-94IA006-000057-B002-170http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/rnucrel1.html 138.80.61.12 19970221190418 text/html 11180HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:34:35 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 11008Last-modified: Tue, 09 Jul 1996 02:26:03 GMT  General characteristics and evaluation of the radioactive release Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). General Characteristics and Evaluation of the Radioactive Release It would be useful to load an illustrative picture here to comparing text with one. Basic information on radionuclide release at the active stage of the accident has been presented in [1]. In this work the data on the amount and composition of the released radionuclides were obtained by calculations, using the models of transfer of contaminants in the atmosphere. The initial information for calculations was: the results of investigations of a small number of aerosol samples taken over the accidental unit; the data of aerial gamma monitoring of the region of the Chernobyl NPP; the results of analysis of samples of depositions in the near zone; the data on weather conditions in the regions of transport of contaminated air masses. The process of release of radioactive substances from the accidental reactor was relatively long and consisted of several stages, which differed in radionuclide composition and intensity [14]. First stage. Second stage. Third stage. Fourth stage. Obviously, the release of radioactive noble gases during the accident was about 100% (we have met no discrepancies in estimation of this value). Since the moment of the accident a high-power stream of hot air was formed, which carried out radioactive substances from the reactor ruins into the atmosphere. Its power was maximal during the first 2-3 days. The height of the stream, as it was indicated in [1], reached more than 1200 m on 27 April, 1986, 30 km to the north-west from the Chernobyl NPP: North-west shadow in progress. The level of radiation in the stream was about 1 mR/h at the height of 1200 m. In the following days the stream height did not exceed 200-400 m, as it is indicated in [14]. The level of radiation in the stream at the distance of 5-10 km and the height of 200 m was about 1 R/h on 27 April, and 0.5 R/h on 28 April. This estimation of the release height is true only for the relatively heavy fuel component. According to the data of [9], iodine and cesium radioisotopes were found at the heights of 6-9 km. The appearance of the iodine-131 traces in Japan and the USA not later 5 May indicates the same fact. The qualitative picture of the release briefly presented above has not been subjected in its main details to changes in subsequent investigations (see, for instance, [2 and 14]), where the data on the initial amount of radionuclides in the reactor and the values of the release components were made more accurate. Only the statement about termination of significant releases in 10 days after the accident (see "New data on radioactive release") raises doubts. The total amount of the radionuclides released out of the bounds of the Chernobyl NPP area is 50 MCi as of 6 May 1986, with the error of 50% (excluding radioactive gases and fuel) and the fuel component estimated in [1, 2] as 3.5%-0.5% of the fuel inventory in the core of the reactor with accuracy -50%, is confirmed by subsequent investigations [4, 5]. The estimation of the fuel release less than 1% presented in the USA work [16] can be explained by the fact that its authors used the data on fallouts and concentration of radionuclides in the air in the far zone, whereas the main part of the fuel component of the release is concentrated inside the 30-km zone. Pu spot inside 30-km zone(press here for more details). Main disagreements belong to the estimation of the release of iodine and cesium isotopes. The estimation of Cs-137 release made in as 13% was the subject of discussion in [1-4, 6, 14, 16-18], and subsequently increased up to 25 % in [4], over 25% in [17], 33% - 13% in [14], 33% - 10% in [2] bases on the measurement of Cs-137 in fuel masses inside the containment structure ("sarcophagus") of unit 4, up to 40% according to the data of [16] based on the data of global fallouts, and reached 60% in [6]. The authors consider the most convincing the estimation presented in [2]: 33% - 10%, or (2.3-0.7) MCi, which coincides with the estimation in [14, 16]. More detailed investigations of fuel masses inside the containment structure could possibly give more accurate information on this subject. Still more ambiguous is the estimation of the release of iodine radioisotopes, first of all of I-131, which has great practical importance for reconstruction of the doses to thyroid in population. Unfortunately, the measurements on 26 April are absent, and the first release of the short-lived radionuclides (not only of iodine-131, but also of tellurium-132, molybdenum-99,etc.) is not outlined completely. Besides, there is a circumstance unfavorable for measurements, the release of iodine partly in the aerosol and partly in the gaseous form. The measurement of the gaseous iodine requires special complex technique. Besides, the relation of the forms noticeably varied with time during the active phase of the accident [7]. The approximate constancy of the ratio of activities of I-131 and Cs-137 in fallouts observed in the majority of European countries gives an opportunity to estimate iodine depositions with cesium maps. In particular, such estimation was performed in [17], where the conclusion was made that the iodine release was least 45%. In different works the iodine release is estimated from 20% to 60% [3-5, 16, 17], up to 80% in [6]. To our opinion, the most realistic is the estimation of iodine-131 release equal to 50-60% of its content in the reactor, or 40-50 MCi as of 26 April 1986. Accounting for radioactive decay in the reactor in the period of active release, this corresponds to total activity of 30-35 MCi released into the atmosphere in different days. According to the general estimation, about 80% of iodine came out from the reactor in the gaseous form.  Estimation of radionuclide activity in release at the Chernobyl accident. The data of total releases are presented as of 26 April 1986.  +--------------------------------------------+ | Nuclide |  [1]  |   [16]   |   [20]        | |         |  MCi  |   MCi    |    MCi        | +---------+-------+----------+---------------+ |Xe-133   |  170  |   ---    |    170        | |Kr-85    |  0.9  |   ---    |    0.9        | |I-131    |   17  |    47    |     45        | |Te-132   |   11  |   11.2   |     11        | |Cs-134   |  0.5  |    1.3   |    1.2        | |Cs-137   |  1.0  |    2.4   |    2.3        | |Zr-95    |  4.2  |    0.24  |    4.5        | |Ru-103   |  3.8  |    0.76  |    4.6        | |Ru-106   |  1.6  |    0.17  |    0.8        | |Ba-140   |  7.3  |    1.17  |    4.6        | |Ce-141   |  3.5  |    0.25  |    5.3        | |Ce-144   |  2.4  |    0.14  |    3.7        | |Sr-89    |  2.5  |   ---    |    2.2        | |Sr-90    |  0.22 |   ---    |    0.22       | |Np-238   |  23   |   ---    |    45         | |Pu-238   |0.8E-3 |   ---    |    0.8E-3     | |Pu-239   |0.7E-3 |   ---    |    0.8E-3     | |Pu-240   |1.0E-3 |   ---    |    1.2E-3     | |Pu-241   |0.14   |   ---    |    0.16       | |Pu-242   |1.0E-8 |   ---    |    2.3E-8     | |Cm-242   |2.1E-2 |   ---    |    2.5E-2     | +---------+-------+----------+---------------+ Table presents numerical data on the release of different radionuclides from the destroyed reactor. The first column presents the estimations from the report by academician Legasov in 1986 [1] decay-corrected to 26 April 1986 [18]. The second column gives the estimations from the work by P.H.Gudiksen et.al. [16] decay-corrected to 26 April 1986. The third column is the summary of the most reliable (as to the expert evaluation of the authors) data on the release. The data on the radionuclide inventory in the reactor are taken in accordance with [12, 18]. Notice that the release of iodine-131, as well as of cesium radioisotopes, was evidently underestimated according to the results of [1, 18]. The release of iodine-131 was assumed equal to the middle (45 MCi) of the estimated interval 40-50 MCi. The estimation of the cesium release as 33%, or 2.3 MCi, is based on the amount of cesium-137 in the destroyed reactor, which coincides with its global distribution in the atmosphere and deposition on surface. The Cs-134 release was assumed equal to 0.53 of the Cs-137 release (see section 3.2), i.e., 1.2 MCi. The data on the tellurium release have no discrepancies. Strong connection with the fuel matrix is assumed for the rest of radionuclides, at least during the explosion, and their release is calculated as 3.5% of the amount accumulated in the core. See also: Core Inventory, Contents. WT03-B20-95IA006-000057-B016-42http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/chrtlist.html 138.80.61.12 19970221184745 text/html 1482HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:17:49 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1311Last-modified: Tue, 09 Jul 1996 02:26:02 GMT A LIST of AVAILABLE CHARTS. Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). A LIST of AVAILABLE CHARTS. The following charts are presented in this database: Activity relations: Ru-103/Cs-137, I-131/Cs-137, Cs-134/Cs-137. Nuclides release had been fixed above the 4-th block within active phase of accident. Release of the following nuclides: Zr95, Nb95 and Ce144. Other illustrative materials are presented in list of pictures, list of maps and list of relationsFor more information about each picture, map or chart see introduction to "Polyn" project and contents of data baseAny comments and questions are welcome by Khramtsov Pavel (e-mail address: dobr@kiae.su ) Access to Data Bases is supported by Relcom (Reliable Communications Asociation). WT03-B20-96IA005-000051-B019-50http://lacebark.ntu.edu.au:80/j_mitroy/sid101/renewable/natta-gu.html 138.80.61.12 19970221152317 text/html 52567HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:53:27 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 52395Last-modified: Tue, 09 Jul 1996 02:25:43 GMT Renewable Energy in the UK: A NATTA Guide to Newcomers Renewable Energy in the UK: A NATTA Guide for Newcomers Brief Introductory Guides to the basic options and state of play in the UK, provided by NATTA,the UK based Network for Alternative Technology and Technology Assessment. UK Renewable Energy Options Overview Solar Energy in the UK The Windfarm Debate in the UK Water Power (Hydro, Wave , Tidal) Energy Crops: pro's and con's With the exception of the first item, which is an extract from a NATTA briefing note ('UK Renewable Energy' ; 50p from NATTA), the items above are based on NATTA leaflets which are availaible free from NATTA c/o EERU, Open University, Milton Keynes, MK 7 6AA NATTA also offers a range of more detailed publications on renewable energy, including its bimonthy journal RENEW. See RENEW ON-LINE for regular extracts. NATTA is an independent membership network with some 500 subscribers, based with the OU Energy and Environment Research Unit. As with all NATTA publications, the views expressed should not be taken to necessarily reflect those of all NATTA members, The Energy and Environment Research Unit or the Open University. UK Renewable Energy Options: A Brief Overview Renewable energy is so called because it relies on natural energy flows and sources in the environment, which, since they are continuously replenished, will never run out. The UK is well placed: it has amongst the world's largest resources of wind, wave and tidal energy. Wind turbines sited in windy parts of the countryside could in principle generate perhaps 20% of the UK's electricity, while the potential for wind turbines mounted in shallow water off-shore is even larger - perhaps up to 50% of UK electricity requirements, although the cost would be higher than for on-land machines. Typical modern wind turbines have a rated output (at full power) of around 400 kilowatts. They are usually grouped together in `wind farms'. Wind moving over water creates waves, and the UK wave energy potential, if suitable floating devices could be located in deep water out to sea, (i.e. `deep-sea' systems) could be up to 20% or more of UK electricity requirements. Smaller amounts of power, at possibly less cost, could be obtained from devices operated nearer to the shore, (`in-shore' or `coastal' systems) and from shore mounted units e.g. sited in gullies, (`on-shore' systems). The gravitational pull of the moon and the sun on the seas, produces two tides per day as the earth rotates. The high tides can be trapped behind a `barrage' on suitable estuaries, creating a head of water which, when released, can be used to drive turbines, as with low head hydro-electric plants. Like some existing hydro schemes, some tidal barrages might be large. For example, the scheme proposed for the Severn Estuary would have around 8000 megawatts of installed generating capacity and could supply 6% of UK electricity requirements. The total UK tidal barrage potential is around 20% of UK electricity requirements. In addition, the fast moving tidal streams in some areas (e.g. around Scotland and the Channel Islands) can also be used to generate power, using propeller type devices located in the flow. If all suitable tidal stream sites were used, they might provide up to 19% of UK electricity requirements. More conventionally, although the energy potential is relatively limited, electricity can be generated form the flow of rivers and even streams, via small low head /'run of the river' hydro electric turbines, as well as via the more familiar and larger hydro electric dams; with their construction costs now paid off, the latter produce the cheapest electricity in the UK. In addition to these electricity supplying options, the UK has, surprisingly, a significant solar energy potential, the currently most cost-effective way of exploiting it being via well insulated `passive solar' houses, designed with large south facing windows to trap solar heat. Solar energy can also be converted into electricity via the photovoltaic `solar cell' , and solar energy is of course the key power source for plant (and animal) life via photosynthesis. The harvesting of `energy crops' looks increasingly likely to provide a significant source of `bio-fuels' of various sorts - liquids (like ethanol and bio-diesel) for transport use, and gases (like methane), or solids (eg wood) used for heating or electricity production. Short rotation 'coppicing' of rapid growing willow or poplar is one option, with the resultant wood chips being converted in a gasifier plant to produce hot gasses, which in turn are used to drive electricity-producing gas and steam turbines. This could become a major new energy source. Some industrial and domestic wastes can also be used also be used as fuel , although there has been some concern expressed by environmentalists over emissions from waste combustion plants, and not everyone would see waste as strictly 'renewable'. Similarly, collecting heat from `geothermal' sources deep underground is not strictly renewable - the geothermal wells would be gradually exhausted. But the energy potential is quite large - perhaps 10% of UK electricity requirement from `hot dry rock' deep underground. The UK already obtains about 2% of its electricity from renewable sources, chiefly from large to medium scale hydro-electric plants. There is some potential for expansion of medium and small scale hydro, but wind and waste projects are now being developed around the country on a significant scale and energy crops seen as the next likely area for development. The Government has however relegated offshore wind, wave and tidal energy to the long shot categeory, on the basis of cost, although it would seems unlikely, given the UK's maritime history and extensive offshore engineering expertise, for these quite considerable energy resources to be ignored indefinitely. Potential Contributions The Department of Trade and Industry's 1994 Energy Paper 62 (DTI 1994), estimated that, in principle, electricity producing renewables might economically supply up to a maximum of 190 TWh/yr by 2025, or around 63% of current consumption. This is the resource at around 4.5 p/kWh at 15% discount rate. This however is the maximum practical resource and in reality there would be a range of technical, environmental and economic constraints on what could actually be achieved in practice. A more realistic, fairly conservative, mid range estimate, adopted by the UK Governments Renewable Energy Advisory Group, is a 20% contribution to electricity by 2025 (60 TWh/yr) involving around 10, 000 MW (net) of renewable generating capacity. By way of comparison,. at present (mid 1990's) the UK has around 65,000 MW of conventional electricity generating capacity: more than 50% of this is coal fired, but natural gas fired combined cycled gas turbines have increasingly been introduced. The nuclear element provides around 20% of UK electricity supply, although as the older plants reach the end of their operating life, this will progressively fall. Currently only one new plant has been built - Sizewell' B'.- and it seems unlikely that, once the nuclear industry is privatised, any further nuclear plants will be ordered in the UK. It seems likely that renewable energy technology will continue to develop in terms of performance and reliability, and become more cost effective. For example the costs of electricity from wind turbines has fallen by around 70% within a decade or so, and similar reductions for photo voltaic solar cells have taken place. However, there are obviously some technical constraints. Some renewable energy sources of course are intermittent e.g. the winds and waves, and of course, the sun! However it has been suggested that intermittency need not be a major operational problem, if the electricity from these devices is fed into the national power grid network. So long as the total contribution from the various intermittent renewables does not exceed around 30-40% of the total electricity on the grid, the grid can in effect 'even out ' local variations, so that the net overall power available from the grid remains more or less constant, without the need for expensive storage systems.. Although renewables generally have much less environmental impact than conventional sources, no technology can be totally benign, and they have varying degree of local impact. The development of some renewable energy technologies is therefore also likely to be constrained by local environmental, planning and land use factors. Indeed there have already been some local planning disputes and local opposition in relation to the spread of wind farms in the UK: see 'the Windpower Debate in the UK'. Equally however, the deployment of renewables may be stimulated by increasing environmental concerns over the generally much more significant global impacts of using conventional energy technologies e.g. global warming from the emission of greenhouse gases like carbon dioxide produced when fossil fuels are burnt. The local and global impacts have to be traded off against each other. Conclusions Obviously the future of renewables will depend on a wide range of technical, economic, environmental and political factors , as well as other policy concerns and political developments, nationally and internationally. Some of the key issues are the question of whether nuclear power can be relied on in the future, the security of supply and balance of payment problems that may face the UK when and if it has to import natural gas from overseas, the role of energy conservation, and the wider environmental issues relating to greenhouse gas emissions, acid rain and so on. The UK is fortunate in having relatively large reserves of oil, gas and coal, and this, and the relative cheapness of gas, has, arguably, led to a degree of complacency, not least on energy conservation. But, quite apart from the fact that these fossil fuel reserves are finite, the question remains whether all of them can be used without producing unacceptable environmental problems, most notably in terms of global warming. New technologies are emerging which use fossil fuels more efficiently, thus reducing net emissions. Obviously in the short term energy conservation and the potential for energy saving via the introduction of more energy -efficient energy -using technology would merit top priority. After all, cost effective savings of between 50-80% are claimed as feasible in many end-use sectors, and it would seem to be foolish to consider any new supply options unless we were also tackling energy waste. However, although much can be done to avoid energy waste, the UK will still need new sources of energy, as old plant is retired. Given that nuclear power seems unlikely to revive in the UK at least, the renewable energy based technologies represent the most promising non -fossil options. The Windfarm Debate The Pro's and Con's of Windfarm The UK's potential resource for wind generated electricity, using windturbines sited in windy parts of the countryside, is put at about 20% of current electricity requirements. That's about what nuclear power provides at present. However the amount of power that can actually be obtained form the winds will depend on how many acceptable sites can be found. Currently nearly thirty windfarms - groups of windturbines on one site - have been set up, in Cornwall, Wales, Yorkshire, Scotland and else where. Most have been welcomed locally, but, in some locations, there have been some strong local protests, chiefly over noise problems and visual intrusion. The Case for Wind Farms is straight forward. Windpower is clean - extracting power from the wind produces no chemical or radioactive emissions, and has minimal physical impacts on the local ecosystem. The land around the windturbines in windfarms can be used for conventional agricultural purposes- indeed sheep seem to welcome them as windbreaks. Birds tend to avoid moving windturbine blades: indeed they seem much more at risk from the large national grid cables. Windturbines are more like bird scarers. When and if needed, decommissioning is easy: when removed, windfarms leave no toxic residues or environmental damage. There are no direct fuel costs, and the cost of extracting power is bound to fall as the technology improves. It is already seen as commercially competitive in the USA and elsewhere around the world. In summary, windpower is sustainable, clean and is increasingly competitive economically. Its local impacts are relatively small compared with the global impacts of using conventional fuels. But there is also a case against windfarms. Firstly, the local impacts are not always insignificant- local residents may be disturbed by noise and the windfarms intrude on the landscape. Some local residents have reported annoying levels of noise from the blades or the gearing systems of some windturbines - with some, for example, finding it hard to sleep. Others have complained that the machines are ugly, and may deter tourists from the area. Some objectors feel the planning bodies have not been sufficiently rigorous in applying the necessary planning controls. Secondly, some say that the wind programme is counterproductive- it would be better to invest in energy conservation.Some opponents feel that the wind farms produce expensive electricity and that the developers have simply taken advantage of the interim cross subsidy scheme introduced by the Government to make easy profits, paid for by consumers, whereas the amount of power generated is small compared to what could be saved if we invested instead in energy conservation measures. Finally, some say that the UK is too densely populated to be able to absorb a significant number of wind farms. Wind farms must inevitably be sited on prominent ridges and hills, and these are usually in attractive areas. These should be protected for everyones use. If we must have windturbines, why not put them off shore? In summary, wind farms are noisy, ugly, expensive and are not needed or appropriate in the UK countryside. Discussion With the foregoing in mind, it seems useful to make the following points. Firstly, the economic of windpower are improving, but profit margins are still tight. Even so the extra cost to consumers is small. The Governments Non Fossil Fuel Obligation (NFFO) provides a protected market for both nuclear power and some renewables, together with a surcharge on fossil fuel generation. This fossil fuel levy is passed on ultimately to consumers, and has been running at around 10% of generating costs. It has raised £1.3 billion or so each year so far, £1.2 billion of this going to Nuclear Electric. The small amount left over has supported just under 200 renewable projects, including the 30 or so windfarms so far built. The cost to the consumer of supporting the wind projects is very small - less than a tenth of a percent extra on their bills. The cost was actually artificially high initially because the Government chose to link renewables to nuclear in the NFFO scheme, and this led to a 1998 deadline being imposed by the European Commission, who objected to what they saw as an unfair subsidy for nuclear power. The result was that the initial wind power schemes had to be offered a relatively high price in order to recoup their investment by 1998. However, the 1998 deadline has now been removed, so the next wave of wind projects have been able to go ahead with less subsidy, and windpower is approaching commercial competitiveness, as is happening, after a period of subsidy, elsewhere in the world. Even so, the profit margins for bold new projects like this are tight: it is hardly a case of easy money for the developers and backers. Secondly, the initial projects were generally welcomed locally - with some of the initial objectors changing their mind once the projects were up an running. The UK's first windfarm was at Deli farm near Delabole in Cornwall. An independent 'before and after' study indicated that 80% of the local people asked said it made no difference to their daily life, 44% approved and 40% approved strongly. In the 'before' study, 40% of local people interviewed thought it was going to be visually intrusive, but this fell to 29% after it was set up and running. Whereas many expected it to present noise problems beforehand, after it was running 80% felt this had turned out not to be a problem. The windfarm had 100,000 visitors in its first year of operation. A more recent Countryside Council for Wales study, in areas of Wales where windfarms have been operating, indicated that 68% of the sample felt that the windfarms had little impact and that they would be prepared to see more. Interestingly many more objections came from a control sample in an area where there were no windfarms. Thirdly, the main objections have been about projects where there have been specific local problems. The LLandinam windfarm is acceptably quiet close up,( most people are surprised at how quiet windfarms are when they first visit them) but down in the valley resonance effects seem occasionally to amplify the noise. Effects like this will have to be responded to carefully, and avoided in future. Similarly, as experience with windfarm deployment grows, siting policy can be improved to avoid interfering with sensitive local views. Full local consultation, well in advance, is an obvious priority. Fourthly, around 70% of the 3000 or so Windturbines in Denmark are owned locally- would local ownership reduce the level of opposition in the UK? 'Your own pigs don't smell' say the Danes. So far all the UK projects have been developed by conventional medium to large scale companies, with some of the funding and the technology coming from overseas eg from Japan. Would a shift to 'co-ops', like the Danish Guilds, improve the situation in the UK, with the local community thereby benefiting directly from the project? Fifthly, the UK has the worlds best wind power resource- with Scotland having more windpower available than the rest of Europe put together. Denmark, which is mainly flat, aims to generate 10% of its electricity from wind turbines by 2005. Surely the UK which has a much better wind regime, can do at least as well? If we turn our back on this option, what are the alternatives? Energy conservation is an obvious priority, but even if we can block up the leaky bucket of current very inefficient energy use system, we will still need sustainable energy supplies to fill it. i.e. we need both conservation and renewables- including offshore wind and the other renewables. Its worth noting in this context that if the UKs current target of installing 1500 Mega watts of renewable generating capacity by the year 2000 is successfully achieved, this would, according to the DTI's 1994 Energy Paper 62, avoid the emission of some 2 Million tonnes of Carbon pa, at a cost of around only one percent extra on consumers electricity bills, via the NFFO. By comparison the Energy Saving Trusts energy conservation programme, if successful, would save around 2.5 mTCpa by 2000 and add 1-2% to consumers bills- possibly more. So renewables and conservation are fairly evenly matched in terms of the cost of cutting emissions, with wind power playing a useful role in this process. Finally, we are not faced with a static situation. As wind turbine technology develops, some of the initial problems should be resolved. For example, new, variable speed, machines are being developed which are more efficient, economic and less noisy. The Wind Farm Debate The debate over windpower has become increasingly polarised in the UK recently, with extreme positions often being taken, particularly by the objectors, who sometimes evidently feel that windprojects are being forced on them. One unsigned leaflet circulated in Wales in 1993 warned that Wales was 'being covered in swathes of ugly turbines to line the pockets of foreigners and greedy owners' A little more moderately, Sir Bernard Ingham, vice president of the Country Guardian anti-wind lobby group, commented 'people who think they're attractive are aesthetically dead'. (Newsweek 28 March,1994)Clearly everyone is entitled to their opinion.: after all, in the end it is often a subjective issue. For example, some people are very sensitive to low level noise- and can't sleep with a fridge running. Some are very sensitive to changes in the landscape- even though of course the current UK landscape is mostly man made, the result of centuries of modifications due to agriculture, land clearance and so on. Some find windfarms very appealing- as witness the large number of tourists visiting them. Some local people have actually objected to not being able to see them. Some see them ugly monstrosities, as 'lavatory brushes in the sky', while other see than as symbols of a sustainable future, and as a clear alternative to nuclear power. Given the wide range of responses to the windfarm issue, it is understandable that the debate can become a little shrill. However, what is needed is a constructive debate on the role of renewables generally, and on exactly what the carrying capacity of a country like the UK is as far as windturbines are concerned. Is the 20% theoretical resource too much to hope for? Will it be cut down in practice to 10% or even less? What about offshore wind? And the other renewables? What would be their impacts? For example, would short rotation arable coppicing be a better bet, more suited than wind farms to farming communities, as some suggest? Or, if carried out on a large scale, wouldn't that be to return to a ' slash and burn' approach, with its own set of environmental dangers? Aren't there likely to be just as many objections to extensive coppice plantations, wood chip incineration plants, and significantly increased local truck traffic? Conclusion All technologies have impacts. In general, however, the impacts of the renewables are much smaller and more local than the usually large and global impacts of conventional energy technologies. Some of the renewables seem likely to have minimal local impacts- as well as offering large energy potentials- offshore wind for example, as well as offshore wave and tidal stream turbines. Unfortunately however, these are all currently seen as expensive options- and have been more or less abandoned in the UK, following the policy outlined in the DTI's Energy Paper 62. Many of the other renewable energy technologies, although currently more economic, have land use implications- most obviously in the case of windfarms, but also in the case of energy crops and arable coppicing. Even given a properly integrated national policy on renewable development, we would still probably have to strike a balance between these various options, and, therefore, to get to grips with land use issue. Hopefully, however, the smaller scale and more local nature of most renewable energy technologies should make this process easier. For unlike conventional power stations, whose often large scale, global, social and environmental costs and risks are often hidden away, the impact of renewables like windfarms is more obvious and local. Basically, the technologies are easier to understand and assess. What you see is what you get: there are no hidden costs. Given that the nature and function of the technology is more transparent, it ought to be possible to have a constructive debate, involving a wide range of people, over how, where, on what scale and by whom renewable energy systems like windfarms should be developed. NATTA produces a bi-monthly journal, Renew, which, amongst other things, covers the windfarm debate. Recent coverage of the debate is brought together in 'Windpower in the UK', Vols 1 & 2, ( 1993 & 1995; 2 pounds each), culled from the pages of RENEW. See also Sue Walkers NATTA report on the LLandinam windfarm, 'Down on the Windfarm' (2 pounds). The windfarm debate is also discussed in John Glover and Peter Daley's NATTA report 'The UK Windfarm Debate' (5 pounds). More recently Dave Elliott produced a paper on 'Public Reactions to Windfarms' for the OU Technology Policy Group, which is also available from NATTA (5 pounds). NATTA c/o Energy and Environment Research Unit, Open University, Milton Keynes, MK7 6AA. Return to index Water Power Of all the elements, water can produce the most dramatic effects on the environment- whether it's floods, rainstorms, or tidal waves. There is obviously a lot of power there. Where does it come from? Solar Power Most of it comes, one way or another, from the sun. The suns heat evaporates off moisture to form clouds, then rain falls, some of it ending up in rivers and streams. And we can use this hydrological cycle -by damning up rivers and streams to produce useful heads of water. That's the basis of hydroelectric power- the mass of water trapped behind a huge damn is passed through a turbine, generating electricity. Its cheap and clean, and the energy source will last for ever-its renewable. And it can be used on a small scale as well as on the more familiar large scale. Currently about 20% of the worlds electricity is generated by hydro plants-so in reality renewable energy has already arrived. But there is another way in which solar energy can provide us with power via water- wave power. Waves are created by the action of wind over water, the winds in turn being the result of the differential heating of different parts of the land and sea by the sun. In fact waves are really a form of stored wind power. We can harness the power of the waves by absorbing the energy in giant floating devices- like the nodding duck developed by Stephen Salter at the University of Edinburgh, or the squeezed airbag 'Clam' system developed at Coventry University. The potential is vast- perhaps 20% of the UK's electricity could be generated in this way, maybe more. The UK initially pioneered wave energy, but Norway and Japan took over the lead following a disputed decision by the UK Government to wind up the UK wave Research Programme. Some smaller inshore and coastal units have been developed, but for the moment the large scale development of wavepower is stalled.. Lunar Power The other main way in which water can be used to provide power is by the use of the tides. Tidal energy shouldn't be confused with wave power: the energy comes from the gravitational pull of the moon which attracts masses of water to create tidal rises and falls., as the earth rotate, modified by the pull of the sun and the shape of the landmass. Again the potential is huge- perhaps 20% of the UKs electricity requirements, with the Severn Estuary being one of the worlds best sites. Of course, given that we are talking about large civil engineering structures across estuaries there will be some local environmental impacts. But the Tidal Barrage on the Rance estuary in Brittany has been operating for more than twenty years with few problems..... Taken together the UKs water power resources-hydro, large and small;wave , deep sea and inshore; and tidal power, could ultimately provide a large proportion of the power we need-and on an indefinitely sustainable basis. Obviously there will and should be debates about the environmental trade offs that might have to be made eg in the case of tidal barrages and large hydro plants, but many of our water power sources can be developed on a smaller scale basis. There are already many dozens of new micro-hydro schemes in operation or under construction and there are scores of potential sites for smaller scale tidal barrages. At present the economics tends to favour larger schemes, but when and if we begin to take the environmental costs of existing forms of power generation into account, it seems likely that most types and scales of water power, will increasingly be seen as viable. For further discussion of the issues see the following reports: 'Wave Energy' (A compliation of reports from back issues of NATTA's journal Renew), 1994, (2 pounds) from NATTA. 'The UK's Wave Energy R&D Programme' Dave Elliott (A critical review of the way in which wave power development has been constrained in the UK), OU Technology Policy Group report, 1995, (4 pounds) from NATTA. 'Tidal Power', NATTA report, 1990, (1 pound) from NATTA. 'Tidal Energy', (A compliation of reports from back issues of Renew), 1994, (1pound)from NATTA. Return to index Solar Power in the UK The sun provides the basis for life on earth and delivers sufficient energy to each square foot to meet all our needs -if we could tap that energy efficiently. Historically human beings have tried to do that via agriculture, using wood as a fuel and by using the indirect solar energy represented by winds and streams More recently we've used the stored solar energy of fossil fuels-coal, oil and gas. But it may be that we can make use of the sun more directly. Around the world recent years have seen large scale experiments with solar power- for example via giant solar heat concentrating mirrors and dishes, tracking the sun across the sky and focusing its rays so as to raise stream for electricity generation. Large scale 'Solar Thermal 'plants like this are becoming increasingly popular in desert areas of the USA. and elsewhere. But solar energy can also be utilised on a smaller scale - and even in cooler climates like the UK. Solar Britain At first glance you would not think that solar power stood much of a chance in the UK, what with our short summers and cloudy days. But the UK receives on average over the year around half the amount of energy from the sun per square foot as countries on the equator, and that heat is likely to be of more use here than in hot countries. During the 1970's there was something of a boom in sales of roof mounted solar collectors in the UK That was hardly surprising, since flat plate solar heat collectors, looking something like a radiator, plugged into a hot water system, could typically cut domestic fuel bills by a half. But it soon became apparent that the overall economics were not that attractive-commercial systems had payback times of five to ten years or more, and there were also some cowboys operating in the field who undermined consumers confidence. The boom consequently faded, although many enthusiasts continued with diy units. The 1980's saw something of a renewal of interest, following the oil price shocks. Local councils made much of the running- with rehab council houses being retrofitted with solar assisted heating and several new solar council house projects were initiated. By the end of the Greater London Council's reign in London for example there were more than 200 solar houses in use, and, meanwhile, Milton Keynes Development Corporation was turning the new city of Milton Keynes into a solar showplace, with more than 300 solar housing projects. However, the emphasis had gradually shifted away from flat plate solar collectors on individual houses. One of the last of the Greater London Council supported schemes was a grouped heating scheme, with an array of solar collectors feeding a common hot water store, shared by 15 separate dwellings. This system averaged out variation in use rates for each house and improved the overall economics. More fundamentally, there was a switch to the even more cost effective passive solar concept. Conventional Roof top solar collectors need small pumps to drive the heated water around the heat circuit, but you can also collect useful amounts of heat, if you have large south facing glazed areas. Its much like the greenhouse concept- and involves no moving parts. Hence the term 'passive' as opposed to conventional 'active' solar collectors, with pumps. Typically, with a well insulated house, you can cut overall fuel bills annually by a third in this way. Milton Keynes has two hundred or so passive solar council houses, and passive solar design has now become almost a byword in modern architectural practice. Large scale solar atriums for office and commercial buildings are now a familiar sight, and, at the more lowly level, many people have added solar heat trapping conservatories to their homes. Slowly Does It. So slowly solar power is becoming a reality in the UK. It has been a slow process in the main because solar power has had to compete unaided against often heavily subsidised conventional fuels-like electricity and gas. Only the more environmentally conscious homeowners have opted for solar out of commitment to a sustainable future. But the balance is slowly tipping, and new techniques and technologies are emerging which could well bring about more radical changes. Already there are more efficient, although more expensive, evacuated tube solar collectors, as well as various clever new selective absorption systems to improve heat collection But the big breakthrough is likely to be in the photo voltaic solar field. Photo cells, like those on cameras and pocket calculators, convert sunlight directly into electricity. The only problem is that they are expensive. They were initially used mainly for powering space satellites, but developments in the semiconductor field have gradually brought prices down. Within a decade or so they are likely to be competitive with conventional power sources. In which case we are likely to see photo voltaic ('pv') cells being used widely- even for domestic supplies. They are already in use in some outlying rural areas where there is no grid electricity and the alternative is diesel, or nothing, in the Australian outback, in dessert areas and in African villages, for water pumping, running fridges for key medical supplies, or powering remote telecommunications equipment. Soon we may see much wider scale use. New types of cell materials have been developed which are more efficient and cheaper and some can be mounted on flexible backings, so it may even be possible to buy electricity generating sheets to hang up wherever you want. Ten Wasted Years? Even if you discount this sort of development, one way or another the potential of solar energy would seem to be considerable. But, as we've seen, it has been a long time coming. at least in the UK. The UK Government has until recently been pretty dismissive of its potential. Photo voltaics have been seen as pretty much irrelevant to the UK and in 1982 the Government decided to abandon further support for active solar projects. Only passive solar continued to receive support, while a few UK companies, like BP, continued to push ahead with pv for the export market. However recently there has been something of a U -turn, following a report which concluded that there might soon be a very significant market for pv. Certainly the USA, Japan and more recently Germany, have been putting billions into pv in recent years.. Environmental Impacts One of the reasons why at least some people have not seen solar as a viable option for the UK is that it has been seen as a very land hungry technology. There has been talk of having to cover the whole of the UK with solar cells in order to generate any significant power. The reality is very different For example, the potential for using roof tops and walls for solar cells is very large: and pv cells can be used as cladding material, thus reducing some building costs and offsetting the cost of the cells. There is already a house (in Oxford) equipped in this way and an office building at the University of Northumbria in Newcastle upon Tyne. Prof Bob Hill at the University of Northumbria has been carrying out aerial photography to identify suitable south facing surfaces around the UK and in London, and he has pointed out that a similar study done in Berlin estimated that the city could generate 2.5 GW of pv power - equivalent to two nuclear power plants. London is likely to have a similar potential, while the potential for the UK as a whole, if fully developed, might be greater than the total UK electricity requirement. Of course that still begs the question of the economics. But environmentally, there would seem to be few problems, at least in terms of the uses of pv. There are however some question marks associated with the manufacture of pv cells. This is very much a high tech industry using exotic and often hazardous chemicals-potentially representing a significant health and safety problem for workers. There could also be ex-plant pollution issues to contend with. But assuming these potential problems can be avoided, pv could become the breakthrough technology of the next decade. What next? The photoelectric effect, on which pv cells are based, was first explained by none other than Albert Einstein, at about the same time as he developed his famous E=MC 2 equation, which lay the basis for nuclear energy. At one time, it was felt that solar cells were going to be a much more likely contender as power generators than nuclear energy. But then came the wartime Manhattan atom bomb project and one way or another, solar cell research lost out. It has only been in recent decades that interest has returned. For the enthusiasts, the future could be one in which not only is pv used widely in the north, but third world countries in the sunny south use electricity from vast solar cell arrays in desert areas to generate hydrogen gas by splitting water into its constituent part via electrolysis. The desert nations, many of them currently locked into oil production, could then become a major exporter of a new form of less damaging energy to the cold north. Certainly there is growing interest in the use of hydrogen as a fuel to replace petrol, and in photo voltaics as a power source for producing the hydrogen -pvh is the buzzword. The attractions are clear: hydrogen can be burnt with no byproduts other than water, it can be used in a fuel cell to generate electricity, again very cleanly, and you can store hydrogen easily, and thereby avoid one of the key problems with all form of solar energy- its intermittancy. Solar hydrogen, and hydrogen generated from indirect solar sources, like the winds and waves, could be piped down the gas main for domestic use, perhaps mixed in with our progressively diminishing north sea gas reserves. It could become the heating fuel of the future. And used directly as a fuel, or with fuel cells in electric cars, it could help us deal with the pollution problems of petrol engines. If these sorts of projects, and others like them, get off the ground, we could be moving towards a future in which the sun plays a major role in helping us live once again in a sustainable way. NATTA has produced a short guide to Milton Keynes' Solar Houses 'Solar in the City',(50p ) and a booklet 'Solar Houses in London' (1 pound) . There is also a compilation of solar articles, 'Solar Energy', culled from back issues of NATTA's journal, RENEW. -60p. All these publications are available from NATTA C/O EERU, Open University ,Walton Hall, Milton Keynes, Bucks MK76AA Return to index Energy Crops Growing fuel rather than food could well become a significant new option for farmers - especially given the European Union's land set aside policy. Liquid biofuels (biodiesel) are already being produced in some parts of the EU from oil seed rape. Solid fuels from `woody biomass' are another option.Short rotation arable coppicing, e.g. using fast growing willows or poplar is currently seen as likely to be an important source of fuel for electricity generation in the UK - indeed the UK Governments Department of Trade and Industry estimated (in Energy Paper 62) that the maximum total realistic UK resource potential by 2025 could be up to 150TWh/yr - half current UK electricity requirements. At first sight increased afforestation would seem a very attractive proposition - trees are beautiful and we need more to help combat global warming. But large scale short rotation arable coppicing does not involve full tree development - instead the thin willowy growths, reaching perhaps 15 feet, are cut back to stumps every 3-5 years. That may not prove to be so visually attractive. Large areas could also be involved with mechanised cutters passing periodically along corridors through the coppice plantations. The resultant wood chips would also have to be transported periodically to combustion plants - with traffic, and its associated pollution, in rural areas therefore perhaps increasing. And combustion itself can involve pollution. Overall then, as with all technologies, even renewable energy based technologies, there are environmental trade offs. The Case for Coppicing Existing forms of power generation are very environmentally damaging.Burning coal, oil and gas inevitably produces carbon dioxide, a key greenhouse gas, as well as other pollutants, including acid rain related gasses.Nuclear power generation inevitably involves the generation of highly dangerous and very long lived nuclear wastes, and there is the risk of major accidents. By contrast, growing and burning energy crops is greenhouse gas neutral, as long as the regrowth rate balances the use rate so that as much carbon dioxide is absorbed as is produced by combustion. Regularly coppiced plantations will actually absorb more carbon dioxide than mature trees - since carbon dioxide absorption slows once a tree nears maturity. All the existing fuels will eventually run out, whereas wood is renewable - it can always be available. The UK's reserves of north sea oil and gas are limited, and even our coal reserves will not last for ever. Used in the current types of nuclear reactors, the worlds uranium reserves will only last around 60 years. Perhaps more importantly the cost of using these fuels will increase as stricter environmental control are applied. By contrast arable coppicing represents a very large new energy resource - which can be sustainably harvested, at reasonable cost, and costs should drop as more experience is gained. In summary energy crops are sustainable, environmentally sound, and represent a huge new energy source. The Case Against Coppicing The local impacts may be significant - depending on the scale. The coppice plantations may not be very visually appealing. It is envisaged that large parts of the set aside land - representing 15% of UK arable land - might be used, and that could change the character of the UK landscape. Fertilizer and Pesticide run off could also be a problem. As could the risk of fire. Wood chips are a bulky fuel and there could also be local environment impacts from the extra truck traffic (and its pollution). Combustion is environmentally undesirable. Burning wood produces toxic pollutants - including, possibly, dioxins from chemical residues from herbicides and pesticides. Focusing on single crops can undermine biodiversity. Monoculture plantations are aesthetically unappealing (as with the vast existing conifer plantations). Being less ecologically diverse than, for example, natural forests, they tend to be at risk of disease and pests, so herbicides and pesticides may have to be used to compensate. Farming should be for food - not energy. With much of the world starving, it is immoral to switch to a new energy cash-crop in order to fuel our wasteful energy habits. Energy conservation would be a better bet - and is a cheaper option. So far coppicing seems unlikely to be economic without subsidy although equally it could become a vast new extension of agri-business. In summary, there are environmental, social and economic question marks. Discussion With the foregoing in mind, it seems useful to make the following points:Only a few experimental projects exist at present - and rapid expansion would take some time. Coppicing would initially have to be subsidised from the EU's set aside scheme, the woodland development grant scheme and via the NFFO `fossil fuel levy' ( as is likely to happen under the third NFFO order from Nov 1994 onwards) and it could take some years to become established on a wide scale. So there is time to discuss the pro's and con's. Arable coppicing is one of several renewable options with land use implications: wind farms are another example. Each of the renewables have their advantages and disadvantages. For example, some might argue that coppicing is more suited to the UK landscape than wind farms, although the best sites for coppicing are usually not the best sites for windturbines and vica versa. Others might feel that what is to some extent a `slash and burn' energy cropping approach is a backward step, although global hunger is a problem of distribution and local politics. We already produce enough food to feed the world. Even so, we need to decide on the right balance between the various renewable options and energy conservation. In technical terms, energy coppicing can be very efficient. The energy ratio (i.e. available energy output in the crop divided by the energy input requirement, for growing, harvesting and transport) has been put at between 10 and 100. And the combustion process can be very efficient, especially given the development of advanced co-generation/CHP techniques. For example with gassified biomass used to power steam injected turbines (the so called BIG-STIG technology), operated in the combined heat and power mode, conversion efficiencies of up to 80% can be obtained. The environmental impacts can be limited - and some can be positive. Herbicides would only be used in the first year of the cycle. Less pesticides would be needed than for conventional arable crops and coppices tend to mop up pollutants, thus reducing 'run off' into water courses. Coppices would be located mainly on marginal land, and can offer habitats eg for birds and insects, especially given the open access corridors that would traverse the plantations. Bio-diversity can also be ensured by using multi-cloned plants or by mixing species: coppices have already been shown to attract woodland animal species.Overall, if degraded or abandoned cropland were used, the environmental and wildlife impact would be positive. The Transport Impact will be low Wood chips would be stored on site and probably be transported annually for combustion. For say a 10 hectare coppice plantation, with 5 hectares harvested annually, only 75 tonnes of wood chip would need transporting annually. This would involve 4X20 tonne loads or 8X10 tonnes loads - not a huge annual volume. And it would replace the transport of the crops that might have been grown otherwise. Conversion into electricity could be done locally. The combustion plants could be on local industrial estates. A 10 MWe unit run continuously at 80% efficiency would require around 60 tonnes of wood chip per day i.e. 3X20 tonne lorry loads per day. It would be unlikely for wood chips to be transported more than 20 miles. And there are prospects for power generation on the farm site itself, thus eliminating transport requirements almost entirely. Either way, local employment would be boosted.Initial studies indicate broad support. A study of the attitudes of country users for ETSU in 1993 indicated that although there were some reservations, in general, there was support for coppicing, as long as the scale was not too great. Some concern was however expressed at leaving its development up to commercial pressures and interests. Conclusion On balance arable coppicing seems to offer significant advantages. However, there are clearly some unknowns: what exactly are the environmental and pollution implications, how economic will it be, how will it be planned, what scale is appropriate? Is it just an extension of conventional farming or something new? Currently, no change of use application need to be sought for switching set aside or arable land to coppicing. At the very least, some form of planning control would seem necessary to avoid land use conflicts and visual disruption. Existing planning rules and emission standards would presumably cover the processing and combustion plants: but an integrated planning approach to the whole system seems called for, with full local consultation and detailed guidelines, as have been developed in the USA and elsewhere.More generally, there would seem a need for a debate over renewable energy development generally. At present the pattern of development has been left primarily to short term market forces to define - with the emphasis being on waste combustion and wind farms, arable coppicing being the next major commercial contender. All have environmental impacts - even if they are much smaller than the global impacts of conventional energy systems. They may have a place in a sustainable energy system, but the correct balance is as yet unclear. As is the fate of some of the larger-scale renewables, most of which, although currently seen as more expensive, have very low environmental impacts - e.g. off-shore wind and deep-sea wave. At present these are receiving no support in the UK. This could be short-sighted, given that environmental concerns e.g. over global warming, may grow.The debate over coppicing should therefore not be carried out in isolation from the broader debate over our energy future. The debate on coppicing. Large scale coppicing is a new concept which the Government clearly favours.The National Farmers Union is also strongly in favour of coppicing - sensing a new opportunity for its members. Clearly employment is a key issue.The environmental and strategic debate has, however, only just started. NATTA has produced a compilation of reports on the Energy Crops and Biofuels from its bi-monthly journal RENEW: 'Energy Crops' (2pounds), available from NATTA, c/o Energy and Environment Research Unit, Open University, Milton Keynes, MK7 6AA Return to index WT03-B20-97IA006-000055-B004-332http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/head_ill.html 138.80.61.12 19970221174116 text/html 1933HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:11:29 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1762Last-modified: Tue, 09 Jul 1996 02:26:03 GMT Header of illustrations list. Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). A LIST of AVAILABLE ILLUSTRATIONS. The following types of illustration are presented in this database: photos (Generally there are pictures of 4-th block, fuel forms inside 4-th block, special equipment implemented during liquidation works, personality and so on) maps(Generally there are maps of territories contamination, monitoring net, dynamic of fallout for different regions charts (General there are charts of nuclide release within active phase of accident, relations between isotopes in release and fallout) graphic relations (Generally there are relations between isotopes in release and fallout, distributions of these relations, dynamics of nuclide release) For more information about each picture, map or chart see introduction to "Polyn" project and contents of data base. Any comments and questions are welcome by Khramtsov Pavel (e-mail address: dobr@kiae.su ) . Access to Data Bases is supported by Relcom (Reliable Communications Asociation). WT03-B20-98IA006-000057-B015-635http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/pictlist.html 138.80.61.12 19970221184640 text/html 1587HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:16:58 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1416Last-modified: Tue, 09 Jul 1996 02:26:03 GMT A LIST of AVAILABLE PICTURES. Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). A LIST of AVAILABLE PICTURES The following pictures are presented in this database: Ruin of the 4-th block / Cascade Wall construction work /. Ruin of the 4-th block / Shelter's Roof construction. /. Fuel form. "Elephant Foot". Helicopter above the 4-th Block of ChNPP. "Shelter". Winter 1986. Other illustrative materials are presented in list of maps, list of charts and list of relations. For more information about each picture, map or chart see introduction to "Polyn" project and contents of data base. Any comments and questions are welcome by Khramtsov Pavel (e-mail address: dobr@kiae.su ) . Access to Data Bases is supported by Relcom (Reliable Communications Asociation). WT03-B20-99IA006-000057-B015-607http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/rbmkshrt.html 138.80.61.12 19970221184627 text/html 3841HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:16:42 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 3670Last-modified: Tue, 09 Jul 1996 02:26:03 GMT  Specific features of the design of the RMBK-1000 reactor Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences .( Project "Polyn" ). Specific features of the design of the RMBK-1000 reactor The unit 4 reactor of the Chernobyl NPP is of the RMBK-1000 type, with thermal power 3200 MW and electric power 1000 MW. It was put into operation in December 1983 [1, 2-15]. Graphitemoderator was placed between vertical channels with fuel and light-water boiling coolant. This scheme has the advantage that it requires no large body of high pressure, as in PWR or BWR reactors, and the graphite moderator allows to use the fuel with low enrichment with uranium-235. Corresponding control and protection systems in such scheme can provide acceptable security. However, according to opinion of a group of international experts, the systems of control and protection which existed at the Chernobyl NPP did not conform to modern security requirements [15]. In a RMBK-1000 reactor thecoolant is circulated in the contour under pressure, and the generated steam is directly applied to the generator turbine. A Scheme of RBMK-1000 Reactor. Uranium dioxide enriched with 2% of uranium-235 is in zirconium cylindrical fuel elements joined in bundles 18 pieces (fuel assemblies) placed in the channels. There were 1659 fuel assemblies with about 114.7 kg of uranium in each assembly in the cylindrical core with the radius of 6 m and height of 7 m at the moment of the accident. The total mass of uranium in the core was 190.2 ton. It is important to note that the reactor design allows local fuel replacement, when a reactor is in operation, and there were fuel assemblies with very different levels of burn-up to 20 NW*day/kg of uranium at the moment of the accident. The reactor core is surrounded by biological shield in the form of cylindrical coaxial tank with water 16.6 m in diameter, which remained practically undamaged after the explosion at the Chernobyl NPP . The tank of biological shield and the core are closed tight from above and below by cylindrical "covers" filled with seprentinit, through which multiple communication pipes pass. During the explosion these parts of the reactor were displaced, and the formed passages served as the ways for egress of the contents of the core out. The system of control and protection of the reactor is based in displacement of 211 rods-absorbers in special channels of the core and is to provide automatic maintain of a given power level, rapid decrease of the power in response to signals of main equipment failure, accidental termination of reactor operation in response to dangerous deviations of parameters. The systems of localization of radioactive releases is intended for condensation and collection of radioactive water in closed spaces under the reactor and is not meant for break-down of the core and for release of considerable amount of radioactive materials out of it. See also: Core Inventory, Release of nuclides, Contents. WT03-B20-100IA006-000057-B012-151http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/ff-oil.html 138.80.61.12 19970221182134 text/html 5221HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:51:44 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5050Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Fossil Fuel: Oil Fossil Fuel: Oil Each day, the United States produces about 9 million barrels of oil (7.2million barrels of crude oil plus 1.7 million barrels of natural gas liquids),while importing almost 8 million barrels. Much of the easiest oil to producehas already been extracted from existing fields, and improved technologies willbe required to recover and refine the generally lower quality remainingresource. Resource Oil is a liquid fossil fuel produced in 31 states and the federal outercontinental shelf from onshore (82%) and offshore (18%) wells. In 1992, the United States produced 8.9 million barrels of oil per day (BOPD),including natural gas liquids; 1.0 million BOPD were exported, mostly toMexico, Japan, and the Virgin Islands. In 1992, the United States imported 7.9 million BOPD; the largest supplierswere Saudi Arabia, Venezuela, Canada, Mexico, and Nigeria. 1992 net imports of6.9 million BOPD accounted for 41% of petroleum products supplied andabout 60% of the U.S. trade deficit. Known domestic in-the-ground oil resources are 350 billion barrels, a portionof which can be recovered with advanced technologies. The cumulative total ofpast domestic oil production to date is 160 billion barrels. At year-end 1992, proven domestic oil reserves were 25 billion barrels. There are an estimated 49 billion barrels of undiscovered recoverable oil inthe United States. Advantages The entire automotive and petroleum industries are built around gasoline andthere is extensive infrastructure already in place. Gasoline, diesel fuel, and other petroleum products are convenient,concentrated energy sources for transportation. Extensive domestic supplies remain to be recovered or developed with propertechnology and higher prices. Current Use and Cost In 1992, the U.S. share of world petroleum consumption was 26%. In 1992, oil provided 26% (17.5 quadrillion Btu) of domestic energy productionand 41% of energy consumption (33.5 quadrillion Btu). The principal uses of oil are in the transportation (64% in 1992), andindustrial sectors (26%). The principal products made from oil are gasoline (42%), distillate fuel oil(17%--for space heating, diesel engines, electrical generation), and jet fuel(9%). The 1992 average crude-oil refiner cost was $18.43 per barrel. Projected Use and Cost Domestic crude oil production is projected to decrease from 7.4 million BOPD in1990 to 4.8 million BOPD in 2006 with relatively low prices, then to 5.1million BOPD in 2010 with rising prices and improved technology. Consumption is projected to increase from 17.0 million BOPD in 1990 to21.3 million BOPD in 2010. Net imports are projected to increase from 6.7 million BOPD in 1990 to12.8 million BOPD in 2010--increasing from 39% to 60% of domesticconsumption. World oil prices are projected to increase from $18.20 in 1992 to $28.16 in2010 (1992 $). Issues Increasing imports of oil create trade deficit problems and foreign-supplydependency. Use of petroleum products is the largest contributor--by fuel source--ofgreenhouse gases (44% of man-made carbon emissions). Opportunities and Challenges Because much of the easiest oil to produce has already been extracted fromexisting fields, oil remaining in those fields--some 60% to 80% of known U.S.resources--could be abandoned by 2010 unless new technology becomesavailable. Both domestic and imported oil is becoming heavier and higher in sulfur,requiring improved refining technologies. New Technologies Computerized three-dimensional seismic surveying and reservoir modeling canidentify significant additional resources. Improved "secondary" water flooding plus "tertiary" recovery techniquesincrease yield from existing reservoirs. As a promising end use, a forthcoming Mercedes gets 75 miles per gallon (mpg)of diesel while meeting California low-emission vehicle standards; a currentlyavailable Audi gets more than 60 mpg of gasoline. Advanced processing technologies reduce pollution from processing. References 1. Annual Energy Review 1992, Energy Information Agency, U.S. Departmentof Energy (DOE), 1992. 2. Annual Energy Outlook 1994, with Projections to 2010, EnergyInformation Agency, DOE, 1994. 3. Oil Technology--Program Profile, Office of Fossil Energy, DOE,1994. 4. Oil Recovery--Program Profile, Office of Fossil Energy, DOE, 1994. 5. Automotive News, February 7, 1993. 6. Energy Information Sheets, Energy Information Agency, DOE, 1993. WT03-B20-101IA006-000057-B015-231http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/copyrigh.html 138.80.61.12 19970221184152 text/html 984HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:11:31 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 814Last-modified: Tue, 09 Jul 1996 02:26:02 GMT  CopyRight </TITLE<H3><IMG SRC  = "polyn0.gif" ALIGN=MIDDLE><I> Russian Research Center "Kurchatov Institute"</I></H3><H2>Hypertext Data base:<B> Chernobyl and its consequences</B>.(<I>Project "Polyn"</I>).</H2><HR><H1>CopyRight</H1><HR>All Internet conditions are employed. Data base files are open for free use but reference on data base is obligatory in publications.<HR>A number of photos and measurements was made in conditions of high irradiation, so we hope that some ethic limits will be applied for documents utilizing.<HR>Some access restrictions to pictures and measurements are applied. For unlimited access contact with data base administration group.<HR>Access to Data Bases is supported by <A HREF="http://www.kiae.su">Relcom (<I>Reliable Communications Asociation</I>).</A><HR></DOC><DOC><DOCNO>WT03-B20-102</DOCNO><DOCOLDNO>IA006-000055-B003-312</DOCOLDNO><DOCHDR>http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/manifest.html 138.80.61.12 19970221173529 text/html 2804HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:05:48 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 2633Last-modified: Tue, 09 Jul 1996 02:26:03 GMT</DOCHDR><TITLE> Manifest Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences .(Project "Polyn"). Manifest Dear Colleagues. We are glad to present a hypertext data base describing an accident at Chernobyl Nuclear Power Plant. As a platform of this data base a recent official version of events published in 1992[20], has been used as well as materials of Chernobyl Kurchatov Institute Expedition and a number of publications in scientific press and mass media. "POLYN" is a Russian word and means "Chernobyl'". After an accident all works in a 30-km zone were named after this word, so we keep it as a name of our data base. This data base is not an official publication but an initiative work of our group. If "Polyn" is useful for anybody , it will be pleasure for us. Any ideas, remarks and corrections are welcome. Your possible choice: Introduction Copyright Contents Literature Data Base Administration Group: Since June of 1986 we had started a work aimed to provide an information support of decisions made by specialists of our institute during liquidation of consequences of Chernobyl Accident. In general, our activity was focused on the following issues: Data Base of Radioactive Samples measurements collected in 30-km zone and at the destroyed block. Bibliographic Data Base of scientific publications. Images of the Block and maps. Distributed Data Bank of Scientific Russian Organizations involved in Chernobyl activity. Since the 1993 year we had started a new work to unify all of the available information resources in a universal form. This data base is an attempt to achieve this goal. Olga Zimina (liai@chern1.msk.su) has an experience in the data base design and its management. She is a designer of PC/AT version of 30-km zone contamination data base and applications for it. Natal'ya Sergeeva (liai@chern1.msk.su) has an experience in design and management of bibliographic data bases. She is designer of Bibliagraphic data base of publications in scientific reports, periodics and mass media. Pavel Khramtsov (dobr@kiae.su) is a head of this group. WT03-B20-103IA006-000055-B004-262http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/placesel.html 138.80.61.12 19970221174035 text/html 5114HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:10:41 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 4943Last-modified: Tue, 09 Jul 1996 02:26:03 GMT Chernobyl Nuclear Power Plant: Design and Construction  CONTENTS OF DATA BASE. Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences .(Project "Polyn"). Chernobyl Nuclear Power Plant: Design and Construction The plan of putting into operation the power generating capacities on thenuclear power plants (NPPs) of the USSR of 11.9 million KW within 1966-1977, including NPPs with the reactors of the new type namely RBMK-1000 of 8 million KW was confirmed by the resolution of the Council of Ministers of the USSR on 29.09.66. The Leningrad NPP became the head nuclear power plant with these reactors. In line with mentioned plan the deficit problem of electrical energy in the Central energy consumption region of the United Energy System (UES) of the south being the largest among UES of the European part of the country by both volume of electrical energy consumption and sizes of installed capacity could be solved by the help of one of the nuclear power plants. UES of the South has covered the area of 27 administrative districts of Ukrainian SSR as well as of Rostov district with 0.7 million square km and with population up to 53 million people.In the European part of the country about 26% total installed capacity and 25% total electricity consumption was fallen to UES share. Electricity consumption was to be increased by approximately 3.5 times in this region from 1970 to 1990 to assessments of the Ukraine department of "Ehnergoset'proekt" Institute of Minehnergo (Ministry of Energy) of USSR made in 1974. Table 1.1. Assessment of electricity consumption for the area of United Energy                           System of the South.+-----------------------------+---------+----------+----------+--------+---------+|        Name                 |  1970   |   1975   |   1980   |  1985  |  1990   |+-----------------------------+---------+----------+----------+--------+---------+|Electricity consumption,     |  136.4  |   193.0  |   266.0  |  381.0 |  473.0  ||milliard KWh                 |         |          |          |        |         ||                             |         |          |          |        |         ||The same relative to 1970,   |    1.0  |     1.4  |     1.9  |    2.67|    3.46 ||rel. unit                    |         |          |          |        |         ||                             |         |          |          |        |         ||Installed capacity of NPPs,  |   27.3  |    39.2  |    54.9  |   74.5 |  101.8  ||GW (el)                      |         |          |          |        |         ||                             |         |          |          |        |         ||The same relative to 1970,   |    1.0  |     1.44 |     2.0  |    2.74|    3.72 ||rel. unit                    |         |          |          |        |         |+-----------------------------+---------+----------+----------+--------+---------+ or in graphical form: The location of the new NPP in Ukraine's Central districts would be economically attractive when it was considered that the rational radius of NPP influence formed 350-450 km. This NPP originally named as Central-Ukrainian was to be consisted of two generating units by 1000 MW each. Place selection for construction of Central-Ukrainian NPP was performed by Kiev departmentof "Teploehnergoproekt" Institute of Minehnergo of USSR and Kiev design bureau "Ehnergoset'proekt". Two construction places were proposed: Ladyzhiny (Vinnitskij district); Kopachi (Kievskij district). The board of Gosplan of Ukr.SSR specifies the NPP location near Kopachi and named it as Chernobyl by resolution on 18.01.67. Resolution of the central Committee of USSR Communist Party and the Council of Ministers of USSRby 02.02.67 stated the resolution of UkrSSR Gosplan. In this case the following aspects were taken into account: Low density of population within the NPP location. Availability of the significant water supply source (Pripyat` river). Low productivity of alienable lands. Proximity of the main line (station Yanov of the South-West railway). Developed network of automobile roads with solid coating. Favourable geographic, climatic and etc. conditions allowing the fast employment of personnel. The industrial site of Chernobyl NPP is located in the eastern part of the region named Belorussko-Ukrainskoe Poles'e, 160 km to the north-east of Kiev, 15 km to the north-west of Chernobyl (Kiev district). See also: Contents, Design and Construction, RBMK-1000 reactor, Accident Cases. WT03-B20-104IA006-000057-B015-530http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/rnucrel0.html 138.80.61.12 19970221184543 text/html 3957HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:15:51 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 3786Last-modified: Tue, 09 Jul 1996 02:26:03 GMT DYNAMIC AND COMPOSITION OF THE RADIOACTIVE RELEASE Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). DYNAMIC AND COMPOSITION OF THE RADIOACTIVE RELEASE The study of radioactive release during the Chernobyl accident is the subject of a considerable number of works [1, 2, 4, 6, 9, 11-14, 16-18]. However, basic actual data on release of radionuclides during the active stage of the accident was mainly given in [1]. The final picture was first presented to world community in report by academician V.A.Legasov with co-authors at the IAEA session in August 1986 and has not been reconsidered since then. During the process of the accident the nuclear fuel from the destroyed reactor was subjected to high temperature and mechanical failure. Heating up to the temperatures of about 2000 C caused complete or partial evaporation of volatile radioactive elements from the fuel. A part of the fuel was released during the explosion in the form of fine-dispersed component which distributed in the atmosphere. Subsequent fires and natural fuel heating by radioactive decay caused that hot air continued to carry out of the reactor the products of graphite burning, radioactive gases, aerosols, and fine-dispersed fuel dust with dimensions of particles from parts of a micron to tens and even hundreds micron. Thus, radioactive releases from the destroyed reactor consisted of at least two different components: radioactive nuclides included into the matrix of the dispersed fuel and released in the form of radioactive dust, separate volatile radioactive substances in the form of gases or aerosols, which evaporated from the hot fuel. The detail description of different radioactive release characteristics is divided on the following parts: General characteristics and evaluation of radioactive release. New data on radioactive release. According to estimations, during the accident: inert gases came out of the destroyed reactor completely [1.2]; iodine-131 release was 50-60%, which is 40-60 MCi (on the average, 45 MCi) as of 26 April 1986, [3, 4, 5, 16, 17]; cesium release was (33 +/- 0.7)MCi; strontium release was about 4%, or 0.2 MCi [1]; fuel release outside the plant area was (3.5 +/- 0.5)% [1, 4, 5]. Besides, some general remarks can be made: In the modelling of radionuclide transport at the Chernobyl accident one should differ the fuel and volatile components, and the transport of relatively refractory fuel component can be described by local and mesoscale models, whereas volatile (cesium and iodine) component require attraction of global transport models; the considerable release of radionuclides could continue even after 5 May 1986. Some related information about core inventory and reactor may be found in: Core Inventory. Specific features of the design of the RBMK-1000 reactor. The causes of the accident and its progress. See also: Contents. WT03-B20-105IA006-000057-B016-63http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/curvlist.html 138.80.61.12 19970221184801 text/html 1378HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:18:22 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 1207Last-modified: Tue, 09 Jul 1996 02:26:02 GMT A LIST of AVAILABLE RELATIONS. Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). A LIST of AVAILABLE RELATIONS. The following relations are presented in this database: Measurements of Cs isotopes in Vilnus and above the 4-th block. Measurements of I isotopes in Kiev and above the 4-th block. Other illustrative materials are presented in list of pictures, list of charts and list of maps For more information about each picture, map or chart see introduction to "Polyn" project and contents of data base. Any comments and questions are welcome by Khramtsov Pavel (e-mail address: dobr@kiae.su ) . Access to Data Bases is supported by Relcom (Reliable Communications Asociation). WT03-B20-106IA005-000051-B020-140http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/energy.html 138.80.61.12 19970221152835 text/html 2624HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:58:55 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 2453Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Energy: Useful Facts--October 1994 Energy: Useful Facts--October 1994  Energy: Useful Facts--October 1994 Energy--Our Future Is Today This electronic package of information is designed to be a helpful resource. The package was produced by the U.S. Department of Energy and the National Renewable Energy Laboratory. The package contains one short speech and two short articles which can be used verbatim or revised according to your particular needs. Also included are 21 papers containing useful facts for various energy technology areas. These useful facts or "Talking Points" are meant to be used to bolster any presentations or public relations activities. The speech, two articles, and 21 "Talking Points" papers are designed to be easily printed from the print option on your computer. General Energy Facts Global Climate Change Transportation Fossil Fuel: Oil Fossil Fuel: Natural Gas Fossil Fuel: Coal Nuclear Power Energy Efficiency Electric Utilities and Energy Efficiency Energy Efficiency and Renewable Energy in Buildings Energy Efficiency in Industrial Technologies Renewable Energy Renewable Energy: Hydropower Renewable Energy: Wind Renewable Energy: Biomass and Biofuels Alternative Fuel Vehicles Hydrogen Energy Renewable Energy: Photovoltaics Renewable Energy: Solar Thermal Renewable Energy: Geothermal Energy Job Creation, Economic Development, and Sustainability Speech Article: Saving Energy in the '90s by Christine A. Ervin Article: Energy Awareness--Why Should We Care?WT03-B20-107IA006-000057-B011-524http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/gen-enrg.html 138.80.61.12 19970221182009 text/html 5526HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:50:03 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5355Last-modified: Tue, 09 Jul 1996 02:25:52 GMT General Energy Facts General Energy Facts Worldwide World energy consumption is expected to increase 40% to 50% by the year 2010,and the global mix of fuels--renewables (18%), nuclear (4%), and fossil(78%)--is projected to remain substantially the same as today; thus globalcarbon dioxide emissions would also increase 50% to 60%. Among industrialized and developing countries, Canada consumes per capita themost energy in the world, and Italy consumes the least among industrializedcountries. Developing countries use 30% of global energy. Rapid population growth,combined with economic growth, will rapidly increase that percentage in thenext 10 years. The World Bank estimates that investments of $1 trillion will be needed in thisdecade and upwards of $4 trillion during the next 30 years to meet developingcountries' electricity needs alone. Traditional biomass fuels, such as wood, crop residues, and animal dung, remainthe primary energy source for more than 2 billion people. United States In 1991, Americans spent about $1,975 per person on all energy purchases. Thisrepresents about 7.7% of the U.S. gross domestic product. For comparison, in1991 Americans spent about $2,280 per capita for housing and about $2,565 formedical needs. Per capita, the United States ranks second in worldwide energy consumptionamong the industrialized nations. America uses about 15 times more energy per person than does the typicaldeveloping country. The United States spends about $440 billion annually for energy. Energy costsU.S. consumers $200 billion and U.S. manufacturers $100 billion annually. The U.S. industrial sector consumed 39% of the nation's energy in 1990. In 1990, commercial buildings accounted for nearly 11% of U.S. total energyconsumption. Residential appliances, including heating and cooling equipment and waterheaters, consume 90% of all energy used in the U.S. residential sector. Today, 36% of U.S. energy is consumed in electricity generation. By 2010, thatwill rise to 41%. The transportation sector consumed 35% of the nation's energy in 1990; thissector is 97% dependent on petroleum. Fossil Fuel Use The United States consumes about 17 million barrels of oil per day, of whichnearly two-thirds is used for transportation. Oil provides more than 40% ofthe nation's primary energy (energy in its naturally occurring form beforeconversion to end-use forms). The United States imports more than seven million barrels of oil per day. Fossil fuels are depleted at a rate that is 100,000 times faster than they areformed. Opportunities A decrease of only 1% in industrial energy use would save the equivalent ofabout 55 million barrels of oil per year, worth about $1 billion. Within 15 years, renewable energy could be generating enough electricity topower 40 million homes and offset 70 days of oil imports. Natural gas supplies the equivalent of 35 million barrels of oil each day, andits use is growing more rapidly than that of any other major source. New Technologies and Initiatives The high initial cost of active solar systems--ranging from $2,000 for smallwater-heating systems to more than $8,000 for residential space-heatingsystems--is the main reason for low sales in the United States. Coolingsystems cost $10,000 to $20,000 for typical homes. Wind power has the potential to supply a large fraction--probably at least20%--of U.S. electricity demand at an economical price. Providing power for villages in developing countries is a fast-growing marketfor photovoltaics. The United Nations estimates that more than 2 millionvillages worldwide are without electric power for water supply, refrigeration,lighting, and other basic needs, and the cost of extending the utility grids isprohibitive, $23,000 to $46,000 per kilometer in 1988. References 1. World Resources, 1990-1991, World Resources Institute, 1992. 2. "Energy and the Developing Countries," Fueling Development: EnergyTechnologies for Developing Countries, 1992. 3. "Energy from the Sun," Scientific American, September 1990. 4. Energy Conservation: Technical Efficiency and Program Effectiveness,Congressional Research Service Issues Brief, April 13, 1993. 5. "In Solar Village, Sunshine is Put in Harness," U.S. News and WorldReport, February 11, 1985. 6. Renewable Energy: A New National Commitment?, Science PolicyResearch Division, Congressional Research Service, April 27, 1993. 7. Expanding Energy Savings by Accelerating Market Diffusion of EfficientTechnologies: Three Case Studies, The Center for Applied Research,February 1992. 8. Statistical Abstract of the United States: 1993, 113th edition, U.S.Department of Commerce, 1994. 9. The World Almanac and Book of Facts: 1994, World Almanac, 1994. WT03-B20-108IA006-000057-B012-50http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/global-c.html 138.80.61.12 19970221182045 text/html 5232HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:50:50 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5061Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Gloabal Climate Change Global Climate Change Climate change refers to the trends that persist for decades or even centuries,over and above natural seasonal and annual changes. Climate changes areinfluenced, among other things, by a natural "greenhouse" effect that maintainsa warm and inhabitable Earth. Climate changes can also adversely affect oceanwater levels and agriculture production. Pollution prevention andenergy-efficient and renewable energy technologies can significantly help inefforts to stabilize and reverse global climate change. The Challenge The amount of atmospheric carbon dioxide has increased about 25% worldwidesince the early 1800s. In just the last 35 years, amounts of the gas haveincreased 10% and are currently increasing nearly half a percent each year. Because of excess greenhouse gas emissions caused by human activities, averageglobal temperatures could rise 2.2 degrees F by 2030, and regional climatechanges could be dramatic. Current Use Among individual countries, the United States is the leading contributor ofgreenhouse gases. Although the United States accounts for only 5% of theworld's population, we contribute 20% of the world's greenhouse gases. The United States, with less than 5% of the world's population, is responsiblefor 25% of the world's carbon dioxide emissions, more than any other nation. The transportation sector produced 32% of U.S. carbon dioxide emissions andwill be the fastest growing source of carbon dioxide emissions through the year2000. The carbon dioxide emission rates for a typical automobile are 1.12 pounds permile. This means that a car traveling a typical commute of 20 miles can emitmore than 20 pounds of carbon dioxide. Including emissions from electricity generation, the industrial sectoraccounted for 33% of U.S. carbon dioxide emissions in 1990. Our homes were responsible for 20% of the U.S. carbon dioxide emissions, andcommercial buildings were responsible for another 16%. Projected Use Net emissions of greenhouse gases such as carbon dioxide, methane, nitrousoxides, and hydrofluorocarbons in the United States are projected to grow byabout 7% between 1990 and 2000. Currently, developing countries account for 30% of global energy use andemissions. However, that rate is projected to rise from 25% to 44% by 2025;this rate is based on the additional electric power needed to maintain moderatelevels of economic growth. The Solution If extensively used, renewable energy sources could save as much as 10% of theemissions produced by the electricity sector during a year. Opportunity is ripe for U.S. businesses to increase exports of energy-efficientand renewable energy technologies to developing countries and to offer supportto these countries to minimize their greenhouse gas emissions without slowingdown needed economic development. Worldwide, the market for environmentally benign technologies is huge. TheCenter for the Exportation of Science and Technology estimates that theEuropean Community will spend more than $400 billion and the United States morethan $770 billion to reduce greenhouse gas emissions this decade. The United Nations is developing a climate fund to channel $200 million todeveloping countries to help them combat global warming. Industrialized nations have recently decreased carbon dioxide emissions byalmost 10%. Smaller, lighter cars have helped improve gas mileage. Moreefficient refrigerators and ovens and better insulated homes have all helpedaccomplish the goal of reducing emissions. Projects and Initiatives The Climate Change Action Plan prepared by President Clinton and Vice PresidentGore in October 1993 outlines a goal to reduce U.S. greenhouse gas emissions to1990 levels by the year 2000. Through the Climate Change Action Plan, U.S. Department of Energy (DOE)initiatives will produce an emissions savings equivalent to the emissionsproduced by electricity generation for all households in California, Texas, andNew York combined. Under "accelerated policies," renewables could reduce greenhouse emissions by40% through 2050, according to a United Nation's Conference on Environment andDevelopment report. References 1. The Climate Change Action Plan, DOE, October 1993. 2. Changing by Degrees: Steps to Reduce Greenhouse Gases, Congress ofthe United States, Office of Technology Assessment, February 1991. 3. Energy Conservation: Technical Efficiency and Program Effectiveness,Congressional Research Service Issues Brief, April 13, 1993. WT03-B20-109IA006-000057-B012-80http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/transprt.html 138.80.61.12 19970221182109 text/html 5241HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:51:23 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5070Last-modified: Tue, 09 Jul 1996 02:25:54 GMT Transportation Transportation General The nation's transportation sector, comprised of approximately 195 millionvehicles of all types, is 97% dependent on petroleum and consumes 63% of alloil used in the United States. Foreign oil constitutes nearly half (49.3%) ofall oil used--and the use of imported oil is growing. Total energy consumption in transportation is expected to grow from 20.1 quadsin 1985 to 24.3 quads in 2010. Efficiency in personal travel is expected toincrease by 44%; efficiency of freight transportation is expected to increaseby only 11%. The average fuel efficiency of the U.S. motor vehicle fleet has alreadyimproved 50% since 1973. But the number of vehicle miles driven has increased45%, offsetting the gains made in efficiency. In 1990, 86.5% of American workers rode in a car, light truck, or van to work.Single-occupancy vehicles made up 73.2% of the total, and only 5.3% used publictransportation. The remainder walked, worked at home, or used a motorcycle orbicycle. Pollution Prevention and Costs A reversible high-occupancy vehicle (HOV) lane can save 6200 gallons ofgasoline and 13,420 pounds of carbon monoxide per day compared to adding a lanein each direction. Rail uses about 25% the energy per ton-mile of trucks and is expected to expandits share of the market for freight. Vehicle emissions continue to exacerbate air pollution. Transportationaccounts for 66% of carbon monoxide emissions, 21% of particulates, 35% ofvolatile organic compounds, and 40% of nitrogen oxides--the precursors toozone and smog. Along with the irritation it creates, traffic congestion costs U.S. communitiesat least $43 billion in lost productivity, wasted gasoline, and increased airpollution. This amount of money equates to the annual sales of the ChryslerCorporation in 1993. Economics and Employment The transportation sector accounts for almost 15% of the nation's GNP andalmost 15% of its jobs. In 1992, Americans spent $424 billion on user-operated transportation. Thisincluded all costs of owning and operating a vehicle. About 25% of this costwas for gasoline and oil. Market Growth Oak Ridge National Laboratory's predicted growth of truck traffic (2.5% peryear from 1995 to 2010) is greater than that of light-duty vehicles, while thefuel economy of trucks increases at a slower rate (0.8% per year from 1995 to2010) than that of light-duty vehicles. Sustainable Alternatives and Advantages Ethanol: By 2000, 500 million gallons of ethanol made frombiomass will displace 13 million barrels of oil; by 2020, 14 billiongallons of ethanol will displace 348 million barrels of oil. Ethanolcurrently supplies nearly 1% of the nation's transportation fuel needs;production is based in the Midwest; about 400 million bushels of corn isconverted to more than 1 billion gallons of ethanol annually. U.S. Jobs: The biomass-to-ethanol industry will create more than2800 new jobs by 2000 and more than 100,000 jobs by 2020. Reduce Global Climate Change: Ethanol made from biomassgenerates 90% less carbon dioxide (the leading cause of global warming) and 70%less sulfur dioxide (the leading agent responsible for acid rain) than doesreformulated gasoline. Ease of Use: Mixing 20% biodiesel with 80% diesel will enableU.S. diesel-fueled fleet operators to comply with stricter emissionsregulations with no major modifications to existing equipment. Opportunities Renewable fuels are a key to achieving sustainable development in the UnitedStates; the Department of Energy's (DOE's) biofuels programs will stimulateinvestment in the nation's agricultural, fuel production, and automobilemanufacturing industries. By 1996, the federal Clean Cities Program anticipates commitments for 250,000alternative fuel vehicles in federal, state, local, and private fleets. References 1. "Conservation Power--A New Look That's Igniting an Energy Revolution,"Business Week, September 16, 1991. 2. OTFA Program-Related Facts, internal paper, DOE, September 27,1991. 3. Energy Efficiency: How Far Can We Go?, Oak Ridge National Laboratory,January 1990. 4. Expanding Energy Savings by Accelerating Market Diffusion of EfficientTechnologies: Three Case Studies, The Center for Applied Research,February 1992. 5. Tomorrow's Energy Today: Energy Efficiency and Renewable Energy,National Renewable Energy Laboratory, November 1993. 6. The World Almanac and Book of Facts: 1994, World Almanac, 1994 WT03-B20-110IA006-000057-B012-192http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/ff-ngas.html 138.80.61.12 19970221182151 text/html 5197HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:52:10 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5026Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Fossil Fuel: Natural Gas Fossil Fuel: Natural Gas The United States has abundant natural gas reserves. And because it isrelatively inexpensive and nonpolluting, natural gas is being used increasinglyfor electricity generation and other industrial uses. New technologies willhelp to extend the types and productivity of gas resources. Resource Natural gas is a combustible, gaseous mixture of hydrocarbons including mostlymethane; it is called "dry" gas when liquid hydrocarbons and nonhydrocarbongases are removed. Total 1992 U.S. natural gas production of 17.8 trillion cubic feet (Tcf) was 27%of total U.S. energy production; that production came from more than 250,000wells in 33 states and in federal waters (Gulf of Mexico, offshore California).Texas produced 6.7 Tcf, Louisiana 5.0 Tcf, and Oklahoma 2.0 Tcf. Year-end 1992 U.S. dry-gas proven reserves were just more than 165 Tcf. Asmuch as 1295 Tcf is technically recoverable in the Lower 48 States--a 60-yearsupply at projected consumption rates. Advantages Natural gas is the most environmentally friendly fossil fuel. There are adequate domestic resources and proven reserves of natural gas. Natural gas can be converted to high-value liquid petroleum products such asautomotive fuel. Current Use and Cost In 1992, the United States consumed 19.5 Tcf of natural gas--more than 24% ofall U.S. energy consumption; industrial use was 7.5 Tcf (39% of total gasconsumption); residential use was 4.7 Tcf (24%); commercial use was 2.8 Tcf(14%); electric utility use was 2.8 Tcf (14%); and other use was 1.7 Tcf (9%) 1992 imports of natural gas were 2.1 Tcf--more than 9% of total U.S.consumption; nearly all of that came from Canada via pipeline. During 1992, residential sector users paid an average price of $5.89 perthousand cubic feet (Mcf); commercial, industrial, and electric utilitiessector users paid $4.88, $2.84, and $2.36 per Mcf, respectively. Projected Use and Cost U.S. gas consumption is expected to increase from 18.7 Tcf in 1990 to 24.1Tcf in 2010. Domestic production is expected to increase from 17.8 Tcf to 20.2Tcf. Imports are expected to provide 16% of consumption in 2010. Residential consumption is forecast to increase 10% from 1990 to 2010;industrial consumption will increase 12%. These increases are attributable togas conversions, continued gas dominance in new housing, and new uses forgas. On January 1, 1993, all price controls on first sale of natural gas wereremoved. Issues Natural gas is not easily transported to or from countries outside NorthAmerica. Pipelines are generally required for distribution to end-users. Opportunities and Challenges Demands for a cleaner environment and advances in gas-fired generationtechnology are increasing the use of gas for the generation of electricity. Atleast half the generating capacity added in the United States in the next 15years will be from gas-fired turbines. Of the estimated 1295 Tcf of recoverable gas in the Lower 48 States, recoveryof more than 200 Tcf depends on developing better technology. Compressed natural gas represents the largest share of alternative-fuelconsumption projected for the transportation sector in 2010. More interregional pipeline capacity is needed to support expansion of gasmarkets. Total pipeline capacity is projected to increase from 89.6 billioncubic feet per day (Bcfpd) in 1992 to 107.2 Bcfpd in 2010. New Technologies Advanced exploration technologies, such as 3-D seismic mapping, can locatepreviously undetected resources. Unconventional gas recovery technologies include production fromlow-permeability reservoirs, recovering methane from coal beds, and recoveringgas from Devonian shales. Research and development in high-efficiency gas technologies (e.g., fuel cells)is strongly supported by the Climate Change Action Plan. Advanced gas turbines operate at higher temperatures and efficiencies; nitrogenoxide emissions will be cut in half. New drilling technologies are reducing the cost of producing gas now. References 1. Energy Information Sheets, Energy Information Administration, DOE,December 1993. 2. Annual Energy Outlook 1994, with Projections to 2010, EnergyInformation Administration, DOE, January 1994. 3. Natural Gas Research--Program Profile: The Resurgence of a DomesticEnergy Option, Office of Fossil Energy, DOE, 1994. 4. Annual Energy Review 1993, Energy Information Administration, DOE,June 1994. WT03-B20-111IA006-000057-B012-285http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/ff-coal.html 138.80.61.12 19970221182237 text/html 4646HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:52:41 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 4475Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Fossil Fuel: Coal Fossil Fuel: Coal The United States has abundant coal resources, and coal is the least expensivefossil fuel on an energy-per-Btu basis. But environmental issues related tocoal-fired power-generating plants could limit its effectiveness. New cleancoal technologies will allow coal to meet emission requirements established bythe Clean Air Act Amendments of 1990. Resource Coal is a solid fossil fuel mined in several parts of the United States fromopen pits (59%) and underground mines (41%). Bituminous/sub-bituminous coalprovides 89% of domestic demonstrated reserves. Mining of western U.S. coal--typically low-sulfur coal from open pits--isprojected to increase from 41% in 1992 to 47% in 2010. Annual domestic coal production is about 1 billion tons, with about 100 milliontons exported to Canada, Japan, and Italy. Domestic in-the-ground coal resources are estimated at 4 trillion tons.Estimated recoverable-coal reserves total 261 billion tons--more than 200 yearssupply at current production rates. Advantages Coal is an abundant domestic resource that strongly supports the miningindustry and provides numerous jobs. Coal is the least expensive electrical energy source. Coal provides a dependable resource and technology for baseload electricalgeneration. Current Use and Cost The principal use of coal is electricity generation by utility companies (87%in 1992); industrial use (11%) is declining. Since 1984, coal has been largest source of domestic energy (33%, or22 quadrillion Btu in 1992). Coal provides more than half the energy consumed by utilities to produceelectricity (55% of total energy, 81% of fossil fuel energy, or 16 quadrillionBtu in 1992). Coal is the least expensive fossil fuel on energy-per-Btu basis ($1.42/millionBtu in 1992). Projected Use and Cost Coal production is projected to increase to 1.2 billion tons in 2010, providing35% of domestic energy production. Coal's cost per Btu is expected to rise by less than 2% until 2010, thusmaintaining its price advantage over other fossil fuels. Issues Coal is not well suited to peaking electrical generation. With today's technology, coal is a major source of air pollution (66% of totalU.S. sulfur dioxide emissions) and greenhouse gases (36% of total man-madecarbon emissions). Opportunities Large-scale retirement of coal and nuclear baseload electric plants will beginaround 2000; these plants could be replaced with clean coal technology. The Clean Air Act Amendments of 1990 place new requirements on future powergeneration; these could be met affordably with clean coal technology. The worldwide market for clean power technologies will range between$270 billion and $750 billion during the next 20 years; this is atremendous opportunity for U.S. industry and jobs. New Technologies Dramatically improved coal liquefaction technologies--to replace importedoil--have cut projected costs in half to $35 per barrel, matching the peak 1981cost for crude oil (recent costs have been around $20 per barrel). Today's coal-fired power plants average 33% efficiency (energy conversion toelectricity); new technologies are projected to greatly improve efficiencywhile reducing air pollution per amount of coal: Low-emission boiler systems (43% efficiency) Pressurized fluidized-bed combustion (50% efficiency) Integrated gasification combined cycle (52% efficiency) Indirectly fired cycles (55% efficiency) Gasification/fuel-cell combinations (60% efficiency, or 85% withcogeneration). References 1. Annual Energy Review 1992, Energy Information Agency, DOE, 1992. 2. Annual Energy Outlook 1994, Energy Information Agency, DOE, 1994. 3. Clean Power Systems--Program Profile, Office of Fossil Energy, DOE,1994. 4. Clean Coal Technology--Program Profile, Office of Fossil Energy, DOE,1994. 5. Energy Information Sheets, Energy Information Agency, DOE, 1993. WT03-B20-112IA006-000057-B012-325http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/nuclear.html 138.80.61.12 19970221182315 text/html 4080HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:53:24 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 3909Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Nuclear Power Nuclear Power Nuclear power uses the energy trapped in the atoms of heavy elements likeuranium. Splitting these atoms to form lighter elements--a process calledfission--causes a release of energy, which is used to boil water into steam.The steam then spins a turbine, which in turn spins a generator to produceelectricity. Resource The 109 operating nuclear power plants in the United States produce about 21%of the nation's electricity--only coal produces more electricity. Nuclear power plant performance is steadily improving. In the early 1970s,nuclear power plants typically operated at only 50% of their total capacity.Today they operate at consistently more than 70% of their capacity. Although no nuclear power plants have been ordered since 1978, many plants thatwere under construction at that time have been completed and are now operating.This steadily increased electricity generation by nuclear power and providedelectricity to meet increased U.S. power needs. Advantages Nuclear power--unlike fossil fuels--generates no air pollutants (such asnitrous oxides or sulfur dioxide) or greenhouse gases, so it does notcontribute to acid rain, smog, or climate change. The nuclear and coal power plants that entered service in the last 20 yearshave cut oil use for electricity production by 1 million barrels per day,helping reduce U.S. dependence on imported oil. Issues There are several barriers to the expanded use of nuclear power. Among theleading ones are: (1) Delays in providing for nuclear waste management (2) Economic competitiveness with other fuels (3) Public concerns about safety, despite actual safety records (4) Investment community concerns about financial risks. Waste management is one of the most difficult problems for nuclear power. Thetechnology exists for the burial of wastes from nuclear power plants andstudies are under way to test the suitability of a potential site for anunderground repository. However, no site has been formally selected, andreaching a final decision requires much more work. Because of the lack of a permanent waste repository, nuclear plants havestockpiled spent fuel on site in large pools of water. These pools were onlybuilt large enough for temporary storage needs, so many are close to capacity,requiring operators to look for other storage options. DOE Projects and Initiatives The U.S. Department of Energy (DOE) is working with the nuclear industry in acost-shared, cooperative program to develop standardized, certified designs forthe next generation of nuclear power plants--advanced light-water reactors.These new, economic plants, incorporating improved safety features, will beavailable for electric utility orders about the turn of the century. DOE is also making major changes in the civilian reactor waste managementprogram to get the program on schedule and avoid further delays. References 1. Monthly Energy Report, Energy Information Administration, DOE, March1994. 2. Edelson, Edward, The Journalist's Guide to Nuclear Energy, AtomicIndustrial Forum, Inc., Bethesda, Maryland, July 1985. 3. Energy Information Sheets, Energy Information Administration, DOE,December 1993. 4. Annual Energy Outlook 1994, with Projections to 2010, EnergyInformation Administration, DOE, January 1994. 5. Annual Energy Review 1993, Energy Information Administration, DOE,June 1994. WT03-B20-113IA006-000057-B012-442http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/enrg-eff.html 138.80.61.12 19970221182416 text/html 5233HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:54:18 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5062Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Energy Efficiency Energy Efficiency Energy efficiency is a measure of how much value we get from energy thatwe consume; any increase in efficiency means a decrease in use andconsequently savings in cost, pollution, carbon emissions, and foreigndependence. Estimates of past and potential future energy and cost savingsresulting from increases in energy efficiency vary widely, but all suggest thatit is one of our most substantial "new energy" resources. Resource One U.S. Department of Energy (DOE) study calculated that energy efficiency andconservation activities from 1973 to 1986 reduced growth of U.S. energy use by30%, saving $225 billion annually. DOE projects that energy efficiency measures will reduce energy use in 2010 by13%, but another DOE study estimated that use of existing cost-effectivetechnology for end-use energy efficiency could reduce energy use by anadditional 13%. Reductions in electricity use through efficiency measures save more than threetimes as much primary energy because of the losses in conversion toelectricity. Primary energy is energy in its naturally occurring form (e.g.,coal or oil) before conversion to end-use forms. An Alliance to Save Energy study concluded that the United States could reduceprimary energy demand by at least 20% using only investment opportunities witha payback of 5 years or less. An American Council for an Energy Efficient Economy study concluded thatvigorous adoption of cost-effective energy efficiency and renewable energymeasures could reduce national energy use in 2030 by 50% compared to1988--saving $2.3 trillion and reducing carbon emissions by 70%. Advantages Reducing energy demand by efficient use is often the cleanest, least expensive,and most effective way to improve the adequacy of energy supply. Energy efficiency reduces air pollution and carbon dioxide emissions andalleviates energy dependence and balance-of-payments problems. Current Use The United States consumes 33 times as much energy per person as India,13 times as much as China, two and a half times as much as Japan, andtwo times as much as Sweden. Only Canada and two small, oil-rich countriesuse as much energy per capita. Projected Use DOE projects a 25% increase in energy consumption by 2010, mostly from theindustrial and transportation sectors; commercial and residential energy growthare expected to be relatively flat. Opportunities The economies of developing countries will be expanding rapidly in the future,and world energy use could expand dramatically. By developing energyefficiency and renewable energy technologies now, the United States can lead byexample, reducing growth of energy use and carbon emissions while creatingmajor markets for the technologies. Challenges Energy use is closely tied to economic activity. But it does not have to bethat way--energy efficiency can hold down growth of energy use and contributeto economic health. From 1977 to 1987, energy use in the United Statesactually decreased slightly while the gross national product increased 27% inreal dollars. New Technologies General Motors and Ford have built prototype five-passenger cars that get90 miles per gallon (mpg); a forthcoming Mercedes will get 75 mpg ofdiesel while meeting California low-emission vehicle standards; an Audi that iscurrently available gets more than 60 mpg of gasoline. 3M Corporation has reduced energy use per unit of production by more than 50%during the last 17 years. A Center for Applied Research study found that fully adopting cost-effectiveefficiency improvements for commercial lighting, industrial motors, andhousehold appliances would reduce energy consumption by 38%. References 1. Energy Conservation: Technical Efficiency and Program Effectiveness,Congressional Research Service, 1993. 2. Alliance to Save Energy, May 13, 1993, Testimony before House Committee onScience, Space, and Technology Subcommittee on Energy, 1993. 3. Energy Efficiency and Job Creation: The Employment and Income Benefitsfrom Investing in Energy Conserving Technologies, American Council for anEnergy-Efficient Economy, October 1992. 4. Annual Energy Outlook 1994, with Projections to 2010, EnergyInformation Agency, DOE, 1994. 5. Automotive News, February 7, 1993. 6. Expanding Energy Savings by Accelerating Market Diffusion of EfficientTechnologies: Three Case Studies, The Center for Applied Research, 1992. 7. Oil Recovery Program Profile: FY 1993, Office of Fossil Energy,DOE. WT03-B20-114IA006-000057-B012-513http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/elec-uti.html 138.80.61.12 19970221182453 text/html 5248HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:54:57 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5077Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Electrical Utilities and EnergyEfficiency Electric Utilities and Energy Efficiency More than one-third (37%) of the energy used in the United States is used toproduce electricity. Roughly 70% of that energy is lost in the process ofconverting it to electricity and distributing the electricity to customers.Any gains in the efficiency of this process could result in significant energysavings for the country. Electric utilities also have a unique ability toencourage energy efficiency among their customers--an approach known asdemand-side management. Both of these approaches help the environment byavoiding power-plant emissions. Energy Efficiency in Production and Distribution of Electricity Although most power plant efficiencies are limited to about 30% to 40%, someefficiency improvements can be gained through the use of more efficient motors,generators, and other electrical equipment. The electrical equipment in powerplants consumes a significant fraction of the generated electricity. Advanced technologies for new fossil-fueled power plants may boost theefficiencies in many cases to 50% (e.g., fuel gasification combined with anadvanced gas turbine), and in some cases to as high as 85% (e.g., fuel cellscombined with cogeneration). The current transmission and distribution systems in the United States aredesigned for safety and reliability, but they are not necessarily energyefficient. The current designs allow power to travel through longer paths thanneeded and even to circulate in loops through the distribution system. Thiscauses energy losses that could be avoided with better technology. New technologies (e.g., thyristor-controlled series capacitor [TCSC]) underdevelopment by industry and the U.S. Department of Energy (DOE) allow an activecontrol of the distribution system to route power more efficiently through thedistribution grid. This will save energy. Demand-Side Management Utility investments in energy efficiency can keep utility bills lower becausethey are more cost effective than building new power plants. For example, theSacramento Municipal Utility District (SMUD) has avoided rate increases for itscustomers for 5 years by carrying out an aggressive energy efficiencyprogram. To meet the nation's projected growth in electricity needs during the next 10years, $100 to $200 billion in new capital investment will be needed. However,new power plants are quite expensive, and there is often strong localresistance to power plant construction. Because of this, many utilities aredoing whatever they can to reduce the demand for electricity and to defer newpower plant construction. The current situation is leading utilities to work with customers to reduce thecustomer's electrical demands or to shift the customer's electricity use awayfrom the periods of high electrical demand. This approach is calleddemand-side management (DSM). Because DSM reduces the customers' electricity bills, it saves the customermoney. Some rate structures allow utilities to capitalize the money that theyspend on DSM, spreading the costs over 30 years. The customer pays this cost,but because it is less than the cost of the electricity saved, the customersaves money. The utility can also profit if it is allowed to collect a profiton the DSM investment. This creates a win-win situation for utilities andtheir customers. DSM is a relatively new and growing service for electric utilities. In 1991,U.S. utilities invested $2 billion in DSM technologies. In 1990, there were more than 1300 utility DSM programs in the United Stateswith 13 million customers participating, saving 0.6% of the nation'selectricity. Experts predict that by 2000, the savings could increase to2.2%. Economic Benefits A 2% reduction in electricity use would cut the nation's electric bills by $6.1billion per year, freeing up this money for investment or other spending. DSM programs are very labor intensive, so they produce new jobs. In 1991alone, utility DSM programs in Massachusetts created 2350 jobs. References 1. 21st-Century Technologies for the "Era of Efficiency," Office ofFossil Energy, DOE. 2. National Energy Strategy, DOE, February 1991. 3. Official Guide to Demand-Side Management Programs and Research,UDI/McGraw-Hill, Washington, DC, 1994. 4. Getting Down to Business: A Strategy for Energy Efficiency in the UnitedStates, United States Energy Association, 1992. 5. The Energy Efficiency Industry and the Massachusetts Economy,Massachusetts Energy Efficiency Council, December 1992. WT03-B20-115IA006-000057-B012-549http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/ee-build.html 138.80.61.12 19970221182531 text/html 5352HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:55:41 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5181Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Energy Efficiency and Renewable Energy inBuildings Energy Efficiency and Renewable Energy in Buildings Buildings include residential, commercial, institutional, and federalstructures. Energy efficiency and renewable energy in buildings encompassbuilding design, building materials, heating, cooling, lighting, andappliances. Current Use and Cost U.S. buildings consume 36% of the country's energy supply at a cost of$193 billion--nearly as much as the combined annual sales of GeneralMotors Corporation and Exxon in 1993, and energy use is growing at a rate of3.3% a year. Including the fossil fuel used to generate the electricity, commercialbuildings account for more than 15% of U.S. carbon-dioxide emissions;residential buildings account for 19%. Building Design Passive solar buildings are being built today that save as much as 50% onheating bills for only 1% more in construction costs. Exemplary buildings--which use the sun, efficient technologies, and theenvironment to provide for a building's heating, cooling, and lightingrequirements--will reduce energy bills by as much as 75%. And these buildingscost no more than conventional buildings. New building technologies, such as low-emissivity (low-E) windows, compactfluorescent lights, and energy-efficient clothes washers and dryers,dishwashers, refrigerators, and freezers have created new businessopportunities and growth markets. Building Materials Poorly insulated windows account for 25% of all heating and coolingrequirements in the United States at a cost of $22 billion--more than theannual sales of Shell Oil Company in 1993. This is equivalent to the amount ofenergy flowing through the Alaskan pipeline each year. New insulating foams do not use chlorofluorocarbon blowing agents or materialsand have the same superior thermal and physical characteristics as those theyreplace. Heating and Cooling Heating and cooling equipment consumes 42% of all building energy use at a costof $81 billion--nearly twice the dollar amount of Chrysler Corporation's annualsales in 1993. A transpired solar collector--which is a dark, perforated metal wall--convertsup to 80% of the sunlight striking it into heated ventilation air forcommercial buildings. Residential natural gas absorption heat pumps with more than a 50% improvementin heating efficiency over the best gas furnaces will be available in 1997. By2000, those units will be improved to provide over 50% more efficient coolingthan the best current absorption air conditioning units. Commercial gas cooling units that are 50% better than the best existingabsorption chillers will be available by 1998. Lighting and Appliances Lighting accounts for about 25% or $44.25 billion of all electricity consumedin the United States--more than the annual sales of the Chrysler Corporation in1993. Compact fluorescent lamps consume 75% to 85% less electricity than doincandescent ones and, typically, last 4 to 5 times longer than incandescentflood lamps and 9 to 13 times longer than ordinary incandescent bulbs. Appliances (other than heating and cooling) use 40% of all electricity consumedin residential and commercial buildings. This equates to a cost of $54.6billion dollars in 1991. Energy efficiency standards for refrigerators, refrigerator-freezers, clotheswashers, clothes dryers, and dishwashers are expected to save approximately 20quadrillion Btu (equal to 24% of the nation's energy use in 1992) of energyduring 1993 to 2015. Beginning with some 1997 models, refrigerator walls and doors will haveadvanced vacuum insulation panels which have five times the insulating value ofcurrent units. Opportunities The Federal Energy Management Program will reduce energy use in federalbuildings 30% by 2005, based on 1985 use. This will save taxpayers more than$1 billion dollars a year by the year 2000. References 1. Scientific American, September 1990. 2. The Climate Change Action Plan, U.S. Department of Energy (DOE),October 1993. 3. OBT Program Overview: Building Materials, DOE, October 1993. 4. OTFA Program-Related Facts, internal paper, Office of Technical andFinancial Assistance, DOE, September 27, 1991. 5. NREL LabTalk, National Renewable Energy Laboratory, August 1994. 6. Expanding Energy Savings by Accelerating Market Diffusion of EfficientTechnologies: Three Case Studies, The Center for Applied Research,February 1992. 7. DOE FY 1995 Internal Budget Review, DOE, July 1993. 8. Energized: Building America, DOE, June 1994. 9. FEMP Program Overview: Fort Lewis Conservation Program, DOE, June1994. WT03-B20-116IA006-000057-B012-586http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/ee-indus.html 138.80.61.12 19970221182557 text/html 5259HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:56:13 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5088Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Energy Efficiency in IndustrialTechnologies Energy Efficiency in Industrial Technologies Industry consumes about 37% of the total energy used in the United States eachyear, at a cost of about $115 billion--nearly as much as the 1993 annual salesof General Motors Incorporated, the largest U.S. company. Helping our nation'sindustries become more productive by using new technologies increases energyefficiency, minimizes or reuses wastes, improves profits, and reducespollution. Additionally, energy efficiency improvements and pollutionprevention measures often complement each other. Opportunities in Industrial Technologies Cogeneration generates steam and electrical energy using 30% lessfuel than if each is produced separately. Industrial cogenerators sell theirexcess power to local electric utilities, generating revenue for their plant. Industrial electric motor systems--motors, speed drives, fans,compressors, and power distribution systems--account for more than 20% of allelectricity used in the United States. That's more electricity than the entireSouth Atlantic region of the United States uses annually. By 2010, using more efficient electric motor systems in theindustrial sector could save 240 billion kilowatt hours of electricity annually(8.5% of total annual electricity production), provide an industrial energycost savings of $13 billion (nearly equal to the 1993 annual sales of theCoca-Cola Corporation), and reduce greenhouse gas emissions by 48.5 milliontons. More than a third of the energy consumed by U.S. industry is used to provideprocess heat, such as hot water and steam. This is about equalto the entire amount of energy used in Texas. Energy use in intensive process industries such as metals, glass, paper,and chemicals, along with petroleum refining and food processing, canaccount for as much as one-fourth of their production costs. If themanufacturing processes used by these energy-intensive industries could be mademore efficient, then these firms could be more competitive. Direct steelmaking, which eliminates steps from the steelproduction process, could reduce energy use by 20%, while reducingair-polluting emissions. Chemical feedstocks, the raw materials used to make otherproducts such as drugs, plastics, or fertilizer, can be made with forestry andagricultural products instead of petroleum-based feedstocks. Thesebiomass-based chemical feedstocks help create significant new markets foragricultural products. Opportunities in Waste Reduction Technologies U.S. industry produces more than 14 billion tons of waste that cost more than$45 billion to treat and dispose of properly in 1990--that's more thanChrysler Corporation's annual sales revenues for 1993. A program operated by the U.S. Department of Energy (DOE) has shown that anenergy/waste assessment can save the average small- to medium-sized plant morethan $20,000 per year. Solar energy can destroy hazardous wastes. Concentrated sunlightcan be used to decontaminate and detoxify water and air. Landfill gas, an environmental and safety problem, can and isbeing captured and converted to energy to produce electricity, heat, or steam.With minimal cleaning, it can be used directly in boilers to create steam forindustrial uses. Using landfill gas does not require a large capitalinvestment for equipment such as generators. Landfills for municipal solid waste are becoming scarce. Today we have about6000 landfills, down from 30,000 in 1976. About 45% of these 6000landfills are close to capacity and may close in the near future. Each year, Americans discard more than 200 million tons of solid waste. Atpresent, about 33% of these wastes are used to generate energy or produce rawmaterials for recycling into new products. The remainder, about 135 milliontons, is sent to landfills and represents a great potential for additionalenergy production or recycling. Municipal solid waste can be converted to energy by (1) directly burning it toproduce steam or electricity, or (2) converting municipal solid waste into fuelpellets and mixing it with coal. Both methods improve the environment byreducing the amount of solid wastes that must be landfilled. References 1. The Motor Challenge, DOE, February 1994. 2. Conservation and Renewable Energy Technologies for Industry, NationalRenewable Energy Laboratory, October 1991. 3. Waste Material Management: Energy and Materials for Industry, DOE,November 1993. WT03-B20-117IA006-000057-B013-85http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/renew-en.html 138.80.61.12 19970221182705 text/html 5364HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:56:39 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5193Last-modified: Tue, 09 Jul 1996 02:25:53 GMT Renewable Energy Renewable Energy Renewable energy sources are either continuously resupplied by the sun or tapinexhaustible resources--as in geothermal energy. In contrast, fossil fuelsform so slowly in comparison to our energy use that we are essentially miningfinite, nonrenewable resources and could exhaust quality supplies within theforeseeable future. The use of renewable energy does not produce greenhousegases and either does not pollute or emits far less pollution than burningfossil fuels. Renewable energy sources also represent a secure and stablesource of energy for our country. These diverse technologies represent animportant source of new industries. Resource Solar thermal and photovoltaics use solar energy directly; wind, biomass, andhydropower indirectly use the products of their energy. For most of the United States, the electrical needs for a typical family couldtheoretically be met by the solar energy shining on about 30 square feet ofroof space. If solar energy systems that were only 10% efficient (well within reach ofcurrent solar cell and solar thermal technology) were placed on 1% of the U.S.land area (such as two or three large counties in Nevada), they could provideenergy equivalent to that used by the United States. Wind energy in North Dakota alone could provide one-third of the U.S.electrical demand. Advantages U.S. industry now spends about $12 billion annually to control airemissions; most renewable energy uses do not pollute at all; others emit farless pollution than burning fossil fuels; use of renewables alleviates airquality and acid rain problems. Federal renewable energy and energy efficiency programs will create 20,000 jobsin 1995. Renewable energy equipment exports reached $245 million in 1992; a massiveworldwide market is anticipated for environmentally benign technologies such asrenewables. Renewable energy uses do not release carbon dioxide or other greenhousegases that threaten to cause global warming, as fossil fuel use does. Renewable energy would all be generated domestically, creating jobs andalleviating energy dependence and balance-of-payments problems. Current Use In the United States, renewables now supply about 8% of the total energy demand(compared to 18% worldwide) and 11% of the electricity generation. Hydroelectric power in the United States accounts for about half of the totalrenewable energy supply and most of the renewable electricity generation. Projected Use The U.S. Department of Energy (DOE) projects that renewable energy productionwill increase to 15% of U.S. energy needs by 2030; a program of price premiumswould increase it to 22%, a program of intensive research and development wouldincrease that projection to 28%. DOE is seeking to double nonhydro renewable energy generating capacity by2000. Within 15 years, renewable energy could be generating enough electricity topower 40 million homes and to offset 70 days of oil imports. Renewables could be saving 65 million tons of carbon emissions--equal to 10% ofthe emissions produced by the electricity sector. President Clinton's Climate Change Action Plan estimates that more than3000 megawatts of electricity will be generated by commercial forms ofrenewable solar thermal energy by the year 2000. Opportunities There will be large-scale retirement of electric generating plants startingaround 2000; nearly one-fourth of the electric utility industry's 2010 capacitywill be built between now and then; this is a major opportunity to installrenewable technologies. The 1990 Clean Air Act Amendments place major new requirements ontransportation vehicles and future power generation; renewables are anexcellent way to meet these demands. Several studies on energy subsidies, while they differ widely in their totalcalculation, agree that renewables have thus far received only a very smallpercentage of energy subsidies. As much as $200 billion in new capital investment will be needed this decade tomeet the nation's growing electricity needs. These investments will determinethe sources of our electrical power for years to come. References 1. Zweibel, Ken, Harnessing Solar Power, 1990. 2. Tomorrow's Energy Today, National Renewable Energy Laboratory,November 1993. 3. FY 1995 Budget Highlights, DOE, 1994. 4. Annual Energy Outlook 1994, with Projections to 2010, EnergyInformation Agency, 1994. 5. The Potential of Renewable Energy: An Interlaboratory White Paper,DOE, 1990. 6. The Climate Change Action Plan, DOE, October 1993. 7. Wind Energy Program Overview: Fiscal Year 1993, DOE, May 1994. WT03-B20-118IA006-000057-B013-140http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/re-hydrp.html 138.80.61.12 19970221182748 text/html 4017HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:57:57 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 3846Last-modified: Tue, 09 Jul 1996 02:25:53 GMT Renewable Energy: Hydropower Renewable Energy: Hydropower Hydropower converts the energy of flowing water into electricity. The flowingwater passes through a turbine that spins like a pinwheel, in turn spinning agenerator to produce electricity. Although most people associate hydropowerwith large dams on rivers, hydropower also includes small dams that can belocated on a diversion from the main river. Current Use Hydropower is the largest renewable energy source in the U.S. It currentlygenerates about 10% of the nation's electricity, and even more during periodsof high electrical demand. Each year, hydropower generates enough power to supply 28 million U.S.households and represents the energy equivalent of nearly 500 million barrelsof oil. Hydropower is the most efficient and reliable of all renewable energy sources.Hydropower plants typically operate at efficiencies of 85% to 95%. Potential Use The Federal Energy Regulatory Commission estimates that the nation's existinghydropower capacity could theoretically be more than doubled. Small-scalehydropower plants with small diversion structures would provide much of thisadditional capacity. New hydropower capacity does not necessarily require new dams: only 2400 ofthe nation's 80,000 existing dams are used to generate power. Many of theothers could be modified to generate power. New technologies can allow existing hydropower plants to operate moreefficiently, producing more electricity. A 1% improvement in the efficiency ofthe existing U.S. hydropower plants would produce enough extra power to supply283,000 households. This would save the energy equivalent of more than5 million barrels of oil each year. Economic Benefits The United States has invested more than $150 billion (in 1993 dollars) inhydropower facilities. This investment, largely made in the 1940s, is nowyielding a significant economic benefit for the nation by providing aninexpensive, sustainable supply of electricity. In 1987, hydropower produced 17% of the electricity in industrialized countriesand 31% of the electricity in developing countries. The World EnergyConference has estimated that this number could increase fivefold. Thisrepresents a large market for the export of U.S. hydropower technologies. Environmental Benefits and Challenges Hydropower plants produce no carbon dioxide, sulfur oxides, or nitrousoxides--no air emissions at all. Because they produce no greenhouse gasemissions, hydropower plants help to minimize global climate change. Hydropower plants produce no solid or liquid wastes. Hydropower projects can have an impact on water quality and fish and wildlifehabitats. To address these concerns, many hydropower projects are beingretrofitted with fish ladders to encourage the upstream migration of fish totheir spawning grounds in the rivers' headwaters. Many projects are alsomaintaining minimum flows through the dams to encourage downstream migrationand maintain downstream wildlife habitats. References 1. Hydropower: America's Leading Renewable Energy Resource, U.S.Department of Energy and the Electric Power Research Institute, April 1993. 2. Profiles in Renewable Energy: Case Studies of Successful Utility-SectorProjects, National Renewable Energy Laboratory, October 1993. 3. "Energy from the Sun," Scientific American, September 1990. WT03-B20-119IA006-000057-B013-175http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/re-wind.html 138.80.61.12 19970221182809 text/html 5097HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:58:24 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 4926Last-modified: Tue, 09 Jul 1996 02:25:53 GMT Renewable Energy: Wind Renewable Energy: Wind Wind has become a viable source of electric energy for utilities. Utility-scalewind power plants consist of 100 to 1000 wind turbines that are located closeto each other, have a single electrical connection to the utility grid, and canbe managed from a single control room. Resource Excellent wind resources are widely dispersed throughout the world. Almostevery country has some areas with good wind resources. Power in the wind is proportional to the cube of the wind speed. Therefore,locations with higher average wind speeds have much higher energy resources.The windier the location, the more kilowatt-hours (kWh) can be produced by thesame equipment, and the lower the overall cost of energy. The total U.S. wind resource is very large. Although every region in thecountry has some windy areas, much of the U.S. wind resources are concentratedin the Great Plains. For example, the state of North Dakota alone hasenough energy from good wind areas to supply 36% of the 1990 electricityconsumption in the lower 48 states. Many windy locations are in remote areas, far from load centers, and do nothave transmission lines nearby. Current Use and Cost Wind power plant electricity generating costs have gone from $0.30/kWh in 1981to $0.05/kWh in 1990--an 84% decrease in cost. In California, 1700 megawatts (MW) of rated capacity is installed (as much asthat of two large coal-fired power plants); in Europe, another 1000 MW isinstalled (as much as that of one nuclear power plant). The wind power plantsin California produce 3.1 billion kWh/year; this is 1.2% of theelectricity used by California or 0.1% of the electricity used by the UnitedStates--enough to supply a residential city the size of San Francisco andWashington, D.C., combined. Existing wind power plants produce electricity at a levelized cost of $0.05 to$0.08/kWh at a site with an average annual wind speed through the rotor of15.4 miles per hour (mph). Those turbines have a first cost as little as$1,000/kW and last as long as 20 years. New turbines are coming on line thatproduce electricity at less than $0.05/kWh; they cost as little as $750/kW andhave expected lifetimes of 20 to 30 years. Turbine availability of power plantsis 95% or more, better than that of most conventional power plants. Operationand maintenance costs are typically lower than those for conventional powerplants. Existing wind power plants exploit very windy locations with average annualwind speeds of at least 16 mph. The United States could supply 20% of itselectricity from these very windy locations with existing technology--producing560,000 million kWh per year. These areas cover 18,000 square miles--0.6%of the lower 48 states. Less than 5% of this land would be used by theequipment and access roads; most of the existing land use, such as ranching andfarming, would continue as it is now. Projected Use and Cost In the United States, 2000 to 5000 MW of new capacity are planned or are underconstruction. A similar amount is planned or under construction in 11 countriesin the European Union. Worldwide, sizable wind power plants are on the drawingboards in Argentina, Chile, China, India, Mexico, and the Ukraine. By 2000, wind-generated electricity will cost as little as $0.04/kWh. At thisprice, it can compete with any type of conventional generation. Technology Development Costs will be reduced through employing advanced wind technology; furtherreducing operation and maintenance associated with new, larger power plants;and manufacturers taking advantage of economies of scale in unit size,purchasing, and production. New wind technologies to improve reliability and reduce costs include improvedaerodynamic performance of blades; improved design of mechanical components;variable-speed and/or low-speed generators; and controls that increase systemefficiency and lifetime. References 1. America Takes Stock of a Vast Energy Resource, Utility Wind InterestGroup, February 1992. 2. Wind Energy Weekly, American Wind Energy Association, Vol. 13, No.605, 1994. 3. Wind Energy Program Overview, Fiscal Year 1993, U.S. Department ofEnergy, May 1994. 4. Integrating an Ever-Changing Resource, Utility Wind Interest Group,July 1992. 5. Economic Lessons from a Decade of Experience, Utility Wind InterestGroup, August 1991. WT03-B20-120IA006-000057-B013-209http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/re-bioms.html 138.80.61.12 19970221182826 text/html 5515HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:58:45 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5344Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Renewable Energy: Biomass andBiofuels Renewable Energy: Biomass and Biofuels The nation's transportation sector is 97% dependent on petroleum and consumes63% of all oil used in the United States. Foreign oil contributes nearly half(49.3%) of all oil used--and the use of imported oil is growing. Biomass isorganic material that can be converted into enough transportation fuels, calledbiofuels, to displace significant amounts of the imported oil withoutencroaching on food and forestry crops. Resource Nonfood agricultural energy crops and more than half of the nation's household,industrial, agricultural, and forestry wastes could produce enough ethanol tosupply most of the nation's current gasoline consumption. Microscopic aquatic plants called microalgae could produce enough biodiesel tosatisfy the entire U.S. diesel market. Electricity produced from biomass has grown from 200 megawatts (MW) in theearly 1980s to more than 8000 MW today, representing a 4000% increase. Advantages Reduce imported oil consumption: By 2000, 500 million gallons ofethanol made from biomass will displace 13 million barrels of oil; by 2020, 14billion gallons of ethanol will displace 348 million barrels of oil. Create U.S. jobs: The biomass-to-ethanol industry will createmore than 2800 new jobs by 2000 and more than 100,000 jobs by 2020. Reduce global climate change: Ethanol made from biomassgenerates 90% less carbon dioxide (the leading cause of global warming) and 70%less sulfur dioxide (the leading agent responsible for acid rain) than doesreformulated gasoline. Ease of use: Mixing 20% biodiesel with 80% diesel will enableU.S. diesel-fueled fleet operators to comply with stricter emissionsregulations with no major modifications to existing equipment. Current Use and Cost Ethanol currently supplies nearly 1% of the nation's transportation fuel needs;production is based in the Midwest; about 400 million bushels of corn isconverted to more than 1 billion gallons of ethanol annually. Current conversion costs: Biomass can be converted to ethanolfor less than $1.00/gallon--down from $3.42/gallon in 1980; biomass can beconverted to methanol for $0.84/gallon; soybeans can be converted to biodieselfor $2.50/gallon. Projected Use and Cost Department of Energy goals: Conversion costs: Biomass to ethanol for $0.67/gallon by 2000;biomass to methanol for $0.55/gallon; microalgae to biodiesel for $1.00/gallonby 2015. 10% of cars on U.S. highways will use alternative fuels by 2000; 25% by 2010. Opportunities Renewable fuels are a key to achieving sustainable development in the UnitedStates; the U.S. Department of Energy's (DOE's) biofuels programs willstimulate investment in the nation's agricultural, fuel production, andautomobile manufacturing industries. More than 36 million acres enrolled in the Conservation Reserve Program (CRP)since 1986 could produce energy crops while improving the quality of thecropland, which is the purpose of the CRP. Energy crops, such as switchgrass, poplars, and willows, will soon be grown bymany of America's farmers to provide fuel for high-efficiency biomass powerplants. These biomass plants will contribute to the battle against globalwarming by reducing greenhouse gas emissions significantly compared toconventional power production. Challenges Costs: Enable biofuels to be cost-competitive with gasoline whenoil is $25/barrel. Developing a fuel industry infrastructure: Farmers want marketsbefore they plant energy crops; fuel producers want guarantees of stablesupplies before they build a new infrastructure. New Technologies Technologies that enable producers to extract and ferment sugar from nonstarchmaterials are being developed now; this would allow use of nonfood crops andother biomass resources. Technologies that can convert oils found in microalgae to biodiesel are beingdeveloped now. Also under development are new gasifier designs and other thermochemicaladvances that enable methanol and biocrude, a crude oil replacement, to be madefrom biomass. References 1. Hinman, Norman, "Biofuels: A Strategy for a Strong America," presentationby N. Hinman, figures based on Biofuels Deployment Plan, National RenewableEnergy Laboratory, 1994. 2. Ervin, Christine, Assistant Secretary, Office of Energy and RenewableEfficiency, Renewable Energy: Vision for the 21st Century, text forspeech presented at 1994 Agricultural Summit of New Uses, June 1994. 3. Energy Policy Act of 1992. 4. Useful Facts on the Impacts of Deploying Energy Efficiency and RenewableEnergy Technologies and Practices, Office of Technical and FinancialAssistance, DOE, January 1994. WT03-B20-121IA006-000057-B013-240http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/altrfuel.html 138.80.61.12 19970221182842 text/html 4534HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:59:01 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 4363Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Alternative Fuel Vehicles Alternative Fuel Vehicles Alternative fuels include ethanol, methanol, natural gas, liquefied petroleumgas, hydrogen, electricity, and any other fuel or energy source that is notproduced from petroleum. Alternative fuel vehicles (AFVs) can be passenger carsor trucks (light-duty vehicles), heavy-duty trucks, or buses that have beendesigned or modified to operate on alternative fuels. Advantages Only 13% market penetration for AFVs in the year 2010 could save 3.5 quads ofenergy by that year. This equals about 15% of the total energy used by thetransportation sector or the total amount of energy used by Illinois. Widespread AFV use will reduce atmospheric pollutants, stimulate job creation,and strengthen domestic energy security. Federal tests showed that AFVs compare favorably to conventional vehicles interms of fuel economy and generally produce fewer harmful emissions thangasoline counterparts. Current Use About 275,000 AFVs were running on American roads at the end of 1993; these arenearly all federal or private fleet vehicles. Most AFVs are powered by liquefied petroleum gas, followed by compressednatural gas, and next, by a blend of 85% gasoline and 15% methanol, also knownas M85. Through the U.S. Department of Energy (DOE), the federal government is leadingthe drive to increase the number of AFVs on road. Projected Use The federal Energy Information Administration projects that there will be 2.5million AFVs on the nation's highways by 2010. AFV use is continuing to increase; for example, an executive order signed byPresident Clinton in 1993 requires that 75% of the vehicles acquired by thefederal fleet by 1999 be AFVs. Opportunities An important piece of legislation, The Alternative Motor Fuels Act of1988, is spurring America's adoption of AFVs. This legislation directed DOE to work with other agencies to promote thedevelopment and widespread use of alternative fuels, thus propelling AFVstoward commercial application and consumer acceptance. Another crucial piece of federal legislation, The Clean Air ActAmendments of 1990, requires modified gasoline content and establishedstricter emissions regulations for vehicles in designated ozone nonattainmentregions of the country. The Energy Policy Act of 1992 requires that federal, state, andlocal governments, alternative fuel providers, and private fleets buy AFVs inincreasing percentages over time. This act also includes tax incentives forbuying AFVs and developing retail service stations. Challenges DOE is working on infrastructure development, cost reduction, and vehicle rangeimprovements to achieve consumer acceptance. DOE Projects and Initiatives The Clean Cities Program: Aims to establish a self-sustaining AFVinfrastructure by involving federal, state, and local governments; fuelsuppliers; vehicle manufacturers; consumers; fleet managers; utilities; andenvironmental groups. The California Pilot Program: Sets nonmethane hydrocarbonemission standards for California fleet vehicles beginning in model year 1996.California will have to acquire 150,000 "clean" vehicles by that year. The Clean-Fuel Fleet Program: Requires one-third of theacquisitions by centrally fueled fleets to use clean fuels and meet morestringent tailpipe standards by 1998. The National Alternative Fuels Hotline/Alternative Fuels Data Center:Establishes a centralized hotline number for nationwide AFV information(1-800-423-1DOE). References 1. "OTFA Program-Related Facts," Internal paper, Office of Technical andFinancial Assistance, DOE, September 1991. 2. Alternatives to Traditional Transportation Fuels: An Overview," DOE,Energy Information Administration, Office of Coal, Nuclear, Electric andAlternate Fuels, June 1994. WT03-B20-122IA006-000057-B013-295http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/hydrogen.html 138.80.61.12 19970221182911 text/html 4561HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 17:59:30 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 4390Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Hydrogen Hydrogen Energy Hydrogen, as an energy carrier, is anticipated to join electricity to becomethe foundation for a national sustainable energy system using renewable energy.Hydrogen can be made safe, environmentally friendly, and versatile, and it hasmany potential energy uses, including powering nonpolluting vehicles, heatinghomes and offices, and fueling aircraft. Resource Energy from renewable sources--sunlight, wind, hydropower, and biomass--must bestored and transported so it is available when and where it is needed.Hydrogen potentially could be produced using renewable sources, then stored andused later in homes, factories, businesses, vehicles, and airplanes. Hydrogen can be produced from water using electricity, in a process calledelectrolysis. Production is also possible from direct sunlight acting on wateror biological organisms and from organisms that create hydrogen in the darkfrom carbon monoxide and water. These processes are all subjects of currentresearch and development programs. Current Use Most of the domestic hydrogen used today is produced from fossil fuels--naturalgas or petroleum derivatives. The major markets for hydrogen are in thepetrochemical and fertilizer industries. The National Aeronautics and Space Administration uses hydrogen to propel itsspace shuttles into orbit and to provide all of the shuttles' electric powerfrom on-board fuel cells. Fuel cells combine hydrogen and oxygen to generateelectricity; the fuel cells' exhaust--pure water--is used for drinking water bythe crew. Advantages and Opportunities The production of hydrogen from renewable electricity and from biomass couldreduce our dependence on imported petroleum. If the U.S. Department of Energy(DOE) reaches its goal of hydrogen energy providing 10% of the total U.S.energy consumption by 2025, our dependence on oil imports could be cut inhalf. Hydrogen can be combined with gasoline, ethanol, methanol, or natural gas; justadding 5% hydrogen to the gasoline-air mixture in an internal combustion engine(ICE) could reduce nitrogen oxide emissions by 30% to 40%. An ICE converted toburn pure hydrogen produces only water and minor amounts of nitrogen oxides asexhaust. California's new "zero-emission" standard for passengercars--requiring that 2% of new cars sold in the state be nonpolluting by1998--could be met by electric vehicles powered by hydrogen fuel cells, orhybrids powered by hydrogen-fueled ICEs and batteries or flywheels.Manufacturing fuel cells to meet the potential demand could add 70,000 new jobsto the state. Hydrogen can be produced from a variety of renewable sources and has many usesin our economy. Because of the versatility of production methods and end use,wide-spread hydrogen energy use will create significant benefits to theagricultural, manufacturing, transportation, and service sectors of the U.S.economy. DOE Projects and Initiatives Large-scale use of renewable hydrogen energy requires advances in production,storage, and utilization technologies. Improvements in stationary and on-boardhydrogen storage technologies are necessary to meet mass market energy demands;transportation requires special tanks or pipelines; and some advances are stillrequired to make vehicles powered by fuel cells practical. Research anddevelopment programs encouraged by DOE are concentrating in all of these keyareas. References 1. Proposal for a Sustainable Energy Future Based on Renewable Hydrogen,Senator Tom Harkin, June 3, 1993. 2. Hoffman, Peter, The Hydrogen Letter, Hyattsville, MD, July 1994. 3. Hydrogen Program Plan--FY 1993-FY 1997, DOE, June 1992. 4. Hoffman, Peter, The Hydrogen Letter, Hyattsville, MD, June 1994. 5. The Los Angeles Times, May 12, 1994. 6. FY 1994 Annual Operating Plan--Hydrogen Program, DOE and NationalRenewable Energy Laboratory, February 1994. WT03-B20-123IA006-000057-B013-507http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/re-solar.html 138.80.61.12 19970221183122 text/html 5062HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:00:41 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 4891Last-modified: Tue, 09 Jul 1996 02:25:53 GMT Renewable Energy: Solar Thermal Renewable Energy: Solar Thermal Solar thermal technology converts sunlight into usable forms of heat. Thisheat can be used directly, for example, in passive solar building design androof-mounted water heating systems used for residential, commercial, andindustrial applications. Other solar thermal systems use the captured heat todrive electric generators, producing electricity on a utility scale; these areknown as solar thermal electric systems. Resource Solar thermal technology is diverse. Small systems can be installed on theroofs of homes to heat water for domestic use. Moderate-size systems cansupply hot water, steam, and hot air to schools, hospitals, businesses, andindustries. Large solar thermal electric installations can generateelectricity in quantities comparable to those generated in intermediate-sizeutility generating plants (that is, 100 to 200 megawatts [MW] ofelectricity). The technology can also be used to destroy environmentalcontaminants in air, water, and soil. Among the first mechanical uses of solar thermal energy was a 20 squaremeter, parabolic concentrating reflector, steam-driven printing press at theWorld's Fair in Paris in 1878. Within 15 years, renewable energy, including solar thermal technologies, couldbe generating enough electricity to power 40 million homes and to offset 70days of oil imports. This technical potential equates to 42% of the number ofhomes in the U.S. in 1994. Solar collectors covering less than half of Nevada could supply all of theUnited States' energy needs. Current Use Passive solar buildings are being built today that save as much as 50% onheating bills for only 1% more in construction costs. More than 350 MW of solar thermal electric systems have been installed in theUnited States, enough to serve the residential needs of the city of Seattle;this equals 90% of the world's installed solar capacity. Solar thermal electric systems operating in the United States today meet theneeds of 350,000 people (equal to the population of the city of Miami) anddisplace the equivalent of 2.3 million barrels of oil annually. The U.S. has about 400 MW of privately financed solar steam-to-electricityfacilities that are interconnected to utility-grade power plants. More than one-half million solar hot water systems have been installed in theUnited States, mostly on single-family homes. The majority of these systemsare used to heat swimming pools. Typically, a homeowner relying on electricity to heat water could save up to$500 in the first year of operation by installing a solar water heating system.The savings over time increases due to increasing electricity rates. Theaverage solar heating system pays for itself in 4 to 7 years. Roof-mounted solar hot water systems are often designed to look like skylights,making them more pleasing in appearance to homeowners and their neighbors. Projected Use Solar thermal electric capacity is predicted to increase 130% worldwide by theyear 2000. The cost of building, operating, and maintaining solar thermal electric systemshas decreased dramatically--in some cases by a factor of ten--during the lastdecade. Some designs will be economically competitive with conventionalelectricity-generating technologies by the year 2000. By the middle of the next decade, some solar thermal electric technologiescould be producing electricity at $0.06 to $0.07 per kilowatt hour (kWh).Average electricity prices were $0.08/kWh for residential users and $0.05 forindustrial users in 1993. The cost of solar water heating systems declined by 30% between 1980 and 1990.Further cost reductions will not be as dramatic, but prices will continue todecrease as demand increases and manufacturers take advantage of economies ofscale. References 1. Solar Thermal Electric: Five Year Program Plan, FY 1993 through1997, Solar Thermal and Biomass Power Division, Office of Solar EnergyConversion, U.S. Department of Energy (DOE), 1993. 2. Economics of Solar Energy Technologies, American Solar EnergySociety, 1992. 3. Cool Energy: The Renewable Solution to Global Warming, Union ofConcerned Scientists, 1990. 4. The Climate Change Action Plan, DOE, October 1993. 5. Eber, Kevin, Renewable Energy: A Guide to a New World of EnergyChoices (draft), National Renewable Energy Laboratory, 1994. WT03-B20-124IA006-000057-B013-589http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/re-geoth.html 138.80.61.12 19970221183215 text/html 5387HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:02:24 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5216Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Renewable Energy: GeothermalEnergy Renewable Energy: Geothermal Energy Geothermal energy is renewable heat energy from the Earth. Geothermalreservoirs of hot water or steam are tapped with wells, and their heat iseither used directly or converted to electricity. Geothermal applications canrange in scale from small, residential heat pumps to large district heatingsystems. Geothermal Electric Power: Current and Potential Use The 45 geothermal power plants in the United States produce enough electricityto power the homes of more than 3.5 million people--that's about how manypeople live in the Dallas/Fort Worth area. Geothermal power plants are very reliable and require minimal maintenance. Theaverage geothermal plant operates 95% of the time, compared to 70% to 80% fornuclear and coal-fired plants. The Geysers, a large steam reservoir north of San Francisco, is the largestsource of geothermal power in the world. California obtains about 7% of itselectricity from geothermal power plants. The total amount of geothermal energy that could be exploited with today'stechnology is estimated at 27 times more than energy used throughout the entirecountry each year. Economic Benefits of Geothermal Electric Power Geothermal plants in the United States employ approximately 3700 people, withelectricity sales of about $1 billion per year. A typical large geothermalplant pays as much as $4.2 million annually in local property taxes. Today's geothermal plants produce power at a cost competitive with conventionalenergy sources and ranging from $0.03 to $0.075 per kilowatt-hour. During the next 20 years, foreign countries are expected to spend $25 to$40 billion constructing geothermal power plants, creating a significantopportunity for U.S. suppliers of geothermal goods and services. U.S. firmsrecently announced contracts totaling approximately $4.5 billion to buildgeothermal power plants in Indonesia and the Philippines. Royalties received by the U.S. government for geothermal leases are more than$30 million annually. Environmental Benefits of Geothermal Electric Power Geothermal power plants have very low air emissions. Producing the same amountof electricity, a typical geothermal power plant would emit no nitrous oxides,only 1% of the sulfur dioxide, and only 5% of the carbon dioxide of acoal-fired plant. Some geothermal power plant designs emit no carbon dioxide, which is agreenhouse gas. This makes geothermal power an ideal technology for helping tominimize global climate change. In 1991, California's Lake County, home to many of the power plants at TheGeysers, became the first and only county to fully meet California's stringentair quality regulations. Lake County received an award in 1992 and 1993 for thecounty with the cleanest air in California. Direct Use of Geothermal Energy: Current and Potential Use Direct uses of geothermal heat include heating buildings and greenhouses,pasteurizing milk, deicing roads, heating water in fish farms, dehydratingfoods, growing mushrooms, secondary oil recovering, and heating leachingsolutions at gold mines. Such applications currently save the energyequivalent of 2 million barrels of oil each year. The known geothermal hot water resources could produce enough energy to replace1.2 trillion cubic feet of natural gas every year for 30 years. This ismore than 60 times the amount of natural gas currently used each year. Geothermal Heat Pumps Geothermal heat pumps (GHPs) use the Earth as a heat source and sink. Theirhigh efficiency can cut annual home heating costs by as much as 50% and cutcooling costs by 25%. GHPs can be used throughout the country and are rapidlygaining popularity in the Midwest and Northeast. GHPs are a major component of President Clinton's Climate Change Action Plan.recently announced contracts totGHP installations are projected to exceed 400,000 annually by the year 2000, upfrom 40,000 units per year today. This would avoid the need to construct fourmedium-size (300-MW) power plants each year. References 1. Geothermal Progress Monitor, U.S. Department of Energy (DOE),December 1993. 2. Accomplishments, DOE Geothermal Fact Sheet, April 1994. 3. Geothermal Electric Power Systems, DOE Geothermal Fact Sheet, April1994. 4. The Potential of Renewable Energy: An Interlaboratory White Paper,DOE, March 1990. 5. Economic Impacts of Geothermal Development, DOE Geothermal FactSheet, April 1994. 6. Opportunities in Developing Countries, DOE Geothermal Fact Sheet,April 1994. 7. Geothermal Energy: Heat from the Earth/Power for the Future,Electric Power Research Institute, February 1992. WT03-B20-125IA006-000057-B014-173http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/job-crea.html 138.80.61.12 19970221183416 text/html 5296HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:03:13 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5125Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Job Creation, Economic Development, andSustainability Job Creation, Economic Development, and Sustainability Current Benefits The wind energy industry presently supports more than 50businesses in California; 1200 people are employed directly in these businessesand another 4300 jobs have been created indirectly. Nearly all jobs arerelated to operating, maintaining, and servicing wind turbines. From biomass alone, 66,000 jobs are supported in the UnitedStates, with a great many in rural areas. By 2010, biomass powercould support more than 283,000 U.S. jobs. The Sacramento Municipal Utility District's (SMUD's) Conservation Power Programhas (1) spent $59 million locally on energy-efficiencymeasures, (2) avoided spending $45 million to purchase power from otherregions, (3) increased regional income by $124 million,(4) created about 880 direct jobs, 250 of which were SMUD employees, and(5) added $22 million to the wage-earning households in the area. The Massachusetts State Energy Office reports that the state has realized a257% growth in energy-efficiency firms (such as energy servicecompanies) between 1988 and 1992. Fox River Mills of Osage, Iowa, has reduced the energy cost of producing apair of socks by 29% since 1984. This cost reduction has contributed toincreased productivity and increased orders from customers. Themill, which employed 110 workers in 1984, employs 310 people today. Theplant owners have expanded the size of the plant twice and added anothershift. Projected Benefits On the national level, a 1992 study cosponsored by the SolarEnergy Industries Association, the American Gas Association, and the Allianceto Save Energy conservatively projects 175,000 new jobs by 2010, with an energyscenario emphasizing efficiency improvements, renewables, and strategic use ofnatural gas. The value of renewable energy equipment exports reached $245million in 1992, including $210 million for photovoltaic equipment. Near-termexport market expansion of $800 million in sales of renewable energy and energyefficiency equipment would create approximately 29,000 new jobs. The U.S. employment base would increase by 67,500 jobs annually from the years1991 to 2000 as a result of the lighting efficiency upgrade undera federal accelerated technology diffusion proposal. Wage and salary incomewould grow $1.15 billion annually during the same period. With an average increase in electric motor efficiency of 6%, theU.S. employment base would increase an average of 9200 jobs per year from 1991to 2000. An associated annual increase in wage and salary income of $190million would also be realized during this same period. Electric bill savingsof approximately $600 million would be realized by the year 2000. The Wisconsin Energy Bureau recently found that with a 75% increase in thestate's renewable energy use, the state would realize more than62,000 new jobs, $1.2 billion in new wages, and $4.6 billion in new sales forWisconsin businesses. By improving automobile and light truck fuel economy from 28miles per gallon (mpg) in 1990 to 40 mpg in 2000, and then to 50 mpg in 2010,large numbers of jobs would be created. Under this fuel economy improvementproposal, 72,000 and 244,000 jobs would be created by 2000 and 2010,respectively. About 293,000 new jobs could be created by 1995, 471,000 new jobs by 2000, andnearly 1.1 million new jobs by 2010 with a major national commitment toenergy efficiency improvements. Less than 10% of the new jobscreated are associated with direct investment in efficiency measures, whilemore than 90% are associated with energy savings and the "respending" of thosesavings. References 1. Energy Efficiency and Job Creation: The Employment and Income Benefitsfrom Investing in Energy Conserving Technologies, American Council for anEnergy-Efficient Economy, October 1992. 2. "Jobs Benefits of Expanding Investment in Solar Energy," Solar IndustryJournal, Fourth Quarter 1992. 3. Electricity From Biomass: Renewable Energy Today and Tomorrow, U.S.Department of Energy, April 1993. 4. Jobs in a Sustainable Economy, Worldwatch Institute, September1991. 5. Economic Impact of SMUD's Conservation Power Program, ResourcePlanning Department, Sacramento Municipal Utility District, December 1993. 6. Expanding Energy Savings by Accelerating Market Diffusion of EfficientTechnologies: Three Case Studies, The Center for Applied Research,February 1992. WT03-B20-126IA006-000057-B014-381http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/oped_ce.html 138.80.61.12 19970221183643 text/html 5549HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:06:49 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 5378Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Article: Saving Energy in the '90s Energy Awareness Month Op-Ed Article Saving Energy in the '90s by Christine A. Ervin It has been nearly 20 years since we experienced our last oil shock in theUnited States. Should we still care about using energy more efficiently, andabout making greater use of solar, wind and other types of renewable energy? You bet. Even though no crisis appears to be lurking around the corner, thereis more reason than ever for each of us to make our country, our community andour personal lives cleaner, wealthier, healthier and more productive with goodenergy choices. Never before have we consumers had so many options for saving energy dollars.Since the last energy crises in the 1970s, America's scientists andentrepreneurs have developed new generations of products--super-savingappliances, incredibly efficient lighting, intelligent designs for buildings,clean industrial processes that prevent the generation of wastes, cars thatoperate on renewable fuels rather than gasoline. These are no longer the gizmos of the future. Many of the new energytechnologies--ranging from wind machines to passive solar buildings, and fromcompact fluorescent lights to cars powered by natural gas--are on the markettoday, providing clean energy at money-saving prices. Today, wind machines are producing electric power as cheaply as coal ornuclear power plants in some parts of the United States. In parts of theMidwest and Northeast, there is enough untapped wind to provide more thanenough electricity for the entire country. Geothermal heat pumps, which heat and cool buildings using the earth's naturaltemperatures, are growing in popularity. Passive solar buildings, which makeuse of sunlight for heat and illumination, continue appearing all around thecountry. And photovoltaic cells--those wafer-thin devices that powercalculators, watches and satellites--are providing economical electricity in alltypes of remote applications, from park shelters to highway signs. One utility--the Sacramento Municipal Utility District--is building its next power plant onthe roofs of its customers, in the form of house-by-house photovoltaicpanels. More revolutionary products are on the horizon. At the U.S. Department ofEnergy (DOE), to give just two examples, we are researching ways to makecompletely recyclable automobiles, cars that get three times more fuel mileagethan the models on the road today, and roofing shingles that produceelectricity when exposed to sunlight. Without a crisis to spur us on, why should we care about improving our energyefficiency and expanding our use of renewable energy systems? There are manyreasons. The money each of us saves on our energy bills is like new tax-free income.That new income not only makes us wealthier, it also creates jobs for others aswe spend the money on other goods and services. Studies done for DOE indicatethat each dollar invested in energy efficiency creates more employment thandollars spent on many other types of investment, including building new powerplants. Efficiency and renewable energy strengthen the economy in other ways, too.Today, the United States imports nearly 50% of its petroleum from elsewhere--more than during the oil shocks of the 1970s--and the amount is projected torise to 60% by 2010. Most of our foreign trade deficit is due to these imports.Reducing oil imports lowers the trade deficit, makes our economy healthier, andkeeps dollars at home, creating jobs. Reducing our dependence on imports may also reduce the number of militaryconflicts the United States is forced to confront in the future. By investing in energy efficiency and renewable energy at home, we can makethese young U.S. industries stronger and more competitive in the globalmarketplace. Today, half the world's people do not have electricity. Massivenations like China and India are undertaking development projects that willopen up vast new markets for energy technologies. By capturing these opportunities, U.S. energy efficiency and renewable energycompanies will not only increase America's exports, they'll also help make surethat newly industrializing nations are built upon foundations of clean,inexhaustible energy supplies. And that leads to another major benefit: Energy efficiency and most renewableenergy systems prevent pollution and many other types of environmental damages.At home and abroad, they are the key to making economic growth compatible withenvironmental health. There have never been more good reasons or more good opportunities for each ofus to take advantage of energy efficiency and renewable resources. If you wantto learn more about your choices, call DOE's Energy Efficiency and RenewableEnergy Clearinghouse at 1-800-363-3732. Christine Ervin is Assistant Secretary for Energy Efficiency and RenewableEnergy at the U.S. Department of Energy. WT03-B20-127IA006-000057-B014-406http://lacebark.ntu.edu.au:80/j_mitroy/sid101/energyfacts/oped_tm.html 138.80.61.12 19970221183703 text/html 4706HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:07:19 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 4535Last-modified: Tue, 09 Jul 1996 02:25:52 GMT Article: Energy Awareness -- Why Should WeCare? Energy Awareness Month Op-Ed Article Energy Awareness--Why Should We Care? For the fifteenth year in a row, the U.S. Department of Energy (DOE) hasdeclared that October is Energy Awareness Month. So what? Prices seemreasonable. There are no gas lines or fuel shortages. Why should peoplecare? The reason is our children, and their children, and the generations thatfollow. The United States and other nations are making decisions today thatwill affect the global economy and environment for centuries to come. In 1860, when Abe Lincoln was running for President, the United States wasusing the equivalent of 111 million barrels of oil annually. Today, we usethat much oil in less than a week. Last year, we spent $51 billion on imported oil. Oil imports accounted for 44%of our foreign trade deficit. By the end of this decade, the amount of U.S.dollars leaking away to oil-producing nations overseas is expected to grow to$94 billion. Today, we spend almost $500 billion annually in the U.S. to lightand heat our homes and offices, run our transportation systems, and operate ourfarms and factories. Even modest energy savings can produce new tax-freedisposable income for our families, new capital for our businesses, and loweroverhead to make our industries more competitive. But money is not the only reason we should be aware of energy. The productionand consumption of energy causes more environmental damage than any othersingle human activity. To give just one example; the energy that runs ourappliances, furnaces, water heaters, air conditioners, and lighting producestwo tons of carbon dioxide each year for every man, woman, and child in theUnited States. This October, the world stands at a crossroads in regard to energy use and, byextension, the quality of the environment and the quality of life. During this decade, the United States is expected to spend as much as $200billion on new electric generating capacity. Worldwide, nations will spend anestimated $1 trillion in the next few years to bring electric power to thebillions of people who don't have it. China and India--the world's two most populous nations--are among the manyunderdeveloped countries undertaking aggressive economic development programsand planning major investments in energy systems. The world investment in energy, with its attendant impact on environment, willkeep growing. Driven by continued population growth and economic development,global energy consumption is expected to increase as much as 50% by the year2010. The choices we make about where that energy comes from and how it is producedand consumed will affect the world environment for a long, long time to come. That's why DOE established this year's Energy Awareness Month theme as "Energy--Our Future Is Today." We are indeed deciding our future by the choices we maketoday. It's also why DOE is sponsoring a series of town hall meetings around thecountry this fall, preparing to write a National Energy Policy Plan forCongress with "sustainable energy" as its theme. Sustainable development--meeting the needs of the present without compromisingthe needs of future generations--is the key to a livable future, and energy isthe key to sustainable development. The United States must lead the world indeveloping and using energy efficiently, and in making the transition to clean,renewable resources like solar, wind, biomass and geothermal energy. As Energy Secretary Hazel O'Leary puts it, "There has never been a greateropportunity for the United States and other nations around the world toharmonize the needs of people with the protection of the environment. If wedon't produce and use energy correctly today, we risk damage to our environmenttomorrow...The discovery and use of sustainable energy resources advanceeconomic growth, energy security and environmental health at the same time." As we celebrate Energy Awareness Month, we all can renew our commitment tousing energy resources wisely for the generations that will follow. WT03-B20-128IA006-000057-B004-238http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/rfostage.html 138.80.61.12 19970221191628 text/html 1152HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:46:47 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 982Last-modified: Tue, 09 Jul 1996 02:26:03 GMT Fourth Stage of Release Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). Fourth Stage of Release. The last, the fourth stage began on 6 May and was characterized by abrupt decrease of the release rate because of the undertaken measures and of formation of more refractory compounds of radio nuclides in the process of interaction with the materials introduced into the core. The reason of such abrupt fall of the release power is still not clear, and noticeable releases of activity continued much later during the whole month. A picture and description of release after 6 May see in "New data on radioactive release". See also: Contents, RBMK-1000. WT03-B20-129IA006-000057-B004-302http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/rnucrel2.html 138.80.61.12 19970221191709 text/html 7801HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 18:47:28 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 7630Last-modified: Tue, 09 Jul 1996 02:26:03 GMT DYNAMIC AND COMPOSITION OF THE RADIOACTIVE RELEASE Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences.(Project "Polyn"). NEW DATA ON RADIOACTIVE RELEASE The well known picture of dynamics of radionuclide release at the Chernobyl accident is the basis for majority of model calculations. In this section the authors would like to present the data on radionuclide composition of the release from the reactor ruins during two months after the accident, which have not been analyzed earlier and considerably compliment and partially change the generally accepted picture of the release. The aerosol samples for gamma spectrometric analysis were obtained by means of filter cars towed by helicopters of air forces at flights directly over the reactor ruins. In the main, the sampling was made at the heights from 300 to 700 m, and the volume of the air pumped for the samples was several thousands of cubic meters. The data of gamma spectrometric analysis of the samples are presented in the form of charts of volume activity. Release of Nuclide. Measurements above reactor. The data were obtained systematically beginning from 27 April 1986 and till the end of June (one, more rarely several measurements per day), but they are insufficient for application of standard mathematical methods for estimation of significance. More fine time structure of release intensity than it is shown in the generally accepted model attracts attention. Another new result is the conclusion about the presence of high-power radionuclide releases outside the limits of ten days after 26 April, whereas the active stage terminates in ten days in the generally accepted model [1]. For instance, there is a peak in 20-21 days after the beginning of the accident, and a less pronounced peak in the region of 25-30 days. The estimation of significance of new data was performed in several ways. At first they were checked by comparing with the results of independent sufficiently reliable measurements of radionuclide activity in the air of Kiev and Vilnus. There is a considerable set of estimated data on measurements of iodine-131 concentration in the air of Kiev ( 140 km to the south from the Chernobyl NPP) during May-June 1986 [8]. If we match the data on iodine-131 concentration over the ruins in the same time scale, then one can see noticeable time correlation in activity peaks even without attraction of weather data and transport models. The analysis of the measurement data of Ru-103 and Cs-137 activity over the reactor and in the air of the near-ground layer in Vilnus [7] and in Berezino biosphere reserve [14] also showed time correlations with the shift of 1-2 days noticeable increase of activity on 16-20 May is seen in all charts. The book [14] indicates that "the measurements at large distances indicate the release of radioactive products (mainly of Cs-137) continuing up to 25 May 1986, which was obviously attributed to new local releases from the mostly heated zones of the reactor." Correlations and ratios of activities of characteristic radio nuclides in samples served from the very beginning as basic bench marks both for identification of the samples proper, and for reconstruction of the values measured more difficulty, for instance, of the levels of plutonium isotopes. It is interesting to consider correlations for cesium-134 and cesium-137 isotopes, which should vary in rather narrow limits, if the considered measurements were performed correctly. The range of the values of the ratio Cs134/Cs137 for fuel assemblies with different level of burn-up at the moment of the accident varied from 0.4 to 0.7 with the average 0.52-0.54. This picture presents these ratios obtained from the results of measurements of wair samples over the reactor, and the difference of the values is whithin - 15%. It is revealing that the ratio of Nb-95 and Zr-95 activities, which are rather close in physical and chemical properties, remains constant, at least within 30%. The ratio of activities of these radio nuclides with Ce-144 is also stable. The dynamic picture of the release of these relatively refractory radionuclides strongly connected with the fuel matrix is quite homogeneous, and in the whole repeats the picture of full release for volatile radio nuclides. As for the ratio of plutonium and Ce-144 activities in samples of air and soil, which was the basis for reconstruction of the amount of released plutonium and, consequently, or fuel, in the initial period on all samples it slightly differed from the average value equal to (9.4+0.8).10 . There are interesting data iodine form of release that was collected by Phys.-Chem. Institute above reactor and in Vilnus. Measurements in Vilnus. The measurements of air filters are represented in table placed below.  Data of measurements above the Chernobyl reactor.            (Phys.Chem. Institute) +---------------+--------------+----------+ | I131          | gaseous form |  aerosol |  |               +--------------+----------+ | Up to 8.05.86 |    0.30      |   0.70   | |               |              |          | | after 8.05.86 |    0.90      |   0.10   | | to 19.05.85   |              |          |  +---------------+--------------+----------+        Concentration of nuclides in air above reactor.                       May 1986.                 (Phys.Chem. Institute)                        bk/sq.m +-------+--------+-------+-------+-------+------+-----+ |Nuclide|     8  |   14  |  15   |  16   |  17  |  19 | Date(May) +-------+--------+-------+-------+-------+------+-----+ |Zr-95  |   525  |  500  |  0.9  | 1470  |  35  |  27 | |Nb-95  |   235  |  590  |  1.6  | 2610  |  50  |  44 | |Ru-103 |  3700  |  230  |  1.0  |  950  |  12  | 120 | |Ru-106 |   130  |   30  |  0.5  |  260  |   3  |  36 | |I-131  |  5030  |  160  |  1.5  |  350  |  17  | 136 | |Te-132 |   690  |   25  |   -   |   -   |   -  |  20 | |Cs-134 |    65  |   10  |  0.07 |   30  |  0.7 |   2 | |Cs-137 |   330  |   35  |  0.18 |   75  |  1.4 |   5 | |Ba-140 |   260  |  230  |  0.6  |  570  |  4.6 |   8 | |La-140 |    90  |  340  |  0.2  |  130  |  1.9 |   4 | |Ce-141 |   255  |  460  |  0.83 | 1200  | 18.5 |  23 | |Ce-144 |   150  |  170  |  0.65 | 1230  | 18.5 |  24 | +-------+--------+-------+-------+-------+------+-----+       See also: Contents, Core Inventory, History. WT03-B20-130IA005-000051-B018-488http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl/contents.html 138.80.61.12 19970221152202 text/html 3778HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:52:11 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 3607Last-modified: Tue, 09 Jul 1996 02:26:02 GMT  CONTENTS OF DATA BASE. Russian Research Center "Kurchatov Institute" Hypertext Data base: Chernobyl and its consequences .(Project "Polyn"). CONTENTS OF DATA BASE. ANNOUNCE: We are planing to fill all positions in placed below list during 1994-1995 year. There is a plan of this filling. But we are ready correct it in accordance with readers needs. Any suggestions, corrections, ideas, error checks are welcome by administration group Introduction Accident cases and it`s progress. Design and construction of Chernobyl Nuclear Plant Specific features of the design of the RBMK-1000 reactor ChNPP reactor safety system Modern viewpoint on ChNPP reactor safety system Chronology of Chernobyl Accident Initial data for Accident simulation Evaluation of Chernobyl Accident Cases knowledges Accident simulation Conclusions Activity of Central Government and Local Authorities during Chernobyl accident and liquidation of its consequences. State commission and operative control group of Communist Party of USSR. Evacuation of people from accident zone Accident source localization Environment monitoring Relocation and Slavutich town building Accident cases investigation Interaction with international organizations and accident consequences liquidation management Activity of Ministry of Internal Affairs. Fire extinguishing Evacuation of people and ownership defence Accident consequences liquidation Army activity. Fire and radiation protection actions Radiation protection of personal and civil people Information support of army activity Activity of Ministry of Energy. Accident consequences liquidation. Design and construction of "Sarcophagus"(Object "Shelter"). Supervision of destructed reactor unit. Estimation of core inventory Estimation of radioactivity remained in destroyed unit Design and construction of "Shelter" Destroyed block diagnostics Fuel distribution inside destroyed block Physical and chemisty conditions of fuel masses Estimation of Nuclear explosion possibilities Fuel-environment interaction Environment Monitoring. Environment monitoring inside 30-km zone State Hydrometeorology Committee and its activity Environment contamination of European part of USSR Forecast of water contamination Scientific problems of Chernobyl environment contamination Medical protection of people during Chernobyl Accident. Decontamination. Territory of ChNPP and Pripyat town Settlements Equipment. People Decontamination of roads and dustdemping Agriculture Food Activity and food production on contaminated territories Chernobyl Science Activity of International Organizations List of illustrations Literature Access to Data Bases is supported by Relcom (Reliable Communications Asociation). WT03-B20-131IA005-000051-B018-524http://lacebark.ntu.edu.au:80/j_mitroy/sid101/chernobyl2/chernobyl.html 138.80.61.12 19970221152243 text/html 7520HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:52:49 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 7349Last-modified: Tue, 09 Jul 1996 02:25:49 GMT THE RADIOLOGICAL AND HEALTH IMPACT OF CHERNOBYL TEN YEARS ON RADIOLOGICAL AND HEALTH IMPACT An Assessment by the NEA Committee on Radiation Protection and Public Health November 1995 OECD NUCLEAR ENERGY AGENCY FOREWORD Several years after the Three Mile Island accident, in the UnitedStates, the Chernobyl accident completely changed the public'sperception of nuclear risk. While the first accident providedthe impetus to develop new research programmes on nuclear safety,the second, with its human death toll and the dispersion of alarge part of the reactor core into the environment, raised alarge number of problems of "management" not only forthe treatment of severely exposed persons, but also for the decisionsthat had to be taken affecting the population. Clearly, the nationalauthorities were not ready to manage an accident whose consequenceswere not confined to their territory. The way the accident was managed and the lack of information provokeda feeling of distrust in the minds of the public that was reinforcedby the fact that radiation cannot be perceived by humans and alsothat it is easily detected even at a very low level. The prospectof contaminated food, aggravated by ambiguous, even contradictoryrecommendations by national authorities, gave rise to a varietyof reactions, and sometimes overreactions, in the management ofthe accident consequences in several European countries. In the country of the accident itself, where political, socialand economic conditions were worsening, the association of theSoviet regime with nuclear activities contributed to raise feelingsof mistrust towards the public authorities. Ten years later, many improvements in radiation protection andemergency preparedness have been made possible by the Chernobylexperience and we are also able to arrive at a more accurate assessmentof the impact of this accident. The fact remains that the futureconsequences in terms of health effects remain imprecise for simpletechnical reasons, and, because of this, lend themselves to acompetition between those who want to minimise the consequencesof the accident and those who wish to promote a catastrophic assessment. In these circumstances, having discussed in 1994 the questionof the future of radiation protection with the dawning of thenext millenium (NE94), the NEA Committee on Radiation Protectionand Public Health (CRPPH) wished to make an honest assessment,ten years on of the accident, on the state of the contaminatedterritories and the state of health of the populations and, onthis basis, to attempt an appreciation of the risks to be expectednot only for man but also for his environment. This review does not end there. The CRPPH also details the lessonsthat have been learned by Member countries and the internationalorganisations such as the ICRP, the IAEA, the EC, the WHO, etc.It has also organised international emergency exercises, the INEXProgramme. Information between States and the public has beenconsiderably expanded. The accident was followed by numerous assistance and researchprogrammes supported by international organisations and bilateralagreements. All these organisations are or will be publishingtheir results. This report differs from those in that it is asynthetic consensus view aimed at those persons who wish to knowthe salient points without having to go into the technical detailswhich one can find elsewhere. We thank all those organisations (UNSCEAR, FAO, WHO, EC) whichhave put information at our disposal so that this report couldbe as up to date as possible. However, those Agencies are stillgenerating a large amount of information to be submitted to theforthcoming international Conference "One Decade After Chernobyl"to be held in April 1996, some of which could not be made availablein time for incorporation into this report. The report was drafted by Dr. Peter Waight (Canada) under thedirection of an editing committee chaired by Dr. Henri Métivier(France). The members of the Editing Committee were: Dr. H. Métivier IPSN, FranceDr. P. Jacob GSF, GermanyDr. G. Souchkevitch WHO, GenevaMr. H. Brunner NAZ, SwitzerlandMr. C. Viktorsson SKI, SwedenDr. B. Bennett UNSCEAR, ViennaDr. R. Hance FAO/IAEA Division of Nuclear Techniques, ViennaMr. S. Kumazawa JAERI, JapanDr. S. Kusumi Institute of Radiation Epidemiology, JapanDr. A. Bouville National Cancer Institute, United StatesDr. J. Sinnaeve EC, BrusselsDr. O. Ilari OECD/NEA, ParisDr. E. Lazo OECD/NEA, Paris TABLE OF CONTENTS FOREWORD (above) The full text is also available in a single large file EXECUTIVE SUMMARY Chapter I. THE SITE AND ACCIDENT SEQUENCE The site The RBMK-1000 reactor Events leading to the accident The accident The graphite fire Chapter II. THE RELEASE, DISPERSION AND DEPOSITION OF RADIONUCLIDES The source term Atmospheric releases Chemical and physical forms Dispersion and deposition Within the former Soviet Union Outside the former Soviet Union Chapter III. REACTIONS OF NATIONAL AUTHORITIES Within the former Soviet Union Outside the former Soviet Union Chapter IV. DOSE ESTIMATES The liquidators The evacuees from the 30-km zone Doses to the thyroid gland Whole-body doses People living in the contaminated areas Doses to the thyroid gland Whole-body doses Populations outside the former Soviet Union Chapter V. HEALTH IMPACT Acute health effects Late health effects Thyroid cancer Other late health effects Other studies Psychological effects Within the former Soviet Union Outside the former Soviet Union Chapter VI. AGRICULTURAL AND ENVIRONMENTAL IMPACTS Agricultural impact Within the former Soviet Union Within Europe Environmental impact Forests Water bodies Chapter VII. POTENTIAL RESIDUAL RISKS The Sarcophagus Radioactive waste storage sites Chapter VIII. LESSONS LEARNED Operational aspects Scientific and technical aspects EXPLANATION OF TERMS LIST OF ACRONYMS REFERENCES WT03-B20-132IA005-000051-B019-84http://lacebark.ntu.edu.au:80/j_mitroy/sid101/renewable/RenewablePolicy.html 138.80.61.12 19970221152332 text/html 21336HTTP/1.0 200 OKDate: Fri, 21 Feb 1997 14:53:49 GMTServer: Apache/1.1.3Content-type: text/htmlContent-length: 21164Last-modified: Tue, 09 Jul 1996 02:25:43 GMT RENEWABLE ENERGY POLICY RENEWABLE ENERGY POLICY Renewables Savaged "We consider that it is very doubtful that the relatively modest increases in new electricity generation justify the large sums spent". So said the all-party House of Commons Public Accounts Committee in its report on the UK Renewable Energy Research, Development and Demonstration (RD&D) programme, on which £340m has been spent so far.The Committee were clearly unimpressed by the prospects for renewables - despite the Department of Trade and Industries stalwart defence. The DTI pointed out that given the support of the NFFO subsidy new renewables should provide 1,500 megawatts of capacity by 2000, this, on top of existing renewable capacity, amounts to a 5% contribution to UK electricity supply. That, in turn, would mean a saving of 3 million tonnes of carbon emission - useful in the fight against global warming. And by the year 2025 the contribution could rise to 20% of UK electricity; renewables thereby continuing to play their part in meeting the UK's committment to climate protection, as well as providing a valuable element of diversity of supply.The Public Accounts Committee was, however, unmoved: its main concern was the cost and short term 'value for money'. Following very much the same line as in the earlier National Audit Office report (see Renew 89), they were sharply critical of the fact that £54 million had been spent on wind power, but with the bulk of this being focussed on one large 3MW machine, and one vertical axis prototype, neither of which were now seen as relevant, since smaller conventional machines had proved to be the most commercially popular. Worse still, 84% of the machines now in operation in the UK were imported - and were getting subsidies, via the NFFO, from consumers.Similarly the Committee was unhappy that £40m was spent on a, now abandoned, hot dry rock geothermal test well. The Committee clearly had it in for renewables: they seemed convinced that they were irrelevant and seized on every problem as further proof. Thus the fact that around half of the projects so far supported by the NFFO had been delayed or halted by planning objections was seen as very significant - even though the DTI's Godfrey Bevan managed to point out during the hearings that the 200MW or so of capacity lost due to the withdrawal of projects supported by NFFO-1 and 2 would be 'offered again during future rounds of the NFFO' so that 'it is not a permanent impediment to the development of 1,500 megawatts'.The DTI is more usually involved in defending its renewable programme from those of us who would like to see much more being done. Facing the Public Accounts Committee it was, however, forced to almost play it down. Thus on wave power the DTI representative, Sir Peter Gregson, depicted it as now only 'a very modest programme' looking at shore line options, which was in any case being wound up, at a cost of only £0.21 million; with just the 'quite small' Islay device being involved. The DTI was hamstrung in its defence of renewables primarily because it couldn't argue the case for expansion effectively. As we have pointed out relentlessly (most recently in Renew 90), the real problem with the UK's programme so far was that it was too small and short term to develop all the options properly. The DTI has increasingly adopted a short term approach, focussing on the near-market options, i.e. those that could be commercially viable without the NFFO by 2005 (which seems to be the new target), at the expense of the generally larger scale longer term big resource (e.g. offshore wind). So inevitably the programme looks marginal.All the DTI would say, rather apologetically, was that the RD & D investment so far allocated ought to pay off in the future, but that it was cutting back on RD & D in future. Not a very robust defence .... But given the hostile tone of the Public Accounts Committee you can see why Energy Paper 62 had announced a 20% RD & D cut back and was desperately trying to shift everything onto a commercial basis, with just the NFFO, and hopefully European money, to keep it going. What is perhaps a little strange is that the Public Accounts Committee is an all-party Committee. One might expect hostility to renewables from Tory backbenchers concerned about 'wasted millions', but surely the opposition party members could have introduced more vision into the review. All we got was negative short termism, coupled with just a slight dig at the comparative level of nuclear spending, and at the location of ETSU within the Atomic Energy Authority, plus some resentment at the UK's failure to attract sufficient EC funds. The absence of any longer term strategic thinking does not bode very well for the future, if and when a Labour administration comes to power...The Renewable Energy Research, Development and Demonstration Programme', Committee of Public Accounts. Session 1993-94 Forty-Second Report (11 July) is available from HMSO for £8.95. AIEP Response The Public Accounts Committee report received quite a lot of press coverage and in a press release produced in response, the Association of Independent Electricity Producers (AIEP) saw the report as 'confusing and misinterpreted' in that it focussed on Government R & D spending from 1975 to 1993, but took less account of what had happened since. In the earlier phase, evidently seen as the 'bad old days' by the AIEP, publicly funded R & D dominated, and although a risk of failure was inevitable and to be expected for all R & D, the AIEP felt that 'Governments are not good at picking winners'. So the Committee was 'absolutely right to question the way that money was spent in the past'.However according to the AIEP things have now changed, and a more commercial approach had been adopted with a small part of the NFFO levy being used to subsidise projects 'which not only work, but are the product of billions of pounds of private investment'. True some projects had not gone ahead. But that was mainly due to planning problems and the fact that the NFFO arrangements were 'complicated and had at times been unpredictable'. Nevertheless, overall privatisation had been beneficial and competition was now identifying viable projects and forcing prices down. So, as far as the AIEP is concerned, the DTI had now got it right ...But if further R & D was to be supported the AIEP felt that its members should advise the DTI on how 'to focus the use of its resources'.The AIEP put the same line on the horrors of direct Government subsidies versus the merits of market enablement strategies, in its evidence to the Governments Nuclear Review. It said it ' regards a process of support which eneables projects to be developed and operated in a demonstration market environment as offering better value for money than direct Government funding of research and devlopemnt of particular technologies.'This seems a little shorsighted. Surely the AIEP will want to make use of more advanced renewable systems in future- and it seems unlikely that the private sector will fund the necessary R&D. It may be, as the DTI argued in Energy Paper 62, that BIG STIG biomass fired gas turbines and the like are now just about ready for 'demonstration' , but they, and certainly the next wave of renewable technologies like wavepower and tidal stream systems, are still some way from the market. NATTA produced its own response to the Public Accounts Committee report taking a longer term view : see Renew 92 for the full text. Technology Foresight As the Public Accounts Committee noted the UK Government has not so far been conspicuously successful at 'picking technological winners'. However, as we noted in Renew 87, a new attempt to improve on the process of picking winners was initiated in 1993, when the Governments Office of Science and Technology announced that it was going to introduce a new `Technology Foresight' scheme, designed to look 20 years ahead in selected fields, so as to provide guidance on future R & D priorities. Fifteen Sector Panels were set up bringing together a wide range of expertise from the civil service, industry and academia. In addition to Godfrey Bevan from the DTI, the Energy panel includes several people known for their support of renewables such as Dr Mary Archer from the National Energy Foundation and the Solar Energy Association, Prof Bob Hill from Northumbria Universities Photovoltic Solar Centre, and Dr Gordon McKerron from the Science Policy Research Unit at the University of Sussex. The next stage in the process is the production preliminary conclusions, for discussion, early next year. DTI cuts Renewables by 73% The Department of Trade and Industry has announced some significant changes in the way official figures for energy resources and usage are to be presented in future, e.g. in the annual 'Energy Trends' and statistical digest booklets. The result is that the renewable contribution will be significantly undervalued.These statistical charges may sound trivial at first glance: following a consultation exercise the UK will now use 'million tonnes of oil equivalent' (mtoe) in line with much European practice, instead of 'kilowatt hours' (kWh) or 'gigajoules' or therms. That's a little annoying since 'kWh's are more familiar to most people - being the unit electricity (and now gas) is sold by. But from now on its 'toe's' (=11,630kWh) and its multiples.Much more significantly however, electricity use figures will be presented in 'energy supplied' terms, rather than in 'substitution' terms. Under the UK's old substitution approach, the figures for electricity supplied, whatever the fuel actually used, were converted to the equivalent fossil fuel energy imput,in reality mostly coal, that would be required to generate it, via a conventional thermal plant.That gave a useful picture of the 'primary' input fuel use - reminding us that thermal conversion systems had huge losses, up to 70% or more, The familiar unit was 'million tonnes of coal equivalent'. Even though nearly 70% of our electricity still comes from coal it is understandable politically why the DTI has shifted to 'million tonnes of oil equivalent'. But the shift to presenting it on a energy supplied basis, i.e. in terms of the energy content of the electricity produced, rather than primary energy input on a substitution basis, has major repercussions.Firstly, for both schemes, a standard 'energy conversion' efficiency factor has to be assumed: in line with international practice 36% was selected for the new 'energy supplied' scheme (this in fact being the figure for the UK's nuclear plants) instead of 34%, the figure used in the UK's old substitution approach (this being the figure for fossil fuel plants). The result is that the new statistics will show the nuclear contribution as 5% higher.At the same time, and here is where it really starts to have an impact, under the new scheme, the contribution of hydro and windpower is reduced by 73% from 1.4 mtoe on a substitution basis to 0.4 mtoe on an energy supplied basis. Other changes at the DTI There has also been a minor organisational reshuffle within the Department of Trade and Industry.While Tim Eggar MP retains overall ministerial responsibility for energy, Ian Taylor MP, Parliamentary Under-Secretary of State for Trade and Technology, now has responsibility for, amongst other things, 'Environmental and Energy Technologies', including renewables. It is unclear whether this amounts to a demotion of renewables. But certainly the Treasury seems keen on cutbacks. According to the Guardian (1/8/94) it wants 'all state R&D spending on non-nuclear energy technologies, including coal, renewables and North Sea oil' axed. As far as renewables go, Energy Paper 62's 20% cut would thus just be a start. That's not a misprint - it's 73% less. A fine example of shifting the goal posts! Basically the 73% or so of waste heat produced by fossil and nuclear plants has been cleverly disguised so that renewables look much worse by comparison, 73% worse! Fortunately the UK statistics will also be presented in substitution terms for references, since it was argued, by objectors during the consultation process, that this conveyed valuable information about the amount of fossil fuel use replaced by nuclear, hydro and other renewables. And it's only for primary input energy - not end use or delivered energy. But the new primary 'energy supplied' figures are likely to be widely used in policy debates - thus, in effect, seriously under representing the renewable contribution. The new system will bring the UK into line with European practice, but it is rather strange that what we seem to be seeing is a shift from a measure which gives an indication of the degree of success we are having in substituting renewables for fossil and nuclear fuel, to a system which is only concerned with the saleable end product, i.e. electricity, regardless of where it came from. Market forces rule OK? New DTI Energy Overview Wave, Tidal and even Offshore wind are once again savaged in a new report from the Governments Department of Trade and Industry.Hot on the heals of the DTI's Energy Paper 62 and ETSU's R82 on Renewable Energy (see Renew 90), the Department of Trade and Industry published Energy Paper 61 'Energy Technologies for the UK'.As EP62 and R82 made clear, they were the renewable energy part of a larger study of all the UK's energy options. EP61 presents the conclusions, with much of the detail being in a back up ETSU report R83.It's an ambitious project, looking at future prospects to 2025, environmental impacts R&D options (or rather RDD&D - Research Development Demonstration and Dissemination) all based on a range of energy supply/demand scenarios including a 'heightened environmental concern' scenario.However EP61 notes that, since energy conservation was now the responsibility of the Department of Environment 'it would not be appropriate for the demand side to be included' in the RDD&D appraisals.EP61 classifies the technologies under four main headings - robust, vulnerable, fragile and unpromising, depending on how well they do in the various scenarios. Unsurprisingly, given the EP62/R82 reviews, tidal barrages, offshore wave, geothermal aquifers and hot dry rocks come under the last category, i.e. unpromising.But, more surprisingly, so does offshore wind.Hydrogen production by electrolysis , hydrogen use in internal combustion engines, and fuel cells for large scale generation are also dumped in this category - which also contains nuclear fusion! All of which offer 'no contribution on any scenario/discount rate' except perhaps under an environmentally constrained future. The robust technologies, include, of course, gas cooled nuclear reactors and PWR's, combined cycle gas turbines, CHP, pumped storage and hydro - along with passive solar design! They allegedly offer a potential contribution under all scenarios, and at all discount rates. The vulnerable technologies include wind power and municipal solid waste in the short term, hydrogen from biomass and photovoltaics in the medium term, these possibly offer a potential in three or more scenario, regardless of discount rate. The fragile technologies, which offer contributions under only one scenario, regardless of discount rate, include landfill gas, agricultural waste as fuel and shore line wave power in the short term, energy crops in the medium term and biodiesel in the long term. EP61 then looks at the implications for RDD&D on the basis of a 'risk versus payback' assessment. It reports that the bulk (55%) of the 1992/3 RDD&D expenditure (both public and private) was in the high risk/low payback zone and comments that 'plans have already been announced to curtail some of these programmes' , quoting fast reactors, tidal and geothermal HDR as examples. These they say accounted for 75% of the high risk/low payback expenditure in 1992/93. Offshore wave was also in the same category. As noted earlier shoreline wave just about scrapes in, and wind, passive solar, land fill gas and photovoltaics are seen as reasonable bets. But of the renewables, only hydro, agricultural wastes and energy crops get the top billing in risk/benefit terms - along strangely with active solar, although it is indicated that the likely 'limited take up' of some of the renewables meant that the scale of expenditure and hence the risk was low. EP61 costs of £15.92 from HMSO. The back up ETSU report R83 comes in 7 volumes. Volume 1 is on fossil fuel technologies, Vol 2 on nuclear, Vol 4 on transmission, including hydrogen, Vols 5, 6 and 7 cover Energy Efficiency in (respectively) buildings, industry and transport and Vol 8 is on methodology. Vol 3 is actually R82, on renewables. Planning Renewables The UK's planning system is gradually adapting to renewables - the rather open ended 'Planning Policy Guideline 22' has been added to by various ministerial statememnts and adjustments, designed to help planners in the difficult process of balancing local and national/global environmental concerns. Energy Minister Tim Eggar has already indicated that holding an NFFO contract did not over-rule local planning objections. Next, it was decided that wind farm proposals could henceforth be required to submit detailed, formal environmental impact assessment, if they feel it could have a significant effect on an Area of Outstanding Natural Beauty, National Park, SSI, Heritage Coast, or if the wind farm consisted of more than 10 turbines, or had a total capacity of more than 5 Mega watts. In parallel, local planners are increasingly now able to draw on more detailed assessments of local and regional renewable resources.In the past it was often difficult for planners faced with a specific proposal for a renewable project, to assess its worth in relation to other possible projects in the area. Now around 50% of local councils in England and Wales have developed or are in the process of developing renewable energy policies in their development plans, which usually include a detailed assessment of the local renewable resource. The pioneering regional studies, like those carried out by SWEB and NORWEB, have been followed by more detailed local studies.Lancashire County Council has been a pioneer in this respect: it has identified zones suited to wind farm deployment, covering 9% of its area, with AONB's etc. being protected. In addition, as we noted in Renew 90, there have been planning studies for the counties of Devon and Durham and studies on Cornwall, Cumbria and Gwynedd will be published shortly. A Regional Planning Study covering the Berks, Oxfordshire, Hampshire, Wilts, Dorset and Isle of Wight area has been initiated, along with a South Wales study looking in detail at landscape issues. Studies by the East Midlands, South East Counties, Yorkshire and South Pennines are also planned. The Devon study is being followed up by a detailed local study of the renewable potential of the green around Hatherleigh. Some of these developments were discussed at a one day seminar back in May, held in Newcastle upon Tyne, and organised for the Northern Branch of the Royal Town Planning Institute : local contact Angela Hull at Newcastle University. Diversity? Renewables have it Investments in renewable electricity supply options offers a significantly more efficient means to foster system diversity than continued support for nuclear power'. So said Andrew Stirling from SPRU in Energy Policy (March 1994).His analysis of diversity is very timely: so far it has been used to justify a significant nuclear contribution - nuclear being seen as the main non-fossil option. Clearly the UK's traditional fuel mix, heavily biased towards coal, is already on the way out, with gas taking a large share of electricity supply;, for example Energy Paper 59 saw gas rising to a 57% share while coal falls back to 27% by 2020 (see Renew 81). But EP59 also saw the nuclear contribution falling to 1% - i.e. essentially just Sizewell B - at least on a 'hands off' business as usual free market scenario, with renewables expanding to 3%, 4% if hydro was included. Stirling goes further, and suggests that optimal diversity, providing a robust flexible balance of sources, would involve a much larger renewable contribution - 17-18% from firm renewables, 13-14% from intermittent renewables, with nuclear charitably, given between a 3 and 14% stake. It's a complex analysis, we've not done justice to it in this brief account: but the implications are clear, even leaving aside issues like strategic flexibility, environmental impact and public acceptability, renewables look very good at providing a diverse range of inputs. WT03-B20-133IA018-000193-B025-24http://www.rssl.co.uk:80/ 194.159.251.194 19970106150501 text/html 1104HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:05:03 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 933Last-modified: Wed, 12 Jun 1996 15:01:59 GMT Reading Scientific Services Ltd Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Please click on logo to continue, orText only viewers click here © In Press PR Ltd1996 WT03-B20-134IA018-000193-B025-36http://www.rssl.co.uk:80/index2.html 194.159.251.194 19970106150513 text/html 2484HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:05:12 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 2312Last-modified: Wed, 12 Jun 1996 15:01:59 GMT RSSL:Index Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Contents Introduction RSSL is the UK's leading independent provider of research, analysis and consultancy services to the worldwide food, drink, consumer goods, healthcare and chemical industries. RSSL's laboratories are amongst the most modern inEurope and RSSL contributes to the wider industryknowledge through training courses, newsletters and press releases. Choose from the contents list or call FREE on 0800 243482 for more information. The company Services Training Laboratories Literature / Brochures Contract, RSSL's Newsletter Heads of Department Job Opportunities with RSSL Current Issues Enquiry form Press Information © In Press PR Ltd1996 WT03-B20-135IA018-000193-B025-48http://www.rssl.co.uk:80/company.html 194.159.251.194 19970106150525 text/html 2438HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:05:27 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 2266Last-modified: Mon, 20 May 1996 07:45:04 GMT RSSL:Company Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Company profile RSSL is the UK's leading independent provider of research, analysis and consultancy services to the worldwide food, drink, consumer goods, healthcare and chemical industries. The company was formed in 1987 and its services are based on the expertise of over 100 staff, many of whom have international reputations. Our laboratories are amongst the most modern in Europe and RSSL continually invests in the latest instrumentation and methodology. We are also constantly searching for talented scientists to strengthen our team. RSSL's multidisciplinary laboratories offer a range of techniques and services and the quality of our work is assured by externally accredited quality systems such as NAMAS and GLP. All our work is undertaken confidentially, and our client base of over 2000 companies, includes many well known internationals. However, RSSL still manages to contribute to the wider industry knowledge through training courses, newsletters and press releases. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-136IA018-000193-B025-66http://www.rssl.co.uk:80/services.html 194.159.251.194 19970106150533 text/html 2985HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:05:37 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 2813Last-modified: Mon, 20 May 1996 07:44:57 GMT RSSL:Services Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Services RSSL's multidisciplinary laboratories provides a wide range of expert services to the food, drink, fmcg, healthcare and chemical industries. We now have NAMAS accreditation for most of the techniques and methods used. Further information on any of these services are featured in brochures available from RSSL. The following areas are also covered on this web site. Research Analysis Consultancy Training Emergency Response Service Food Intelligence Service Select QA - Supplier Quality Assurance Service Services to the Healthcare industries Microbiology Authenticity Sensory Evaluation and Consumer Research Taints and Off-flavours Foreign Body Analysis Pesticides Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-137IA018-000193-B025-87http://www.rssl.co.uk:80/training.html 194.159.251.194 19970106150542 text/html 2201HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:05:44 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 2029Last-modified: Mon, 20 May 1996 07:45:06 GMT RSSL:Training Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Training RSSL is very active in providing training courses for the food industry. The company has secured an enviable reputation for its HACCP training. Courses are run regularly at RSSL's headquarters and on-site at clients' premises. The courses have been designed and are lead by Carol Wallace, a well known authority on HACCP and co-author of "HACCP - A Practical Approach." Food Industry Training Reading (FITR) combines the expertise of RSSL and the food Science and Technology department of the University of Reading in the provision of short courses, trainig and seminars for food industry professionals. More information and course details can be found on FITR's own web site. For more information on courses run by Food Industry Training Reading, please call the Training Centre Manager, Karen Masters on 01189 318217, or email : FITR@afnovell.reading.ac.uk.. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-138IA018-000193-B025-116http://www.rssl.co.uk:80/literature.html 194.159.251.194 19970106150603 text/html 2158HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:06:07 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1986Last-modified: Mon, 20 May 1996 07:44:59 GMT RSSL:Literature Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Literature and Brochures Literature is available on the following subjects. Please fill in the enquiry form to receive copies of any of these. Research The brewing & beverage industry The healthcare industry The chemical industry Oils and fats industry Analysis Microbiology Composition / nutrition / product labelling Pesticide residues Fruit juice authenticity Sensory evaluation Environmental Foreign bodies Taints and off-flavours Mycotoxins Microscopy Particle characterisation Rheology Consumer products Spectroscopy Consultancy Emergency Response Service Product development HACCP consultancy Consumer & Market Research Information Services Training Select QA - Supplier Quality Assurance enquiry form Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-139IA018-000193-B025-133http://www.rssl.co.uk:80/contract.html 194.159.251.194 19970106150610 text/html 23069HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:06:14 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 22896Last-modified: Mon, 20 May 1996 07:45:03 GMT Contract:The RSSL Newsletter Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Contract:The RSSL Newsletter Issue 14 Contract is the RSSL newsletter. For ease of reading the ' electronic ' version of this has been produced as a plain text document. For a hard copy, please email Nicola.E.Shearman@rssl.sprint.com, or telephone us on +44 118 986 8541. Contents Introduction The Food Intelligence Service - Access to Food Law Fruit Juice Adulteration : New Detection Methods Announced Visiting Worker : Nicholas Low The Portuguese Connection Taints and Off Flavours Preservative Efficacy Testing Guide to Taints and Off Flavours Emergency Response Service Update on Select QA New Video Release : HACCP Healthcare Club Meeting Good Laboratory Practice FIS Briefing Notes Meetings Diary Introduction RSSL provides research analysis and consultancy for the European food, consumer goods, healthcare and chemical industries. Our business is based on scientific excellence and confidentiality, accredited quality systems and first class customer service. This newsletter is designed to highlight some of the services that we offer. If you want to discuss any aspect of the services available from RSSL please contact our Customer Service Desk. Phone free on 0800 243482. The Food Intelligence Service - Access to Food Law To expand the range of information services that RSSL is able to offer customers, the Food Intelligence Service has joined forces with food lawyers at Ford & Warren, to provide a legal advice centre. This move recognises the need for companies to have access to expert legal interpretation of legislation and will help customers to clarify points of food law. The service aims to demystify some of the legal jargon and enable people to understand what the law really means for their businesses. The legal information services includes: six monthly FREE Review of EU Food Legislative Issues access to legal databases in-house documentation centre staff trained in food law links with expert food lawyers The service will be available to all companies with no membership or subscription fees and is organised through RSSL on the freephone number 0800 243482. Fruit Juice Adulteration : New Detection Methods Announced As one of the leading centres in the world for the authentication of fruit juice and fruit products, RSSL is continually examining new methods and procedures which could be used to detect adulteration. As part of this investigation RSSL recently invited Professor Nick Low from the University of Saskatchewan in Canada, to spend part of his sabbatical year at its Research Centre in Reading. To coincide with his stay, 55 guests from the food industry attended a special seminar to review new areas of interest in the detection of juice adulteration. Over the last 15 years there have been ongoing problems associated with both the misrepresentation and adulteration of juices. Dr Roger Evans, RSSL's Science Director discussed the ways in which juices are routinely adulterated by the addition of sugar, other cheaper juices, pulpwash, second extracts of fruit (particularly in the case of citrus juices) and misrepresented as being natural juice when in fact they have been derived from concentrates. Recent work by Professor Low at RSSL on oligosaccharide fingerprinting has enabled the detection of a new form of adulteration i.e. the addition of high fructose syrups derived from inulin. The technique used by Professor Low is a highly sensitive capillary gas chromatography procedure. Dr David Hammond described the use of a phenolic marker called phlorin to detect the addition of peel extract and pulpwash and the use of HPAEC-PAD chromatography to isolate oligogalacturonide fragments from adulterated juices. The final speaker of the day, Dr Andrew Lea, showed how phenolic marker compounds like phloridzin and arbutin could pinpoint adulteration of citrus, soft fruit juices and pur�es with cheaper apple and pear juices and how the use of carotenoid screening could be used to characterise particular citrus juices. For a profile of Professor Nick Low see the following story in this issue of Contract. For further information about any authenticity issues contact Dr David Hammond at RSSL or call the Customer Service Desk freefone on 0800 243482. Visiting Worker : Nicholas Low Adulteration of foods is not a new problem. In the early 1800s unscrupulous tea traders would treat used tea leaves with black lead to restore the colour in order to resell the leaves as 'fresh'. Similar situations occurred with other foods which were of poor quality or spoiled. In recent years, adulteration of foods rich in carbohydrate, such as citrus and apple juices, honey and maple syrup, has become a significant problem for the food industry. Of particular importance to reputable food producers and processors is the undeclared addition of inexpensive sweeteners / syrups to high carbohydrate foods. Considering that the price of frozen concentrated orange juice is around $3.08/kg solids and that of sugar syrups is $0.75/kg solids, it is easy to see the economic attraction for adulteration. Low cost sweeteners include sucrose and invert sugar and syrups with varying levels of glucose, fructose and sucrose derived from beet or cane sugar by acid or enzymatic hydrolysis. High fructose syrups, manufactured from raw materials like Jerusalem artichoke, chicory, corn, potato, cassava or palm can also be used. Acid or enzymatic hydrolysis of starch (a glucose polymer) or inulin (a fructose polymer) is sometimes involved. Carbohydrates comprising two to ten monomeric units are called oligosaccharides. These can arise by the transglycosylation action of hydrolase enzymes in the food itself, or by the use of hydrolase enzyme in sweetener / syrup production. For instance, glucosidase is present in the honey sac of bees; amylase / glucoamylase is used in high fructose syrup production. Oligosaccharides can also form chemically by reversion reactions from the action of acid on sucrose, fructose and glucose. Therefore commercial conditions used to produce total invert sugar can result in the formation of oligosaccharides. It is the differences between the natural oligosaccharide patterns or 'fingerprints' of the pure foods and the inexpensive sweeteners/syrups which can allow detection of the fraudulent addition of such materials. Oligosaccharide analysis of foods is accomplished by high performance anion exchange liquid chromatography (HPAE), capillary gas chromatography (CGC) or capillary zone electrophoresis (CZE). Although these oligosaccharides make up only a fraction (0.1 - 0.001%) of the total carbohydrate content of foods they can be 'seen' by pulsed amperometric / flame ionization detectors or by UV/fluorescence. Nick Low was born in the UK and moved to Canada at the age of eight. He has a Chemistry Degree from the Simon Fraser University, an MSc in Nucleic Acid Chemistry from the University of Alberta and a PhD in Food Chemistry, also from Alberta. In 1987 he joined the Department of Applied Microbiology and Food Science of the University of Saskatchewan, and was appointed Professor of Food Chemistry in 1994. His career has encompassed a wide area of research including food authenticity, enzymatic synthesis of complex carbohydrates, chlorophyll and its relationship to edible oil stability, nutraceuticals and the instrumental analysis of foods. In 1990, Professor Low was invited to spend nine months at the European Community Joint Research Centre in Ispra, Italy, training European scientists in the use of oligosaccharide fingerprinting for fruit juice authentication and he is currently on sabbatical leave at RSSL where he has been investigating the adulteration of fruit juices with high fructose syrups derived from inulin. The Portuguese Connection RSSL regularly provides work experience for students as part of HND/MSc degree courses. Of particular interest is our link with the Escola Superior de Biotechnologia which is part of the University of Catolica in Porto, Portugal. We have taken students on placement as part of their course in Chemical Analysis and some of them have returned as permanent members of staff on completion of their courses. Our photo shows Susana Guedes who originally joined on a six month placement in RSSL's Product Chemistry section and Patrica Correia who originally carried out a six-month placement in Beverage Research and is currently working on a MAFF project. Rodolfo Rosa and Joana Azvedo are currently in the middle of six-month placements in Pharmaceutical Analysis and Lipid Chemistry, respectively. Taints and Off Flavours The sources of taints and off-flavours in foodstuffs are manifold and the range and type of products affected equally varied. During 1995, the Flavour and Trace Analysis Laboratory at RSSL investigated over 100 individual cases of taint in foods as diverse as dairy products, confectionery, snack foods, baked goods, distilled beverages and soft drinks. A prominent feature of the casework during this period was the high incidence of taints in dairy products. Many of the cases involved contamination of raw and processed milks with a disinfectant-like taint. The taint was invariably caused by the presence of either chlorophenols or chlorocresol. However, the source of these tainting species differed in each case. In one example a disinfectant like taint in milk from a large dairy was found to be caused by the presence of low levels of 6-orthochlorocresol, an extremely powerful taint with a taste threshold of 0.05 ppb. The taint was found not to originate in the dairy but was traced to milk from a specific farm and thence to a particular batch of animal feed used on the farm. Another example of taint in a dairy product was a natural style yoghurt that developed an objectionable goaty / sweaty and farmyard / faecal odour. The presence of skatole, indole and paracresol in the yoghurt was found to be responsible for the faecal / farmyard component of the taint whereas elevated levels of branched chain volatile fatty acids was responsible for the goaty / sweaty odour. On-going microbiological activity in the yoghurt was the source of the tainting chemicals. A different dairy product, cheddar cheese, was found to possess a strong petroleum / kerosene like aroma and taste. Flavour analysis failed to identify any of the components typically found in commercial petrochemical fractions. However, careful analysis for volatile hydrocarbons identified cis- and trans-1,3-pentadiene in the cheese. Trans-1,3-pentadiene has a bp of 42% and possesses a distinct paraffin / kerosene aroma and taste. It was generated in the cheese via degradation of potassium sorbate. For more information please call the RSSL Customer Service Desk on 0800 243482. Preservative Efficacy Testing If a pharmaceutical preparation or healthcare product does not itself have adequate antimicrobial activity, preservatives may be added to prevent proliferation or to limit microbiological contamination. Unpreserved aqueous preparations and multidose containers are particularly susceptible to microbial spoilage and could present a health hazard to a patient or consumer. In order to reduce the risk of both spoilage and microbial hazards, preservatives are added to a wide variety of preparations from nasal sprays to toothpaste. It is important to assess the effectiveness of the preservative(s) in production batches of the preparations or products. RSSL's Microbiology laboratory carries out Preservative Efficacy Testing to both the British and European Pharmacopoeia methods. The Preservative Efficacy Test involves challenging the preparation or product with a prescribed inoculum of micro-organisms (105 to 106 per g or per ml), storing the product at a prescribed temperature, withdrawing samples at specified time intervals and counting the micro-organisms in the withdrawn samples. The prescribed organisms include the fungus Aspergillus niger, the yeast Candida albicans and the bacteria Pseudomonas aeruginosa and Staphylococcus aureus. In addition, Escherichia coli is included for all oral preparations and the osmotolerant yeast Zygosaccharomyces rouxii for those oral preparations containing a high concentration of sugar. Some of our clients also include factory spoilage isolates in addition to the prescribed organisms. The criteria for the evaluation of antimicrobial activity are given in the terms of log reduction in the number of micro-organisms from the original inoculum level and vary for the type of preparation under test. The test result is given as a pass or fail. RSSL will shortly be applying to NAMAS for an extension to its accreditation scope to include both Preservative Efficacy and Microbial Limit Tests. This work can also be carried out according to GLP with raw data being archived with the client. Guide toTaints and Off Flavours A selection of tainting compounds identified at RSSL Food contaminated Compounds involved Origin Cakes Styrene Glass fibre resin Carbonated beverages in PET bottles Hydrocarbons Storage in proximity to fuel and solvent spills Cereals Indoles, skatole Microbiological Cheese Chlorophenols Proprietary cleaner Chocolates Aliphatic hydrocarbons Printed cartonboard Chocolate crumb Dichlorophenols Walls of metal container Chocolate eggs Xylenols Factory flooring Cola Orthocresol Can lacquer Custard Guaiacol Microbiological Desiccated coconut Chloroanisoles Paper sacks Distilled spirit Methyl isoborneol Process water Emulsifying agent Chloroanisoles Packaging Fruit juice Guaiacol Microbiological Ice cream 2-Butoxyethanol Printed cartonboard Jam filling 6-Orthochlorocresol Contaminated tanker Milk Alkoxy alcohols Paperboard Milk Paracresol Microbiological Orange juice Ethyl acetate, ethanol Microbiological Peanut butter Methacrylic acid Sealing foil Spring water Chloroanisoles Cardboard boxes used to store empty bottles Spring water 2-Ethylhexanol Liner of closure For further information about taints and off-flavours issues please contact Dr Brian Baigre at RSSL on 01734 868541 or call the Customer Service Desk freefone on 0800 243482. Emergency Response Service 1995 saw an unprecedented rise in ERS cases A record breaking 116 companies tasked the service 319 incidents were handled by RSSL - a 95% increase on the 1994 figures 62% of incidents involved foods, 25% beverages and 10% pharmaceuticals or cosmetics For more information about RSSL's Emergency Response Service ring Ray Gibson on 01734 868541 or the Customer Service Desk freefone on 0800 243482. Update on Select QA Interest in Select QA, RSSL's Supplier Quality Assurance Scheme has been growing steadily since its launch last year. Spurred on by the requirement of retailers for their suppliers to register with Third Party Quality Assurance Schemes, enquiries have been received from a wide range of manufacturers and suppliers of foods, beverages, fresh produce and non-food products. Supported by Lead Assessor Trained Auditors from a variety of backgrounds, Select QA provides a competitively priced package of SQA services which can be tailored to your commercial needs. Please contact the RSSL Customer Service Desk on 0800 243482 for a brochure and free copy of the Select QA Guide to Standards of Operation. Please state your requirement for the Food or Non-foods Standard. New Video Release : HACCP This helpful new video unravels the mystique which has historically surrounded the HACCP System. It is aimed at managers and shop floor workers who need a general understanding of the HACCP philosophy and how it relates to their role in the food manufacturing process. Call our Customer Service Desk freefone on 0800 243482 for further information. HACCP - How the System Works is a joint project between RSSL and Shield Video Training Ltd. Healthcare Club Meeting AUDITING IN THE HEALTHCARE INDUSTRY Friday 19 April 1996 at RSSL 09.00 Registration and coffee 09.30 Introduction 09.45 Introduction to BS EN ISO 9001 and pharmaceutical C.O.P. 10.15 Lead Assessor training 10.45 Third party supplier quality assurance 11.15 Coffee 11.45 Debate on the development of third party auditing for the healthcare industry 13.00 Questions 13.15 Lunch & Tour of RSSL 14.00 Workshop syndicate groups 15.00 Feedback session 15.45 Closing discussions and tea More information from Dr Marek Walach at the Customer Service Desk freefone 0800 243482. Good Laboratory Practice GLP was originally introduced to ensure the quality and integrity of safety data generated in non-clinical laboratory studies in selected industry sectors. The scope has since been extended to cover a broad range of scientific work from classical toxicology and pharmaceuticals to environmental studies and the generation of safety data for any 'new chemical substance'. RSSL has an extremely wide range of test methods accredited by the United Kingdom Accreditation Service through NAMAS, and although some of these test methods relate directly to the Pharmaceutical / Healthcare industry, often clients require that work is carried out to GLP standards. RSSL has extended its Quality System to incorporate the additional procedures that are specific to GLP (e.g. Study Plan, Study Director, Master Schedule). These procedures are used most frequently by RSSL's Pharmaceutical laboratory. However, other laboratories are also proficient in their use, thus ensuring that any work required to this standard is fully GLP compliant. FIS Briefing Notes Briefing Notes available free of charge on: Peanut Allergy National Study on Ready to Eat Meat Products: HACCP and Microbiological Quality EU ban on hormones in meat For copies call the RSSL Customer Service Desk on 0800 243482. Meetings Diary Microscopy Applications Group Meeting Full day meeting at RSSL on 1 May 1996 Jill Webb of RSSL's microscopy laboratory will be giving a talk on: "Preparation Techniques" Current Strategies in Food Microscopy Full day meeting of the Oxford & Reading Microscopy Group organised by Jill Webb of RSSL's Microscopy Laboratory on 15 May 1996, Palmer Building, University of Reading. Hilary Holgate of RSSL will be giving a talk on:"The Role of Microscopy in Foreign Body Analysis" Juiceworld 2000: IFU Congress 96Interlaken, Switzerland, 20 - 24 May 1996 Dr Andrew Lea and Dr David Hammond from RSSL's Beverage Research and Authenticity Group will be attending this major international meeting and presenting posters entitled "Detection of oligosaccharide patterns in fruit juices by gas chromatography", and "Detection of the addition of pulpwash to orange juice using two new HPLC procedures". They will also be manning an exhibition stand at the Congress. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-140IA018-000193-B025-153http://www.rssl.co.uk:80/heads.html 194.159.251.194 19970106150620 text/html 8724HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:06:23 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 8552Last-modified: Mon, 20 May 1996 07:44:59 GMT RSSL:Heads Of Department Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Heads of Department Prof Colin Gutteridge Prof Colin Gutteridge is Managing Director of RSSL, and has overseen and initiated much of the expansion in RSSL's services in recent years. Under his management RSSL has introduced new services such as the Food Intelligence Service, HACCP consultancy, and Food Industry Training Reading. It has also built on the strengths of the Emergency Response Service, and services in authenticity, taints and off-flavours, microbiology, pesticide analysis, sensory evaluation, consumer research and microscopy. He has also directed the expansion of RSSL's services to the healthcare, and chemical industries. Dr Ray Gibson Dr Ray Gibson is Business Development Director at RSSL. His wide experience of managing projects for the food industry includes leading RSSL's Emergency Response Service, which he originated in 1987. Through the ERS he is directly involved in investigating and understanding quality and safety issues in the food industry. He is also head of RSSL's Quality Assurance Division, and has recently launched the Select QA Supplier Quality Assurance service. Dr Roger Evans Dr Roger Evans is Science Director at RSSL, having particular responsibility for the company's analytical chemistry resources including leading edge instrumentation. His degree and research interests were in chemistry and enzymology, and on joining RSSL, Dr Evans worked on citrus chemistry/biochemistry research programmes. As such he is well known in the food industry for work on fruit juice authenticity. Mike Arnott Mike Arnott heads the consumer research department at RSSL. He is in overall charge of the sensory evaluation department which he helped establish at RSSL, and also consumer products testing which tests and compares non-food items in specialist laboratories. The Consumer Research Department was one of the first sections within RSSL to do significant contract work, and has continued to offer innovative services to the food industry such as 'mood and performance testing' as well as increasingly applying consumer research techniques to the healthcare industry, and developing its market research capability. Dr Graham Pettipher Dr Graham Pettipher heads RSSL's Microbiology and Quality section. He has specialised in developing rapid microbiological techniques, including the direct epifluorescent filter technique (DEFT). He is also directly concerned with implementing quality systems within RSSL (such as NAMAS), and acts as a consultant to other laboratories seeking NAMAS accreditation. Dr John Sheridan Dr John Sheridan is a Principal Scientist with RSSL and runs the Product Chemistry Laboratory, which covers RSSL's routine services in spectroscopy, nutritional analysis and pharmaceutical analysis. His analytical expertise is often called on in the Emergency Response Service, where ad hoc method development is frequently required. He is a member of the Royal Society of Chemistry, has written many papers on photochemistry and biosynthesis and contributed a chapter on soft drinks analysis to the Encyclopaedia of Analytical Science. Dr Brian Baigrie Dr Brian Baigrie heads the taints and off-flavours laboratories at RSSL. His interest in flavour chemistry began in 1979 when he carried out fundamental research into cocoa and chocolate flavours. Dr Baigrie is now acknowledged as an international expert in food taints - one of the most complex and difficult branches of chemistry. He wrote two chapters on taints in the Encyclopaedia of Food Science, Food Technology and Nutrition, (1993) Academic Press, London. Dr Andrew Lea Dr Andrew Lea is Principal Scientist at RSSL, in charge of beverage research. His research interests have centred on the use of HPLC techniques in studies of pigments and flavour chemicals, especially polyphenolics in tea, cider and fruit products. HPLC techniques have also been applied to patulin analysis. He has written extensively for several standard reference texts, including chapters for 'HPLC in Food Analysis' (Academic Press), 'Processed Apple Products' (AVI van Nostrand), 'Enzymes in Food Processing' (Blackie). He was co-editor of 'Fermented Beverage Production' (Blackie). Dr David Hammond Dr David Hammond is Principal Scientist RSSL with particular responsibility for RSSL's authenticity services. He is recognised throughout the world as an authority on fruit juice authentication, and helped to develop a technique for detecting high fructose sugars in fruit juice, derived from adulteration with inulin. He is a member of the International Fruit Juice Union Analytical Committee, and convenor for the working group of the Technical Committee 174 of the EC. He has written extensively on the subject, most recently a chapter on fruit juice adulteration in 'Food Authenticity' (Blackie). Carol Wallace Carol Wallace is the Food Safety and Quality Consultant at RSSL and is responsible for the development and management of RSSL's training and consultancy services.She has a BSc in microbiology, and her background in applied microbiology, food safety and quality systems spans 10 years in the manufacturing and retail sectors of the food industry. She is recognised as an authority on the application of HACCP to product safety management, and co-author of the highly acclaimed text book, 'HACCP - A Practical Approach'. Dr Ian Smith Dr Ian Smith is Principal Scientist in RSSL's physical science laboratory, which encompasses physical chemistry, microscopy and lipids. Dr Smith's background is in surface and colloid science applied to food systems, and his work has resulted in several patents. Since joining RSSL he has secured two patents on oat emulsifiers, as well as helping RSSL expand its services in studying the rheology and structure of chocolate and sugar confectionery, and in the area of emulsion stability of beverages. Hilary Holgate Hilary Holgate is Senior Scientist in RSSL's Structural Studies Unit, where she has developed considerable experience in foreign body analysis, as well as in conducting structural studies for new product development. She helped establish RSSL's procedures for glass identification, and in setting up the substantial database which is often used in ERS investigations. Dr Marek Walach Dr Marek Walach is RSSL's marketing manager, with particular responsibility for developing RSSL's services to the healthcare industry. His post-doctoral research interests in fermentation studies formed the basis for a technology transfer company, and since joining RSSL, he has helped establish specialist biotechnology facilities for growing algae, which may be a source of novel pharmaceutical compounds. He recently set-up RSSL's Healthcare Club, which provides a forum for industry debate and information, and has already run a number of successful meetings. Dr Marek Walach, Lynn White, Nicola Shearman, Di Amor Dr Marek Walach, Lynn White, Nicola Shearman, and Di Amor are variously responsible for marketing RSSL's services and for dealing with customer enquiries, either for our analytical or information services. Your first point of contact with RSSL is likely to be with Marek, Lynn, Nicola or Di. If you have any queries call free on 0800 243482. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-141IA018-000193-B025-169http://www.rssl.co.uk:80/jobs.html 194.159.251.194 19970106150631 text/html 3971HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:06:33 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 3799Last-modified: Tue, 08 Oct 1996 08:05:02 GMT RSSL:Job Opportunities Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Job opportunities To provide its clients with the best science services RSSL aims to recruit expert scientists with excellent interpersonal skills and a commitment to customer service. There are always opportunities for the right people in the following disciplines. microscopy microbiology spectroscopy analytical chemistry physical chemistry sensory evaluation / consumer research For more information about current vacancies contact Melanie Wade on 0118 9868541 or fax your CV on 0118 9868932. Current Vacancies Mass Spectroscopist Reading c. 20k plus bonus, pension, medical insurance, sharesave, 25 days' leave Reading Scientific Sevices Limited is the leading suplier of research, analysis and consultancy to a wide variety of companies from the food, consumer goods, chemical and healthcare industries. As a company we are committed to the delivery of scientific excellence, first class customer service and the operation of accredited quality systems. We recognise the need to recruit and invest in good quality people. Our flavour and trace analysis department is currently looking for a Mass Spectroscopist to provide a high level Mass Spectroscopy service to both the Flavour Chemistry department and an in house service to other departments. The role will involve operationof VG Trio, IS, Fisions MD800, VG7070E and other similar instruments as well as the ATD400 Thermal desorption instrument and modern gas chromatographs, in particular Hewlett Packard 5890 and 6890 GCs. The job holder will need to be able to provide quotes for external work, analyse samples using appropriate instrumetation, interpret and communicate results directly to clients, both verbally and in report format and manage the throughput of work to demanding timescales. Educated to at least degree level in chemistry or other relevant subject, candidates should have a minimum of 5 years experience of the operation of Mass Spectrometers, preferably gained within the food or fragrance industry. The fast turnaround of samples demands excellent organisational skills coupled with the ability to work well under pressure. You must also be an excellent communicator, commercially aware, customer oriented and a team player. If you meet all the abve criterisa, we would welcome hearing from you. Please apply in writing, enclosing a current CV to Melanie Wade, Human Resources Manager. Even if there are no vacancies at present, we'd still like to hear from you if you think you have the appropriate skills to make a career with RSSL. Reading Scientific Services Ltd is committed to providing equal employment opportunities. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-142IA018-000193-B025-180http://www.rssl.co.uk:80/current.html 194.159.251.194 19970106150638 text/html 1492HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:06:42 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1320Last-modified: Tue, 24 Dec 1996 09:39:30 GMT RSSL:Current Topics Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Current Topics Genetically Modified Organisms - Implications for the food industry (Dec 1996) Detection of Meat Mixtures (Nov 1996) HACCP - The Video (Oct 1996) Review of EU Legislative Issues : Jan - June 96 (Aug 1996) Phthalates (June 1996) Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-143IA018-000193-B026-15http://www.rssl.co.uk:80/enquiry.html 194.159.251.194 19970106150650 text/html 5772HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:06:51 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 5600Last-modified: Tue, 08 Oct 1996 08:05:02 GMT RSSL:Enquiry Form RSSL Enquiry Form This form enables you to request more information about RSSL's services. Note : This is an HTML form. If your browser does not support forms, send mail directly to Lynn.M.White@rssl.sprint.com, or telephone us on +44 118 986 8541. Alternatively print out the form, fill in, and fax back on +44 118 986 8932. Information request Please check the boxes of the services that you would like to receive more information on. Research The brewing & beverage industry The healthcare industry The chemical industry Oils and fats industry Analysis Microbiology Composition / nutrition / product labelling Pesticide residues Fruit juice authenticity Sensory evaluation Environmental Foreign bodies Taints and off-flavours Mycotoxins Microscopy Particle characterisation Rheology Consumer products Spectroscopy Consultancy Emergency Response Service Product development HACCP consultancy Consumer & Market Research Information Services Training Select QA - Supplier Quality Assurance Other Information Personal details Please enter your personal details in the fields below. Title MrMsMrsMissDrProfOther First name Surname Job Title Company Address 1 Address 2 Address 3 Address 4 Postcode/Zip Country E-mail Please call me to discuss. Please send me information about RSSL's services on a regular basis. Click to register your enquiry. Click to clearyour form. © In Press PR Ltd1996 WT03-B20-144IA018-000193-B026-28http://www.rssl.co.uk:80/research.html 194.159.251.194 19970106150657 text/html 2001HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:07:01 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1829Last-modified: Mon, 20 May 1996 07:44:57 GMT RSSL:Research Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Research RSSL's research strength relies on our ability to bring together expertise in many scientific disciplines. The range of skills at our disposal include chemistry (analytical, organic, physical), biochemistry, microbiology, microscopy (electron and light), food science, food technology, sensory evaluation, mathematics/statistics, product development, market research, fermentation technology, information science. This multidisciplinary approach and broad based industry experience means we are able to address research issues and solve problems efficiently and expertly. For more information on our laboratories or how we can address your specific research problem call 0800 243482 or complete the enquiry form. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-145IA018-000193-B026-48http://www.rssl.co.uk:80/analysis.html 194.159.251.194 19970106150704 text/html 2072HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:07:08 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1900Last-modified: Mon, 20 May 1996 07:45:05 GMT RSSL:Analysis Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Analysis Much of our analytical work is directed at helping manufacturers meet their statutory obligations and at achieving the high standards expected by their customers. Checking the safety, composition and purity (authenticity) of products is a regular requirement, as is troubleshooting. RSSL's laboratories have the range and depth of expertise to undertake analysis from simple compositional studies to the determination of analytes at parts per billion or lower. Our analysts work to recognised methods (eg AOAC, BP, USP etc) and to procedures developed in-house. Externally accredited quality systems such as NAMAS and GLP are applied where appropriate. For more information on how we can address your specific analytical requirements call 0800 243482 or complete the enquiry form. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-146IA018-000193-B026-61http://www.rssl.co.uk:80/consultancy.html 194.159.251.194 19970106150713 text/html 3282HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:07:14 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 3110Last-modified: Mon, 20 May 1996 07:45:04 GMT RSSL:Consultancy Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Consultancy RSSL provides a variety of consultancy services, particularly in the area of product safety and product acceptability. Hazard Analysis Critical Control Point (HACCP) HACCP is the internationally recognised system for ensuring product safety and widely accepted as forming the basis for a 'due diligence' defence. RSSL is an internationally recognised authority in HACCP training and consultancy. Emergency Response Service (ERS) The ERS provides immediate analytical suport during product emergencies, 24 hours a day, 365 days a year. RSSL also provides consultancy in helping companies avoid problems. New product development Consumer and market research RSSL uses sensory evaluation methods to help you understand product characteristics, and demonstrate which characteristics drive consumer preferences. Information services Information is key to good decision making in industry, and RSSL can help through the Food Intelligence Service Quality systems RSSL provides consultancy in setting up, and carrying out Supplier Quality Assurance and due diligence programmes, and offers advice on and support to laboratories seeking NAMAS accreditation. Select QA provides a range of third party supplier quality assurance services for both manufacturers and retailers. Training Through FITR RSSL offers a number of courses in food safety/quality related topics Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-147IA018-000193-B026-76http://www.rssl.co.uk:80/healthcare.html 194.159.251.194 19970106150720 text/html 2035HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:07:23 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1863Last-modified: Mon, 20 May 1996 07:44:59 GMT RSSL:Healthcare Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Healthcare RSSL can carry out a variety of routine quality control analyses and contamination investigations to free in-house R&D staff for more productive work. A number of pharmaceutical manufacturers use RSSL for these routine services, and increasingly many more companies are also using our sensory evaluation services to help formulate medicines that taste more acceptable to the public. RSSL has also been successful in transferring other disciplines from the food industry to the healthcare industry including HACCP systems, training, product stability studies and the Emergency Response Service. RSSL also has a biotechnology unit which has the expertise and failities to supply biomass for screening programmes. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-148IA018-000193-B026-91http://www.rssl.co.uk:80/laboratories.html 194.159.251.194 19970106150728 text/html 3271HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:07:31 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 3099Last-modified: Wed, 10 Jul 1996 10:45:52 GMT RSSL:Laboratories Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Laboratories RSSL's multidisciplinary laboratories are amongst the best equipped in Europe. Continuing investment in new technologies and methodologies ensures that RSSL is always able to provide the most up to date analytical techniques to its customers, and can address virtually any analytical problem. Our facilities are divided across a number of broad disciplines Analytical chemistry Gas chromatography, HPLC, SFC, ion chromatography, ATD, mass spectrometry (GC-MS, Py-MS, selective ion monitoring, fast atom bombardment, high resolution, ICP-MS, isotope MS), NMR (400 MHz), FT-IR, UV spectrophotometry, AA spectroscopy. Many of these techniques are used in authenticity studies, investigations of taints and off-flavours, and pesticide residue analysis. Physical chemistry Rheology, tensile and compression testing, particle sizing, viscometry, melting point, tristimulus colorimetry, Differential Scanning Calorimetry, surface tension, moisture content, density, particle size reduction. Microscopy & image analysis Light microscopy, fluorescence, SEM, TEM, X-ray microanalysis, FT-IR, surface interferometry. Microscopy is often the first step for the ERS in addressing product contamination incidents and identifying foreign bodies. Microbiology Rapid microbiological methods include conductance, ATP bioluminescence, DEFT, flow cytometry, ELISA methods for Listeria and Salmonella, toxicology kits. Sensory evaluation Dedicated taste test laboratories equipped with PC's running RSSL's bespoke software TASTE. In addition to its role in formulating products, sensory evaluation is often the first stage in taint and off-flavour investigations. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-149IA018-000193-B026-108http://www.rssl.co.uk:80/ers.html 194.159.251.194 19970106150735 text/html 2400HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:07:39 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 2228Last-modified: Fri, 05 Jul 1996 15:50:59 GMT RSSL:Emergency Response Service Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Emergency Response Service (ERS) Members of the ERS have instant access to RSSL's multidisciplinary laboratories, 24 hours a day, every day of the year. Since its initiation in 1987 the ERS has helped many companies by the provision of analytical and information support during a wide variety of product emergencies in over 1000 cases. The type of case dealt with by the ERS varies widely, but can be classified generally as investigations of serious customer complaints, taints and off-flavour, packaging/processing problems, foreign body analysis, threatened or actual tamper and extortion, authenticity, counterfeiting and pesticide residue analysis. Many of these investigations involve the full range of analytical services available through RSSL, and demand the expertise of scientists able to address unique and novel problems with an appropriate analytical response and rapid turnaround. Where required the ERS acts as a focal point and coordinator of a network of agencies providing a variety of associated services including medical, forensic and poisons information. There is a brochure available. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-150IA018-000193-B026-129http://www.rssl.co.uk:80/fis.html 194.159.251.194 19970106150744 text/html 1748HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:07:47 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1576Last-modified: Mon, 20 May 1996 07:45:00 GMT RSSL:Food Intelligence Service Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Food Intelligence Service The Food Intelligence Service provides rapid answers to business and technical questions in the world-wide food industry. The service will research and interpret information to provide management briefings in response to specific questions from directors and managers in the food industry. The most frequently addressed issues include food safety, production and processing, quality assurance, worldwide new product launches, EC and international legislation, global market trends and patents. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-151IA018-000193-B026-142http://www.rssl.co.uk:80/selectqa.html 194.159.251.194 19970106150752 text/html 2008HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:07:55 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1836Last-modified: Wed, 10 Jul 1996 10:40:09 GMT RSSL:SelectQA Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Select QA Select QA is RSSL's Supplier Quality Assurance service. Select QA is used by retailers, manufacturers and their suppliers and provides a cost effective way to carry out essential SQA activities to industry recognised standards. The service covers both food and non-food products, with RSSL's technologists and auditors having expertise in food and drink, cosmetics and toiletries, pharmaceuticals, healthcare and non-food consumer goods. The Select QA service involves professional auditing at an agreed frequency against the Select QA Guide to Standards of Operation, provision of a detailed performance report and post audit follow-up. The service can be tailored to individual requirements and clients may also wish to add further options such as Customer Complaint Review and one-off Investigative Audits. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-152IA018-000193-B026-157http://www.rssl.co.uk:80/microbiology.html 194.159.251.194 19970106150801 text/html 1947HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:08:03 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1775Last-modified: Mon, 20 May 1996 07:44:58 GMT RSSL:Microbiology Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Microbiology RSSL's microbiology laboratory operates seven days a week offering routine services aimed at assuring the safety and integrity of ingredients and products. The service aims to provide a fast response and quality testing at competitive prices. Routine tests include enumeration (total bacterial counts etc), detection of specific pathogens (Listeria, Salmonella, E. coli etc) and identification (Listeria species, Gram negative and Gram positive bacteria, spoilage yeasts etc). Other services include shelf-life testing, investigation of product spoilage, challenge testing, environmental and hygiene monitoring. The microbiology department also has a critical role to play in RSSL's ERS, HACCP and mycotoxin analysis services. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-153IA018-000193-B026-171http://www.rssl.co.uk:80/authenticity.html 194.159.251.194 19970106150814 text/html 2442HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:08:17 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 2270Last-modified: Mon, 20 May 1996 07:45:05 GMT RSSL:Authenticity Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Authenticity The authenticity of foods is a major issue for the industry. There is always a profit incentive for expensive commodities to be extended by the addition of cheaper ingredients, and such adulteration is becoming increasingly sophisticated. RSSL is acknowledged world-wide for its services in fruit juice authenticity. The full service involves tests of over 15 key parameters which enables our expert analysts to determine both the purity of a juice and its country of origin. These tests can be applied to a number of juices, and fruit products. RSSL also has authenticity services for oils and fats. Certain oils are traded at a premium because of their alleged quality and/or health properties, and again, adulteration with cheaper oils is a potential problem. RSSL's services can check the purity, quality, safety and properties of all these products. Other authenticity services are available for coffee, fish, pasta, rice, meat, wines, and honey. Similar techniques may also be applied in the area of compositional / nutritional analysis, and these are frequently necessary both for accurate labelling and in the detection of counterfeit products, especially perfumes and premium spirits. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-154IA018-000193-B026-186http://www.rssl.co.uk:80/sensory.html 194.159.251.194 19970106150823 text/html 2010HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:08:27 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1838Last-modified: Mon, 20 May 1996 07:44:57 GMT RSSL:Sensory Analysis Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Sensory evaluation The taste, smell, touch, and feel of consumer goods are ultimately the factors that influence repeat purchases. RSSL's sensory evaluation studies are conducted using trained analysts who are able to detect and describe accurately the sensory characteristics of products. These methods may be used to compare one product against its competitors, and further, to understand the factors that drive consumer preference for one product over another. This enables companies to reformulate products having due regard for those characterisitics that will make the product preferred by a majority of consumers. Sensory evaluation is also used in taint and off-flavour investigations giving analysts the vital first clues that allow them to direct chemical investigations at the most likely cause of the problem. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-155IA018-000193-B026-198http://www.rssl.co.uk:80/taint.html 194.159.251.194 19970106150834 text/html 2163HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:08:38 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1991Last-modified: Mon, 20 May 1996 07:44:57 GMT RSSL:Taints & Off Flavours Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Taints and off-flavours Taints and off-flavours (and off odours) are a growing problem for the food, drink and cosmetic industries. The problem can occur from a variety of sources, including packaging migration, environmental pollutants, microbial spoilage, process problems, factory paints, cleaning solvents. Trace levels of a contaminant can affect many tonnes of production so it is vital that the contaminant and its source are identified as quickly as possible. RSSL has particular expertise in this field and has helped many many companies over the years to resolve specific problems. Using sensory evaluation techniques, followed by sophisticated chemical extraction and identification procedures, RSSL can identify the most potent tainting chemicals down to parts per trillion concentrations. The result of these investigations is often critical in helping manufacturers secure compensation from suppliers or distributors for their part in causing the problem. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-156IA018-000193-B026-219http://www.rssl.co.uk:80/foreign.html 194.159.251.194 19970106150843 text/html 2142HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:08:46 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1970Last-modified: Mon, 20 May 1996 07:45:00 GMT RSSL:Foreign Body Analysis Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Foreign Body Analysis Foreign bodies in products are a constant cause for concern to manufacturers and a worry to consumers. Even the best manufacturing plant is vulnerable to the problem, and will require expert assistance to identify the nature of a foreign body and its source. Expert analysis is essential since the appropriate response to finding a glass fragment in a product may be very different from that to finding a plastic fragment in a product - yet both may appear identical to the naked eye. RSSL employs a variety of specialist techniques to foreign body investigations, most especially, light and electron microscopy and Fourier Transform - Infra Red (FT-IR) spectroscopy. We have developed a number of specialised procedures for dealing with foreign body incidents, and have established a database of glass samples which enables rapid identification of fragments brought to us by our clients. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-157IA018-000193-B026-232http://www.rssl.co.uk:80/pesticides.html 194.159.251.194 19970106150850 text/html 1817HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:08:54 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1645Last-modified: Mon, 20 May 1996 07:44:58 GMT RSSL:Pesticides Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Pesticides Consumer groups and regulators are incresingly concerned about the levels of pesticides found in food stuffs. Maximum Residue Levels have been set for more than 80 pesticide chemicals, and routine screening of raw materials and ingredients is essential for ensuring 'due diligence ' and product safety. RSSL's appraoch is to provide the most cost effective service by tailoring its pesticide residue testing programmes to our clients' specific needs. The service is NAMAS accredited, and uses gas chromatography in multi-residue screens for organochlorines and organophosphates, and GC-MS to confirm the presence of individual residues. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-158IA018-000193-B026-249http://www.rssl.co.uk:80/directions.html 194.159.251.194 19970106150859 text/html 1580HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:09:02 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1408Last-modified: Mon, 20 May 1996 07:45:01 GMT RSSL:How to find us Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Directions The University Of Reading can be reached by car from the M4 leaving at either junction 10 (westbound) or junction 11 (eastbound) and following directions on the map below. Reading railway station is on a direct line from London Paddington (30 minutes) and connects to Heathrow and Gatwick airports (60 minutes and 90 minutes respectively). Visitors arriving at Reading railway station are advised to take a taxi to the university campus. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-159IA018-000193-B027-1http://www.rssl.co.uk:80/gmo.html 194.159.251.194 19970106150909 text/html 8857HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:09:10 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 8685Last-modified: Tue, 24 Dec 1996 09:39:30 GMT Genetically Modified Organisms Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 What are the implications for the food industry of genetically modified organisms ? The advent of genetically modified organisms (GMOs) presents several challenges to the food industry, and concern to many consumers. The challenges for the industry are ultimately concerned with securing consumer acceptance and commercial advantage. Consumer acceptance will be the ultimate judge of whether GMOs have a place on the supermarket shelves. However, in order to achieve that acceptance, suppliers, manufacturers and retailers must inform themselves first about the health, safety, authenticity, quality and legal implications of using GMOs. Then they must consider how to educate consumers. This education process may prove difficult, not least because there is a vociferous lobby that objects to GMOs. Some people object to the idea in principle. There is a popular science-fiction myth that geneticists are meddling with nature to produce dangerous new life forms. The fact that altering, adding or deleting one gene comes nowhere close to creating a new organism is irrelevant to these objectors. It is also the case at the moment that genetic modification of farm animals has not progressed as far as it has in plants. This is partly because the gene transfer technology is more complicated in animals, and partly because commercially important genes have yet to be identified. In the case of plants, some have attempted to address public concerns by arguing that genetic engineering is nothing more than a sophisticated form of the cross pollination science that has been practiced for years. However, this is an over simplification. There are two basic methods of inserting genes into a recipient plant tissue. One involves use of a bacterium (Agrobacterium) that naturally infects many dicotyledonous plants. The other involves bombarding cell nuclei with tiny gold or tungsten particles coated with genetic material. The bombardment technique is necessary for inserting genes into cereal crop tissues, since these are not infected by Agrobacterium. In both techniques only some of the cells in a tissue are transformed and these have to be selected out from the mass. This is achieved by inserting a second gene at the same time as the first. The second gene codes for a protein that confers resistance to a chemical which is toxic to the untransformed cells. When this chemical - usually an antibiotic or herbicide - is added, the untransformed cells are killed, leaving the transformed cells behind and available for growth into new plants. A third gene is also usually included in the DNA used for transformation. This will encode for a protein which can be assayed readily to confirm that transformation has occurred. It is the use of the second marker gene that raises objections to GMOs amongst more informed objectors. It is argued that this gene could cross from the GMO to other plant species by natural means. The benefit of having a crop with some quality or processing improvement would be neglible against the downside of creating a strain of herbicide resistant 'superweeds'. A further concern about GMOs is that almost by definition, the long term health impact of consumption is not known. There are fears that the 'foreign' genes could bring problems as well as benefits. The most cited example is that of a soybean containing genetic material from a Brazil nut, which caused allergies in individuals who consumed the soya made from the beans. In this case the soya product was withdrawn voluntarily from sale. This example may prove exceptional, but is interesting from a number of perspectives. First, it shows that gene transfer can confer unwanted characteristics as well as desirable ones. How many of these characteristics will have long term or indeed short term health implications will only be determined after the event. However it is likely that the media and consumers will be on the look out for risks associated with GMOs and the first sign of problems could spell disaster for a given GMO. Second, it raises a specific concern about genetic modification of soy beans. Since soya products are amongst the most widely used food ingredients, genetically modified soybean could find its way into many, many food products. This begs the question of whether legislation is required to ensure that foods containing GMOs are labelled as such. Currently, labelling is voluntary. The soybean/Brazil nut case suggests there is a case for declaring the source of transferred genes, as well as the recipient plant. Such labelling would impose massive administrative burdens on food manufacturers in setting up complex traceability systems, but these might prove unavoidable if the health of a minority of consumers is to be protected. The labelling issue has been thrown into focus by Monsanto's recent introduction of a genetically modified soybean which has resistance to the company's own herbicide product. The company plans to mix the new bean with existing varieties without labelling the mixture. Industry groups are divided on the issue. The British Retail Consortium has expressed the view that the two types of soybean should be kept separate, whereas the Food and Drink Federation has stated that separation is 'impractical', 'unnecessary' and 'unenforceable'. Retailers in general have taken the view that foods containing GMOs should be labelled as such in order to inform consumers. However, Monsanto's policy will make it impossible for the retailers to give this information. They will either have to source supplies that contain no GMOs, or adopt a labelling policy that takes account of the possibility that products contain ingredients sourced from GMOs. The first option raises the issue of authenticity. It may never be possible to state categorically that a given food product contains no GMOs. Even with the best intentions, it is conceivable that some traders may deal in products derived from a GMO source without declaring it to their own customers. It is likely that tests, perhaps based on detection of the marker gene proteins, will have to be developed in much the same way that authenticity tests are required for fruit juices etc. Perhaps the most insidious aspect of the development of GMOs is the extent to which a few very powerful biotechnology companies might be in a position to exploit the international food industry. For example, if a genetically modified crop is so superior to the alternatives, commercial necessity may force the GMO variety to be grown. The company that holds the 'plant variety right' in the GMO stands to make significant gains from its use. It is interesting to speculate on the extent to which that company would also be liable for damages if there were health problems arising from use of one of its GMOs. It is undoubtedly the case that GMOs have great potential benefit. A growing world population depends on sustainable food resources. Crops that are better able to resist disease, to survive drought, or remain wholesome during transportation have clear benefits. Much of the plant breeding technology that has been practiced for centuries has been directed at producing these better crops. The new technology of genetic engineering offers the potential to tailor make produce with even greater refinement. Crops can be developed to have better flavours, easier processing characteristics, stronger colours etc. Whether consumers are prepared to accept an uncertain level of risk in order to receive these relatively modest gains remains to be seen. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-160IA018-000193-B027-20http://www.rssl.co.uk:80/meat.html 194.159.251.194 19970106150918 text/html 6065HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:09:21 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 5893Last-modified: Tue, 24 Dec 1996 09:39:30 GMT Meat Mixtures Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 How easy is it to detect meat mixtures ? It has long been recognised that unscrupulous traders might be tempted to adulterate meat products. There are potential profits to made from substituting a cheaper meat for a more expensive cut of the same species, or of higher-value species. Particularly in minced meat, canned meat, sausages and pies etc, it is relatively easy to see how a cheaper meat might be introduced in place of the meat supposed to be present. Clearly, deliberate mixing is fraudulent and an offence unless the meat product is accurately labelled. In the case of cross species adulteration, it is also potentially offensive to any religious community that might have a prohibition on the consumption of certain animal products. Isolated cases of mixing may also arise from inadequate cleaning of mincing equipment etc. Though accidental, such contamination would be no more acceptable to consumers or trading standards authorities. It would also point to an alarming failure to observe basic hygiene controls during processing. The recent press coverage of the finding by Hounslow trading standards officers of beef mince in lamb mince has alerted the general public to the issue of mixing. Six out of the ten lamb mince products purchased were found to contain beef. In one case the beef content was claimed to be as high as 10%. The results were made public in August although the survey took place in April/May. Not surprisingly, the finding of such a high percentage of beef content raised the issue beyond that of misrepresentation of products. The suspicion was that otherwise unsaleable beef was being deliberately added to lamb mince. Regardless of any intent to deceive the public, the finding added further to the concerns about BSE. Whilst there was no evidence to suggest that the beef concerned was in any way infected with BSE, the findings did nothing to improve consumer confidence in the meat trade. What it did do was confirm the importance of methods for identifying mixing. Laboratory techniques for identifying different species of meat are based on the genetic and chemical differences between the species. A thorough account of appropriate methods is given in the text book 'Food Authentication ' (Blackie) in the chapter on 'Authenticity of meat and meat products'. The most widely used methods are immunological assays (looking at species specific proteins, again most commonly in raw meat although cooked meats may be assayed using different test kits), electrophoretic (looking at protein markers in raw meat), and DNA methods (applicable to both raw and cooked meats). Lipid analysis has limited application in identifying the fatty acid and triglyceride components of meats but the technique is relatively insensitive. The sensitivity of these techniques will vary, often according to the quality of reagents used as much as the quality of the meats being tested. High performance liquid chromatography, electron spray mass spectrometry and near infra red spectrometry have also been used or studied as speciation techniques with varying degrees of success. However, these techniques are unlikely to be commercially viable at least in the short term. In any scientific analysis it is important to appreciate the limitations of the technique for answering a given question. Therefore, it is vital to run detection limit controls, and false positive / false negative controls especially with immunological techniques. None of the techniques as described is able to differentiate between meat from different breeds, although DNA analysis has the potential to differentiate between breeds. However there is enough concern about mixing meats between species to provide enough impetus for further research in this area. An EU Standards Measurement and Testing project is currently underway which aims to prepare homogeneous and stable meat species reference materials for use in intercomparison studies to investigate and evaluate the analytical procedures for meat speciation commonly used within in the EU. The project is being coordinated by Ian Lumley of the Laboratory of the Government Chemist, and author of the chapter referred to above. This programme is well advanced and preparation of the required samples has been completed. A number of laboratories across Europe have participated in the first stages of the study, and a second stage of interlaboratory comparisons and further method developments are yet to be proposed. In the meantime, the meat industry must do all it can to regain and preserve consumer confidence in its products. At the very least that must mean taking every step to ensure the authenticity of its products. Where authenticity is in doubt, any of the rapid screening techniques referred to above might be used to identify the suspect products, which can then be subjected to more thorough testing. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-161IA018-000193-B027-37http://www.rssl.co.uk:80/haccpvideo.html 194.159.251.194 19970106150929 text/html 1768HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:09:29 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1596Last-modified: Tue, 24 Dec 1996 09:39:30 GMT HACCP - The Video Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 HACCP - the video A new training video explaining the principles and practice of HACCP has been released by RSSL. "HACCP - How The System Works" is an ideal tool for introducing HACCP to the wider workforce, senior management and the staff charged with implementing HACCP systems. The video explains the theory behind HACCP, demonstrates its importance to food safety, and gives a clear guide on setting up HACCP teams, establishing HACCP procedures and monitoring their effectiveness. For more information on "HACCP - How The System Works" email Lynn.M.White@rssl.sprint.com, or call on +44 118 986 8541. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-162IA018-000193-B027-59http://www.rssl.co.uk:80/legislation.html 194.159.251.194 19970106150938 text/html 29173HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:09:41 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 29000Last-modified: Mon, 05 Aug 1996 10:40:39 GMT European Union Legislative Issues Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Review of European Union Legislative Issues: January - June 1996 Authenticity Official Control of Foodstuffs A question raised by Gerhard Schmid in the European Parliament (Official Journal 96/C 9/48 of 15.1.96 ) on the official control of foodstuffs Directive elicited the response from the Commission that the Coca-Cola Company are technically obliged to divulge details of their formulae. Fraud The European Commission has issued a proposal (COM 95 690 ) for a Regulation to enable them to make spot checks and inspections for the detection of frauds and irregularities to back up the efforts made by the member states in this area. It has been suggested that the Commission is taking action because of the apparent ineffectiveness of the member states in combatting fraud. Chocolate After three years of prevarication, the Commission has finally adopted proposals to streamline unwieldy legislation in 7 vertical foodstuffs Directives. The proposals have been held up because of the difficulty in reaching agreement on the definition of chocolate. Europe is fairly evenly split on what constitutes chocolate. One half believes it should contain no other fat than cocoa butter while the other half believe that up to 5% of other vegetable fat is necessary to meet market conditions, particularly in the hotter climates. A compromise has been reached allowing chocolate products containing up to 5% vegetable fats as a substitute for cocoa butter but they will have to carry a label saying so. Member states can still deny their national manufacturers the right to use vegetable fats but they cannot deny market access to imported chocolate. This compromise proposal is likely to secure a qualified majority in Council but under the co-decision procedure it could have problems in the European Parliament. Specification Loophole The European Court of Justice has ruled that national product specifications are unenforceable if the national government has not given copies of all the technical rules that it adopts to the Commission. Thus national courts have been advised to ensure that national standards have been deposited with the Commission before proceeding with local infringements. Contaminants The European Commission has published in the Official Journal L64 the Recommendation 96/199/EU on the 1996 coordinated programme of inspection to comply with maximum pesticide residue levels in foodstuffs. The Economic and Social Committee has given its opinion (96/C 82/01) on the proposed Directive (95/C 201/07) amending Directive 76/363/EEC on the maximum levels of pesticide residues on fruit and vegetables, Directive 86/362/EEC on cereals, Directive 86/363/EEC on products of animal origin, and Directive 90/642/EEC on products of plant origin including fruit and vegetables. The draft Directive, 96/C 87/02 on integrated pollution prevention and control has reached the common position stage. It will now go to the European Paliament for its second reading through the co-decision process. Member of the European Parliament, Amedeo Amadeo, was assured on asking about the unauthorised use of pesticides in agriculture that the system was indeed under control (96/C 112/19). Consumer Protection Consumer Power When Emma Bonino was appointed the European Commissioner for Consumer Affairs she was shocked at the low status of consumer matters within the Commission. She is determined to simplify the resolution of individual consumer complaints and announced plans to launch discussions on an action plan for this area. To this end, she is shepherding a Directive to give consumers equal access to justice in all EU member states. There would be mutual recognition of consumer associations which would be able to represent European citizens before the courts in any of the 15 member states. The Directive will affect advertising and consumer credit. Bonino sees this as an important step in fostering the citizen's belief in belonging to the EU. Understandably the European Consumers Council (BEUC) in Brussels is pleased. Member states will have to designate their approved consumer associations and notify the Commission who will ensure publication in the Official Journal. The proposals for the Directive are published in Commission document COM 95 712. The Economic and Social Committee gave its opinion on the proposed legislation in the Official Journal 96/C 39/12 of 12.2.96, and the proposed revised Directive on the coordination of laws, regulations and administrative provisions of Member States relating to injunctions for the protection of consumers interests was published in the Official Journal 96/C 107/03 of 13.4.96. Action Plan A proposed action plan (Communication from the Commission, COM 96 13) proposed on 14 February complementing the above legislation aims to encourage amicable resolution of conflicts by offering a working plan for a voluntary system of additional legal help for handling consumer complaints. A simplified form something like the one used to report car accidents, is proposed to facilitate individual consumer-related disputes in the EU. A legal action guide is already available on the Internet to enhance legal protection for European consumers. This is a stage two sequel from the draft Directive (COM 95 712) initiated by the Commission in January of this year aimed at protecting consumers' collective interests by the mutual recognition of cross border legal actions. Unit Pricing The Council reached a Common Position on 23 April on the Proposed Directive on consumer protection in the indication of the prices of products offered to consumers. The Directive will replace three earlier Directives 79/581/EEC as amended by 88/315/EEC and 88 314/EEC. It has not yet been published in the Official Journal but the European Parliament has already considered it and put forward amendments. The Directive will require the indication of both the selling price and the price per unit of measurement of most products offered by traders to consumers to improve consumer information and to facilitate comparison of prices. There are openings for derogations for street traders, small businesses, for products for which the Directive would be inappropriate, etc. The Economic and Social Committee put forward its comments on the proposed legislation in the Official Journal 96/C 82/08 of 19.3.96. But European MP, Riccardo Garasci, in a question (96/C 66/136) to the Commission asserted that the multiplicity of prices confronting the consumer will only mislead. Open Forum on Consumer Policy A three year plan on protection of the consumer will focus on quality, information and openness. At a ministerial open forum, four of ten main issues discussed were: protecting consumers from unscrupulous financial services traders; getting better quality of service from the public utilities; facilitating access to new technologies for the information society; and restoring public confidence in food products following the mad cow scare. The lone dissenting voice was the British Minister who regretted the unnecessary burden imposed on industry by yet more Community intervention. Food Additives Miscellaneous Additives The Commission is drafting an amendment to the Miscellaneous Additives Directive (Commission Document COM 96 212). A draft was circulated by the UK government to interested parties in January. The three additive Directives (sweeteners, colours, and miscellaneous) are now more or less adopted in practice throughout the EU, except France and Portugal. Flavourings for use in Foodstuffs A basis for compiling a list of approved flavourings and procedues for the use of such flavours in foodstuffs has been reached. A regulation to effect was adopted on 25 June 1996. The common position stage was published in the Official Journal 96/C 59/03 of 28.2.96. Derogation on the use of Additives Some Member States have insisted on maintaining national laws prohibiting the use of additives in certain foodstuffs. An amended proposal for a Decision to regularise this has been published, COM 96 50, and a common position was adopted on 18 June 1996. Similar products from other member states containing additives permitted by EU law cannot be denied access to those markets. Thus for products which benefit from the inclusion of an additive, the restrictive member states are effectively hindering their own industry. Sweeteners Directive The amendment to the Sweeteners Directive has reached the Common Position. The amendments are a tidying up operation and make no major changes - there are no new sweeteners included. Throughout the Annex, the definition of "breakfast cereals" has been modified to include a minimum threshold of 15% fibre (the 20% minimum bran remains). Three categories of permitted foods, breakfast cereals, energy reduced soups, and distilled drinks containing less than 15% alcohol, have been deleted from that part of the Annex covering cyclamate. For the use of cyclamate in the category "food supplements/diet integrators based on vitamins and or mineral elements, syrup-type or chewable" the maximum permitted level has been reduced from 2000 mg/kg to 1250 mg/kg cyclamate. Food Imports Edible Oils For the transport of bulk edible oils and fats by sea, there has been a derogation (96/3/EU published on 27.1.96 in Official Journal L 21) from the hygiene of foodstuffs Directive 93/43/EU. Import Tariffs Regulation 3009/95 published on 30.12.95 in Official Journal L 319, amends Annex I of Regulation 2658/87 on tariff and statistical nomenclature and on common customs tariff. This refers to the importation of food. Illegal Banana Imports Until recently, Germany has had a derogation allowing the importation of "dollar" bananas into the European Union on the basis of long standing custom. This derogation has now been rescinded much to the annoyance of the German importers who have apparently been defying the change in EU law, and have been importing bananas out of quota and duty free through Hamburg! The matter was raised in the European Parliament by Fernando Fernandez Martin who asked ( Official Journal 96/C 9/50 of 15.1.96) about measures to combat fraudulent import of non-EU bananas. Maize Residues Following the increased importation of residues from maize starch manufacture from the USA, the EU has placed restrictions on the volume. These restrictions embodied in Regulation 2019/94 have been amended in Regulation 396/96 published on 5.3.96 in Official Journal L 54. TARIC The European Union system of tariffs (TARIC) is updated periodically. The most recent update appeared in the Official Journal 96/C 98/01 of 1.4.96. The GATT Uruguay Round has made necessary certain changes to the trading regulations of the European Union. The Commission has proposed (COM 96 23) amending Regulation 519/94 on common rules for imports fom certain third countries as a result. Food Law Food Law Among the items on the Commission's programme for 1996 will be a proposal for a general Directive on food law to establish (in retrospect!) a framework of principles for European food legislation. A discussion paper has been drawn up by the Commissioners of Industry, Agriculture, Internal Market, and Consumer Policy as a basis for an internal policy debate, setting out the range of options open to the Commission including both the scope of the proposed Green Paper and the regulatory approach to be adopted. Long standing differences between the Agriculture and Industry Directorates General will have to be resolved before the Paper is tabled by the Commission for more general discussion. Once made public it will be the basis for far-reaching consultations with all interested parties. Legislation Streamlining - SLIM The Commission has launched a new initiative called SLIM (Simpler Legislation for the Internal Market) published in Commission document COM 96 204 to cut down the quantity of legislation and to improve its quality. A number of test cases will be considered this year and the results assessed by the Internal Market ministers in November. If successful, the scheme will be applied to most other areas of EU legislation. Foodstuffs A Green Paper on Foodstuffs is expected this year from the Commission. In it will be an assessment of EU action in the food sector since 1962, an analysis of the workings of the Internal Market, an identification of gaps in legislation and an overview of the international dimension. The objective of the Paper is to elicit from Member States, consumer groups, industry and farmers their views on what type of framework is needed for making the any changes. Vertical Directives After three years of prevarication, the Commission has finally made a Proposal to Council (COM 96 722) for a Directive to amend all 7 vertical foodstuffs Directives affecting coffee and chicory extracts, tinned milk, jam jellies and marmalade, fruit juices, sugars and honey to streamline legislation in those Directives which have become out-dated. Existing Directives contain a number of details which have been superseded due to the adoption of horizontal Directives on labelling, hygiene, additives etc. The proposals have been held up because of the difficulty in reaching agreement on the definition of chocolate. Health & Safety Legislation In the last six months, a number of legislative changes have been progressed. Directive 95/63/EU was published on 30.12.95 in Official Journal L 335, amending Directive 89/655/EEC on minimum safety and health requirements for equipment. A common position has been reached (Official Journal 96/C 147/01 published on 21.5.96) on a Directive on pressure equipment. The Economic and Social Committee has given its opinion (Official journal 96/C 39/05 published on 12.2.96) on the European Union programme on safety, hygiene and health at work. Finally, proposals hae been put forward (Official Journal 96/C 23/07 published on 27.1.96) to amend Directive 89/686 on personal protective equipment. Working Conditions Commission proposals on flexible working hours and safety at work for part-time workers, employees with fixed term contracts and temporary workers are now the subject of consultation. Employers see no need for further legislation but the Commission and the unions disagree. All agree that any obstacles to flexible working have to be removed as long as the basic principle of equal treatment for people engaged in flexible working is maintained. Radiation A Directive setting out basic standards for the protection of the general population and workers from the dangers of ionising radiation was adopted in the middle of May by the Council of Ministers. It follows recommendations made by the several international authorities on the subject. Labelling Trade Marks Joseph Dalby, a barrister based in Brussels, has been cautioning industry on some of the pitfalls in the trade mark arena. The new Community Trade Mark (CTM) which has recently been introduced can take precedence over national marks. At the moment the CTM is running in parallel with national schemes. Not all holders of national trade marks will be able to upgrade their industrial property. The granting of a CTM, as in national applications, has to be justified. There is a case before the Court of Justice currently concerning the right of a manufacturer to prevent the resale of his trademarked goods when they have been repackaged. Interim opinion deems it to be lawful. Food Claims Directive After two abortive attempts to draft legislation on this subject, the Commission is looking at other ways of dealing with it. One suggestion is through an amendment to the legislation already in force. QUID Directive At the second reading of the proposed quantitative labelling amendment to the labelling Directive 79/112/EEC, the European Parliament adopted five amendments. The Commission has published its formal opinion on these, in which it accepts two of them - on sales denomination and quantitative ingredient declaration for products consisting of a single ingredient, but opposes the other three stating the reasons why. The Council would have to agree unanimously to overturn the Commission's opposition. That is highly unlikely. Labelling Confusion European Parliament MP,Riccardo Garusci, in a question to the Commission on the Directive on labelling of prices on foods and other products (unitary pricing), believes the multiplicity of prices on products is going to confuse consumers and add to the burden on retailers. Protected Products Regulation 2091/92 sets out the basis for those categories of foods which enjoy special protection in the European Union. For example feta cheese can only be used to describe cheese made by a special technique in Greece from goats milk, but cheddar cheese is considered not to be regional specific. The Comission has now made a proposal for a Decision (COM 96 38) on a non-exhaustive list of generic products under Article 3(3) of Regulation 2081/92; and a proposal for a Regulation (COM 96 48) on the registration of geographical indications and designations of origin under Article 17 of Regulation 2081/92. Recently, the registration of geographical indications and designations of origin has been published as a regulation 1107/96 published on 21.6.96 in the Official Journal L 148. Sweeteners The Labelling Directive 94/54/EC on the compulsory indication on labels of foodstuffs of particulars other than prescribed in Directive 79/112/EEC was amended by Directive 96/21/EU published on 5.4.96 in the Official Journal L 88. The substance of the change was to ensure that the phrase with sweeteners should accompany the name under which the product is sold for all foodstuffs containing a sweeteners or sweeteners permitted by Directive 94/35/EC. Furthermore foodstuffs containing both added sugar(s) and a sweetener or sweeteners requires the accompanying title with sugar(s) and sweetener(s). Additionally there are mandatory warning messages when a product contains aspartame and more than 10% added polyols. Coeliac Disease In answering a European Parliamentary question by Martin Grechler on labelling of gluten in foodstuffs for sufferers from coeliac disease (Official Journal 96/C 109/24 published on 15.4.96 ), the Commission adsvised that changes in the labelling Directive 79/112/EEC are in hand to deal with this. Genetic Modification In a European Parliamentary question (96/C 173/24 published on 17.6.96), Martin Grechler asked if the labelling of genetically modified foodstuffs should be mandatory. He made particular reference to sugar beet genetically modified to counter viral yellowing of the leaves. The Commission replied that the genetic modification had no effect on the purity of the resulting sugar and so special labelling was inappropriate. Labelling of Additives Following the coming into force of the EU regulations on food labelling, there have been numerous enquiries aboout how additives should be labelled in the ingredients list of a product. The UK MAFF have prepared a set of criteria in conjunction with the European Commision to clarify the situation. These can be obtained from MAFF by telephoning (44) 171 238 6463. Nutrition Dietary Supplements 96/C 56/103 of 26.2.96 Ian White in a European Parliamentary question asked about pending legislation on dietary supplements. The Commission in answer said that a discussion paper is in preparation (by the Scientific Committee for Food ) but no legislation is planned. Diabetic Food In the PARNUTS area, there have been two proposals (Official Journal 96/C 37/07 of 8.2.96 and 96/C 41/09 of 13.2.96) to amend Directive 89/398/EEC on PARNUTS to include diabetic foods as a result of pressure from the European Paliament. Council however, apart from Germany, seems to be against the proposal. At the Internal Market meeting at the end of May, Germany stood out against the other member states who wanted no specific Directive on foods for diabetics. Unanimity is required on this issue since there is a direct clash between the Commission (really the European Parliament) and the Council. Because of this, it was suggested in discussions recently at the CIAA with Dr Crauser of DG III/E, that the Commission may have to withdraw the proposed amendment to the PARNUTS Directive 89/398/EEC on diabetic food. Infant and Baby Food Commission Directives have been adopted on infant and follow-up formulae, (96/4/EC published on 28.2.96 in Official Journal L 49), and cereal-based baby foods, (96/5/EC published on 28.2.96 in Official Journal L 49), using the Standing Committee procedure. The first Directive, 96/4/EC amends the original Directive 91/321/EC on infant formulae and follow-on formulae, but 96/5/EC lays down uniform rules for the first time on cereal-based formulae and baby food for infants and young children. It specifies basic requirements on the addition of minerals, vitamins, etc, and ingredients they may not contain. The criteria governing the addition of ingredients are: no risk to health, effective availability of the ingredients, perfect solubility, stability and lack of interaction. Low-calorie Food The Directive, 96/8/EU published on 6.3.96 in Official Journal L 55, on foods intended for use in energy restricted diets for weight reduction, means tighter control on slimming products. The deadline for incorporation into member state law is 30 September 1997. The daily energy from such food has to fall between 800 and 1200 kcal, with a dietary fibre between 10 and 30 g. Minimum and maximum values are required for fats and proteins and minimum values for vitamins and minerals. This Directive falls under the PARNUTS umbrella. Novel Ingredients Novel Foods Following the publication of the common position on the novel foods and novel ingredients regulation last November, the regulation has had its second reading in the European Parliament where further changes were proposed. The Commission has published its opinion on the Parliament s modifications to the Regulation (COM 96 229). Genetically Engineered Corn Several EU member states have decided against allowing Ciba Geigy to market its genetically engineered maize which is resistant to the European corn borer and more resistant to the Ciba weedkiller Basta (glufosinate ammonium.) According to Greenpeace, the maize also contains a toxin gene of Bacillus thuringiensis and another gene making it resistant to Ampicillin, an antibiotic. Although the Commission has approved the marketing of the maize (COM 96 206: Proposal for a Council Decision to allow marketing of genetically modified maize with insecticidal properties and tolerance to the herbicide glufosinate ammonium with respect to Directive 90/220), Austria, Denmark, Sweden and the UK are opposed to it on the lack of conclusive evidence that it poses no threat to the environment or human health. However it should be noted that the UK Advisory Committee on Novel Foods and Processes has given clearance for food products processed from the maize. Greenpeace will be lobbying the other member states against allowing it when it comes up for discussion at the June Environment Council meeting on the basis that the safety testing was flawed. Packaging Plastic Materials A Directive (96/11/EU published on 12.3.96 in the Official Journal L 61) amends Directive 90/128/EEC on plastic materials and articles intended to come into contact with foodstuffs by elaborating on the list of chemicals that can be used. Packaging Turmoil Two new drafts on the proposed identification system for packaging and the formats for databases have angered national experts when they were told that they would not be voluntary after all. All packaging would have to carry an identification number if the new draft is adopted, to allow waste packaging to be sorted efficiently for recycling. There will also be an obligation to measure the amount of packaging which is reused. Recycling The European Commision, in response to a Parliamentary question by Anita Pollock on packaging directive for uniform recycling symbol (96/C 173/55 published on 17.6.96 ), advised that work is in hand. Product Safety Official Control of Foodstuffs In Directive 96/290/EC, published on 3.5.96 in the Official Journal L 109, the Commission recommends that member states take samples of or inspect the following sectors during 1996: Microbiological assessment of dried and fermented ready-to-eat meat and meat products Migration of plasticisers into foods Temperature of chilled foods on display for sale Benzopyrene in smoked pork products Safety of Food in the UK The UK Committee on the Microbiological Safety of Food has published its Annual Report for 1995. Subjects covered during that year were trends in human foodborne infectious intestinal disease and relevant animal data; microbial antibiotic resistance in relation to food safety; foodborne viral infections; and Mycobacterium paratuberculosis in milk. Hydrolysed Vegetable Protein The European Union Scientific Committee for Food has recommended that levels of the most common chloropropanol, 3-monochloropropane-1,2-diol which has been shown at high doses and on prolonged dosing to cause cancers in rats, be reduced to undetectable levels. Chloropropanols are found at very low levels in hydrolysed vegetable protein which is used in small amounts to flavour savoury products. The UK Food Advisory Committee at its meeting in May has initiated discussion with industry to find means of further reducing the already low levels of chloroprpanols in HVP, if necessary by changing the manufacturing process. Malachite Green Trout farmers are advised to take steps to reduce the concentrations of malachite green (used to control fungal growth on mature fish) in fish reaching the market. Although the UK Committee on Toxicology has raised no concern, the UK Veterinary Medicines Directorate, the British Trout Association and the major retailers are cooperating to investigate the reason for the higher than expected levels of the residues. Water Water Resource Planning The Commission has issued a Communication (COM 96 59) on EU water policy, addressing not just quality but quantity as well. The use of water has to be coordinated across the Union to ensure there are no shortages and environmental damage. The objective is to ensure that one member state does not move its problem into another state. The proposed framework Directive is expected to cover the definition of quality objectives, monitoring requirements, the management of water programmes, public participation, and openness in the management of water resources. Water Quality The Economic and Social Committee has given its opinion on the proposed Directive on water quality for human consumption, (Official Journal 96/C 82/12 of 19.3.96). Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-163IA018-000193-B027-77http://www.rssl.co.uk:80/phthalates.html 194.159.251.194 19970106150947 text/html 5867HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:09:49 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 5695Last-modified: Mon, 17 Jun 1996 10:12:12 GMT RSSL:Phthalates Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Phthalates Analytical Support During Food Contamination Scares Responding to the current phthalate controversy The recent incident concerning levels of phthalate in baby milk formulae has once again illustrated how the media can distort a story to create a major food scare. Many food manufacturers and retailers have been monitoring the levels of phthalates in their products for some time and have taken steps to reduce levels either from packaging materials or environmental sources. The potential damage to brands from this type of media attention can do untold harm to responsible manufacturers who endeavour to produce safe and high quality products. RSSL first reported on phthalate contamination through our company newsletter Contract in 1994 and have for some years developed GC-MS methodology to analyse for the whole range of phthalates in foods and food contact materials. The methods are not straightforward because of the need to eliminate accidental contamination from phthalates already present in the laboratory environment. These methods have enabled levels of potentially toxic phthalates such as di-n-butyl phthalate and di (2-ethylhexyl) phthalate to be monitored at mg/kg levels. By providing these services on a routine basis, RSSL has assisted companies to monitor levels of phthalates in their products as part of 'due diligence' screening programmes. We also offer companies an Emergency Response Service for the immediate analysis of products. This service is available 24 hours a day, 365 days of the year and provides emergency cover in situations where companies need all the analytical facts before they can determine their most appropriate response. Data on the toxicity of phthalic acid esters have been collected over many years with references dating back to the 1930's. RSSL has compiled an extensive information file on phthalates covering toxicity information and surveillance reports. If you would like some of this data for your own records, please contact Di Amor on 0118 986 8541. To give a brief history of the situation, the majority of the work has been carried out on rodents which showed that high doses of certain phthalate esters can lower blood lipids and have adverse effects on the testes and embryonic survival. In their Food Surveillance Paper in 1987 (No. 21) MAFF reported that there was around a 100,000-fold margin between the no-effect levels for reproductive toxicity and the maximum likely human intakes. Nevertheless an update on food contact materials in Food Surveillance Paper No 30 (1990) showed that certain foodstuffs (potato snacks and chocolates) contained up to 18.6 mg/kg of dicyclohexyl phthalate and it was recommended that steps be taken to reduce the chance of any contamination. During 1994, MAFF further acknowledged the widespread occurrence of very low levels of phthalates in the environment, including food packaging materials, and stated that it intended to include routine phthalate determinations in its Total Diet Study from early in 1995. It also continues to look at the migration of phthalates and other printing ink components as part of its surveillance of paper and board food contact materials. The debate on the dangers of so-called "environmental oestrogens" was re-opened in 1994 by American scientists who claimed that a cocktail of pollutants from food and water were damaging the reproductive functions of the Western male. Although the original articles focussed on DDT and PCBs, which are chemically distinct from phthalates, concern about the reproductive toxicity of phthalates has resurfaced in the UK media and teams of biologists at Brunel and Edinburgh Universities are currently looking at the hazards posed by longer term, cumulative effects of phthalates and other plastic components. The recent controversy over infant formulae has arisen from a MAFF survey which indicated that dietary intakes of phthalates by formula-fed infants expressed on a body weight basis would be proportionally higher than intakes by adults on a mixed diet. MAFF have recommended that sources of phthalates in the formulae should be established and steps taken to reduce levels found where possible. To reassure customers, retailers can encourage their suppliers to test products for phthalate levels. In this way companies can determine whether they have a widespread problem in their own products and if they do, to test samples at various stages of the production process to establish the sources of contamination. For further information on RSSL's analytical services for phthalates, please call our Customer Service Desk, free of charge, on 0800 243482. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-164IA018-000193-B027-95http://www.rssl.co.uk:80/biotechnology.html 194.159.251.194 19970106150957 text/html 1657HTTP/1.0 200 OKDate: Mon, 06 Jan 1997 15:09:59 GMTServer: Apache/0.8.14Content-type: text/htmlContent-length: 1485Last-modified: Mon, 20 May 1996 07:45:05 GMT RSSL:Biotechnology Reading Scientific Services Ltd The Lord Zuckerman Research Centre, Whiteknights, PO Box 234, Reading. RG6 6LA. Tel : +44 118 986 8541 Fax:+44 118 986 8932 Biotechnology RSSL's biotechnology unit can isolate and grow algae under carefully controlled conditions to supply the pharmaceutical industry with biomass or extracts for screening programmes to identify novel bioactive compounds. RSSL also has a unique culture collection and can offer samples to clients on an exclusive basis. The unit has fermentation facilities which enable it to optimise and scale up the production of potentially novel metabolites. Biomass can also be produced from other micro-organisms including bacteria, fungi, and yeasts. Quicklink: |Index |The Company |Services |Enquiry Form | © In Press PR Ltd1996 WT03-B20-165IA050-000862-B003-154http://www.smokemag.com:80/index.html 192.41.17.142 19970111130721 text/html 2783HTTP/1.0 200 Document followsDate: Sat, 11 Jan 1997 13:07:21 GMTServer: ISERVER/1.5.1Last-modified: Wed, 04 Dec 1996 06:47:46 GMTContent-type: text/htmlContent-length: 2597 SMOKE Magazine Online! Welcome to... SMOKE Magazine Online! A fun, contemporary magazine geared to the rising executive who is interested in cigars and the lifestyle surrounding them.... We invite you visit the online edition of SMOKE and sample the qualities of a first class cigar magazine. Our Winter issue preview! A letter from our Editor Le Cigar Noir dinners... Feature Article Online Shopping Guide... SMOKE Sportswear... SMOKE's Mailroom... Subscribe to SMOKE! Also check out our... SMOKE's Archives - Including past issues, and our popular cigar reviews... SMOKE Links - Some useful resources... Audio/Video (AVI) welcome... (800k) A message from our Publisher Winner of the "Best of the Web" Award, and Yahoo's Top Five Cigar Sites of the Internet! SMOKE Magazine is a proud supporter of the Internet Cigar Group HTML production and Copyright © 1996, Keys Technologies. All rights reserved.WT03-B20-166IA050-000857-B009-147http://www.smokemag.com:80/subscrib.htm 192.41.17.142 19970111124930 text/html 2294HTTP/1.0 200 Document followsDate: Sat, 11 Jan 1997 12:49:22 GMTServer: ISERVER/1.5.1Last-modified: Wed, 04 Dec 1996 07:00:13 GMTContent-type: text/htmlContent-length: 2108 Subscribe to SMOKE Magazine! Sign up now to receive SMOKE at home! Receive 1 year (4 issues) for only $13.99! Name:Address1:Address2:City:State:,   Zip:   Country:Telephone:email: (optional) SMOKE Magazine will bill you $13.99 for a one year's subscription. Canadian orders add $5.50, all other foreign orders add $12. Please allow six to eight weeks for processing. For those using browsers that do not support forms such as this, mail yoursubscription order to: Smoke Magazine P.O. Box 56510 Bolder, CO 80323-6510 Or email our subscription department with your name and address... Inquiries? Call us toll-free at: (800) 766-2633 HTML production and Copyright © 1996, Keys Technologies. All rights reserved.WT03-B20-167IA050-000857-B009-230http://www.smokemag.com:80/s-links.htm 192.41.17.142 19970111125027 text/html 3178HTTP/1.0 200 Document followsDate: Sat, 11 Jan 1997 12:50:25 GMTServer: ISERVER/1.5.1Last-modified: Sun, 28 Jul 1996 15:25:16 GMTContent-type: text/htmlContent-length: 2992 SMOKE-Links SMOKE Around the World SMOKED on the Internet SMOKE CIGARS SMOKE General Cigar Information "Frequently Asked Questions" on cigars, by Bob Curtis, noted Internet author... "Cigars and Cancer" by Marc Schneiderman, M.D. The "alt.smokers.cigars" cigar database, a tremendous on-line resource of information on cigar brands, sizes, wrappers, etc. SMOKE Cigar WWW Pages Learn about Operation Cigar Lift,bringing cigars to our peacekeeping troops in Bosnia... Looking for a cigar-friendly establishment while traveling? VisitCigarFriendly.Com, and discoverthe best places to enjoy your smoking pleasures. The Internet is all about information. Visit Bob Curtis' Personal Page - a tremendous resource of cigar information - without the hype! A Cigar Is -Never Merely- A Cigar WWW Page - more resources for the cigar smoker. The famous Alt.Smokers.Cigars! The Internet's largest cigar discussion group. Want more links? The Cigar/Web List includes every major cigar site on the web! Traveling? Jeff Friedman's International Tobacconist List might be just what you need! SMOKE PIPES SMOKE Pipe Information This introductory "how to"is full of information for the novice or experienced pipe smoker. This is an excellent pipe Frequently Asked Questions article with thorough coverage of what you need to know. The Pipe's Digest Resource Guide contains useful "where-to-go" infromation. SMOKE Pipe WWW Pages Pipes Digest WWW Page. Organization of Online Pipe Smokers Alt.Smokers.Pipes - Pipes discussion group (if your browser agrees). subscribe to The Pipes Digest Mailing List. WT03-B20-168IA050-000857-B008-62http://www.smokemag.com:80/ 192.41.17.142 19970111124619 text/html 2783HTTP/1.0 200 Document followsDate: Sat, 11 Jan 1997 12:46:21 GMTServer: ISERVER/1.5.1Last-modified: Wed, 04 Dec 1996 06:47:46 GMTContent-type: text/htmlContent-length: 2597 SMOKE Magazine Online! Welcome to... SMOKE Magazine Online! A fun, contemporary magazine geared to the rising executive who is interested in cigars and the lifestyle surrounding them.... We invite you visit the online edition of SMOKE and sample the qualities of a first class cigar magazine. Our Winter issue preview! A letter from our Editor Le Cigar Noir dinners... Feature Article Online Shopping Guide... SMOKE Sportswear... SMOKE's Mailroom... Subscribe to SMOKE! Also check out our... SMOKE's Archives - Including past issues, and our popular cigar reviews... SMOKE Links - Some useful resources... Audio/Video (AVI) welcome... (800k) A message from our Publisher Winner of the "Best of the Web" Award, and Yahoo's Top Five Cigar Sites of the Internet! SMOKE Magazine is a proud supporter of the Internet Cigar Group HTML production and Copyright © 1996, Keys Technologies. All rights reserved.WT03-B20-169IA050-000862-B002-371http://www.smokemag.com:80/1296/index.html 192.41.17.142 19970111130456 text/html 2607HTTP/1.0 200 Document followsDate: Sat, 11 Jan 1997 13:04:50 GMTServer: ISERVER/1.5.1Last-modified: Thu, 05 Dec 1996 04:31:06 GMTContent-type: text/htmlContent-length: 2421 Smoke Magazine "Welcome" Page - Volume 2, No.1 - Winter 1996-1997 Welcome to the Anniversary Issue of SMOKE Magazine! We invite you visit our online edition, and sample the qualities of a first class cigar magazine. Find out how to SUBSCRIBE to SMOKE. Table of Contents Visit our Anniversary Issue's Table of Contents. Read selected articles and view illustrations from this special anniversary issue. Read special features, and regular columns on page 2 of SMOKE'S Table of Contents, including our famous Cigar Reviews. Year-end Musings from The Editor of SMOKE. Stay in touch with SMOKE! We want to hear from you! You can get in touch with most of our departments directlythrough the links of our electronic mailroom! Webpage Design by Joseph M. Johnston johnston@netreach.net Copyright © 1996, Keys Technologies. All rights reserved. WT03-B20-170IA050-000857-B012-175http://www.smokemag.com:80/1296/toc12961.htm 192.41.17.142 19970111125758 text/html 3770HTTP/1.0 200 Document followsDate: Sat, 11 Jan 1997 12:58:02 GMTServer: ISERVER/1.5.1Last-modified: Thu, 05 Dec 1996 04:35:05 GMTContent-type: text/htmlContent-length: 3584 SMOKE Table of Contents - Page 1 - Winter 1996-1997 HOW TO USE THIS PAGE Many pictures or items lead to either a featured picture from SMOKE or one of the featured articles included in the online edition. Click on any picture or title to link to these features. Or why not subscribe to SMOKE. Get in touch with SMOKE at our new electronic mailroom. Cover... Mel the Magnificent After landing a pair of gold statuettes at last year's Academy Awards, actor/director/producer Mel Gibson shows no signs of slowing down. Hollywood's most bankable commodity sits with Seven McDonald for an exclusive interview. Features... SMOKE America: Lone Star Cigars (article) Bob Ashley's back with our continuing series on USA born & bred cigar companies. This issue, Bob heads deep into the heart of Texas for a look at Finck Cigar Co. SMOKE Pictorial: Made in Japan (picture) Kiseru pipes once dominated Japanese tobacco tradition; today they are mainly found in souvenir shops and museums I Spy for the FBI (picture) What do James Bond, Maxwell Smart, and Derek Flint have in common? Find out as Chris Rubin looks at the latest in high-tech spy gadgetry. Big Apple Shopping (article) New York, New York, it's a helluva town, particularly for stogie lovers. Sean T. Barry scopes out NYC's illustrious smokeshops and lounges. The Fine Art of Smoking Magritte may not have had his pipe, but Ruby Tuesday has his cigar. Jim Mauro and John Scotello explore the relationship between fine art and fine cigars. Briar Patch (picture) Alan Schwartz goes to Saint-Claude, France the birthplace of briar. Magnifique! Plus, Ultimate author Richard Carlton Hacker explains how Santa got his pipe. A Woman's World Entertainment editor Seven McDonald wrangles an open panel of gal pals who discuss the finer points of cigars, men, and oral sex. That Voodoo That You Don't Santeria, everybody's favorite spooky religion, puts cigar smoking in a whole new light. Michael Karnow gets ritualistic with a Santeria priest and proves he's no chicken. Age of Enlightenment (article) Dr. Adrian Bartoli's continuing guide to cigar maturation further explains the science behind aging your precious smokes. Click to go to page 2 of the Table of Contents.